# from tvm.script import ir as I
# from tvm.script import tir as T
# from tvm.script import relax as R

@I.ir_module
class Module:
    @T.prim_func
    def apply_bitmask_inplace(var_logits: T.handle, var_seq_ids: T.handle, var_bitmask: T.handle):
        T.func_attr({"target": T.target({"arch": "sm_75", "host": {"keys": ["cpu"], "kind": "llvm", "mcpu": "skylake", "mtriple": "x86_64-conda-linux-gnu", "tag": ""}, "keys": ["cuda", "gpu"], "kind": "cuda", "libs": ["thrust"], "max_num_threads": 1024, "max_shared_memory_per_block": 49152, "max_threads_per_block": 1024, "tag": "", "thread_warp_size": 32}), "tir.is_scheduled": T.bool(True), "tir.noalias": T.bool(True)})
        batch_size, vocab_size = T.int32(is_size_var=True), T.int32(is_size_var=True)
        logits = T.match_buffer(var_logits, (batch_size, vocab_size))
        num_seq = T.int32(is_size_var=True)
        seq_ids = T.match_buffer(var_seq_ids, (num_seq,), "int32")
        bitmask = T.match_buffer(var_bitmask, (batch_size, (vocab_size + 31) // 32), "int32")
        # with T.block("root"):
        for fused_s_v_0 in T.thread_binding((num_seq * vocab_size + 1023) // 1024, thread="blockIdx.x"):
            for fused_s_v_1 in T.thread_binding(1024, thread="threadIdx.x"):
                with T.block("block"):
                    vs = T.axis.spatial(num_seq, (fused_s_v_0 * 1024 + fused_s_v_1) // vocab_size)
                    vv = T.axis.spatial(vocab_size, (fused_s_v_0 * 1024 + fused_s_v_1) % vocab_size)
                    T.where(fused_s_v_0 * 1024 + fused_s_v_1 < num_seq * vocab_size)
                    T.reads(bitmask[seq_ids[vs], vv // 32], seq_ids[vs], logits[seq_ids[vs], vv])
                    T.writes(logits[seq_ids[vs], vv])
                    logits[seq_ids[vs], vv] = T.if_then_else(T.bitwise_and(T.shift_right(bitmask[seq_ids[vs], vv // 32], vv % 32), 1) == 1, logits[seq_ids[vs], vv], T.float32(-340282346638528859811704183484516925440.0))

    @T.prim_func
    def apply_logit_bias_inplace(var_logits: T.handle, var_pos2seq_id: T.handle, var_token_ids: T.handle, var_logit_bias: T.handle):
        T.func_attr({"target": T.target({"arch": "sm_75", "host": {"keys": ["cpu"], "kind": "llvm", "mcpu": "skylake", "mtriple": "x86_64-conda-linux-gnu", "tag": ""}, "keys": ["cuda", "gpu"], "kind": "cuda", "libs": ["thrust"], "max_num_threads": 1024, "max_shared_memory_per_block": 49152, "max_threads_per_block": 1024, "tag": "", "thread_warp_size": 32}), "tir.is_scheduled": T.bool(True), "tir.noalias": T.bool(True)})
        batch_size, vocab_size = T.int32(is_size_var=True), T.int32(is_size_var=True)
        logits = T.match_buffer(var_logits, (batch_size, vocab_size))
        num_token = T.int32(is_size_var=True)
        pos2seq_id = T.match_buffer(var_pos2seq_id, (num_token,), "int32")
        token_ids = T.match_buffer(var_token_ids, (num_token,), "int32")
        logit_bias = T.match_buffer(var_logit_bias, (num_token,))
        # with T.block("root"):
        for p0 in T.thread_binding((num_token + 1023) // 1024, thread="blockIdx.x"):
            for p1 in T.thread_binding(1024, thread="threadIdx.x"):
                with T.block("block"):
                    vp = T.axis.spatial(num_token, p0 * 1024 + p1)
                    T.where(p0 * 1024 + p1 < num_token)
                    T.reads(logits[pos2seq_id[vp], token_ids[vp]], pos2seq_id[vp], token_ids[vp], logit_bias[vp])
                    T.writes(logits[pos2seq_id[vp], token_ids[vp]])
                    logits[pos2seq_id[vp], token_ids[vp]] = logits[pos2seq_id[vp], token_ids[vp]] + logit_bias[vp]

    @T.prim_func
    def apply_penalty_inplace(var_logits: T.handle, var_seq_ids: T.handle, var_pos2seq_id: T.handle, var_token_ids: T.handle, var_token_cnt: T.handle, var_penalties: T.handle):
        T.func_attr({"target": T.target({"arch": "sm_75", "host": {"keys": ["cpu"], "kind": "llvm", "mcpu": "skylake", "mtriple": "x86_64-conda-linux-gnu", "tag": ""}, "keys": ["cuda", "gpu"], "kind": "cuda", "libs": ["thrust"], "max_num_threads": 1024, "max_shared_memory_per_block": 49152, "max_threads_per_block": 1024, "tag": "", "thread_warp_size": 32}), "tir.is_scheduled": T.bool(True), "tir.noalias": T.bool(True)})
        batch_size, vocab_size = T.int32(is_size_var=True), T.int32(is_size_var=True)
        logits = T.match_buffer(var_logits, (batch_size, vocab_size))
        num_seq = T.int32(is_size_var=True)
        seq_ids = T.match_buffer(var_seq_ids, (num_seq,), "int32")
        num_token = T.int32(is_size_var=True)
        pos2seq_id = T.match_buffer(var_pos2seq_id, (num_token,), "int32")
        token_ids = T.match_buffer(var_token_ids, (num_token,), "int32")
        token_cnt = T.match_buffer(var_token_cnt, (num_token,), "int32")
        penalties = T.match_buffer(var_penalties, (num_seq, 3))
        # with T.block("root"):
        for p0 in T.thread_binding((num_token + 1023) // 1024, thread="blockIdx.x"):
            for p1 in T.thread_binding(1024, thread="threadIdx.x"):
                with T.block("block"):
                    vp = T.axis.spatial(num_token, p0 * 1024 + p1)
                    T.where(p0 * 1024 + p1 < num_token)
                    T.reads(logits[seq_ids[pos2seq_id[vp]], token_ids[vp]], seq_ids[pos2seq_id[vp]], pos2seq_id[vp], token_ids[vp], penalties[pos2seq_id[vp], 0:3], token_cnt[vp])
                    T.writes(logits[seq_ids[pos2seq_id[vp]], token_ids[vp]])
                    logits[seq_ids[pos2seq_id[vp]], token_ids[vp]] = logits[seq_ids[pos2seq_id[vp]], token_ids[vp]] - (penalties[pos2seq_id[vp], 0] + T.Cast("float32", token_cnt[vp]) * penalties[pos2seq_id[vp], 1])
                    logits[seq_ids[pos2seq_id[vp]], token_ids[vp]] = T.if_then_else(logits[seq_ids[pos2seq_id[vp]], token_ids[vp]] < T.float32(0.0), logits[seq_ids[pos2seq_id[vp]], token_ids[vp]] * penalties[pos2seq_id[vp], 2], logits[seq_ids[pos2seq_id[vp]], token_ids[vp]] / penalties[pos2seq_id[vp], 2])

    @T.prim_func(private=True)
    def argsort(var_probs: T.handle, var_argsort_gpu_v1: T.handle):
        T.func_attr({"tir.noalias": T.bool(True)})
        batch_size, vocab_size = T.int64(), T.int64()
        probs = T.match_buffer(var_probs, (batch_size, vocab_size), offset_factor=1)
        out_buf = T.match_buffer(var_argsort_gpu_v1, (batch_size, vocab_size), "int32", align=8)
        # with T.block("root"):
        value_buf = T.alloc_buffer((batch_size, vocab_size), align=8)
        value_swap_buf = T.alloc_buffer((batch_size, vocab_size), align=8)
        out_swap_buf = T.alloc_buffer((batch_size, vocab_size), "int32", align=8)
        with T.block("argsort_gpu"):
            T.reads()
            T.writes()
            if vocab_size > T.int64(0):
                with T.launch_thread("threadIdx.x", T.int64(1024)) as threadIdx_x:
                    blockIdx_x = T.launch_thread("blockIdx.x", T.max(T.int64(1), (vocab_size + T.int64(1023)) // T.int64(1024)))
                    blockIdx_y = T.launch_thread("blockIdx.y", T.max(T.int64(1), batch_size))
                    if blockIdx_x * T.int64(1024) + threadIdx_x < vocab_size:
                        value_buf[(blockIdx_y % batch_size * vocab_size + (blockIdx_x * T.int64(1024) + threadIdx_x) + blockIdx_y // batch_size) // vocab_size, (blockIdx_y % batch_size * vocab_size + (blockIdx_x * T.int64(1024) + threadIdx_x) + blockIdx_y // batch_size) % vocab_size] = probs[(blockIdx_y % batch_size * vocab_size + (blockIdx_x * T.int64(1024) + threadIdx_x) + blockIdx_y // batch_size) // vocab_size, (blockIdx_y % batch_size * vocab_size + (blockIdx_x * T.int64(1024) + threadIdx_x) + blockIdx_y // batch_size) % vocab_size]
                        out_buf[(blockIdx_y % batch_size * vocab_size + (blockIdx_x * T.int64(1024) + threadIdx_x) + blockIdx_y // batch_size) // vocab_size, (blockIdx_y % batch_size * vocab_size + (blockIdx_x * T.int64(1024) + threadIdx_x) + blockIdx_y // batch_size) % vocab_size] = T.Cast("int32", blockIdx_x * T.int64(1024) + threadIdx_x)
                with T.attr(0, "hand_threaded", 0):
                    threadIdx_x = T.launch_thread("threadIdx.x", T.int64(64))
                    blockIdx_x = T.launch_thread("blockIdx.x", T.max(T.int64(1), (vocab_size + T.int64(127)) // T.int64(128)))
                    blockIdx_y = T.launch_thread("blockIdx.y", T.max(T.int64(1), batch_size))
                    temp_keys_swap = T.allocate([T.int64(128)], "float32", "shared")
                    temp_values_swap = T.allocate([T.int64(128)], "int32", "shared")
                    temp_keys = T.allocate([T.int64(1)], "float32", "local")
                    temp_values = T.allocate([T.int64(1)], "int32", "local")
                    temp_cond1 = T.allocate([T.int64(1)], "float32", "local")
                    temp_cond2 = T.allocate([T.int64(1)], "float32", "local")
                    temp_keys_swap_1 = T.Buffer((128,), data=temp_keys_swap, scope="shared")
                    temp_values_swap_1 = T.Buffer((128,), "int32", data=temp_values_swap, scope="shared")
                    for i in range(T.int64(2)):
                        if T.int64(2) * threadIdx_x + i + blockIdx_x * T.int64(128) < vocab_size:
                            temp_keys_swap_1[T.int64(2) * threadIdx_x + i] = value_buf[(blockIdx_y % batch_size * vocab_size + blockIdx_y // batch_size + (T.int64(2) * threadIdx_x + i + blockIdx_x * T.int64(128))) // vocab_size, (blockIdx_y % batch_size * vocab_size + blockIdx_y // batch_size + (T.int64(2) * threadIdx_x + i + blockIdx_x * T.int64(128))) % vocab_size]
                            temp_values_swap_1[T.int64(2) * threadIdx_x + i] = out_buf[(blockIdx_y % batch_size * vocab_size + blockIdx_y // batch_size + (T.int64(2) * threadIdx_x + i + blockIdx_x * T.int64(128))) // vocab_size, (blockIdx_y % batch_size * vocab_size + blockIdx_y // batch_size + (T.int64(2) * threadIdx_x + i + blockIdx_x * T.int64(128))) % vocab_size]
                    T.tvm_storage_sync("shared")
                    for j in range(T.min(T.int64(128), vocab_size - blockIdx_x * T.int64(128))):
                        if T.int64(2) * threadIdx_x + (T.int64(2) * threadIdx_x + j) % T.int64(2) < T.min(T.int64(128), vocab_size - blockIdx_x * T.int64(128)) - T.int64(1):
                            temp_cond1_1 = T.Buffer((1,), data=temp_cond1, scope="local")
                            temp_cond1_1[T.int64(0)] = temp_keys_swap_1[T.int64(2) * threadIdx_x + (T.int64(2) * threadIdx_x + j) % T.int64(2)]
                            temp_cond2_1 = T.Buffer((1,), data=temp_cond2, scope="local")
                            temp_cond2_1[T.int64(0)] = temp_keys_swap_1[T.int64(2) * threadIdx_x + (T.int64(2) * threadIdx_x + j) % T.int64(2) + T.int64(1)]
                            if temp_cond1_1[T.int64(0)] < temp_cond2_1[T.int64(0)]:
                                temp_keys_1 = T.Buffer((1,), data=temp_keys, scope="local")
                                temp_keys_1[T.int64(0)] = temp_keys_swap_1[T.int64(2) * threadIdx_x + (T.int64(2) * threadIdx_x + j) % T.int64(2)]
                                temp_keys_swap_1[T.int64(2) * threadIdx_x + (T.int64(2) * threadIdx_x + j) % T.int64(2)] = temp_keys_swap_1[T.int64(2) * threadIdx_x + (T.int64(2) * threadIdx_x + j) % T.int64(2) + T.int64(1)]
                                temp_keys_swap_1[T.int64(2) * threadIdx_x + (T.int64(2) * threadIdx_x + j) % T.int64(2) + T.int64(1)] = temp_keys_1[T.int64(0)]
                                temp_values_1 = T.Buffer((1,), "int32", data=temp_values, scope="local")
                                temp_values_1[T.int64(0)] = temp_values_swap_1[T.int64(2) * threadIdx_x + (T.int64(2) * threadIdx_x + j) % T.int64(2)]
                                temp_values_swap_1[T.int64(2) * threadIdx_x + (T.int64(2) * threadIdx_x + j) % T.int64(2)] = temp_values_swap_1[T.int64(2) * threadIdx_x + (T.int64(2) * threadIdx_x + j) % T.int64(2) + T.int64(1)]
                                temp_values_swap_1[T.int64(2) * threadIdx_x + (T.int64(2) * threadIdx_x + j) % T.int64(2) + T.int64(1)] = temp_values_1[T.int64(0)]
                        T.tvm_storage_sync("shared")
                    for k in range(T.int64(2)):
                        if T.int64(2) * threadIdx_x + k + blockIdx_x * T.int64(128) < vocab_size:
                            value_buf[(blockIdx_y % batch_size * vocab_size + blockIdx_y // batch_size + (T.int64(2) * threadIdx_x + k + blockIdx_x * T.int64(128))) // vocab_size, (blockIdx_y % batch_size * vocab_size + blockIdx_y // batch_size + (T.int64(2) * threadIdx_x + k + blockIdx_x * T.int64(128))) % vocab_size] = temp_keys_swap_1[T.int64(2) * threadIdx_x + k]
                            value_swap_buf[(blockIdx_y % batch_size * vocab_size + blockIdx_y // batch_size + (T.int64(2) * threadIdx_x + k + blockIdx_x * T.int64(128))) // vocab_size, (blockIdx_y % batch_size * vocab_size + blockIdx_y // batch_size + (T.int64(2) * threadIdx_x + k + blockIdx_x * T.int64(128))) % vocab_size] = temp_keys_swap_1[T.int64(2) * threadIdx_x + k]
                            out_buf[(blockIdx_y % batch_size * vocab_size + blockIdx_y // batch_size + (T.int64(2) * threadIdx_x + k + blockIdx_x * T.int64(128))) // vocab_size, (blockIdx_y % batch_size * vocab_size + blockIdx_y // batch_size + (T.int64(2) * threadIdx_x + k + blockIdx_x * T.int64(128))) % vocab_size] = temp_values_swap_1[T.int64(2) * threadIdx_x + k]
                            out_swap_buf[(blockIdx_y % batch_size * vocab_size + blockIdx_y // batch_size + (T.int64(2) * threadIdx_x + k + blockIdx_x * T.int64(128))) // vocab_size, (blockIdx_y % batch_size * vocab_size + blockIdx_y // batch_size + (T.int64(2) * threadIdx_x + k + blockIdx_x * T.int64(128))) % vocab_size] = temp_values_swap_1[T.int64(2) * threadIdx_x + k]
                for i_0 in range(T.Cast("int64", T.ceil(T.log2(T.Cast("float64", vocab_size)))) - T.Cast("int64", T.ceil(T.log2(T.float64(128.0))))):
                    threadIdx_x = T.launch_thread("threadIdx.x", T.max(T.int64(1), T.min(T.int64(1024), T.shift_left(T.int64(2), i_0 + T.Cast("int64", T.ceil(T.log2(T.float64(128.0))))))))
                    blockIdx_x = T.launch_thread("blockIdx.x", T.max(T.int64(1), (T.shift_left(T.int64(2), i_0 + T.Cast("int64", T.ceil(T.log2(T.float64(128.0))))) + T.int64(4095)) // T.int64(4096)))
                    blockIdx_y = T.launch_thread("blockIdx.y", T.max(T.int64(1), batch_size * ((vocab_size + (T.shift_left(T.int64(2), i_0 + T.Cast("int64", T.ceil(T.log2(T.float64(128.0))))) - T.int64(1))) // T.shift_left(T.int64(2), i_0 + T.Cast("int64", T.ceil(T.log2(T.float64(128.0))))))))
                    if T.shift_left(T.int64(2), i_0 + T.Cast("int64", T.ceil(T.log2(T.float64(128.0))))) * (blockIdx_y // batch_size) < vocab_size:
                        if (T.shift_left(T.int64(2), i_0 + T.Cast("int64", T.ceil(T.log2(T.float64(128.0))))) + T.int64(4095)) // T.int64(4096) == T.int64(1):
                            if i_0 % T.int64(2) == T.int64(0):
                                first = T.allocate([T.int64(1)], "int64", "local")
                                mid = T.allocate([T.int64(1)], "int64", "local")
                                last = T.allocate([T.int64(1)], "int64", "local")
                                first_1 = T.Buffer((1,), "int64", data=first, scope="local")
                                first_1[T.int64(0)] = T.max(T.int64(0), threadIdx_x * ((T.shift_left(T.int64(2), i_0 + T.Cast("int64", T.ceil(T.log2(T.float64(128.0))))) + (T.min(T.int64(1024), T.shift_left(T.int64(2), i_0 + T.Cast("int64", T.ceil(T.log2(T.float64(128.0)))))) - T.int64(1))) // T.min(T.int64(1024), T.shift_left(T.int64(2), i_0 + T.Cast("int64", T.ceil(T.log2(T.float64(128.0))))))) - (T.min(T.shift_left(T.int64(2), i_0 + T.Cast("int64", T.ceil(T.log2(T.float64(128.0))))) * (blockIdx_y // batch_size) + T.shift_left(T.int64(2), i_0 + T.Cast("int64", T.ceil(T.log2(T.float64(128.0))))), vocab_size) - T.min(T.shift_left(T.int64(2), i_0 + T.Cast("int64", T.ceil(T.log2(T.float64(128.0))))) * (blockIdx_y // batch_size) + T.shift_left(T.int64(2), i_0 + T.Cast("int64", T.ceil(T.log2(T.float64(128.0))))) // T.int64(2), vocab_size)))
                                last_1 = T.Buffer((1,), "int64", data=last, scope="local")
                                last_1[T.int64(0)] = T.min(threadIdx_x * ((T.shift_left(T.int64(2), i_0 + T.Cast("int64", T.ceil(T.log2(T.float64(128.0))))) + (T.min(T.int64(1024), T.shift_left(T.int64(2), i_0 + T.Cast("int64", T.ceil(T.log2(T.float64(128.0)))))) - T.int64(1))) // T.min(T.int64(1024), T.shift_left(T.int64(2), i_0 + T.Cast("int64", T.ceil(T.log2(T.float64(128.0))))))), T.min(T.shift_left(T.int64(2), i_0 + T.Cast("int64", T.ceil(T.log2(T.float64(128.0))))) * (blockIdx_y // batch_size) + T.shift_left(T.int64(2), i_0 + T.Cast("int64", T.ceil(T.log2(T.float64(128.0))))) // T.int64(2), vocab_size) - T.shift_left(T.int64(2), i_0 + T.Cast("int64", T.ceil(T.log2(T.float64(128.0))))) * (blockIdx_y // batch_size))
                                while first_1[T.int64(0)] < last_1[T.int64(0)]:
                                    if value_buf[(blockIdx_y % batch_size * vocab_size + (T.min(T.shift_left(T.int64(2), i_0 + T.Cast("int64", T.ceil(T.log2(T.float64(128.0))))) * (blockIdx_y // batch_size) + T.shift_left(T.int64(2), i_0 + T.Cast("int64", T.ceil(T.log2(T.float64(128.0))))) // T.int64(2), vocab_size) + threadIdx_x * ((T.shift_left(T.int64(2), i_0 + T.Cast("int64", T.ceil(T.log2(T.float64(128.0))))) + (T.min(T.int64(1024), T.shift_left(T.int64(2), i_0 + T.Cast("int64", T.ceil(T.log2(T.float64(128.0)))))) - T.int64(1))) // T.min(T.int64(1024), T.shift_left(T.int64(2), i_0 + T.Cast("int64", T.ceil(T.log2(T.float64(128.0))))))) - T.int64(1) - T.shift_right(first_1[T.int64(0)] + last_1[T.int64(0)], T.int64(1)))) // vocab_size, (blockIdx_y % batch_size * vocab_size + (T.min(T.shift_left(T.int64(2), i_0 + T.Cast("int64", T.ceil(T.log2(T.float64(128.0))))) * (blockIdx_y // batch_size) + T.shift_left(T.int64(2), i_0 + T.Cast("int64", T.ceil(T.log2(T.float64(128.0))))) // T.int64(2), vocab_size) + threadIdx_x * ((T.shift_left(T.int64(2), i_0 + T.Cast("int64", T.ceil(T.log2(T.float64(128.0))))) + (T.min(T.int64(1024), T.shift_left(T.int64(2), i_0 + T.Cast("int64", T.ceil(T.log2(T.float64(128.0)))))) - T.int64(1))) // T.min(T.int64(1024), T.shift_left(T.int64(2), i_0 + T.Cast("int64", T.ceil(T.log2(T.float64(128.0))))))) - T.int64(1) - T.shift_right(first_1[T.int64(0)] + last_1[T.int64(0)], T.int64(1)))) % vocab_size] <= value_buf[(blockIdx_y % batch_size * vocab_size + (T.shift_left(T.int64(2), i_0 + T.Cast("int64", T.ceil(T.log2(T.float64(128.0))))) * (blockIdx_y // batch_size) + T.shift_right(first_1[T.int64(0)] + last_1[T.int64(0)], T.int64(1)))) // vocab_size, (blockIdx_y % batch_size * vocab_size + (T.shift_left(T.int64(2), i_0 + T.Cast("int64", T.ceil(T.log2(T.float64(128.0))))) * (blockIdx_y // batch_size) + T.shift_right(first_1[T.int64(0)] + last_1[T.int64(0)], T.int64(1)))) % vocab_size]:
                                        first_1[T.int64(0)] = T.shift_right(first_1[T.int64(0)] + last_1[T.int64(0)], T.int64(1)) + T.int64(1)
                                    else:
                                        last_1[T.int64(0)] = T.shift_right(first_1[T.int64(0)] + last_1[T.int64(0)], T.int64(1))
                                i = T.allocate([T.int64(1)], "int64", "local")
                                j = T.allocate([T.int64(1)], "int64", "local")
                                i_1 = T.Buffer((1,), "int64", data=i, scope="local")
                                i_1[T.int64(0)] = T.shift_left(T.int64(2), i_0 + T.Cast("int64", T.ceil(T.log2(T.float64(128.0))))) * (blockIdx_y // batch_size) + first_1[T.int64(0)]
                                j_1 = T.Buffer((1,), "int64", data=j, scope="local")
                                j_1[T.int64(0)] = T.min(T.shift_left(T.int64(2), i_0 + T.Cast("int64", T.ceil(T.log2(T.float64(128.0))))) * (blockIdx_y // batch_size) + T.shift_left(T.int64(2), i_0 + T.Cast("int64", T.ceil(T.log2(T.float64(128.0))))) // T.int64(2), vocab_size) + threadIdx_x * ((T.shift_left(T.int64(2), i_0 + T.Cast("int64", T.ceil(T.log2(T.float64(128.0))))) + (T.min(T.int64(1024), T.shift_left(T.int64(2), i_0 + T.Cast("int64", T.ceil(T.log2(T.float64(128.0)))))) - T.int64(1))) // T.min(T.int64(1024), T.shift_left(T.int64(2), i_0 + T.Cast("int64", T.ceil(T.log2(T.float64(128.0))))))) - last_1[T.int64(0)]
                                for i_1_1 in range(T.min(T.min(T.shift_left(T.int64(2), i_0 + T.Cast("int64", T.ceil(T.log2(T.float64(128.0))))) * (blockIdx_y // batch_size) + T.shift_left(T.int64(2), i_0 + T.Cast("int64", T.ceil(T.log2(T.float64(128.0))))) // T.int64(2), vocab_size) - T.shift_left(T.int64(2), i_0 + T.Cast("int64", T.ceil(T.log2(T.float64(128.0))))) * (blockIdx_y // batch_size) + (T.min(T.shift_left(T.int64(2), i_0 + T.Cast("int64", T.ceil(T.log2(T.float64(128.0))))) * (blockIdx_y // batch_size) + T.shift_left(T.int64(2), i_0 + T.Cast("int64", T.ceil(T.log2(T.float64(128.0))))), vocab_size) - T.min(T.shift_left(T.int64(2), i_0 + T.Cast("int64", T.ceil(T.log2(T.float64(128.0))))) * (blockIdx_y // batch_size) + T.shift_left(T.int64(2), i_0 + T.Cast("int64", T.ceil(T.log2(T.float64(128.0))))) // T.int64(2), vocab_size)) - threadIdx_x * ((T.shift_left(T.int64(2), i_0 + T.Cast("int64", T.ceil(T.log2(T.float64(128.0))))) + (T.min(T.int64(1024), T.shift_left(T.int64(2), i_0 + T.Cast("int64", T.ceil(T.log2(T.float64(128.0)))))) - T.int64(1))) // T.min(T.int64(1024), T.shift_left(T.int64(2), i_0 + T.Cast("int64", T.ceil(T.log2(T.float64(128.0))))))), (T.shift_left(T.int64(2), i_0 + T.Cast("int64", T.ceil(T.log2(T.float64(128.0))))) + (T.min(T.int64(1024), T.shift_left(T.int64(2), i_0 + T.Cast("int64", T.ceil(T.log2(T.float64(128.0)))))) - T.int64(1))) // T.min(T.int64(1024), T.shift_left(T.int64(2), i_0 + T.Cast("int64", T.ceil(T.log2(T.float64(128.0)))))))):
                                    if i_1[T.int64(0)] < T.shift_left(T.int64(2), i_0 + T.Cast("int64", T.ceil(T.log2(T.float64(128.0))))) * (blockIdx_y // batch_size) + (T.min(T.shift_left(T.int64(2), i_0 + T.Cast("int64", T.ceil(T.log2(T.float64(128.0))))) * (blockIdx_y // batch_size) + T.shift_left(T.int64(2), i_0 + T.Cast("int64", T.ceil(T.log2(T.float64(128.0))))) // T.int64(2), vocab_size) - T.shift_left(T.int64(2), i_0 + T.Cast("int64", T.ceil(T.log2(T.float64(128.0))))) * (blockIdx_y // batch_size)) and j_1[T.int64(0)] < T.min(T.shift_left(T.int64(2), i_0 + T.Cast("int64", T.ceil(T.log2(T.float64(128.0))))) * (blockIdx_y // batch_size) + T.shift_left(T.int64(2), i_0 + T.Cast("int64", T.ceil(T.log2(T.float64(128.0))))) // T.int64(2), vocab_size) + (T.min(T.shift_left(T.int64(2), i_0 + T.Cast("int64", T.ceil(T.log2(T.float64(128.0))))) * (blockIdx_y // batch_size) + T.shift_left(T.int64(2), i_0 + T.Cast("int64", T.ceil(T.log2(T.float64(128.0))))), vocab_size) - T.min(T.shift_left(T.int64(2), i_0 + T.Cast("int64", T.ceil(T.log2(T.float64(128.0))))) * (blockIdx_y // batch_size) + T.shift_left(T.int64(2), i_0 + T.Cast("int64", T.ceil(T.log2(T.float64(128.0))))) // T.int64(2), vocab_size)):
                                        if value_buf[(blockIdx_y % batch_size * vocab_size + j_1[T.int64(0)]) // vocab_size, (blockIdx_y % batch_size * vocab_size + j_1[T.int64(0)]) % vocab_size] <= value_buf[(blockIdx_y % batch_size * vocab_size + i_1[T.int64(0)]) // vocab_size, (blockIdx_y % batch_size * vocab_size + i_1[T.int64(0)]) % vocab_size]:
                                            value_swap_buf[(blockIdx_y % batch_size * vocab_size + (T.shift_left(T.int64(2), i_0 + T.Cast("int64", T.ceil(T.log2(T.float64(128.0))))) * (blockIdx_y // batch_size) + threadIdx_x * ((T.shift_left(T.int64(2), i_0 + T.Cast("int64", T.ceil(T.log2(T.float64(128.0))))) + (T.min(T.int64(1024), T.shift_left(T.int64(2), i_0 + T.Cast("int64", T.ceil(T.log2(T.float64(128.0)))))) - T.int64(1))) // T.min(T.int64(1024), T.shift_left(T.int64(2), i_0 + T.Cast("int64", T.ceil(T.log2(T.float64(128.0))))))) + i_1_1)) // vocab_size, (blockIdx_y % batch_size * vocab_size + (T.shift_left(T.int64(2), i_0 + T.Cast("int64", T.ceil(T.log2(T.float64(128.0))))) * (blockIdx_y // batch_size) + threadIdx_x * ((T.shift_left(T.int64(2), i_0 + T.Cast("int64", T.ceil(T.log2(T.float64(128.0))))) + (T.min(T.int64(1024), T.shift_left(T.int64(2), i_0 + T.Cast("int64", T.ceil(T.log2(T.float64(128.0)))))) - T.int64(1))) // T.min(T.int64(1024), T.shift_left(T.int64(2), i_0 + T.Cast("int64", T.ceil(T.log2(T.float64(128.0))))))) + i_1_1)) % vocab_size] = value_buf[(blockIdx_y % batch_size * vocab_size + i_1[T.int64(0)]) // vocab_size, (blockIdx_y % batch_size * vocab_size + i_1[T.int64(0)]) % vocab_size]
                                            out_swap_buf[(blockIdx_y % batch_size * vocab_size + (T.shift_left(T.int64(2), i_0 + T.Cast("int64", T.ceil(T.log2(T.float64(128.0))))) * (blockIdx_y // batch_size) + threadIdx_x * ((T.shift_left(T.int64(2), i_0 + T.Cast("int64", T.ceil(T.log2(T.float64(128.0))))) + (T.min(T.int64(1024), T.shift_left(T.int64(2), i_0 + T.Cast("int64", T.ceil(T.log2(T.float64(128.0)))))) - T.int64(1))) // T.min(T.int64(1024), T.shift_left(T.int64(2), i_0 + T.Cast("int64", T.ceil(T.log2(T.float64(128.0))))))) + i_1_1)) // vocab_size, (blockIdx_y % batch_size * vocab_size + (T.shift_left(T.int64(2), i_0 + T.Cast("int64", T.ceil(T.log2(T.float64(128.0))))) * (blockIdx_y // batch_size) + threadIdx_x * ((T.shift_left(T.int64(2), i_0 + T.Cast("int64", T.ceil(T.log2(T.float64(128.0))))) + (T.min(T.int64(1024), T.shift_left(T.int64(2), i_0 + T.Cast("int64", T.ceil(T.log2(T.float64(128.0)))))) - T.int64(1))) // T.min(T.int64(1024), T.shift_left(T.int64(2), i_0 + T.Cast("int64", T.ceil(T.log2(T.float64(128.0))))))) + i_1_1)) % vocab_size] = out_buf[(blockIdx_y % batch_size * vocab_size + i_1[T.int64(0)]) // vocab_size, (blockIdx_y % batch_size * vocab_size + i_1[T.int64(0)]) % vocab_size]
                                            i_1[T.int64(0)] = i_1[T.int64(0)] + T.int64(1)
                                        else:
                                            value_swap_buf[(blockIdx_y % batch_size * vocab_size + (T.shift_left(T.int64(2), i_0 + T.Cast("int64", T.ceil(T.log2(T.float64(128.0))))) * (blockIdx_y // batch_size) + threadIdx_x * ((T.shift_left(T.int64(2), i_0 + T.Cast("int64", T.ceil(T.log2(T.float64(128.0))))) + (T.min(T.int64(1024), T.shift_left(T.int64(2), i_0 + T.Cast("int64", T.ceil(T.log2(T.float64(128.0)))))) - T.int64(1))) // T.min(T.int64(1024), T.shift_left(T.int64(2), i_0 + T.Cast("int64", T.ceil(T.log2(T.float64(128.0))))))) + i_1_1)) // vocab_size, (blockIdx_y % batch_size * vocab_size + (T.shift_left(T.int64(2), i_0 + T.Cast("int64", T.ceil(T.log2(T.float64(128.0))))) * (blockIdx_y // batch_size) + threadIdx_x * ((T.shift_left(T.int64(2), i_0 + T.Cast("int64", T.ceil(T.log2(T.float64(128.0))))) + (T.min(T.int64(1024), T.shift_left(T.int64(2), i_0 + T.Cast("int64", T.ceil(T.log2(T.float64(128.0)))))) - T.int64(1))) // T.min(T.int64(1024), T.shift_left(T.int64(2), i_0 + T.Cast("int64", T.ceil(T.log2(T.float64(128.0))))))) + i_1_1)) % vocab_size] = value_buf[(blockIdx_y % batch_size * vocab_size + j_1[T.int64(0)]) // vocab_size, (blockIdx_y % batch_size * vocab_size + j_1[T.int64(0)]) % vocab_size]
                                            out_swap_buf[(blockIdx_y % batch_size * vocab_size + (T.shift_left(T.int64(2), i_0 + T.Cast("int64", T.ceil(T.log2(T.float64(128.0))))) * (blockIdx_y // batch_size) + threadIdx_x * ((T.shift_left(T.int64(2), i_0 + T.Cast("int64", T.ceil(T.log2(T.float64(128.0))))) + (T.min(T.int64(1024), T.shift_left(T.int64(2), i_0 + T.Cast("int64", T.ceil(T.log2(T.float64(128.0)))))) - T.int64(1))) // T.min(T.int64(1024), T.shift_left(T.int64(2), i_0 + T.Cast("int64", T.ceil(T.log2(T.float64(128.0))))))) + i_1_1)) // vocab_size, (blockIdx_y % batch_size * vocab_size + (T.shift_left(T.int64(2), i_0 + T.Cast("int64", T.ceil(T.log2(T.float64(128.0))))) * (blockIdx_y // batch_size) + threadIdx_x * ((T.shift_left(T.int64(2), i_0 + T.Cast("int64", T.ceil(T.log2(T.float64(128.0))))) + (T.min(T.int64(1024), T.shift_left(T.int64(2), i_0 + T.Cast("int64", T.ceil(T.log2(T.float64(128.0)))))) - T.int64(1))) // T.min(T.int64(1024), T.shift_left(T.int64(2), i_0 + T.Cast("int64", T.ceil(T.log2(T.float64(128.0))))))) + i_1_1)) % vocab_size] = out_buf[(blockIdx_y % batch_size * vocab_size + j_1[T.int64(0)]) // vocab_size, (blockIdx_y % batch_size * vocab_size + j_1[T.int64(0)]) % vocab_size]
                                            j_1[T.int64(0)] = j_1[T.int64(0)] + T.int64(1)
                                    else:
                                        if i_1[T.int64(0)] < T.shift_left(T.int64(2), i_0 + T.Cast("int64", T.ceil(T.log2(T.float64(128.0))))) * (blockIdx_y // batch_size) + (T.min(T.shift_left(T.int64(2), i_0 + T.Cast("int64", T.ceil(T.log2(T.float64(128.0))))) * (blockIdx_y // batch_size) + T.shift_left(T.int64(2), i_0 + T.Cast("int64", T.ceil(T.log2(T.float64(128.0))))) // T.int64(2), vocab_size) - T.shift_left(T.int64(2), i_0 + T.Cast("int64", T.ceil(T.log2(T.float64(128.0))))) * (blockIdx_y // batch_size)):
                                            value_swap_buf[(blockIdx_y % batch_size * vocab_size + (T.shift_left(T.int64(2), i_0 + T.Cast("int64", T.ceil(T.log2(T.float64(128.0))))) * (blockIdx_y // batch_size) + threadIdx_x * ((T.shift_left(T.int64(2), i_0 + T.Cast("int64", T.ceil(T.log2(T.float64(128.0))))) + (T.min(T.int64(1024), T.shift_left(T.int64(2), i_0 + T.Cast("int64", T.ceil(T.log2(T.float64(128.0)))))) - T.int64(1))) // T.min(T.int64(1024), T.shift_left(T.int64(2), i_0 + T.Cast("int64", T.ceil(T.log2(T.float64(128.0))))))) + i_1_1)) // vocab_size, (blockIdx_y % batch_size * vocab_size + (T.shift_left(T.int64(2), i_0 + T.Cast("int64", T.ceil(T.log2(T.float64(128.0))))) * (blockIdx_y // batch_size) + threadIdx_x * ((T.shift_left(T.int64(2), i_0 + T.Cast("int64", T.ceil(T.log2(T.float64(128.0))))) + (T.min(T.int64(1024), T.shift_left(T.int64(2), i_0 + T.Cast("int64", T.ceil(T.log2(T.float64(128.0)))))) - T.int64(1))) // T.min(T.int64(1024), T.shift_left(T.int64(2), i_0 + T.Cast("int64", T.ceil(T.log2(T.float64(128.0))))))) + i_1_1)) % vocab_size] = value_buf[(blockIdx_y % batch_size * vocab_size + i_1[T.int64(0)]) // vocab_size, (blockIdx_y % batch_size * vocab_size + i_1[T.int64(0)]) % vocab_size]
                                            out_swap_buf[(blockIdx_y % batch_size * vocab_size + (T.shift_left(T.int64(2), i_0 + T.Cast("int64", T.ceil(T.log2(T.float64(128.0))))) * (blockIdx_y // batch_size) + threadIdx_x * ((T.shift_left(T.int64(2), i_0 + T.Cast("int64", T.ceil(T.log2(T.float64(128.0))))) + (T.min(T.int64(1024), T.shift_left(T.int64(2), i_0 + T.Cast("int64", T.ceil(T.log2(T.float64(128.0)))))) - T.int64(1))) // T.min(T.int64(1024), T.shift_left(T.int64(2), i_0 + T.Cast("int64", T.ceil(T.log2(T.float64(128.0))))))) + i_1_1)) // vocab_size, (blockIdx_y % batch_size * vocab_size + (T.shift_left(T.int64(2), i_0 + T.Cast("int64", T.ceil(T.log2(T.float64(128.0))))) * (blockIdx_y // batch_size) + threadIdx_x * ((T.shift_left(T.int64(2), i_0 + T.Cast("int64", T.ceil(T.log2(T.float64(128.0))))) + (T.min(T.int64(1024), T.shift_left(T.int64(2), i_0 + T.Cast("int64", T.ceil(T.log2(T.float64(128.0)))))) - T.int64(1))) // T.min(T.int64(1024), T.shift_left(T.int64(2), i_0 + T.Cast("int64", T.ceil(T.log2(T.float64(128.0))))))) + i_1_1)) % vocab_size] = out_buf[(blockIdx_y % batch_size * vocab_size + i_1[T.int64(0)]) // vocab_size, (blockIdx_y % batch_size * vocab_size + i_1[T.int64(0)]) % vocab_size]
                                            i_1[T.int64(0)] = i_1[T.int64(0)] + T.int64(1)
                                        else:
                                            value_swap_buf[(blockIdx_y % batch_size * vocab_size + (T.shift_left(T.int64(2), i_0 + T.Cast("int64", T.ceil(T.log2(T.float64(128.0))))) * (blockIdx_y // batch_size) + threadIdx_x * ((T.shift_left(T.int64(2), i_0 + T.Cast("int64", T.ceil(T.log2(T.float64(128.0))))) + (T.min(T.int64(1024), T.shift_left(T.int64(2), i_0 + T.Cast("int64", T.ceil(T.log2(T.float64(128.0)))))) - T.int64(1))) // T.min(T.int64(1024), T.shift_left(T.int64(2), i_0 + T.Cast("int64", T.ceil(T.log2(T.float64(128.0))))))) + i_1_1)) // vocab_size, (blockIdx_y % batch_size * vocab_size + (T.shift_left(T.int64(2), i_0 + T.Cast("int64", T.ceil(T.log2(T.float64(128.0))))) * (blockIdx_y // batch_size) + threadIdx_x * ((T.shift_left(T.int64(2), i_0 + T.Cast("int64", T.ceil(T.log2(T.float64(128.0))))) + (T.min(T.int64(1024), T.shift_left(T.int64(2), i_0 + T.Cast("int64", T.ceil(T.log2(T.float64(128.0)))))) - T.int64(1))) // T.min(T.int64(1024), T.shift_left(T.int64(2), i_0 + T.Cast("int64", T.ceil(T.log2(T.float64(128.0))))))) + i_1_1)) % vocab_size] = value_buf[(blockIdx_y % batch_size * vocab_size + j_1[T.int64(0)]) // vocab_size, (blockIdx_y % batch_size * vocab_size + j_1[T.int64(0)]) % vocab_size]
                                            out_swap_buf[(blockIdx_y % batch_size * vocab_size + (T.shift_left(T.int64(2), i_0 + T.Cast("int64", T.ceil(T.log2(T.float64(128.0))))) * (blockIdx_y // batch_size) + threadIdx_x * ((T.shift_left(T.int64(2), i_0 + T.Cast("int64", T.ceil(T.log2(T.float64(128.0))))) + (T.min(T.int64(1024), T.shift_left(T.int64(2), i_0 + T.Cast("int64", T.ceil(T.log2(T.float64(128.0)))))) - T.int64(1))) // T.min(T.int64(1024), T.shift_left(T.int64(2), i_0 + T.Cast("int64", T.ceil(T.log2(T.float64(128.0))))))) + i_1_1)) // vocab_size, (blockIdx_y % batch_size * vocab_size + (T.shift_left(T.int64(2), i_0 + T.Cast("int64", T.ceil(T.log2(T.float64(128.0))))) * (blockIdx_y // batch_size) + threadIdx_x * ((T.shift_left(T.int64(2), i_0 + T.Cast("int64", T.ceil(T.log2(T.float64(128.0))))) + (T.min(T.int64(1024), T.shift_left(T.int64(2), i_0 + T.Cast("int64", T.ceil(T.log2(T.float64(128.0)))))) - T.int64(1))) // T.min(T.int64(1024), T.shift_left(T.int64(2), i_0 + T.Cast("int64", T.ceil(T.log2(T.float64(128.0))))))) + i_1_1)) % vocab_size] = out_buf[(blockIdx_y % batch_size * vocab_size + j_1[T.int64(0)]) // vocab_size, (blockIdx_y % batch_size * vocab_size + j_1[T.int64(0)]) % vocab_size]
                                            j_1[T.int64(0)] = j_1[T.int64(0)] + T.int64(1)
                            else:
                                first = T.allocate([T.int64(1)], "int64", "local")
                                mid = T.allocate([T.int64(1)], "int64", "local")
                                last = T.allocate([T.int64(1)], "int64", "local")
                                first_1 = T.Buffer((1,), "int64", data=first, scope="local")
                                first_1[T.int64(0)] = T.max(T.int64(0), threadIdx_x * ((T.shift_left(T.int64(2), i_0 + T.Cast("int64", T.ceil(T.log2(T.float64(128.0))))) + (T.min(T.int64(1024), T.shift_left(T.int64(2), i_0 + T.Cast("int64", T.ceil(T.log2(T.float64(128.0)))))) - T.int64(1))) // T.min(T.int64(1024), T.shift_left(T.int64(2), i_0 + T.Cast("int64", T.ceil(T.log2(T.float64(128.0))))))) - (T.min(T.shift_left(T.int64(2), i_0 + T.Cast("int64", T.ceil(T.log2(T.float64(128.0))))) * (blockIdx_y // batch_size) + T.shift_left(T.int64(2), i_0 + T.Cast("int64", T.ceil(T.log2(T.float64(128.0))))), vocab_size) - T.min(T.shift_left(T.int64(2), i_0 + T.Cast("int64", T.ceil(T.log2(T.float64(128.0))))) * (blockIdx_y // batch_size) + T.shift_left(T.int64(2), i_0 + T.Cast("int64", T.ceil(T.log2(T.float64(128.0))))) // T.int64(2), vocab_size)))
                                last_1 = T.Buffer((1,), "int64", data=last, scope="local")
                                last_1[T.int64(0)] = T.min(threadIdx_x * ((T.shift_left(T.int64(2), i_0 + T.Cast("int64", T.ceil(T.log2(T.float64(128.0))))) + (T.min(T.int64(1024), T.shift_left(T.int64(2), i_0 + T.Cast("int64", T.ceil(T.log2(T.float64(128.0)))))) - T.int64(1))) // T.min(T.int64(1024), T.shift_left(T.int64(2), i_0 + T.Cast("int64", T.ceil(T.log2(T.float64(128.0))))))), T.min(T.shift_left(T.int64(2), i_0 + T.Cast("int64", T.ceil(T.log2(T.float64(128.0))))) * (blockIdx_y // batch_size) + T.shift_left(T.int64(2), i_0 + T.Cast("int64", T.ceil(T.log2(T.float64(128.0))))) // T.int64(2), vocab_size) - T.shift_left(T.int64(2), i_0 + T.Cast("int64", T.ceil(T.log2(T.float64(128.0))))) * (blockIdx_y // batch_size))
                                while first_1[T.int64(0)] < last_1[T.int64(0)]:
                                    if value_swap_buf[(blockIdx_y % batch_size * vocab_size + (T.min(T.shift_left(T.int64(2), i_0 + T.Cast("int64", T.ceil(T.log2(T.float64(128.0))))) * (blockIdx_y // batch_size) + T.shift_left(T.int64(2), i_0 + T.Cast("int64", T.ceil(T.log2(T.float64(128.0))))) // T.int64(2), vocab_size) + threadIdx_x * ((T.shift_left(T.int64(2), i_0 + T.Cast("int64", T.ceil(T.log2(T.float64(128.0))))) + (T.min(T.int64(1024), T.shift_left(T.int64(2), i_0 + T.Cast("int64", T.ceil(T.log2(T.float64(128.0)))))) - T.int64(1))) // T.min(T.int64(1024), T.shift_left(T.int64(2), i_0 + T.Cast("int64", T.ceil(T.log2(T.float64(128.0))))))) - T.int64(1) - T.shift_right(first_1[T.int64(0)] + last_1[T.int64(0)], T.int64(1)))) // vocab_size, (blockIdx_y % batch_size * vocab_size + (T.min(T.shift_left(T.int64(2), i_0 + T.Cast("int64", T.ceil(T.log2(T.float64(128.0))))) * (blockIdx_y // batch_size) + T.shift_left(T.int64(2), i_0 + T.Cast("int64", T.ceil(T.log2(T.float64(128.0))))) // T.int64(2), vocab_size) + threadIdx_x * ((T.shift_left(T.int64(2), i_0 + T.Cast("int64", T.ceil(T.log2(T.float64(128.0))))) + (T.min(T.int64(1024), T.shift_left(T.int64(2), i_0 + T.Cast("int64", T.ceil(T.log2(T.float64(128.0)))))) - T.int64(1))) // T.min(T.int64(1024), T.shift_left(T.int64(2), i_0 + T.Cast("int64", T.ceil(T.log2(T.float64(128.0))))))) - T.int64(1) - T.shift_right(first_1[T.int64(0)] + last_1[T.int64(0)], T.int64(1)))) % vocab_size] <= value_swap_buf[(blockIdx_y % batch_size * vocab_size + (T.shift_left(T.int64(2), i_0 + T.Cast("int64", T.ceil(T.log2(T.float64(128.0))))) * (blockIdx_y // batch_size) + T.shift_right(first_1[T.int64(0)] + last_1[T.int64(0)], T.int64(1)))) // vocab_size, (blockIdx_y % batch_size * vocab_size + (T.shift_left(T.int64(2), i_0 + T.Cast("int64", T.ceil(T.log2(T.float64(128.0))))) * (blockIdx_y // batch_size) + T.shift_right(first_1[T.int64(0)] + last_1[T.int64(0)], T.int64(1)))) % vocab_size]:
                                        first_1[T.int64(0)] = T.shift_right(first_1[T.int64(0)] + last_1[T.int64(0)], T.int64(1)) + T.int64(1)
                                    else:
                                        last_1[T.int64(0)] = T.shift_right(first_1[T.int64(0)] + last_1[T.int64(0)], T.int64(1))
                                i = T.allocate([T.int64(1)], "int64", "local")
                                j = T.allocate([T.int64(1)], "int64", "local")
                                i_1 = T.Buffer((1,), "int64", data=i, scope="local")
                                i_1[T.int64(0)] = T.shift_left(T.int64(2), i_0 + T.Cast("int64", T.ceil(T.log2(T.float64(128.0))))) * (blockIdx_y // batch_size) + first_1[T.int64(0)]
                                j_1 = T.Buffer((1,), "int64", data=j, scope="local")
                                j_1[T.int64(0)] = T.min(T.shift_left(T.int64(2), i_0 + T.Cast("int64", T.ceil(T.log2(T.float64(128.0))))) * (blockIdx_y // batch_size) + T.shift_left(T.int64(2), i_0 + T.Cast("int64", T.ceil(T.log2(T.float64(128.0))))) // T.int64(2), vocab_size) + threadIdx_x * ((T.shift_left(T.int64(2), i_0 + T.Cast("int64", T.ceil(T.log2(T.float64(128.0))))) + (T.min(T.int64(1024), T.shift_left(T.int64(2), i_0 + T.Cast("int64", T.ceil(T.log2(T.float64(128.0)))))) - T.int64(1))) // T.min(T.int64(1024), T.shift_left(T.int64(2), i_0 + T.Cast("int64", T.ceil(T.log2(T.float64(128.0))))))) - last_1[T.int64(0)]
                                for i_2 in range(T.min(T.min(T.shift_left(T.int64(2), i_0 + T.Cast("int64", T.ceil(T.log2(T.float64(128.0))))) * (blockIdx_y // batch_size) + T.shift_left(T.int64(2), i_0 + T.Cast("int64", T.ceil(T.log2(T.float64(128.0))))) // T.int64(2), vocab_size) - T.shift_left(T.int64(2), i_0 + T.Cast("int64", T.ceil(T.log2(T.float64(128.0))))) * (blockIdx_y // batch_size) + (T.min(T.shift_left(T.int64(2), i_0 + T.Cast("int64", T.ceil(T.log2(T.float64(128.0))))) * (blockIdx_y // batch_size) + T.shift_left(T.int64(2), i_0 + T.Cast("int64", T.ceil(T.log2(T.float64(128.0))))), vocab_size) - T.min(T.shift_left(T.int64(2), i_0 + T.Cast("int64", T.ceil(T.log2(T.float64(128.0))))) * (blockIdx_y // batch_size) + T.shift_left(T.int64(2), i_0 + T.Cast("int64", T.ceil(T.log2(T.float64(128.0))))) // T.int64(2), vocab_size)) - threadIdx_x * ((T.shift_left(T.int64(2), i_0 + T.Cast("int64", T.ceil(T.log2(T.float64(128.0))))) + (T.min(T.int64(1024), T.shift_left(T.int64(2), i_0 + T.Cast("int64", T.ceil(T.log2(T.float64(128.0)))))) - T.int64(1))) // T.min(T.int64(1024), T.shift_left(T.int64(2), i_0 + T.Cast("int64", T.ceil(T.log2(T.float64(128.0))))))), (T.shift_left(T.int64(2), i_0 + T.Cast("int64", T.ceil(T.log2(T.float64(128.0))))) + (T.min(T.int64(1024), T.shift_left(T.int64(2), i_0 + T.Cast("int64", T.ceil(T.log2(T.float64(128.0)))))) - T.int64(1))) // T.min(T.int64(1024), T.shift_left(T.int64(2), i_0 + T.Cast("int64", T.ceil(T.log2(T.float64(128.0)))))))):
                                    if i_1[T.int64(0)] < T.shift_left(T.int64(2), i_0 + T.Cast("int64", T.ceil(T.log2(T.float64(128.0))))) * (blockIdx_y // batch_size) + (T.min(T.shift_left(T.int64(2), i_0 + T.Cast("int64", T.ceil(T.log2(T.float64(128.0))))) * (blockIdx_y // batch_size) + T.shift_left(T.int64(2), i_0 + T.Cast("int64", T.ceil(T.log2(T.float64(128.0))))) // T.int64(2), vocab_size) - T.shift_left(T.int64(2), i_0 + T.Cast("int64", T.ceil(T.log2(T.float64(128.0))))) * (blockIdx_y // batch_size)) and j_1[T.int64(0)] < T.min(T.shift_left(T.int64(2), i_0 + T.Cast("int64", T.ceil(T.log2(T.float64(128.0))))) * (blockIdx_y // batch_size) + T.shift_left(T.int64(2), i_0 + T.Cast("int64", T.ceil(T.log2(T.float64(128.0))))) // T.int64(2), vocab_size) + (T.min(T.shift_left(T.int64(2), i_0 + T.Cast("int64", T.ceil(T.log2(T.float64(128.0))))) * (blockIdx_y // batch_size) + T.shift_left(T.int64(2), i_0 + T.Cast("int64", T.ceil(T.log2(T.float64(128.0))))), vocab_size) - T.min(T.shift_left(T.int64(2), i_0 + T.Cast("int64", T.ceil(T.log2(T.float64(128.0))))) * (blockIdx_y // batch_size) + T.shift_left(T.int64(2), i_0 + T.Cast("int64", T.ceil(T.log2(T.float64(128.0))))) // T.int64(2), vocab_size)):
                                        if value_swap_buf[(blockIdx_y % batch_size * vocab_size + j_1[T.int64(0)]) // vocab_size, (blockIdx_y % batch_size * vocab_size + j_1[T.int64(0)]) % vocab_size] <= value_swap_buf[(blockIdx_y % batch_size * vocab_size + i_1[T.int64(0)]) // vocab_size, (blockIdx_y % batch_size * vocab_size + i_1[T.int64(0)]) % vocab_size]:
                                            value_buf[(blockIdx_y % batch_size * vocab_size + (T.shift_left(T.int64(2), i_0 + T.Cast("int64", T.ceil(T.log2(T.float64(128.0))))) * (blockIdx_y // batch_size) + threadIdx_x * ((T.shift_left(T.int64(2), i_0 + T.Cast("int64", T.ceil(T.log2(T.float64(128.0))))) + (T.min(T.int64(1024), T.shift_left(T.int64(2), i_0 + T.Cast("int64", T.ceil(T.log2(T.float64(128.0)))))) - T.int64(1))) // T.min(T.int64(1024), T.shift_left(T.int64(2), i_0 + T.Cast("int64", T.ceil(T.log2(T.float64(128.0))))))) + i_2)) // vocab_size, (blockIdx_y % batch_size * vocab_size + (T.shift_left(T.int64(2), i_0 + T.Cast("int64", T.ceil(T.log2(T.float64(128.0))))) * (blockIdx_y // batch_size) + threadIdx_x * ((T.shift_left(T.int64(2), i_0 + T.Cast("int64", T.ceil(T.log2(T.float64(128.0))))) + (T.min(T.int64(1024), T.shift_left(T.int64(2), i_0 + T.Cast("int64", T.ceil(T.log2(T.float64(128.0)))))) - T.int64(1))) // T.min(T.int64(1024), T.shift_left(T.int64(2), i_0 + T.Cast("int64", T.ceil(T.log2(T.float64(128.0))))))) + i_2)) % vocab_size] = value_swap_buf[(blockIdx_y % batch_size * vocab_size + i_1[T.int64(0)]) // vocab_size, (blockIdx_y % batch_size * vocab_size + i_1[T.int64(0)]) % vocab_size]
                                            out_buf[(blockIdx_y % batch_size * vocab_size + (T.shift_left(T.int64(2), i_0 + T.Cast("int64", T.ceil(T.log2(T.float64(128.0))))) * (blockIdx_y // batch_size) + threadIdx_x * ((T.shift_left(T.int64(2), i_0 + T.Cast("int64", T.ceil(T.log2(T.float64(128.0))))) + (T.min(T.int64(1024), T.shift_left(T.int64(2), i_0 + T.Cast("int64", T.ceil(T.log2(T.float64(128.0)))))) - T.int64(1))) // T.min(T.int64(1024), T.shift_left(T.int64(2), i_0 + T.Cast("int64", T.ceil(T.log2(T.float64(128.0))))))) + i_2)) // vocab_size, (blockIdx_y % batch_size * vocab_size + (T.shift_left(T.int64(2), i_0 + T.Cast("int64", T.ceil(T.log2(T.float64(128.0))))) * (blockIdx_y // batch_size) + threadIdx_x * ((T.shift_left(T.int64(2), i_0 + T.Cast("int64", T.ceil(T.log2(T.float64(128.0))))) + (T.min(T.int64(1024), T.shift_left(T.int64(2), i_0 + T.Cast("int64", T.ceil(T.log2(T.float64(128.0)))))) - T.int64(1))) // T.min(T.int64(1024), T.shift_left(T.int64(2), i_0 + T.Cast("int64", T.ceil(T.log2(T.float64(128.0))))))) + i_2)) % vocab_size] = out_swap_buf[(blockIdx_y % batch_size * vocab_size + i_1[T.int64(0)]) // vocab_size, (blockIdx_y % batch_size * vocab_size + i_1[T.int64(0)]) % vocab_size]
                                            i_1[T.int64(0)] = i_1[T.int64(0)] + T.int64(1)
                                        else:
                                            value_buf[(blockIdx_y % batch_size * vocab_size + (T.shift_left(T.int64(2), i_0 + T.Cast("int64", T.ceil(T.log2(T.float64(128.0))))) * (blockIdx_y // batch_size) + threadIdx_x * ((T.shift_left(T.int64(2), i_0 + T.Cast("int64", T.ceil(T.log2(T.float64(128.0))))) + (T.min(T.int64(1024), T.shift_left(T.int64(2), i_0 + T.Cast("int64", T.ceil(T.log2(T.float64(128.0)))))) - T.int64(1))) // T.min(T.int64(1024), T.shift_left(T.int64(2), i_0 + T.Cast("int64", T.ceil(T.log2(T.float64(128.0))))))) + i_2)) // vocab_size, (blockIdx_y % batch_size * vocab_size + (T.shift_left(T.int64(2), i_0 + T.Cast("int64", T.ceil(T.log2(T.float64(128.0))))) * (blockIdx_y // batch_size) + threadIdx_x * ((T.shift_left(T.int64(2), i_0 + T.Cast("int64", T.ceil(T.log2(T.float64(128.0))))) + (T.min(T.int64(1024), T.shift_left(T.int64(2), i_0 + T.Cast("int64", T.ceil(T.log2(T.float64(128.0)))))) - T.int64(1))) // T.min(T.int64(1024), T.shift_left(T.int64(2), i_0 + T.Cast("int64", T.ceil(T.log2(T.float64(128.0))))))) + i_2)) % vocab_size] = value_swap_buf[(blockIdx_y % batch_size * vocab_size + j_1[T.int64(0)]) // vocab_size, (blockIdx_y % batch_size * vocab_size + j_1[T.int64(0)]) % vocab_size]
                                            out_buf[(blockIdx_y % batch_size * vocab_size + (T.shift_left(T.int64(2), i_0 + T.Cast("int64", T.ceil(T.log2(T.float64(128.0))))) * (blockIdx_y // batch_size) + threadIdx_x * ((T.shift_left(T.int64(2), i_0 + T.Cast("int64", T.ceil(T.log2(T.float64(128.0))))) + (T.min(T.int64(1024), T.shift_left(T.int64(2), i_0 + T.Cast("int64", T.ceil(T.log2(T.float64(128.0)))))) - T.int64(1))) // T.min(T.int64(1024), T.shift_left(T.int64(2), i_0 + T.Cast("int64", T.ceil(T.log2(T.float64(128.0))))))) + i_2)) // vocab_size, (blockIdx_y % batch_size * vocab_size + (T.shift_left(T.int64(2), i_0 + T.Cast("int64", T.ceil(T.log2(T.float64(128.0))))) * (blockIdx_y // batch_size) + threadIdx_x * ((T.shift_left(T.int64(2), i_0 + T.Cast("int64", T.ceil(T.log2(T.float64(128.0))))) + (T.min(T.int64(1024), T.shift_left(T.int64(2), i_0 + T.Cast("int64", T.ceil(T.log2(T.float64(128.0)))))) - T.int64(1))) // T.min(T.int64(1024), T.shift_left(T.int64(2), i_0 + T.Cast("int64", T.ceil(T.log2(T.float64(128.0))))))) + i_2)) % vocab_size] = out_swap_buf[(blockIdx_y % batch_size * vocab_size + j_1[T.int64(0)]) // vocab_size, (blockIdx_y % batch_size * vocab_size + j_1[T.int64(0)]) % vocab_size]
                                            j_1[T.int64(0)] = j_1[T.int64(0)] + T.int64(1)
                                    else:
                                        if i_1[T.int64(0)] < T.shift_left(T.int64(2), i_0 + T.Cast("int64", T.ceil(T.log2(T.float64(128.0))))) * (blockIdx_y // batch_size) + (T.min(T.shift_left(T.int64(2), i_0 + T.Cast("int64", T.ceil(T.log2(T.float64(128.0))))) * (blockIdx_y // batch_size) + T.shift_left(T.int64(2), i_0 + T.Cast("int64", T.ceil(T.log2(T.float64(128.0))))) // T.int64(2), vocab_size) - T.shift_left(T.int64(2), i_0 + T.Cast("int64", T.ceil(T.log2(T.float64(128.0))))) * (blockIdx_y // batch_size)):
                                            value_buf[(blockIdx_y % batch_size * vocab_size + (T.shift_left(T.int64(2), i_0 + T.Cast("int64", T.ceil(T.log2(T.float64(128.0))))) * (blockIdx_y // batch_size) + threadIdx_x * ((T.shift_left(T.int64(2), i_0 + T.Cast("int64", T.ceil(T.log2(T.float64(128.0))))) + (T.min(T.int64(1024), T.shift_left(T.int64(2), i_0 + T.Cast("int64", T.ceil(T.log2(T.float64(128.0)))))) - T.int64(1))) // T.min(T.int64(1024), T.shift_left(T.int64(2), i_0 + T.Cast("int64", T.ceil(T.log2(T.float64(128.0))))))) + i_2)) // vocab_size, (blockIdx_y % batch_size * vocab_size + (T.shift_left(T.int64(2), i_0 + T.Cast("int64", T.ceil(T.log2(T.float64(128.0))))) * (blockIdx_y // batch_size) + threadIdx_x * ((T.shift_left(T.int64(2), i_0 + T.Cast("int64", T.ceil(T.log2(T.float64(128.0))))) + (T.min(T.int64(1024), T.shift_left(T.int64(2), i_0 + T.Cast("int64", T.ceil(T.log2(T.float64(128.0)))))) - T.int64(1))) // T.min(T.int64(1024), T.shift_left(T.int64(2), i_0 + T.Cast("int64", T.ceil(T.log2(T.float64(128.0))))))) + i_2)) % vocab_size] = value_swap_buf[(blockIdx_y % batch_size * vocab_size + i_1[T.int64(0)]) // vocab_size, (blockIdx_y % batch_size * vocab_size + i_1[T.int64(0)]) % vocab_size]
                                            out_buf[(blockIdx_y % batch_size * vocab_size + (T.shift_left(T.int64(2), i_0 + T.Cast("int64", T.ceil(T.log2(T.float64(128.0))))) * (blockIdx_y // batch_size) + threadIdx_x * ((T.shift_left(T.int64(2), i_0 + T.Cast("int64", T.ceil(T.log2(T.float64(128.0))))) + (T.min(T.int64(1024), T.shift_left(T.int64(2), i_0 + T.Cast("int64", T.ceil(T.log2(T.float64(128.0)))))) - T.int64(1))) // T.min(T.int64(1024), T.shift_left(T.int64(2), i_0 + T.Cast("int64", T.ceil(T.log2(T.float64(128.0))))))) + i_2)) // vocab_size, (blockIdx_y % batch_size * vocab_size + (T.shift_left(T.int64(2), i_0 + T.Cast("int64", T.ceil(T.log2(T.float64(128.0))))) * (blockIdx_y // batch_size) + threadIdx_x * ((T.shift_left(T.int64(2), i_0 + T.Cast("int64", T.ceil(T.log2(T.float64(128.0))))) + (T.min(T.int64(1024), T.shift_left(T.int64(2), i_0 + T.Cast("int64", T.ceil(T.log2(T.float64(128.0)))))) - T.int64(1))) // T.min(T.int64(1024), T.shift_left(T.int64(2), i_0 + T.Cast("int64", T.ceil(T.log2(T.float64(128.0))))))) + i_2)) % vocab_size] = out_swap_buf[(blockIdx_y % batch_size * vocab_size + i_1[T.int64(0)]) // vocab_size, (blockIdx_y % batch_size * vocab_size + i_1[T.int64(0)]) % vocab_size]
                                            i_1[T.int64(0)] = i_1[T.int64(0)] + T.int64(1)
                                        else:
                                            value_buf[(blockIdx_y % batch_size * vocab_size + (T.shift_left(T.int64(2), i_0 + T.Cast("int64", T.ceil(T.log2(T.float64(128.0))))) * (blockIdx_y // batch_size) + threadIdx_x * ((T.shift_left(T.int64(2), i_0 + T.Cast("int64", T.ceil(T.log2(T.float64(128.0))))) + (T.min(T.int64(1024), T.shift_left(T.int64(2), i_0 + T.Cast("int64", T.ceil(T.log2(T.float64(128.0)))))) - T.int64(1))) // T.min(T.int64(1024), T.shift_left(T.int64(2), i_0 + T.Cast("int64", T.ceil(T.log2(T.float64(128.0))))))) + i_2)) // vocab_size, (blockIdx_y % batch_size * vocab_size + (T.shift_left(T.int64(2), i_0 + T.Cast("int64", T.ceil(T.log2(T.float64(128.0))))) * (blockIdx_y // batch_size) + threadIdx_x * ((T.shift_left(T.int64(2), i_0 + T.Cast("int64", T.ceil(T.log2(T.float64(128.0))))) + (T.min(T.int64(1024), T.shift_left(T.int64(2), i_0 + T.Cast("int64", T.ceil(T.log2(T.float64(128.0)))))) - T.int64(1))) // T.min(T.int64(1024), T.shift_left(T.int64(2), i_0 + T.Cast("int64", T.ceil(T.log2(T.float64(128.0))))))) + i_2)) % vocab_size] = value_swap_buf[(blockIdx_y % batch_size * vocab_size + j_1[T.int64(0)]) // vocab_size, (blockIdx_y % batch_size * vocab_size + j_1[T.int64(0)]) % vocab_size]
                                            out_buf[(blockIdx_y % batch_size * vocab_size + (T.shift_left(T.int64(2), i_0 + T.Cast("int64", T.ceil(T.log2(T.float64(128.0))))) * (blockIdx_y // batch_size) + threadIdx_x * ((T.shift_left(T.int64(2), i_0 + T.Cast("int64", T.ceil(T.log2(T.float64(128.0))))) + (T.min(T.int64(1024), T.shift_left(T.int64(2), i_0 + T.Cast("int64", T.ceil(T.log2(T.float64(128.0)))))) - T.int64(1))) // T.min(T.int64(1024), T.shift_left(T.int64(2), i_0 + T.Cast("int64", T.ceil(T.log2(T.float64(128.0))))))) + i_2)) // vocab_size, (blockIdx_y % batch_size * vocab_size + (T.shift_left(T.int64(2), i_0 + T.Cast("int64", T.ceil(T.log2(T.float64(128.0))))) * (blockIdx_y // batch_size) + threadIdx_x * ((T.shift_left(T.int64(2), i_0 + T.Cast("int64", T.ceil(T.log2(T.float64(128.0))))) + (T.min(T.int64(1024), T.shift_left(T.int64(2), i_0 + T.Cast("int64", T.ceil(T.log2(T.float64(128.0)))))) - T.int64(1))) // T.min(T.int64(1024), T.shift_left(T.int64(2), i_0 + T.Cast("int64", T.ceil(T.log2(T.float64(128.0))))))) + i_2)) % vocab_size] = out_swap_buf[(blockIdx_y % batch_size * vocab_size + j_1[T.int64(0)]) // vocab_size, (blockIdx_y % batch_size * vocab_size + j_1[T.int64(0)]) % vocab_size]
                                            j_1[T.int64(0)] = j_1[T.int64(0)] + T.int64(1)
                        else:
                            if i_0 % T.int64(2) == T.int64(0):
                                first = T.allocate([T.int64(1)], "int64", "local")
                                mid = T.allocate([T.int64(1)], "int64", "local")
                                last = T.allocate([T.int64(1)], "int64", "local")
                                first_1 = T.Buffer((1,), "int64", data=first, scope="local")
                                first_1[T.int64(0)] = T.max(T.int64(0), blockIdx_x * T.int64(4096) - (T.min(T.shift_left(T.int64(2), i_0 + T.Cast("int64", T.ceil(T.log2(T.float64(128.0))))) * (blockIdx_y // batch_size) + T.shift_left(T.int64(2), i_0 + T.Cast("int64", T.ceil(T.log2(T.float64(128.0))))), vocab_size) - T.min(T.shift_left(T.int64(2), i_0 + T.Cast("int64", T.ceil(T.log2(T.float64(128.0))))) * (blockIdx_y // batch_size) + T.shift_left(T.int64(2), i_0 + T.Cast("int64", T.ceil(T.log2(T.float64(128.0))))) // T.int64(2), vocab_size)))
                                last_1 = T.Buffer((1,), "int64", data=last, scope="local")
                                last_1[T.int64(0)] = T.min(blockIdx_x * T.int64(4096), T.min(T.shift_left(T.int64(2), i_0 + T.Cast("int64", T.ceil(T.log2(T.float64(128.0))))) * (blockIdx_y // batch_size) + T.shift_left(T.int64(2), i_0 + T.Cast("int64", T.ceil(T.log2(T.float64(128.0))))) // T.int64(2), vocab_size) - T.shift_left(T.int64(2), i_0 + T.Cast("int64", T.ceil(T.log2(T.float64(128.0))))) * (blockIdx_y // batch_size))
                                while first_1[T.int64(0)] < last_1[T.int64(0)]:
                                    if value_buf[(blockIdx_y % batch_size * vocab_size + (T.min(T.shift_left(T.int64(2), i_0 + T.Cast("int64", T.ceil(T.log2(T.float64(128.0))))) * (blockIdx_y // batch_size) + T.shift_left(T.int64(2), i_0 + T.Cast("int64", T.ceil(T.log2(T.float64(128.0))))) // T.int64(2), vocab_size) + blockIdx_x * T.int64(4096) - T.int64(1) - T.shift_right(first_1[T.int64(0)] + last_1[T.int64(0)], T.int64(1)))) // vocab_size, (blockIdx_y % batch_size * vocab_size + (T.min(T.shift_left(T.int64(2), i_0 + T.Cast("int64", T.ceil(T.log2(T.float64(128.0))))) * (blockIdx_y // batch_size) + T.shift_left(T.int64(2), i_0 + T.Cast("int64", T.ceil(T.log2(T.float64(128.0))))) // T.int64(2), vocab_size) + blockIdx_x * T.int64(4096) - T.int64(1) - T.shift_right(first_1[T.int64(0)] + last_1[T.int64(0)], T.int64(1)))) % vocab_size] <= value_buf[(blockIdx_y % batch_size * vocab_size + (T.shift_left(T.int64(2), i_0 + T.Cast("int64", T.ceil(T.log2(T.float64(128.0))))) * (blockIdx_y // batch_size) + T.shift_right(first_1[T.int64(0)] + last_1[T.int64(0)], T.int64(1)))) // vocab_size, (blockIdx_y % batch_size * vocab_size + (T.shift_left(T.int64(2), i_0 + T.Cast("int64", T.ceil(T.log2(T.float64(128.0))))) * (blockIdx_y // batch_size) + T.shift_right(first_1[T.int64(0)] + last_1[T.int64(0)], T.int64(1)))) % vocab_size]:
                                        first_1[T.int64(0)] = T.shift_right(first_1[T.int64(0)] + last_1[T.int64(0)], T.int64(1)) + T.int64(1)
                                    else:
                                        last_1[T.int64(0)] = T.shift_right(first_1[T.int64(0)] + last_1[T.int64(0)], T.int64(1))
                                if i_0 % T.int64(2) == T.int64(0):
                                    first_2 = T.allocate([T.int64(1)], "int64", "local")
                                    mid_1 = T.allocate([T.int64(1)], "int64", "local")
                                    last_2 = T.allocate([T.int64(1)], "int64", "local")
                                    first_3 = T.Buffer((1,), "int64", data=first_2, scope="local")
                                    first_3[T.int64(0)] = T.max(T.int64(0), threadIdx_x * T.int64(4) - T.min(T.min(T.shift_left(T.int64(2), i_0 + T.Cast("int64", T.ceil(T.log2(T.float64(128.0))))) * (blockIdx_y // batch_size) + T.shift_left(T.int64(2), i_0 + T.Cast("int64", T.ceil(T.log2(T.float64(128.0))))), vocab_size) - (T.min(T.shift_left(T.int64(2), i_0 + T.Cast("int64", T.ceil(T.log2(T.float64(128.0))))) * (blockIdx_y // batch_size) + T.shift_left(T.int64(2), i_0 + T.Cast("int64", T.ceil(T.log2(T.float64(128.0))))) // T.int64(2), vocab_size) + blockIdx_x * T.int64(4096) - last_1[T.int64(0)]), T.int64(4096)))
                                    last_3 = T.Buffer((1,), "int64", data=last_2, scope="local")
                                    last_3[T.int64(0)] = T.min(threadIdx_x * T.int64(4), T.min(T.min(T.shift_left(T.int64(2), i_0 + T.Cast("int64", T.ceil(T.log2(T.float64(128.0))))) * (blockIdx_y // batch_size) + T.shift_left(T.int64(2), i_0 + T.Cast("int64", T.ceil(T.log2(T.float64(128.0))))) // T.int64(2), vocab_size) - (T.shift_left(T.int64(2), i_0 + T.Cast("int64", T.ceil(T.log2(T.float64(128.0))))) * (blockIdx_y // batch_size) + first_1[T.int64(0)]), T.int64(4096)))
                                    while first_3[T.int64(0)] < last_3[T.int64(0)]:
                                        if value_buf[(blockIdx_y % batch_size * vocab_size + (T.min(T.shift_left(T.int64(2), i_0 + T.Cast("int64", T.ceil(T.log2(T.float64(128.0))))) * (blockIdx_y // batch_size) + T.shift_left(T.int64(2), i_0 + T.Cast("int64", T.ceil(T.log2(T.float64(128.0))))) // T.int64(2), vocab_size) + blockIdx_x * T.int64(4096) - last_1[T.int64(0)] + threadIdx_x * T.int64(4) - T.int64(1) - T.shift_right(first_3[T.int64(0)] + last_3[T.int64(0)], T.int64(1)))) // vocab_size, (blockIdx_y % batch_size * vocab_size + (T.min(T.shift_left(T.int64(2), i_0 + T.Cast("int64", T.ceil(T.log2(T.float64(128.0))))) * (blockIdx_y // batch_size) + T.shift_left(T.int64(2), i_0 + T.Cast("int64", T.ceil(T.log2(T.float64(128.0))))) // T.int64(2), vocab_size) + blockIdx_x * T.int64(4096) - last_1[T.int64(0)] + threadIdx_x * T.int64(4) - T.int64(1) - T.shift_right(first_3[T.int64(0)] + last_3[T.int64(0)], T.int64(1)))) % vocab_size] <= value_buf[(blockIdx_y % batch_size * vocab_size + (T.shift_left(T.int64(2), i_0 + T.Cast("int64", T.ceil(T.log2(T.float64(128.0))))) * (blockIdx_y // batch_size) + first_1[T.int64(0)] + T.shift_right(first_3[T.int64(0)] + last_3[T.int64(0)], T.int64(1)))) // vocab_size, (blockIdx_y % batch_size * vocab_size + (T.shift_left(T.int64(2), i_0 + T.Cast("int64", T.ceil(T.log2(T.float64(128.0))))) * (blockIdx_y // batch_size) + first_1[T.int64(0)] + T.shift_right(first_3[T.int64(0)] + last_3[T.int64(0)], T.int64(1)))) % vocab_size]:
                                            first_3[T.int64(0)] = T.shift_right(first_3[T.int64(0)] + last_3[T.int64(0)], T.int64(1)) + T.int64(1)
                                        else:
                                            last_3[T.int64(0)] = T.shift_right(first_3[T.int64(0)] + last_3[T.int64(0)], T.int64(1))
                                    i = T.allocate([T.int64(1)], "int64", "local")
                                    j = T.allocate([T.int64(1)], "int64", "local")
                                    i_1 = T.Buffer((1,), "int64", data=i, scope="local")
                                    i_1[T.int64(0)] = T.shift_left(T.int64(2), i_0 + T.Cast("int64", T.ceil(T.log2(T.float64(128.0))))) * (blockIdx_y // batch_size) + first_1[T.int64(0)] + first_3[T.int64(0)]
                                    j_1 = T.Buffer((1,), "int64", data=j, scope="local")
                                    j_1[T.int64(0)] = T.min(T.shift_left(T.int64(2), i_0 + T.Cast("int64", T.ceil(T.log2(T.float64(128.0))))) * (blockIdx_y // batch_size) + T.shift_left(T.int64(2), i_0 + T.Cast("int64", T.ceil(T.log2(T.float64(128.0))))) // T.int64(2), vocab_size) + blockIdx_x * T.int64(4096) - last_1[T.int64(0)] + threadIdx_x * T.int64(4) - last_3[T.int64(0)]
                                    for i_3 in range(T.min(T.min(T.min(T.shift_left(T.int64(2), i_0 + T.Cast("int64", T.ceil(T.log2(T.float64(128.0))))) * (blockIdx_y // batch_size) + T.shift_left(T.int64(2), i_0 + T.Cast("int64", T.ceil(T.log2(T.float64(128.0))))) // T.int64(2), vocab_size) - (T.shift_left(T.int64(2), i_0 + T.Cast("int64", T.ceil(T.log2(T.float64(128.0))))) * (blockIdx_y // batch_size) + first_1[T.int64(0)]), T.int64(4096)) + T.min(T.min(T.shift_left(T.int64(2), i_0 + T.Cast("int64", T.ceil(T.log2(T.float64(128.0))))) * (blockIdx_y // batch_size) + T.shift_left(T.int64(2), i_0 + T.Cast("int64", T.ceil(T.log2(T.float64(128.0))))), vocab_size) - (T.min(T.shift_left(T.int64(2), i_0 + T.Cast("int64", T.ceil(T.log2(T.float64(128.0))))) * (blockIdx_y // batch_size) + T.shift_left(T.int64(2), i_0 + T.Cast("int64", T.ceil(T.log2(T.float64(128.0))))) // T.int64(2), vocab_size) + blockIdx_x * T.int64(4096) - last_1[T.int64(0)]), T.int64(4096)) - threadIdx_x * T.int64(4), T.int64(4))):
                                        if i_1[T.int64(0)] < T.shift_left(T.int64(2), i_0 + T.Cast("int64", T.ceil(T.log2(T.float64(128.0))))) * (blockIdx_y // batch_size) + first_1[T.int64(0)] + T.min(T.min(T.shift_left(T.int64(2), i_0 + T.Cast("int64", T.ceil(T.log2(T.float64(128.0))))) * (blockIdx_y // batch_size) + T.shift_left(T.int64(2), i_0 + T.Cast("int64", T.ceil(T.log2(T.float64(128.0))))) // T.int64(2), vocab_size) - (T.shift_left(T.int64(2), i_0 + T.Cast("int64", T.ceil(T.log2(T.float64(128.0))))) * (blockIdx_y // batch_size) + first_1[T.int64(0)]), T.int64(4096)) and j_1[T.int64(0)] < T.min(T.shift_left(T.int64(2), i_0 + T.Cast("int64", T.ceil(T.log2(T.float64(128.0))))) * (blockIdx_y // batch_size) + T.shift_left(T.int64(2), i_0 + T.Cast("int64", T.ceil(T.log2(T.float64(128.0))))) // T.int64(2), vocab_size) + blockIdx_x * T.int64(4096) - last_1[T.int64(0)] + T.min(T.min(T.shift_left(T.int64(2), i_0 + T.Cast("int64", T.ceil(T.log2(T.float64(128.0))))) * (blockIdx_y // batch_size) + T.shift_left(T.int64(2), i_0 + T.Cast("int64", T.ceil(T.log2(T.float64(128.0))))), vocab_size) - (T.min(T.shift_left(T.int64(2), i_0 + T.Cast("int64", T.ceil(T.log2(T.float64(128.0))))) * (blockIdx_y // batch_size) + T.shift_left(T.int64(2), i_0 + T.Cast("int64", T.ceil(T.log2(T.float64(128.0))))) // T.int64(2), vocab_size) + blockIdx_x * T.int64(4096) - last_1[T.int64(0)]), T.int64(4096)):
                                            if value_buf[(blockIdx_y % batch_size * vocab_size + j_1[T.int64(0)]) // vocab_size, (blockIdx_y % batch_size * vocab_size + j_1[T.int64(0)]) % vocab_size] <= value_buf[(blockIdx_y % batch_size * vocab_size + i_1[T.int64(0)]) // vocab_size, (blockIdx_y % batch_size * vocab_size + i_1[T.int64(0)]) % vocab_size]:
                                                value_swap_buf[(blockIdx_y % batch_size * vocab_size + (T.shift_left(T.int64(2), i_0 + T.Cast("int64", T.ceil(T.log2(T.float64(128.0))))) * (blockIdx_y // batch_size) + blockIdx_x * T.int64(4096) + threadIdx_x * T.int64(4) + i_3)) // vocab_size, (blockIdx_y % batch_size * vocab_size + (T.shift_left(T.int64(2), i_0 + T.Cast("int64", T.ceil(T.log2(T.float64(128.0))))) * (blockIdx_y // batch_size) + blockIdx_x * T.int64(4096) + threadIdx_x * T.int64(4) + i_3)) % vocab_size] = value_buf[(blockIdx_y % batch_size * vocab_size + i_1[T.int64(0)]) // vocab_size, (blockIdx_y % batch_size * vocab_size + i_1[T.int64(0)]) % vocab_size]
                                                out_swap_buf[(blockIdx_y % batch_size * vocab_size + (T.shift_left(T.int64(2), i_0 + T.Cast("int64", T.ceil(T.log2(T.float64(128.0))))) * (blockIdx_y // batch_size) + blockIdx_x * T.int64(4096) + threadIdx_x * T.int64(4) + i_3)) // vocab_size, (blockIdx_y % batch_size * vocab_size + (T.shift_left(T.int64(2), i_0 + T.Cast("int64", T.ceil(T.log2(T.float64(128.0))))) * (blockIdx_y // batch_size) + blockIdx_x * T.int64(4096) + threadIdx_x * T.int64(4) + i_3)) % vocab_size] = out_buf[(blockIdx_y % batch_size * vocab_size + i_1[T.int64(0)]) // vocab_size, (blockIdx_y % batch_size * vocab_size + i_1[T.int64(0)]) % vocab_size]
                                                i_1[T.int64(0)] = i_1[T.int64(0)] + T.int64(1)
                                            else:
                                                value_swap_buf[(blockIdx_y % batch_size * vocab_size + (T.shift_left(T.int64(2), i_0 + T.Cast("int64", T.ceil(T.log2(T.float64(128.0))))) * (blockIdx_y // batch_size) + blockIdx_x * T.int64(4096) + threadIdx_x * T.int64(4) + i_3)) // vocab_size, (blockIdx_y % batch_size * vocab_size + (T.shift_left(T.int64(2), i_0 + T.Cast("int64", T.ceil(T.log2(T.float64(128.0))))) * (blockIdx_y // batch_size) + blockIdx_x * T.int64(4096) + threadIdx_x * T.int64(4) + i_3)) % vocab_size] = value_buf[(blockIdx_y % batch_size * vocab_size + j_1[T.int64(0)]) // vocab_size, (blockIdx_y % batch_size * vocab_size + j_1[T.int64(0)]) % vocab_size]
                                                out_swap_buf[(blockIdx_y % batch_size * vocab_size + (T.shift_left(T.int64(2), i_0 + T.Cast("int64", T.ceil(T.log2(T.float64(128.0))))) * (blockIdx_y // batch_size) + blockIdx_x * T.int64(4096) + threadIdx_x * T.int64(4) + i_3)) // vocab_size, (blockIdx_y % batch_size * vocab_size + (T.shift_left(T.int64(2), i_0 + T.Cast("int64", T.ceil(T.log2(T.float64(128.0))))) * (blockIdx_y // batch_size) + blockIdx_x * T.int64(4096) + threadIdx_x * T.int64(4) + i_3)) % vocab_size] = out_buf[(blockIdx_y % batch_size * vocab_size + j_1[T.int64(0)]) // vocab_size, (blockIdx_y % batch_size * vocab_size + j_1[T.int64(0)]) % vocab_size]
                                                j_1[T.int64(0)] = j_1[T.int64(0)] + T.int64(1)
                                        else:
                                            if i_1[T.int64(0)] < T.shift_left(T.int64(2), i_0 + T.Cast("int64", T.ceil(T.log2(T.float64(128.0))))) * (blockIdx_y // batch_size) + first_1[T.int64(0)] + T.min(T.min(T.shift_left(T.int64(2), i_0 + T.Cast("int64", T.ceil(T.log2(T.float64(128.0))))) * (blockIdx_y // batch_size) + T.shift_left(T.int64(2), i_0 + T.Cast("int64", T.ceil(T.log2(T.float64(128.0))))) // T.int64(2), vocab_size) - (T.shift_left(T.int64(2), i_0 + T.Cast("int64", T.ceil(T.log2(T.float64(128.0))))) * (blockIdx_y // batch_size) + first_1[T.int64(0)]), T.int64(4096)):
                                                value_swap_buf[(blockIdx_y % batch_size * vocab_size + (T.shift_left(T.int64(2), i_0 + T.Cast("int64", T.ceil(T.log2(T.float64(128.0))))) * (blockIdx_y // batch_size) + blockIdx_x * T.int64(4096) + threadIdx_x * T.int64(4) + i_3)) // vocab_size, (blockIdx_y % batch_size * vocab_size + (T.shift_left(T.int64(2), i_0 + T.Cast("int64", T.ceil(T.log2(T.float64(128.0))))) * (blockIdx_y // batch_size) + blockIdx_x * T.int64(4096) + threadIdx_x * T.int64(4) + i_3)) % vocab_size] = value_buf[(blockIdx_y % batch_size * vocab_size + i_1[T.int64(0)]) // vocab_size, (blockIdx_y % batch_size * vocab_size + i_1[T.int64(0)]) % vocab_size]
                                                out_swap_buf[(blockIdx_y % batch_size * vocab_size + (T.shift_left(T.int64(2), i_0 + T.Cast("int64", T.ceil(T.log2(T.float64(128.0))))) * (blockIdx_y // batch_size) + blockIdx_x * T.int64(4096) + threadIdx_x * T.int64(4) + i_3)) // vocab_size, (blockIdx_y % batch_size * vocab_size + (T.shift_left(T.int64(2), i_0 + T.Cast("int64", T.ceil(T.log2(T.float64(128.0))))) * (blockIdx_y // batch_size) + blockIdx_x * T.int64(4096) + threadIdx_x * T.int64(4) + i_3)) % vocab_size] = out_buf[(blockIdx_y % batch_size * vocab_size + i_1[T.int64(0)]) // vocab_size, (blockIdx_y % batch_size * vocab_size + i_1[T.int64(0)]) % vocab_size]
                                                i_1[T.int64(0)] = i_1[T.int64(0)] + T.int64(1)
                                            else:
                                                value_swap_buf[(blockIdx_y % batch_size * vocab_size + (T.shift_left(T.int64(2), i_0 + T.Cast("int64", T.ceil(T.log2(T.float64(128.0))))) * (blockIdx_y // batch_size) + blockIdx_x * T.int64(4096) + threadIdx_x * T.int64(4) + i_3)) // vocab_size, (blockIdx_y % batch_size * vocab_size + (T.shift_left(T.int64(2), i_0 + T.Cast("int64", T.ceil(T.log2(T.float64(128.0))))) * (blockIdx_y // batch_size) + blockIdx_x * T.int64(4096) + threadIdx_x * T.int64(4) + i_3)) % vocab_size] = value_buf[(blockIdx_y % batch_size * vocab_size + j_1[T.int64(0)]) // vocab_size, (blockIdx_y % batch_size * vocab_size + j_1[T.int64(0)]) % vocab_size]
                                                out_swap_buf[(blockIdx_y % batch_size * vocab_size + (T.shift_left(T.int64(2), i_0 + T.Cast("int64", T.ceil(T.log2(T.float64(128.0))))) * (blockIdx_y // batch_size) + blockIdx_x * T.int64(4096) + threadIdx_x * T.int64(4) + i_3)) // vocab_size, (blockIdx_y % batch_size * vocab_size + (T.shift_left(T.int64(2), i_0 + T.Cast("int64", T.ceil(T.log2(T.float64(128.0))))) * (blockIdx_y // batch_size) + blockIdx_x * T.int64(4096) + threadIdx_x * T.int64(4) + i_3)) % vocab_size] = out_buf[(blockIdx_y % batch_size * vocab_size + j_1[T.int64(0)]) // vocab_size, (blockIdx_y % batch_size * vocab_size + j_1[T.int64(0)]) % vocab_size]
                                                j_1[T.int64(0)] = j_1[T.int64(0)] + T.int64(1)
                                else:
                                    first_2 = T.allocate([T.int64(1)], "int64", "local")
                                    mid_1 = T.allocate([T.int64(1)], "int64", "local")
                                    last_2 = T.allocate([T.int64(1)], "int64", "local")
                                    first_3 = T.Buffer((1,), "int64", data=first_2, scope="local")
                                    first_3[T.int64(0)] = T.max(T.int64(0), threadIdx_x * T.int64(4) - T.min(T.min(T.shift_left(T.int64(2), i_0 + T.Cast("int64", T.ceil(T.log2(T.float64(128.0))))) * (blockIdx_y // batch_size) + T.shift_left(T.int64(2), i_0 + T.Cast("int64", T.ceil(T.log2(T.float64(128.0))))), vocab_size) - (T.min(T.shift_left(T.int64(2), i_0 + T.Cast("int64", T.ceil(T.log2(T.float64(128.0))))) * (blockIdx_y // batch_size) + T.shift_left(T.int64(2), i_0 + T.Cast("int64", T.ceil(T.log2(T.float64(128.0))))) // T.int64(2), vocab_size) + blockIdx_x * T.int64(4096) - last_1[T.int64(0)]), T.int64(4096)))
                                    last_3 = T.Buffer((1,), "int64", data=last_2, scope="local")
                                    last_3[T.int64(0)] = T.min(threadIdx_x * T.int64(4), T.min(T.min(T.shift_left(T.int64(2), i_0 + T.Cast("int64", T.ceil(T.log2(T.float64(128.0))))) * (blockIdx_y // batch_size) + T.shift_left(T.int64(2), i_0 + T.Cast("int64", T.ceil(T.log2(T.float64(128.0))))) // T.int64(2), vocab_size) - (T.shift_left(T.int64(2), i_0 + T.Cast("int64", T.ceil(T.log2(T.float64(128.0))))) * (blockIdx_y // batch_size) + first_1[T.int64(0)]), T.int64(4096)))
                                    while first_3[T.int64(0)] < last_3[T.int64(0)]:
                                        if value_swap_buf[(blockIdx_y % batch_size * vocab_size + (T.min(T.shift_left(T.int64(2), i_0 + T.Cast("int64", T.ceil(T.log2(T.float64(128.0))))) * (blockIdx_y // batch_size) + T.shift_left(T.int64(2), i_0 + T.Cast("int64", T.ceil(T.log2(T.float64(128.0))))) // T.int64(2), vocab_size) + blockIdx_x * T.int64(4096) - last_1[T.int64(0)] + threadIdx_x * T.int64(4) - T.int64(1) - T.shift_right(first_3[T.int64(0)] + last_3[T.int64(0)], T.int64(1)))) // vocab_size, (blockIdx_y % batch_size * vocab_size + (T.min(T.shift_left(T.int64(2), i_0 + T.Cast("int64", T.ceil(T.log2(T.float64(128.0))))) * (blockIdx_y // batch_size) + T.shift_left(T.int64(2), i_0 + T.Cast("int64", T.ceil(T.log2(T.float64(128.0))))) // T.int64(2), vocab_size) + blockIdx_x * T.int64(4096) - last_1[T.int64(0)] + threadIdx_x * T.int64(4) - T.int64(1) - T.shift_right(first_3[T.int64(0)] + last_3[T.int64(0)], T.int64(1)))) % vocab_size] <= value_swap_buf[(blockIdx_y % batch_size * vocab_size + (T.shift_left(T.int64(2), i_0 + T.Cast("int64", T.ceil(T.log2(T.float64(128.0))))) * (blockIdx_y // batch_size) + first_1[T.int64(0)] + T.shift_right(first_3[T.int64(0)] + last_3[T.int64(0)], T.int64(1)))) // vocab_size, (blockIdx_y % batch_size * vocab_size + (T.shift_left(T.int64(2), i_0 + T.Cast("int64", T.ceil(T.log2(T.float64(128.0))))) * (blockIdx_y // batch_size) + first_1[T.int64(0)] + T.shift_right(first_3[T.int64(0)] + last_3[T.int64(0)], T.int64(1)))) % vocab_size]:
                                            first_3[T.int64(0)] = T.shift_right(first_3[T.int64(0)] + last_3[T.int64(0)], T.int64(1)) + T.int64(1)
                                        else:
                                            last_3[T.int64(0)] = T.shift_right(first_3[T.int64(0)] + last_3[T.int64(0)], T.int64(1))
                                    i = T.allocate([T.int64(1)], "int64", "local")
                                    j = T.allocate([T.int64(1)], "int64", "local")
                                    i_1 = T.Buffer((1,), "int64", data=i, scope="local")
                                    i_1[T.int64(0)] = T.shift_left(T.int64(2), i_0 + T.Cast("int64", T.ceil(T.log2(T.float64(128.0))))) * (blockIdx_y // batch_size) + first_1[T.int64(0)] + first_3[T.int64(0)]
                                    j_1 = T.Buffer((1,), "int64", data=j, scope="local")
                                    j_1[T.int64(0)] = T.min(T.shift_left(T.int64(2), i_0 + T.Cast("int64", T.ceil(T.log2(T.float64(128.0))))) * (blockIdx_y // batch_size) + T.shift_left(T.int64(2), i_0 + T.Cast("int64", T.ceil(T.log2(T.float64(128.0))))) // T.int64(2), vocab_size) + blockIdx_x * T.int64(4096) - last_1[T.int64(0)] + threadIdx_x * T.int64(4) - last_3[T.int64(0)]
                                    for i_4 in range(T.min(T.min(T.min(T.shift_left(T.int64(2), i_0 + T.Cast("int64", T.ceil(T.log2(T.float64(128.0))))) * (blockIdx_y // batch_size) + T.shift_left(T.int64(2), i_0 + T.Cast("int64", T.ceil(T.log2(T.float64(128.0))))) // T.int64(2), vocab_size) - (T.shift_left(T.int64(2), i_0 + T.Cast("int64", T.ceil(T.log2(T.float64(128.0))))) * (blockIdx_y // batch_size) + first_1[T.int64(0)]), T.int64(4096)) + T.min(T.min(T.shift_left(T.int64(2), i_0 + T.Cast("int64", T.ceil(T.log2(T.float64(128.0))))) * (blockIdx_y // batch_size) + T.shift_left(T.int64(2), i_0 + T.Cast("int64", T.ceil(T.log2(T.float64(128.0))))), vocab_size) - (T.min(T.shift_left(T.int64(2), i_0 + T.Cast("int64", T.ceil(T.log2(T.float64(128.0))))) * (blockIdx_y // batch_size) + T.shift_left(T.int64(2), i_0 + T.Cast("int64", T.ceil(T.log2(T.float64(128.0))))) // T.int64(2), vocab_size) + blockIdx_x * T.int64(4096) - last_1[T.int64(0)]), T.int64(4096)) - threadIdx_x * T.int64(4), T.int64(4))):
                                        if i_1[T.int64(0)] < T.shift_left(T.int64(2), i_0 + T.Cast("int64", T.ceil(T.log2(T.float64(128.0))))) * (blockIdx_y // batch_size) + first_1[T.int64(0)] + T.min(T.min(T.shift_left(T.int64(2), i_0 + T.Cast("int64", T.ceil(T.log2(T.float64(128.0))))) * (blockIdx_y // batch_size) + T.shift_left(T.int64(2), i_0 + T.Cast("int64", T.ceil(T.log2(T.float64(128.0))))) // T.int64(2), vocab_size) - (T.shift_left(T.int64(2), i_0 + T.Cast("int64", T.ceil(T.log2(T.float64(128.0))))) * (blockIdx_y // batch_size) + first_1[T.int64(0)]), T.int64(4096)) and j_1[T.int64(0)] < T.min(T.shift_left(T.int64(2), i_0 + T.Cast("int64", T.ceil(T.log2(T.float64(128.0))))) * (blockIdx_y // batch_size) + T.shift_left(T.int64(2), i_0 + T.Cast("int64", T.ceil(T.log2(T.float64(128.0))))) // T.int64(2), vocab_size) + blockIdx_x * T.int64(4096) - last_1[T.int64(0)] + T.min(T.min(T.shift_left(T.int64(2), i_0 + T.Cast("int64", T.ceil(T.log2(T.float64(128.0))))) * (blockIdx_y // batch_size) + T.shift_left(T.int64(2), i_0 + T.Cast("int64", T.ceil(T.log2(T.float64(128.0))))), vocab_size) - (T.min(T.shift_left(T.int64(2), i_0 + T.Cast("int64", T.ceil(T.log2(T.float64(128.0))))) * (blockIdx_y // batch_size) + T.shift_left(T.int64(2), i_0 + T.Cast("int64", T.ceil(T.log2(T.float64(128.0))))) // T.int64(2), vocab_size) + blockIdx_x * T.int64(4096) - last_1[T.int64(0)]), T.int64(4096)):
                                            if value_swap_buf[(blockIdx_y % batch_size * vocab_size + j_1[T.int64(0)]) // vocab_size, (blockIdx_y % batch_size * vocab_size + j_1[T.int64(0)]) % vocab_size] <= value_swap_buf[(blockIdx_y % batch_size * vocab_size + i_1[T.int64(0)]) // vocab_size, (blockIdx_y % batch_size * vocab_size + i_1[T.int64(0)]) % vocab_size]:
                                                value_buf[(blockIdx_y % batch_size * vocab_size + (T.shift_left(T.int64(2), i_0 + T.Cast("int64", T.ceil(T.log2(T.float64(128.0))))) * (blockIdx_y // batch_size) + blockIdx_x * T.int64(4096) + threadIdx_x * T.int64(4) + i_4)) // vocab_size, (blockIdx_y % batch_size * vocab_size + (T.shift_left(T.int64(2), i_0 + T.Cast("int64", T.ceil(T.log2(T.float64(128.0))))) * (blockIdx_y // batch_size) + blockIdx_x * T.int64(4096) + threadIdx_x * T.int64(4) + i_4)) % vocab_size] = value_swap_buf[(blockIdx_y % batch_size * vocab_size + i_1[T.int64(0)]) // vocab_size, (blockIdx_y % batch_size * vocab_size + i_1[T.int64(0)]) % vocab_size]
                                                out_buf[(blockIdx_y % batch_size * vocab_size + (T.shift_left(T.int64(2), i_0 + T.Cast("int64", T.ceil(T.log2(T.float64(128.0))))) * (blockIdx_y // batch_size) + blockIdx_x * T.int64(4096) + threadIdx_x * T.int64(4) + i_4)) // vocab_size, (blockIdx_y % batch_size * vocab_size + (T.shift_left(T.int64(2), i_0 + T.Cast("int64", T.ceil(T.log2(T.float64(128.0))))) * (blockIdx_y // batch_size) + blockIdx_x * T.int64(4096) + threadIdx_x * T.int64(4) + i_4)) % vocab_size] = out_swap_buf[(blockIdx_y % batch_size * vocab_size + i_1[T.int64(0)]) // vocab_size, (blockIdx_y % batch_size * vocab_size + i_1[T.int64(0)]) % vocab_size]
                                                i_1[T.int64(0)] = i_1[T.int64(0)] + T.int64(1)
                                            else:
                                                value_buf[(blockIdx_y % batch_size * vocab_size + (T.shift_left(T.int64(2), i_0 + T.Cast("int64", T.ceil(T.log2(T.float64(128.0))))) * (blockIdx_y // batch_size) + blockIdx_x * T.int64(4096) + threadIdx_x * T.int64(4) + i_4)) // vocab_size, (blockIdx_y % batch_size * vocab_size + (T.shift_left(T.int64(2), i_0 + T.Cast("int64", T.ceil(T.log2(T.float64(128.0))))) * (blockIdx_y // batch_size) + blockIdx_x * T.int64(4096) + threadIdx_x * T.int64(4) + i_4)) % vocab_size] = value_swap_buf[(blockIdx_y % batch_size * vocab_size + j_1[T.int64(0)]) // vocab_size, (blockIdx_y % batch_size * vocab_size + j_1[T.int64(0)]) % vocab_size]
                                                out_buf[(blockIdx_y % batch_size * vocab_size + (T.shift_left(T.int64(2), i_0 + T.Cast("int64", T.ceil(T.log2(T.float64(128.0))))) * (blockIdx_y // batch_size) + blockIdx_x * T.int64(4096) + threadIdx_x * T.int64(4) + i_4)) // vocab_size, (blockIdx_y % batch_size * vocab_size + (T.shift_left(T.int64(2), i_0 + T.Cast("int64", T.ceil(T.log2(T.float64(128.0))))) * (blockIdx_y // batch_size) + blockIdx_x * T.int64(4096) + threadIdx_x * T.int64(4) + i_4)) % vocab_size] = out_swap_buf[(blockIdx_y % batch_size * vocab_size + j_1[T.int64(0)]) // vocab_size, (blockIdx_y % batch_size * vocab_size + j_1[T.int64(0)]) % vocab_size]
                                                j_1[T.int64(0)] = j_1[T.int64(0)] + T.int64(1)
                                        else:
                                            if i_1[T.int64(0)] < T.shift_left(T.int64(2), i_0 + T.Cast("int64", T.ceil(T.log2(T.float64(128.0))))) * (blockIdx_y // batch_size) + first_1[T.int64(0)] + T.min(T.min(T.shift_left(T.int64(2), i_0 + T.Cast("int64", T.ceil(T.log2(T.float64(128.0))))) * (blockIdx_y // batch_size) + T.shift_left(T.int64(2), i_0 + T.Cast("int64", T.ceil(T.log2(T.float64(128.0))))) // T.int64(2), vocab_size) - (T.shift_left(T.int64(2), i_0 + T.Cast("int64", T.ceil(T.log2(T.float64(128.0))))) * (blockIdx_y // batch_size) + first_1[T.int64(0)]), T.int64(4096)):
                                                value_buf[(blockIdx_y % batch_size * vocab_size + (T.shift_left(T.int64(2), i_0 + T.Cast("int64", T.ceil(T.log2(T.float64(128.0))))) * (blockIdx_y // batch_size) + blockIdx_x * T.int64(4096) + threadIdx_x * T.int64(4) + i_4)) // vocab_size, (blockIdx_y % batch_size * vocab_size + (T.shift_left(T.int64(2), i_0 + T.Cast("int64", T.ceil(T.log2(T.float64(128.0))))) * (blockIdx_y // batch_size) + blockIdx_x * T.int64(4096) + threadIdx_x * T.int64(4) + i_4)) % vocab_size] = value_swap_buf[(blockIdx_y % batch_size * vocab_size + i_1[T.int64(0)]) // vocab_size, (blockIdx_y % batch_size * vocab_size + i_1[T.int64(0)]) % vocab_size]
                                                out_buf[(blockIdx_y % batch_size * vocab_size + (T.shift_left(T.int64(2), i_0 + T.Cast("int64", T.ceil(T.log2(T.float64(128.0))))) * (blockIdx_y // batch_size) + blockIdx_x * T.int64(4096) + threadIdx_x * T.int64(4) + i_4)) // vocab_size, (blockIdx_y % batch_size * vocab_size + (T.shift_left(T.int64(2), i_0 + T.Cast("int64", T.ceil(T.log2(T.float64(128.0))))) * (blockIdx_y // batch_size) + blockIdx_x * T.int64(4096) + threadIdx_x * T.int64(4) + i_4)) % vocab_size] = out_swap_buf[(blockIdx_y % batch_size * vocab_size + i_1[T.int64(0)]) // vocab_size, (blockIdx_y % batch_size * vocab_size + i_1[T.int64(0)]) % vocab_size]
                                                i_1[T.int64(0)] = i_1[T.int64(0)] + T.int64(1)
                                            else:
                                                value_buf[(blockIdx_y % batch_size * vocab_size + (T.shift_left(T.int64(2), i_0 + T.Cast("int64", T.ceil(T.log2(T.float64(128.0))))) * (blockIdx_y // batch_size) + blockIdx_x * T.int64(4096) + threadIdx_x * T.int64(4) + i_4)) // vocab_size, (blockIdx_y % batch_size * vocab_size + (T.shift_left(T.int64(2), i_0 + T.Cast("int64", T.ceil(T.log2(T.float64(128.0))))) * (blockIdx_y // batch_size) + blockIdx_x * T.int64(4096) + threadIdx_x * T.int64(4) + i_4)) % vocab_size] = value_swap_buf[(blockIdx_y % batch_size * vocab_size + j_1[T.int64(0)]) // vocab_size, (blockIdx_y % batch_size * vocab_size + j_1[T.int64(0)]) % vocab_size]
                                                out_buf[(blockIdx_y % batch_size * vocab_size + (T.shift_left(T.int64(2), i_0 + T.Cast("int64", T.ceil(T.log2(T.float64(128.0))))) * (blockIdx_y // batch_size) + blockIdx_x * T.int64(4096) + threadIdx_x * T.int64(4) + i_4)) // vocab_size, (blockIdx_y % batch_size * vocab_size + (T.shift_left(T.int64(2), i_0 + T.Cast("int64", T.ceil(T.log2(T.float64(128.0))))) * (blockIdx_y // batch_size) + blockIdx_x * T.int64(4096) + threadIdx_x * T.int64(4) + i_4)) % vocab_size] = out_swap_buf[(blockIdx_y % batch_size * vocab_size + j_1[T.int64(0)]) // vocab_size, (blockIdx_y % batch_size * vocab_size + j_1[T.int64(0)]) % vocab_size]
                                                j_1[T.int64(0)] = j_1[T.int64(0)] + T.int64(1)
                            else:
                                first = T.allocate([T.int64(1)], "int64", "local")
                                mid = T.allocate([T.int64(1)], "int64", "local")
                                last = T.allocate([T.int64(1)], "int64", "local")
                                first_1 = T.Buffer((1,), "int64", data=first, scope="local")
                                first_1[T.int64(0)] = T.max(T.int64(0), blockIdx_x * T.int64(4096) - (T.min(T.shift_left(T.int64(2), i_0 + T.Cast("int64", T.ceil(T.log2(T.float64(128.0))))) * (blockIdx_y // batch_size) + T.shift_left(T.int64(2), i_0 + T.Cast("int64", T.ceil(T.log2(T.float64(128.0))))), vocab_size) - T.min(T.shift_left(T.int64(2), i_0 + T.Cast("int64", T.ceil(T.log2(T.float64(128.0))))) * (blockIdx_y // batch_size) + T.shift_left(T.int64(2), i_0 + T.Cast("int64", T.ceil(T.log2(T.float64(128.0))))) // T.int64(2), vocab_size)))
                                last_1 = T.Buffer((1,), "int64", data=last, scope="local")
                                last_1[T.int64(0)] = T.min(blockIdx_x * T.int64(4096), T.min(T.shift_left(T.int64(2), i_0 + T.Cast("int64", T.ceil(T.log2(T.float64(128.0))))) * (blockIdx_y // batch_size) + T.shift_left(T.int64(2), i_0 + T.Cast("int64", T.ceil(T.log2(T.float64(128.0))))) // T.int64(2), vocab_size) - T.shift_left(T.int64(2), i_0 + T.Cast("int64", T.ceil(T.log2(T.float64(128.0))))) * (blockIdx_y // batch_size))
                                while first_1[T.int64(0)] < last_1[T.int64(0)]:
                                    if value_swap_buf[(blockIdx_y % batch_size * vocab_size + (T.min(T.shift_left(T.int64(2), i_0 + T.Cast("int64", T.ceil(T.log2(T.float64(128.0))))) * (blockIdx_y // batch_size) + T.shift_left(T.int64(2), i_0 + T.Cast("int64", T.ceil(T.log2(T.float64(128.0))))) // T.int64(2), vocab_size) + blockIdx_x * T.int64(4096) - T.int64(1) - T.shift_right(first_1[T.int64(0)] + last_1[T.int64(0)], T.int64(1)))) // vocab_size, (blockIdx_y % batch_size * vocab_size + (T.min(T.shift_left(T.int64(2), i_0 + T.Cast("int64", T.ceil(T.log2(T.float64(128.0))))) * (blockIdx_y // batch_size) + T.shift_left(T.int64(2), i_0 + T.Cast("int64", T.ceil(T.log2(T.float64(128.0))))) // T.int64(2), vocab_size) + blockIdx_x * T.int64(4096) - T.int64(1) - T.shift_right(first_1[T.int64(0)] + last_1[T.int64(0)], T.int64(1)))) % vocab_size] <= value_swap_buf[(blockIdx_y % batch_size * vocab_size + (T.shift_left(T.int64(2), i_0 + T.Cast("int64", T.ceil(T.log2(T.float64(128.0))))) * (blockIdx_y // batch_size) + T.shift_right(first_1[T.int64(0)] + last_1[T.int64(0)], T.int64(1)))) // vocab_size, (blockIdx_y % batch_size * vocab_size + (T.shift_left(T.int64(2), i_0 + T.Cast("int64", T.ceil(T.log2(T.float64(128.0))))) * (blockIdx_y // batch_size) + T.shift_right(first_1[T.int64(0)] + last_1[T.int64(0)], T.int64(1)))) % vocab_size]:
                                        first_1[T.int64(0)] = T.shift_right(first_1[T.int64(0)] + last_1[T.int64(0)], T.int64(1)) + T.int64(1)
                                    else:
                                        last_1[T.int64(0)] = T.shift_right(first_1[T.int64(0)] + last_1[T.int64(0)], T.int64(1))
                                if i_0 % T.int64(2) == T.int64(0):
                                    first_2 = T.allocate([T.int64(1)], "int64", "local")
                                    mid_1 = T.allocate([T.int64(1)], "int64", "local")
                                    last_2 = T.allocate([T.int64(1)], "int64", "local")
                                    first_3 = T.Buffer((1,), "int64", data=first_2, scope="local")
                                    first_3[T.int64(0)] = T.max(T.int64(0), threadIdx_x * T.int64(4) - T.min(T.min(T.shift_left(T.int64(2), i_0 + T.Cast("int64", T.ceil(T.log2(T.float64(128.0))))) * (blockIdx_y // batch_size) + T.shift_left(T.int64(2), i_0 + T.Cast("int64", T.ceil(T.log2(T.float64(128.0))))), vocab_size) - (T.min(T.shift_left(T.int64(2), i_0 + T.Cast("int64", T.ceil(T.log2(T.float64(128.0))))) * (blockIdx_y // batch_size) + T.shift_left(T.int64(2), i_0 + T.Cast("int64", T.ceil(T.log2(T.float64(128.0))))) // T.int64(2), vocab_size) + blockIdx_x * T.int64(4096) - last_1[T.int64(0)]), T.int64(4096)))
                                    last_3 = T.Buffer((1,), "int64", data=last_2, scope="local")
                                    last_3[T.int64(0)] = T.min(threadIdx_x * T.int64(4), T.min(T.min(T.shift_left(T.int64(2), i_0 + T.Cast("int64", T.ceil(T.log2(T.float64(128.0))))) * (blockIdx_y // batch_size) + T.shift_left(T.int64(2), i_0 + T.Cast("int64", T.ceil(T.log2(T.float64(128.0))))) // T.int64(2), vocab_size) - (T.shift_left(T.int64(2), i_0 + T.Cast("int64", T.ceil(T.log2(T.float64(128.0))))) * (blockIdx_y // batch_size) + first_1[T.int64(0)]), T.int64(4096)))
                                    while first_3[T.int64(0)] < last_3[T.int64(0)]:
                                        if value_buf[(blockIdx_y % batch_size * vocab_size + (T.min(T.shift_left(T.int64(2), i_0 + T.Cast("int64", T.ceil(T.log2(T.float64(128.0))))) * (blockIdx_y // batch_size) + T.shift_left(T.int64(2), i_0 + T.Cast("int64", T.ceil(T.log2(T.float64(128.0))))) // T.int64(2), vocab_size) + blockIdx_x * T.int64(4096) - last_1[T.int64(0)] + threadIdx_x * T.int64(4) - T.int64(1) - T.shift_right(first_3[T.int64(0)] + last_3[T.int64(0)], T.int64(1)))) // vocab_size, (blockIdx_y % batch_size * vocab_size + (T.min(T.shift_left(T.int64(2), i_0 + T.Cast("int64", T.ceil(T.log2(T.float64(128.0))))) * (blockIdx_y // batch_size) + T.shift_left(T.int64(2), i_0 + T.Cast("int64", T.ceil(T.log2(T.float64(128.0))))) // T.int64(2), vocab_size) + blockIdx_x * T.int64(4096) - last_1[T.int64(0)] + threadIdx_x * T.int64(4) - T.int64(1) - T.shift_right(first_3[T.int64(0)] + last_3[T.int64(0)], T.int64(1)))) % vocab_size] <= value_buf[(blockIdx_y % batch_size * vocab_size + (T.shift_left(T.int64(2), i_0 + T.Cast("int64", T.ceil(T.log2(T.float64(128.0))))) * (blockIdx_y // batch_size) + first_1[T.int64(0)] + T.shift_right(first_3[T.int64(0)] + last_3[T.int64(0)], T.int64(1)))) // vocab_size, (blockIdx_y % batch_size * vocab_size + (T.shift_left(T.int64(2), i_0 + T.Cast("int64", T.ceil(T.log2(T.float64(128.0))))) * (blockIdx_y // batch_size) + first_1[T.int64(0)] + T.shift_right(first_3[T.int64(0)] + last_3[T.int64(0)], T.int64(1)))) % vocab_size]:
                                            first_3[T.int64(0)] = T.shift_right(first_3[T.int64(0)] + last_3[T.int64(0)], T.int64(1)) + T.int64(1)
                                        else:
                                            last_3[T.int64(0)] = T.shift_right(first_3[T.int64(0)] + last_3[T.int64(0)], T.int64(1))
                                    i = T.allocate([T.int64(1)], "int64", "local")
                                    j = T.allocate([T.int64(1)], "int64", "local")
                                    i_1 = T.Buffer((1,), "int64", data=i, scope="local")
                                    i_1[T.int64(0)] = T.shift_left(T.int64(2), i_0 + T.Cast("int64", T.ceil(T.log2(T.float64(128.0))))) * (blockIdx_y // batch_size) + first_1[T.int64(0)] + first_3[T.int64(0)]
                                    j_1 = T.Buffer((1,), "int64", data=j, scope="local")
                                    j_1[T.int64(0)] = T.min(T.shift_left(T.int64(2), i_0 + T.Cast("int64", T.ceil(T.log2(T.float64(128.0))))) * (blockIdx_y // batch_size) + T.shift_left(T.int64(2), i_0 + T.Cast("int64", T.ceil(T.log2(T.float64(128.0))))) // T.int64(2), vocab_size) + blockIdx_x * T.int64(4096) - last_1[T.int64(0)] + threadIdx_x * T.int64(4) - last_3[T.int64(0)]
                                    for i_5 in range(T.min(T.min(T.min(T.shift_left(T.int64(2), i_0 + T.Cast("int64", T.ceil(T.log2(T.float64(128.0))))) * (blockIdx_y // batch_size) + T.shift_left(T.int64(2), i_0 + T.Cast("int64", T.ceil(T.log2(T.float64(128.0))))) // T.int64(2), vocab_size) - (T.shift_left(T.int64(2), i_0 + T.Cast("int64", T.ceil(T.log2(T.float64(128.0))))) * (blockIdx_y // batch_size) + first_1[T.int64(0)]), T.int64(4096)) + T.min(T.min(T.shift_left(T.int64(2), i_0 + T.Cast("int64", T.ceil(T.log2(T.float64(128.0))))) * (blockIdx_y // batch_size) + T.shift_left(T.int64(2), i_0 + T.Cast("int64", T.ceil(T.log2(T.float64(128.0))))), vocab_size) - (T.min(T.shift_left(T.int64(2), i_0 + T.Cast("int64", T.ceil(T.log2(T.float64(128.0))))) * (blockIdx_y // batch_size) + T.shift_left(T.int64(2), i_0 + T.Cast("int64", T.ceil(T.log2(T.float64(128.0))))) // T.int64(2), vocab_size) + blockIdx_x * T.int64(4096) - last_1[T.int64(0)]), T.int64(4096)) - threadIdx_x * T.int64(4), T.int64(4))):
                                        if i_1[T.int64(0)] < T.shift_left(T.int64(2), i_0 + T.Cast("int64", T.ceil(T.log2(T.float64(128.0))))) * (blockIdx_y // batch_size) + first_1[T.int64(0)] + T.min(T.min(T.shift_left(T.int64(2), i_0 + T.Cast("int64", T.ceil(T.log2(T.float64(128.0))))) * (blockIdx_y // batch_size) + T.shift_left(T.int64(2), i_0 + T.Cast("int64", T.ceil(T.log2(T.float64(128.0))))) // T.int64(2), vocab_size) - (T.shift_left(T.int64(2), i_0 + T.Cast("int64", T.ceil(T.log2(T.float64(128.0))))) * (blockIdx_y // batch_size) + first_1[T.int64(0)]), T.int64(4096)) and j_1[T.int64(0)] < T.min(T.shift_left(T.int64(2), i_0 + T.Cast("int64", T.ceil(T.log2(T.float64(128.0))))) * (blockIdx_y // batch_size) + T.shift_left(T.int64(2), i_0 + T.Cast("int64", T.ceil(T.log2(T.float64(128.0))))) // T.int64(2), vocab_size) + blockIdx_x * T.int64(4096) - last_1[T.int64(0)] + T.min(T.min(T.shift_left(T.int64(2), i_0 + T.Cast("int64", T.ceil(T.log2(T.float64(128.0))))) * (blockIdx_y // batch_size) + T.shift_left(T.int64(2), i_0 + T.Cast("int64", T.ceil(T.log2(T.float64(128.0))))), vocab_size) - (T.min(T.shift_left(T.int64(2), i_0 + T.Cast("int64", T.ceil(T.log2(T.float64(128.0))))) * (blockIdx_y // batch_size) + T.shift_left(T.int64(2), i_0 + T.Cast("int64", T.ceil(T.log2(T.float64(128.0))))) // T.int64(2), vocab_size) + blockIdx_x * T.int64(4096) - last_1[T.int64(0)]), T.int64(4096)):
                                            if value_buf[(blockIdx_y % batch_size * vocab_size + j_1[T.int64(0)]) // vocab_size, (blockIdx_y % batch_size * vocab_size + j_1[T.int64(0)]) % vocab_size] <= value_buf[(blockIdx_y % batch_size * vocab_size + i_1[T.int64(0)]) // vocab_size, (blockIdx_y % batch_size * vocab_size + i_1[T.int64(0)]) % vocab_size]:
                                                value_swap_buf[(blockIdx_y % batch_size * vocab_size + (T.shift_left(T.int64(2), i_0 + T.Cast("int64", T.ceil(T.log2(T.float64(128.0))))) * (blockIdx_y // batch_size) + blockIdx_x * T.int64(4096) + threadIdx_x * T.int64(4) + i_5)) // vocab_size, (blockIdx_y % batch_size * vocab_size + (T.shift_left(T.int64(2), i_0 + T.Cast("int64", T.ceil(T.log2(T.float64(128.0))))) * (blockIdx_y // batch_size) + blockIdx_x * T.int64(4096) + threadIdx_x * T.int64(4) + i_5)) % vocab_size] = value_buf[(blockIdx_y % batch_size * vocab_size + i_1[T.int64(0)]) // vocab_size, (blockIdx_y % batch_size * vocab_size + i_1[T.int64(0)]) % vocab_size]
                                                out_swap_buf[(blockIdx_y % batch_size * vocab_size + (T.shift_left(T.int64(2), i_0 + T.Cast("int64", T.ceil(T.log2(T.float64(128.0))))) * (blockIdx_y // batch_size) + blockIdx_x * T.int64(4096) + threadIdx_x * T.int64(4) + i_5)) // vocab_size, (blockIdx_y % batch_size * vocab_size + (T.shift_left(T.int64(2), i_0 + T.Cast("int64", T.ceil(T.log2(T.float64(128.0))))) * (blockIdx_y // batch_size) + blockIdx_x * T.int64(4096) + threadIdx_x * T.int64(4) + i_5)) % vocab_size] = out_buf[(blockIdx_y % batch_size * vocab_size + i_1[T.int64(0)]) // vocab_size, (blockIdx_y % batch_size * vocab_size + i_1[T.int64(0)]) % vocab_size]
                                                i_1[T.int64(0)] = i_1[T.int64(0)] + T.int64(1)
                                            else:
                                                value_swap_buf[(blockIdx_y % batch_size * vocab_size + (T.shift_left(T.int64(2), i_0 + T.Cast("int64", T.ceil(T.log2(T.float64(128.0))))) * (blockIdx_y // batch_size) + blockIdx_x * T.int64(4096) + threadIdx_x * T.int64(4) + i_5)) // vocab_size, (blockIdx_y % batch_size * vocab_size + (T.shift_left(T.int64(2), i_0 + T.Cast("int64", T.ceil(T.log2(T.float64(128.0))))) * (blockIdx_y // batch_size) + blockIdx_x * T.int64(4096) + threadIdx_x * T.int64(4) + i_5)) % vocab_size] = value_buf[(blockIdx_y % batch_size * vocab_size + j_1[T.int64(0)]) // vocab_size, (blockIdx_y % batch_size * vocab_size + j_1[T.int64(0)]) % vocab_size]
                                                out_swap_buf[(blockIdx_y % batch_size * vocab_size + (T.shift_left(T.int64(2), i_0 + T.Cast("int64", T.ceil(T.log2(T.float64(128.0))))) * (blockIdx_y // batch_size) + blockIdx_x * T.int64(4096) + threadIdx_x * T.int64(4) + i_5)) // vocab_size, (blockIdx_y % batch_size * vocab_size + (T.shift_left(T.int64(2), i_0 + T.Cast("int64", T.ceil(T.log2(T.float64(128.0))))) * (blockIdx_y // batch_size) + blockIdx_x * T.int64(4096) + threadIdx_x * T.int64(4) + i_5)) % vocab_size] = out_buf[(blockIdx_y % batch_size * vocab_size + j_1[T.int64(0)]) // vocab_size, (blockIdx_y % batch_size * vocab_size + j_1[T.int64(0)]) % vocab_size]
                                                j_1[T.int64(0)] = j_1[T.int64(0)] + T.int64(1)
                                        else:
                                            if i_1[T.int64(0)] < T.shift_left(T.int64(2), i_0 + T.Cast("int64", T.ceil(T.log2(T.float64(128.0))))) * (blockIdx_y // batch_size) + first_1[T.int64(0)] + T.min(T.min(T.shift_left(T.int64(2), i_0 + T.Cast("int64", T.ceil(T.log2(T.float64(128.0))))) * (blockIdx_y // batch_size) + T.shift_left(T.int64(2), i_0 + T.Cast("int64", T.ceil(T.log2(T.float64(128.0))))) // T.int64(2), vocab_size) - (T.shift_left(T.int64(2), i_0 + T.Cast("int64", T.ceil(T.log2(T.float64(128.0))))) * (blockIdx_y // batch_size) + first_1[T.int64(0)]), T.int64(4096)):
                                                value_swap_buf[(blockIdx_y % batch_size * vocab_size + (T.shift_left(T.int64(2), i_0 + T.Cast("int64", T.ceil(T.log2(T.float64(128.0))))) * (blockIdx_y // batch_size) + blockIdx_x * T.int64(4096) + threadIdx_x * T.int64(4) + i_5)) // vocab_size, (blockIdx_y % batch_size * vocab_size + (T.shift_left(T.int64(2), i_0 + T.Cast("int64", T.ceil(T.log2(T.float64(128.0))))) * (blockIdx_y // batch_size) + blockIdx_x * T.int64(4096) + threadIdx_x * T.int64(4) + i_5)) % vocab_size] = value_buf[(blockIdx_y % batch_size * vocab_size + i_1[T.int64(0)]) // vocab_size, (blockIdx_y % batch_size * vocab_size + i_1[T.int64(0)]) % vocab_size]
                                                out_swap_buf[(blockIdx_y % batch_size * vocab_size + (T.shift_left(T.int64(2), i_0 + T.Cast("int64", T.ceil(T.log2(T.float64(128.0))))) * (blockIdx_y // batch_size) + blockIdx_x * T.int64(4096) + threadIdx_x * T.int64(4) + i_5)) // vocab_size, (blockIdx_y % batch_size * vocab_size + (T.shift_left(T.int64(2), i_0 + T.Cast("int64", T.ceil(T.log2(T.float64(128.0))))) * (blockIdx_y // batch_size) + blockIdx_x * T.int64(4096) + threadIdx_x * T.int64(4) + i_5)) % vocab_size] = out_buf[(blockIdx_y % batch_size * vocab_size + i_1[T.int64(0)]) // vocab_size, (blockIdx_y % batch_size * vocab_size + i_1[T.int64(0)]) % vocab_size]
                                                i_1[T.int64(0)] = i_1[T.int64(0)] + T.int64(1)
                                            else:
                                                value_swap_buf[(blockIdx_y % batch_size * vocab_size + (T.shift_left(T.int64(2), i_0 + T.Cast("int64", T.ceil(T.log2(T.float64(128.0))))) * (blockIdx_y // batch_size) + blockIdx_x * T.int64(4096) + threadIdx_x * T.int64(4) + i_5)) // vocab_size, (blockIdx_y % batch_size * vocab_size + (T.shift_left(T.int64(2), i_0 + T.Cast("int64", T.ceil(T.log2(T.float64(128.0))))) * (blockIdx_y // batch_size) + blockIdx_x * T.int64(4096) + threadIdx_x * T.int64(4) + i_5)) % vocab_size] = value_buf[(blockIdx_y % batch_size * vocab_size + j_1[T.int64(0)]) // vocab_size, (blockIdx_y % batch_size * vocab_size + j_1[T.int64(0)]) % vocab_size]
                                                out_swap_buf[(blockIdx_y % batch_size * vocab_size + (T.shift_left(T.int64(2), i_0 + T.Cast("int64", T.ceil(T.log2(T.float64(128.0))))) * (blockIdx_y // batch_size) + blockIdx_x * T.int64(4096) + threadIdx_x * T.int64(4) + i_5)) // vocab_size, (blockIdx_y % batch_size * vocab_size + (T.shift_left(T.int64(2), i_0 + T.Cast("int64", T.ceil(T.log2(T.float64(128.0))))) * (blockIdx_y // batch_size) + blockIdx_x * T.int64(4096) + threadIdx_x * T.int64(4) + i_5)) % vocab_size] = out_buf[(blockIdx_y % batch_size * vocab_size + j_1[T.int64(0)]) // vocab_size, (blockIdx_y % batch_size * vocab_size + j_1[T.int64(0)]) % vocab_size]
                                                j_1[T.int64(0)] = j_1[T.int64(0)] + T.int64(1)
                                else:
                                    first_2 = T.allocate([T.int64(1)], "int64", "local")
                                    mid_1 = T.allocate([T.int64(1)], "int64", "local")
                                    last_2 = T.allocate([T.int64(1)], "int64", "local")
                                    first_3 = T.Buffer((1,), "int64", data=first_2, scope="local")
                                    first_3[T.int64(0)] = T.max(T.int64(0), threadIdx_x * T.int64(4) - T.min(T.min(T.shift_left(T.int64(2), i_0 + T.Cast("int64", T.ceil(T.log2(T.float64(128.0))))) * (blockIdx_y // batch_size) + T.shift_left(T.int64(2), i_0 + T.Cast("int64", T.ceil(T.log2(T.float64(128.0))))), vocab_size) - (T.min(T.shift_left(T.int64(2), i_0 + T.Cast("int64", T.ceil(T.log2(T.float64(128.0))))) * (blockIdx_y // batch_size) + T.shift_left(T.int64(2), i_0 + T.Cast("int64", T.ceil(T.log2(T.float64(128.0))))) // T.int64(2), vocab_size) + blockIdx_x * T.int64(4096) - last_1[T.int64(0)]), T.int64(4096)))
                                    last_3 = T.Buffer((1,), "int64", data=last_2, scope="local")
                                    last_3[T.int64(0)] = T.min(threadIdx_x * T.int64(4), T.min(T.min(T.shift_left(T.int64(2), i_0 + T.Cast("int64", T.ceil(T.log2(T.float64(128.0))))) * (blockIdx_y // batch_size) + T.shift_left(T.int64(2), i_0 + T.Cast("int64", T.ceil(T.log2(T.float64(128.0))))) // T.int64(2), vocab_size) - (T.shift_left(T.int64(2), i_0 + T.Cast("int64", T.ceil(T.log2(T.float64(128.0))))) * (blockIdx_y // batch_size) + first_1[T.int64(0)]), T.int64(4096)))
                                    while first_3[T.int64(0)] < last_3[T.int64(0)]:
                                        if value_swap_buf[(blockIdx_y % batch_size * vocab_size + (T.min(T.shift_left(T.int64(2), i_0 + T.Cast("int64", T.ceil(T.log2(T.float64(128.0))))) * (blockIdx_y // batch_size) + T.shift_left(T.int64(2), i_0 + T.Cast("int64", T.ceil(T.log2(T.float64(128.0))))) // T.int64(2), vocab_size) + blockIdx_x * T.int64(4096) - last_1[T.int64(0)] + threadIdx_x * T.int64(4) - T.int64(1) - T.shift_right(first_3[T.int64(0)] + last_3[T.int64(0)], T.int64(1)))) // vocab_size, (blockIdx_y % batch_size * vocab_size + (T.min(T.shift_left(T.int64(2), i_0 + T.Cast("int64", T.ceil(T.log2(T.float64(128.0))))) * (blockIdx_y // batch_size) + T.shift_left(T.int64(2), i_0 + T.Cast("int64", T.ceil(T.log2(T.float64(128.0))))) // T.int64(2), vocab_size) + blockIdx_x * T.int64(4096) - last_1[T.int64(0)] + threadIdx_x * T.int64(4) - T.int64(1) - T.shift_right(first_3[T.int64(0)] + last_3[T.int64(0)], T.int64(1)))) % vocab_size] <= value_swap_buf[(blockIdx_y % batch_size * vocab_size + (T.shift_left(T.int64(2), i_0 + T.Cast("int64", T.ceil(T.log2(T.float64(128.0))))) * (blockIdx_y // batch_size) + first_1[T.int64(0)] + T.shift_right(first_3[T.int64(0)] + last_3[T.int64(0)], T.int64(1)))) // vocab_size, (blockIdx_y % batch_size * vocab_size + (T.shift_left(T.int64(2), i_0 + T.Cast("int64", T.ceil(T.log2(T.float64(128.0))))) * (blockIdx_y // batch_size) + first_1[T.int64(0)] + T.shift_right(first_3[T.int64(0)] + last_3[T.int64(0)], T.int64(1)))) % vocab_size]:
                                            first_3[T.int64(0)] = T.shift_right(first_3[T.int64(0)] + last_3[T.int64(0)], T.int64(1)) + T.int64(1)
                                        else:
                                            last_3[T.int64(0)] = T.shift_right(first_3[T.int64(0)] + last_3[T.int64(0)], T.int64(1))
                                    i = T.allocate([T.int64(1)], "int64", "local")
                                    j = T.allocate([T.int64(1)], "int64", "local")
                                    i_1 = T.Buffer((1,), "int64", data=i, scope="local")
                                    i_1[T.int64(0)] = T.shift_left(T.int64(2), i_0 + T.Cast("int64", T.ceil(T.log2(T.float64(128.0))))) * (blockIdx_y // batch_size) + first_1[T.int64(0)] + first_3[T.int64(0)]
                                    j_1 = T.Buffer((1,), "int64", data=j, scope="local")
                                    j_1[T.int64(0)] = T.min(T.shift_left(T.int64(2), i_0 + T.Cast("int64", T.ceil(T.log2(T.float64(128.0))))) * (blockIdx_y // batch_size) + T.shift_left(T.int64(2), i_0 + T.Cast("int64", T.ceil(T.log2(T.float64(128.0))))) // T.int64(2), vocab_size) + blockIdx_x * T.int64(4096) - last_1[T.int64(0)] + threadIdx_x * T.int64(4) - last_3[T.int64(0)]
                                    for i_6 in range(T.min(T.min(T.min(T.shift_left(T.int64(2), i_0 + T.Cast("int64", T.ceil(T.log2(T.float64(128.0))))) * (blockIdx_y // batch_size) + T.shift_left(T.int64(2), i_0 + T.Cast("int64", T.ceil(T.log2(T.float64(128.0))))) // T.int64(2), vocab_size) - (T.shift_left(T.int64(2), i_0 + T.Cast("int64", T.ceil(T.log2(T.float64(128.0))))) * (blockIdx_y // batch_size) + first_1[T.int64(0)]), T.int64(4096)) + T.min(T.min(T.shift_left(T.int64(2), i_0 + T.Cast("int64", T.ceil(T.log2(T.float64(128.0))))) * (blockIdx_y // batch_size) + T.shift_left(T.int64(2), i_0 + T.Cast("int64", T.ceil(T.log2(T.float64(128.0))))), vocab_size) - (T.min(T.shift_left(T.int64(2), i_0 + T.Cast("int64", T.ceil(T.log2(T.float64(128.0))))) * (blockIdx_y // batch_size) + T.shift_left(T.int64(2), i_0 + T.Cast("int64", T.ceil(T.log2(T.float64(128.0))))) // T.int64(2), vocab_size) + blockIdx_x * T.int64(4096) - last_1[T.int64(0)]), T.int64(4096)) - threadIdx_x * T.int64(4), T.int64(4))):
                                        if i_1[T.int64(0)] < T.shift_left(T.int64(2), i_0 + T.Cast("int64", T.ceil(T.log2(T.float64(128.0))))) * (blockIdx_y // batch_size) + first_1[T.int64(0)] + T.min(T.min(T.shift_left(T.int64(2), i_0 + T.Cast("int64", T.ceil(T.log2(T.float64(128.0))))) * (blockIdx_y // batch_size) + T.shift_left(T.int64(2), i_0 + T.Cast("int64", T.ceil(T.log2(T.float64(128.0))))) // T.int64(2), vocab_size) - (T.shift_left(T.int64(2), i_0 + T.Cast("int64", T.ceil(T.log2(T.float64(128.0))))) * (blockIdx_y // batch_size) + first_1[T.int64(0)]), T.int64(4096)) and j_1[T.int64(0)] < T.min(T.shift_left(T.int64(2), i_0 + T.Cast("int64", T.ceil(T.log2(T.float64(128.0))))) * (blockIdx_y // batch_size) + T.shift_left(T.int64(2), i_0 + T.Cast("int64", T.ceil(T.log2(T.float64(128.0))))) // T.int64(2), vocab_size) + blockIdx_x * T.int64(4096) - last_1[T.int64(0)] + T.min(T.min(T.shift_left(T.int64(2), i_0 + T.Cast("int64", T.ceil(T.log2(T.float64(128.0))))) * (blockIdx_y // batch_size) + T.shift_left(T.int64(2), i_0 + T.Cast("int64", T.ceil(T.log2(T.float64(128.0))))), vocab_size) - (T.min(T.shift_left(T.int64(2), i_0 + T.Cast("int64", T.ceil(T.log2(T.float64(128.0))))) * (blockIdx_y // batch_size) + T.shift_left(T.int64(2), i_0 + T.Cast("int64", T.ceil(T.log2(T.float64(128.0))))) // T.int64(2), vocab_size) + blockIdx_x * T.int64(4096) - last_1[T.int64(0)]), T.int64(4096)):
                                            if value_swap_buf[(blockIdx_y % batch_size * vocab_size + j_1[T.int64(0)]) // vocab_size, (blockIdx_y % batch_size * vocab_size + j_1[T.int64(0)]) % vocab_size] <= value_swap_buf[(blockIdx_y % batch_size * vocab_size + i_1[T.int64(0)]) // vocab_size, (blockIdx_y % batch_size * vocab_size + i_1[T.int64(0)]) % vocab_size]:
                                                value_buf[(blockIdx_y % batch_size * vocab_size + (T.shift_left(T.int64(2), i_0 + T.Cast("int64", T.ceil(T.log2(T.float64(128.0))))) * (blockIdx_y // batch_size) + blockIdx_x * T.int64(4096) + threadIdx_x * T.int64(4) + i_6)) // vocab_size, (blockIdx_y % batch_size * vocab_size + (T.shift_left(T.int64(2), i_0 + T.Cast("int64", T.ceil(T.log2(T.float64(128.0))))) * (blockIdx_y // batch_size) + blockIdx_x * T.int64(4096) + threadIdx_x * T.int64(4) + i_6)) % vocab_size] = value_swap_buf[(blockIdx_y % batch_size * vocab_size + i_1[T.int64(0)]) // vocab_size, (blockIdx_y % batch_size * vocab_size + i_1[T.int64(0)]) % vocab_size]
                                                out_buf[(blockIdx_y % batch_size * vocab_size + (T.shift_left(T.int64(2), i_0 + T.Cast("int64", T.ceil(T.log2(T.float64(128.0))))) * (blockIdx_y // batch_size) + blockIdx_x * T.int64(4096) + threadIdx_x * T.int64(4) + i_6)) // vocab_size, (blockIdx_y % batch_size * vocab_size + (T.shift_left(T.int64(2), i_0 + T.Cast("int64", T.ceil(T.log2(T.float64(128.0))))) * (blockIdx_y // batch_size) + blockIdx_x * T.int64(4096) + threadIdx_x * T.int64(4) + i_6)) % vocab_size] = out_swap_buf[(blockIdx_y % batch_size * vocab_size + i_1[T.int64(0)]) // vocab_size, (blockIdx_y % batch_size * vocab_size + i_1[T.int64(0)]) % vocab_size]
                                                i_1[T.int64(0)] = i_1[T.int64(0)] + T.int64(1)
                                            else:
                                                value_buf[(blockIdx_y % batch_size * vocab_size + (T.shift_left(T.int64(2), i_0 + T.Cast("int64", T.ceil(T.log2(T.float64(128.0))))) * (blockIdx_y // batch_size) + blockIdx_x * T.int64(4096) + threadIdx_x * T.int64(4) + i_6)) // vocab_size, (blockIdx_y % batch_size * vocab_size + (T.shift_left(T.int64(2), i_0 + T.Cast("int64", T.ceil(T.log2(T.float64(128.0))))) * (blockIdx_y // batch_size) + blockIdx_x * T.int64(4096) + threadIdx_x * T.int64(4) + i_6)) % vocab_size] = value_swap_buf[(blockIdx_y % batch_size * vocab_size + j_1[T.int64(0)]) // vocab_size, (blockIdx_y % batch_size * vocab_size + j_1[T.int64(0)]) % vocab_size]
                                                out_buf[(blockIdx_y % batch_size * vocab_size + (T.shift_left(T.int64(2), i_0 + T.Cast("int64", T.ceil(T.log2(T.float64(128.0))))) * (blockIdx_y // batch_size) + blockIdx_x * T.int64(4096) + threadIdx_x * T.int64(4) + i_6)) // vocab_size, (blockIdx_y % batch_size * vocab_size + (T.shift_left(T.int64(2), i_0 + T.Cast("int64", T.ceil(T.log2(T.float64(128.0))))) * (blockIdx_y // batch_size) + blockIdx_x * T.int64(4096) + threadIdx_x * T.int64(4) + i_6)) % vocab_size] = out_swap_buf[(blockIdx_y % batch_size * vocab_size + j_1[T.int64(0)]) // vocab_size, (blockIdx_y % batch_size * vocab_size + j_1[T.int64(0)]) % vocab_size]
                                                j_1[T.int64(0)] = j_1[T.int64(0)] + T.int64(1)
                                        else:
                                            if i_1[T.int64(0)] < T.shift_left(T.int64(2), i_0 + T.Cast("int64", T.ceil(T.log2(T.float64(128.0))))) * (blockIdx_y // batch_size) + first_1[T.int64(0)] + T.min(T.min(T.shift_left(T.int64(2), i_0 + T.Cast("int64", T.ceil(T.log2(T.float64(128.0))))) * (blockIdx_y // batch_size) + T.shift_left(T.int64(2), i_0 + T.Cast("int64", T.ceil(T.log2(T.float64(128.0))))) // T.int64(2), vocab_size) - (T.shift_left(T.int64(2), i_0 + T.Cast("int64", T.ceil(T.log2(T.float64(128.0))))) * (blockIdx_y // batch_size) + first_1[T.int64(0)]), T.int64(4096)):
                                                value_buf[(blockIdx_y % batch_size * vocab_size + (T.shift_left(T.int64(2), i_0 + T.Cast("int64", T.ceil(T.log2(T.float64(128.0))))) * (blockIdx_y // batch_size) + blockIdx_x * T.int64(4096) + threadIdx_x * T.int64(4) + i_6)) // vocab_size, (blockIdx_y % batch_size * vocab_size + (T.shift_left(T.int64(2), i_0 + T.Cast("int64", T.ceil(T.log2(T.float64(128.0))))) * (blockIdx_y // batch_size) + blockIdx_x * T.int64(4096) + threadIdx_x * T.int64(4) + i_6)) % vocab_size] = value_swap_buf[(blockIdx_y % batch_size * vocab_size + i_1[T.int64(0)]) // vocab_size, (blockIdx_y % batch_size * vocab_size + i_1[T.int64(0)]) % vocab_size]
                                                out_buf[(blockIdx_y % batch_size * vocab_size + (T.shift_left(T.int64(2), i_0 + T.Cast("int64", T.ceil(T.log2(T.float64(128.0))))) * (blockIdx_y // batch_size) + blockIdx_x * T.int64(4096) + threadIdx_x * T.int64(4) + i_6)) // vocab_size, (blockIdx_y % batch_size * vocab_size + (T.shift_left(T.int64(2), i_0 + T.Cast("int64", T.ceil(T.log2(T.float64(128.0))))) * (blockIdx_y // batch_size) + blockIdx_x * T.int64(4096) + threadIdx_x * T.int64(4) + i_6)) % vocab_size] = out_swap_buf[(blockIdx_y % batch_size * vocab_size + i_1[T.int64(0)]) // vocab_size, (blockIdx_y % batch_size * vocab_size + i_1[T.int64(0)]) % vocab_size]
                                                i_1[T.int64(0)] = i_1[T.int64(0)] + T.int64(1)
                                            else:
                                                value_buf[(blockIdx_y % batch_size * vocab_size + (T.shift_left(T.int64(2), i_0 + T.Cast("int64", T.ceil(T.log2(T.float64(128.0))))) * (blockIdx_y // batch_size) + blockIdx_x * T.int64(4096) + threadIdx_x * T.int64(4) + i_6)) // vocab_size, (blockIdx_y % batch_size * vocab_size + (T.shift_left(T.int64(2), i_0 + T.Cast("int64", T.ceil(T.log2(T.float64(128.0))))) * (blockIdx_y // batch_size) + blockIdx_x * T.int64(4096) + threadIdx_x * T.int64(4) + i_6)) % vocab_size] = value_swap_buf[(blockIdx_y % batch_size * vocab_size + j_1[T.int64(0)]) // vocab_size, (blockIdx_y % batch_size * vocab_size + j_1[T.int64(0)]) % vocab_size]
                                                out_buf[(blockIdx_y % batch_size * vocab_size + (T.shift_left(T.int64(2), i_0 + T.Cast("int64", T.ceil(T.log2(T.float64(128.0))))) * (blockIdx_y // batch_size) + blockIdx_x * T.int64(4096) + threadIdx_x * T.int64(4) + i_6)) // vocab_size, (blockIdx_y % batch_size * vocab_size + (T.shift_left(T.int64(2), i_0 + T.Cast("int64", T.ceil(T.log2(T.float64(128.0))))) * (blockIdx_y // batch_size) + blockIdx_x * T.int64(4096) + threadIdx_x * T.int64(4) + i_6)) % vocab_size] = out_swap_buf[(blockIdx_y % batch_size * vocab_size + j_1[T.int64(0)]) // vocab_size, (blockIdx_y % batch_size * vocab_size + j_1[T.int64(0)]) % vocab_size]
                                                j_1[T.int64(0)] = j_1[T.int64(0)] + T.int64(1)
                if T.Cast("int64", T.ceil(T.log2(T.Cast("float64", vocab_size)))) > T.Cast("int64", T.ceil(T.log2(T.float64(128.0)))) and (T.Cast("int64", T.ceil(T.log2(T.Cast("float64", vocab_size)))) - T.Cast("int64", T.ceil(T.log2(T.float64(128.0))))) % T.int64(2) == T.int64(1):
                    threadIdx_x = T.launch_thread("threadIdx.x", T.int64(1024))
                    blockIdx_x = T.launch_thread("blockIdx.x", T.max(T.int64(1), (vocab_size + T.int64(1023)) // T.int64(1024)))
                    blockIdx_y = T.launch_thread("blockIdx.y", T.max(T.int64(1), batch_size))
                    if blockIdx_x * T.int64(1024) + threadIdx_x < vocab_size:
                        value_buf[(blockIdx_y * vocab_size + (blockIdx_x * T.int64(1024) + threadIdx_x)) // vocab_size, (blockIdx_y * vocab_size + (blockIdx_x * T.int64(1024) + threadIdx_x)) % vocab_size] = value_swap_buf[(blockIdx_y * vocab_size + (blockIdx_x * T.int64(1024) + threadIdx_x)) // vocab_size, (blockIdx_y * vocab_size + (blockIdx_x * T.int64(1024) + threadIdx_x)) % vocab_size]
                        out_buf[(blockIdx_y * vocab_size + (blockIdx_x * T.int64(1024) + threadIdx_x)) // vocab_size, (blockIdx_y * vocab_size + (blockIdx_x * T.int64(1024) + threadIdx_x)) % vocab_size] = out_swap_buf[(blockIdx_y * vocab_size + (blockIdx_x * T.int64(1024) + threadIdx_x)) // vocab_size, (blockIdx_y * vocab_size + (blockIdx_x * T.int64(1024) + threadIdx_x)) % vocab_size]

    @T.prim_func
    def batch_decode_paged_kv(_0: T.int32, Q_handle: T.handle, pages_handle: T.handle, page_table_indptr_handle: T.handle, page_table_values_handle: T.handle, var_length_info: T.handle, k_rope_pos_offset_handle: T.handle, q_rope_position_handle: T.handle, output_handle: T.handle, lse_handle: T.handle, rotary_mode: T.int32, rope_scale: T.float32, rope_theta: T.float32, attn_score_scaling_factor: T.float32):
        T.func_attr({"target": T.target({"arch": "sm_75", "host": {"keys": ["cpu"], "kind": "llvm", "mcpu": "skylake", "mtriple": "x86_64-conda-linux-gnu", "tag": ""}, "keys": ["cuda", "gpu"], "kind": "cuda", "libs": ["thrust"], "max_num_threads": 1024, "max_shared_memory_per_block": 49152, "max_threads_per_block": 1024, "tag": "", "thread_warp_size": 32}), "tir.is_scheduled": 1})
        B = T.int32(is_size_var=True)
        Q = T.match_buffer(Q_handle, (B, 32, 80), "float16")
        max_num_pages = T.int32(is_size_var=True)
        pages = T.match_buffer(pages_handle, (max_num_pages, 2, 32, 16, 80), "float16")
        page_table_indptr = T.match_buffer(page_table_indptr_handle, (B + 1,), "int32", offset_factor=1)
        nnz_pages = T.int32(is_size_var=True)
        page_table_values = T.match_buffer(page_table_values_handle, (nnz_pages,), "int32", offset_factor=1)
        length_info = T.match_buffer(var_length_info, (B,), "int32", offset_factor=1)
        k_rope_pos_offset = T.match_buffer(k_rope_pos_offset_handle, (B,), "int32", offset_factor=1)
        q_rope_position = T.match_buffer(q_rope_position_handle, (B,), "int32", offset_factor=1)
        output = T.match_buffer(output_handle, (B, 32, 80), "float16")
        lse = T.match_buffer(lse_handle, (B, 32))
        # with T.block("root"):
        sm_scale: T.float32 = T.float32(0.16129820911147805)
        for bx in T.thread_binding(B, thread="blockIdx.x"):
            for fused_by_bz in T.thread_binding(32, thread="blockIdx.y"):
                for ty in T.thread_binding(1, thread="threadIdx.y"):
                    for tx in T.thread_binding(20, thread="threadIdx.x"):
                        for tz in T.thread_binding(25, thread="threadIdx.z"):
                            with T.block("attn"):
                                T.reads(page_table_indptr[bx:bx + 2], length_info[bx], q_rope_position[bx], Q[bx, fused_by_bz // 32 + ty + fused_by_bz % 32, tx * 4 - 40:tx * 4 - 40 + 84])
                                T.writes(output[bx, fused_by_bz % 32 + fused_by_bz // 32 + ty, tx * 4:tx * 4 + 4], lse[bx, fused_by_bz % 32 + fused_by_bz // 32 + ty])
                                Q_local = T.alloc_buffer((4,), "float16", scope="local")
                                kv_chunk_len = T.alloc_buffer((1,), "int32", scope="local")
                                K_smem = T.alloc_buffer((50, 80), "float16", scope="shared")
                                V_smem = T.alloc_buffer((50, 80), "float16", scope="shared")
                                O_allreduce = T.alloc_buffer((25, 1, 80), scope="shared")
                                md_allreduce = T.alloc_buffer((25, 1, 2), scope="shared")
                                S_reduce_local = T.alloc_buffer((1,), scope="local")
                                t0 = T.alloc_buffer((1,), scope="local")
                                S_local = T.alloc_buffer((2,), scope="local")
                                QK_local = T.alloc_buffer((4,), scope="local")
                                V_local = T.alloc_buffer((4,), "float16", scope="local")
                                m_prev = T.alloc_buffer((1,), scope="local")
                                d_prev = T.alloc_buffer((1,), scope="local")
                                other_m = T.alloc_buffer((1,), scope="local")
                                other_d = T.alloc_buffer((1,), scope="local")
                                exp_mprev = T.alloc_buffer((1,), scope="local")
                                exp_otherm = T.alloc_buffer((1,), scope="local")
                                other_o = T.alloc_buffer((4,), scope="local")
                                st_m = T.alloc_buffer((1,), scope="local")
                                st_d = T.alloc_buffer((1,), scope="local")
                                O_local = T.alloc_buffer((4,), scope="local")
                                by: T.int32 = fused_by_bz % 32
                                bz: T.int32 = fused_by_bz // 32
                                batch_idx: T.int32 = bx
                                cur_page_indptr_begin: T.int32 = page_table_indptr[batch_idx]
                                cur_page_indptr_end: T.int32 = page_table_indptr[batch_idx + 1]
                                kv_chunk_len[0] = T.if_then_else(cur_page_indptr_begin != cur_page_indptr_end, (cur_page_indptr_end - cur_page_indptr_begin - 1) * 16 + length_info[batch_idx], 0)
                                st_m[0] = T.float32(-50000.0)
                                st_d[0] = T.float32(1.0)
                                for vec in T.vectorized(4):
                                    O_local[vec] = T.float32(0.0)
                                for vec in T.vectorized(4):
                                    freq = T.float32()
                                    Q_local[vec] = T.if_then_else(rotary_mode == 1, T.Let(T.Cast("float16", T.cos(freq) * T.Cast("float32", Q[bx, by + bz + ty, tx * 4 + vec]) + T.sin(freq) * T.Cast("float32", T.if_then_else(tx * 4 + vec < 40, Q[bx, by + bz + ty, tx * 4 + vec + 40] * T.float16(-1.0), Q[bx, by + bz + ty, tx * 4 + vec - 40]))), where={freq: T.Cast("float32", q_rope_position[batch_idx]) * rope_scale / T.pow(rope_theta, T.Cast("float32", (tx * 4 + vec) * 2 % 80) / T.float32(80.0))}), Q[bx, by + bz + ty, tx * 4 + vec])
                                for iterator in range((kv_chunk_len[0] + 49) // 50):
                                    tile_start_s: T.int32 = (tz + ty) * 2
                                    tile_start_g: T.int32 = (iterator * 25 + tz + ty) * 2
                                    for j in range(2):
                                        with T.block("KV_load"):
                                            T.reads()
                                            T.writes()
                                            row_g: T.int32 = tile_start_g + j
                                            if row_g < kv_chunk_len[0]:
                                                seq_offset: T.int32 = row_g
                                                page_no: T.int32 = page_table_values[cur_page_indptr_begin + seq_offset // 16]
                                                page_offset: T.int32 = seq_offset % 16
                                                for vec in T.vectorized(4):
                                                    freq = T.float32()
                                                    K_smem[tile_start_s + j, tx * 4 + vec] = T.if_then_else(rotary_mode == 1, T.Let(T.Cast("float16", T.cos(freq) * T.Cast("float32", pages[page_no, 0, by, page_offset, tx * 4 + vec]) + T.sin(freq) * T.Cast("float32", T.if_then_else(tx * 4 + vec < 40, pages[page_no, 0, by, page_offset, tx * 4 + vec + 40] * T.float16(-1.0), pages[page_no, 0, by, page_offset, tx * 4 + vec - 40]))), where={freq: T.Cast("float32", k_rope_pos_offset[batch_idx] + row_g) * rope_scale / T.pow(rope_theta, T.Cast("float32", (tx * 4 + vec) * 2 % 80) / T.float32(80.0))}), pages[page_no, 0, by, page_offset, tx * 4 + vec])
                                                    V_smem[tile_start_s + j, tx * 4 + vec] = pages[page_no, 1, by, page_offset, tx * 4 + vec]
                                            else:
                                                for vec in T.vectorized(4):
                                                    K_smem[tile_start_s + j, tx * 4 + vec] = T.float16(0.0)
                                                    V_smem[tile_start_s + j, tx * 4 + vec] = T.float16(0.0)
                                    T.tvm_storage_sync("shared")
                                    m_prev[0] = st_m[0]
                                    for j in range(2):
                                        for vec in T.vectorized(4):
                                            QK_local[vec] = T.Cast("float32", Q_local[vec]) * T.Cast("float32", K_smem[tz * 2 + j, tx * 4 + vec]) * attn_score_scaling_factor * sm_scale
                                        S_reduce_local[0] = T.float32(0.0)
                                        for vec in T.unroll(4):
                                            S_reduce_local[0] = S_reduce_local[0] + QK_local[vec]
                                        with T.block("block_cross_thread"):
                                            T.reads(S_reduce_local[0])
                                            T.writes(t0[0])
                                            T.attr(T.comm_reducer(lambda x0, y0: x0 + y0, [T.float32(0.0)]), "reduce_scope", T.reinterpret("handle", T.uint64(0)))
                                            T.tvm_thread_allreduce(T.uint32(1), S_reduce_local[0], T.bool(True), t0[0], tx)
                                        S_local[j] = T.float32(-50000.0)
                                        if (iterator * 25 + tz) * 2 + j < kv_chunk_len[0]:
                                            S_local[j] = t0[0]
                                        st_m[0] = T.max(st_m[0], S_local[j])
                                    o_scale: T.float32 = T.exp2(m_prev[0] - st_m[0])
                                    st_d[0] = st_d[0] * o_scale
                                    for j in range(2):
                                        S_local[j] = T.exp2(S_local[j] - st_m[0])
                                        st_d[0] = st_d[0] + S_local[j]
                                    for j in T.vectorized(4):
                                        O_local[j] = O_local[j] * o_scale
                                    for j in range(2):
                                        for vec in T.vectorized(4):
                                            V_local[vec] = V_smem[tz * 2 + j, tx * 4 + vec]
                                        for vec in T.vectorized(4):
                                            O_local[vec] = O_local[vec] + T.Cast("float32", V_local[vec]) * S_local[j]
                                for vec in T.vectorized(4):
                                    O_allreduce[tz, ty, tx * 4 + vec] = O_local[vec]
                                md_allreduce[tz, ty, 0] = st_m[0]
                                md_allreduce[tz, ty, 1] = st_d[0]
                                T.tvm_storage_sync("shared")
                                st_m[0] = T.float32(-50000.0)
                                st_d[0] = T.float32(1.0)
                                for vec in T.vectorized(4):
                                    O_local[vec] = T.float32(0.0)
                                for j in range(25):
                                    m_prev[0] = st_m[0]
                                    d_prev[0] = st_d[0]
                                    other_m[0] = md_allreduce[j, ty, 0]
                                    other_d[0] = md_allreduce[j, ty, 1]
                                    for vec in T.vectorized(4):
                                        other_o[vec] = O_allreduce[j, ty, tx * 4 + vec]
                                    st_m[0] = T.max(st_m[0], other_m[0])
                                    st_d[0] = d_prev[0] * T.exp2(m_prev[0] - st_m[0]) + other_d[0] * T.exp2(other_m[0] - st_m[0])
                                    exp_mprev[0] = T.exp2(m_prev[0] - st_m[0])
                                    exp_otherm[0] = T.exp2(other_m[0] - st_m[0])
                                    for vec in T.vectorized(4):
                                        O_local[vec] = O_local[vec] * exp_mprev[0] + other_o[vec] * exp_otherm[0]
                                for vec in T.vectorized(4):
                                    O_local[vec] = O_local[vec] / st_d[0]
                                for vec in T.vectorized(4):
                                    output[batch_idx, by + bz + ty, tx * 4 + vec] = T.Cast("float16", O_local[vec])
                                lse[batch_idx, by + bz + ty] = st_m[0] + T.log2(st_d[0])

    @T.prim_func
    def batch_decode_paged_kv_sliding_window(_0: T.int32, Q_handle: T.handle, pages_handle: T.handle, page_table_indptr_handle: T.handle, page_table_values_handle: T.handle, var_length_info: T.handle, k_rope_pos_offset_handle: T.handle, q_rope_position_handle: T.handle, output_handle: T.handle, lse_handle: T.handle, rotary_mode: T.int32, rope_scale: T.float32, rope_theta: T.float32, attn_score_scaling_factor: T.float32):
        T.func_attr({"target": T.target({"arch": "sm_75", "host": {"keys": ["cpu"], "kind": "llvm", "mcpu": "skylake", "mtriple": "x86_64-conda-linux-gnu", "tag": ""}, "keys": ["cuda", "gpu"], "kind": "cuda", "libs": ["thrust"], "max_num_threads": 1024, "max_shared_memory_per_block": 49152, "max_threads_per_block": 1024, "tag": "", "thread_warp_size": 32}), "tir.is_scheduled": 1})
        B = T.int32(is_size_var=True)
        Q = T.match_buffer(Q_handle, (B, 32, 80), "float16")
        max_num_pages = T.int32(is_size_var=True)
        pages = T.match_buffer(pages_handle, (max_num_pages, 2, 32, 16, 80), "float16")
        page_table_indptr = T.match_buffer(page_table_indptr_handle, (B + 1,), "int32", offset_factor=1)
        nnz_pages = T.int32(is_size_var=True)
        page_table_values = T.match_buffer(page_table_values_handle, (nnz_pages,), "int32", offset_factor=1)
        length_info = T.match_buffer(var_length_info, (3, B), "int32", offset_factor=1)
        k_rope_pos_offset = T.match_buffer(k_rope_pos_offset_handle, (B,), "int32", offset_factor=1)
        q_rope_position = T.match_buffer(q_rope_position_handle, (B,), "int32", offset_factor=1)
        output = T.match_buffer(output_handle, (B, 32, 80), "float16")
        lse = T.match_buffer(lse_handle, (B, 32))
        # with T.block("root"):
        sm_scale: T.float32 = T.float32(0.16129820911147805)
        for bx in T.thread_binding(B, thread="blockIdx.x"):
            for fused_by_bz in T.thread_binding(32, thread="blockIdx.y"):
                for ty in T.thread_binding(1, thread="threadIdx.y"):
                    for tx in T.thread_binding(20, thread="threadIdx.x"):
                        for tz in T.thread_binding(25, thread="threadIdx.z"):
                            with T.block("attn"):
                                T.reads(page_table_indptr[bx:bx + 2], length_info[0:3, bx], q_rope_position[bx], Q[bx, fused_by_bz // 32 + ty + fused_by_bz % 32, tx * 4 - 40:tx * 4 - 40 + 84])
                                T.writes(output[bx, fused_by_bz % 32 + fused_by_bz // 32 + ty, tx * 4:tx * 4 + 4], lse[bx, fused_by_bz % 32 + fused_by_bz // 32 + ty])
                                Q_local = T.alloc_buffer((4,), "float16", scope="local")
                                kv_chunk_len = T.alloc_buffer((1,), "int32", scope="local")
                                K_smem = T.alloc_buffer((50, 80), "float16", scope="shared")
                                V_smem = T.alloc_buffer((50, 80), "float16", scope="shared")
                                O_allreduce = T.alloc_buffer((25, 1, 80), scope="shared")
                                md_allreduce = T.alloc_buffer((25, 1, 2), scope="shared")
                                S_reduce_local = T.alloc_buffer((1,), scope="local")
                                t0 = T.alloc_buffer((1,), scope="local")
                                S_local = T.alloc_buffer((2,), scope="local")
                                QK_local = T.alloc_buffer((4,), scope="local")
                                V_local = T.alloc_buffer((4,), "float16", scope="local")
                                m_prev = T.alloc_buffer((1,), scope="local")
                                d_prev = T.alloc_buffer((1,), scope="local")
                                other_m = T.alloc_buffer((1,), scope="local")
                                other_d = T.alloc_buffer((1,), scope="local")
                                exp_mprev = T.alloc_buffer((1,), scope="local")
                                exp_otherm = T.alloc_buffer((1,), scope="local")
                                other_o = T.alloc_buffer((4,), scope="local")
                                st_m = T.alloc_buffer((1,), scope="local")
                                st_d = T.alloc_buffer((1,), scope="local")
                                O_local = T.alloc_buffer((4,), scope="local")
                                by: T.int32 = fused_by_bz % 32
                                bz: T.int32 = fused_by_bz // 32
                                batch_idx: T.int32 = bx
                                cur_page_indptr_begin: T.int32 = page_table_indptr[batch_idx]
                                cur_page_indptr_end: T.int32 = page_table_indptr[batch_idx + 1]
                                kv_chunk_len[0] = T.if_then_else(cur_page_indptr_begin != cur_page_indptr_end, (cur_page_indptr_end - cur_page_indptr_begin - 1) * 16 + length_info[0, batch_idx] - length_info[1, batch_idx] + length_info[2, batch_idx], 0)
                                st_m[0] = T.float32(-50000.0)
                                st_d[0] = T.float32(1.0)
                                for vec in T.vectorized(4):
                                    O_local[vec] = T.float32(0.0)
                                for vec in T.vectorized(4):
                                    freq = T.float32()
                                    Q_local[vec] = T.if_then_else(rotary_mode == 1, T.Let(T.Cast("float16", T.cos(freq) * T.Cast("float32", Q[bx, by + bz + ty, tx * 4 + vec]) + T.sin(freq) * T.Cast("float32", T.if_then_else(tx * 4 + vec < 40, Q[bx, by + bz + ty, tx * 4 + vec + 40] * T.float16(-1.0), Q[bx, by + bz + ty, tx * 4 + vec - 40]))), where={freq: T.Cast("float32", q_rope_position[batch_idx]) * rope_scale / T.pow(rope_theta, T.Cast("float32", (tx * 4 + vec) * 2 % 80) / T.float32(80.0))}), Q[bx, by + bz + ty, tx * 4 + vec])
                                for iterator in range((kv_chunk_len[0] + 49) // 50):
                                    tile_start_s: T.int32 = (tz + ty) * 2
                                    tile_start_g: T.int32 = (iterator * 25 + tz + ty) * 2
                                    for j in range(2):
                                        with T.block("KV_load"):
                                            T.reads()
                                            T.writes()
                                            row_g: T.int32 = tile_start_g + j
                                            if row_g < kv_chunk_len[0]:
                                                seq_offset: T.int32 = T.if_then_else(row_g < length_info[2, batch_idx], row_g, row_g - length_info[2, batch_idx] + length_info[1, batch_idx])
                                                page_no: T.int32 = page_table_values[cur_page_indptr_begin + seq_offset // 16]
                                                page_offset: T.int32 = seq_offset % 16
                                                for vec in T.vectorized(4):
                                                    freq = T.float32()
                                                    K_smem[tile_start_s + j, tx * 4 + vec] = T.if_then_else(rotary_mode == 1, T.Let(T.Cast("float16", T.cos(freq) * T.Cast("float32", pages[page_no, 0, by, page_offset, tx * 4 + vec]) + T.sin(freq) * T.Cast("float32", T.if_then_else(tx * 4 + vec < 40, pages[page_no, 0, by, page_offset, tx * 4 + vec + 40] * T.float16(-1.0), pages[page_no, 0, by, page_offset, tx * 4 + vec - 40]))), where={freq: T.Cast("float32", k_rope_pos_offset[batch_idx] + row_g) * rope_scale / T.pow(rope_theta, T.Cast("float32", (tx * 4 + vec) * 2 % 80) / T.float32(80.0))}), pages[page_no, 0, by, page_offset, tx * 4 + vec])
                                                    V_smem[tile_start_s + j, tx * 4 + vec] = pages[page_no, 1, by, page_offset, tx * 4 + vec]
                                            else:
                                                for vec in T.vectorized(4):
                                                    K_smem[tile_start_s + j, tx * 4 + vec] = T.float16(0.0)
                                                    V_smem[tile_start_s + j, tx * 4 + vec] = T.float16(0.0)
                                    T.tvm_storage_sync("shared")
                                    m_prev[0] = st_m[0]
                                    for j in range(2):
                                        for vec in T.vectorized(4):
                                            QK_local[vec] = T.Cast("float32", Q_local[vec]) * T.Cast("float32", K_smem[tz * 2 + j, tx * 4 + vec]) * attn_score_scaling_factor * sm_scale
                                        S_reduce_local[0] = T.float32(0.0)
                                        for vec in T.unroll(4):
                                            S_reduce_local[0] = S_reduce_local[0] + QK_local[vec]
                                        with T.block("block_cross_thread"):
                                            T.reads(S_reduce_local[0])
                                            T.writes(t0[0])
                                            T.attr(T.comm_reducer(lambda x0, y0: x0 + y0, [T.float32(0.0)]), "reduce_scope", T.reinterpret("handle", T.uint64(0)))
                                            T.tvm_thread_allreduce(T.uint32(1), S_reduce_local[0], T.bool(True), t0[0], tx)
                                        S_local[j] = T.float32(-50000.0)
                                        if (iterator * 25 + tz) * 2 + j < kv_chunk_len[0]:
                                            S_local[j] = t0[0]
                                        st_m[0] = T.max(st_m[0], S_local[j])
                                    o_scale: T.float32 = T.exp2(m_prev[0] - st_m[0])
                                    st_d[0] = st_d[0] * o_scale
                                    for j in range(2):
                                        S_local[j] = T.exp2(S_local[j] - st_m[0])
                                        st_d[0] = st_d[0] + S_local[j]
                                    for j in T.vectorized(4):
                                        O_local[j] = O_local[j] * o_scale
                                    for j in range(2):
                                        for vec in T.vectorized(4):
                                            V_local[vec] = V_smem[tz * 2 + j, tx * 4 + vec]
                                        for vec in T.vectorized(4):
                                            O_local[vec] = O_local[vec] + T.Cast("float32", V_local[vec]) * S_local[j]
                                for vec in T.vectorized(4):
                                    O_allreduce[tz, ty, tx * 4 + vec] = O_local[vec]
                                md_allreduce[tz, ty, 0] = st_m[0]
                                md_allreduce[tz, ty, 1] = st_d[0]
                                T.tvm_storage_sync("shared")
                                st_m[0] = T.float32(-50000.0)
                                st_d[0] = T.float32(1.0)
                                for vec in T.vectorized(4):
                                    O_local[vec] = T.float32(0.0)
                                for j in range(25):
                                    m_prev[0] = st_m[0]
                                    d_prev[0] = st_d[0]
                                    other_m[0] = md_allreduce[j, ty, 0]
                                    other_d[0] = md_allreduce[j, ty, 1]
                                    for vec in T.vectorized(4):
                                        other_o[vec] = O_allreduce[j, ty, tx * 4 + vec]
                                    st_m[0] = T.max(st_m[0], other_m[0])
                                    st_d[0] = d_prev[0] * T.exp2(m_prev[0] - st_m[0]) + other_d[0] * T.exp2(other_m[0] - st_m[0])
                                    exp_mprev[0] = T.exp2(m_prev[0] - st_m[0])
                                    exp_otherm[0] = T.exp2(other_m[0] - st_m[0])
                                    for vec in T.vectorized(4):
                                        O_local[vec] = O_local[vec] * exp_mprev[0] + other_o[vec] * exp_otherm[0]
                                for vec in T.vectorized(4):
                                    O_local[vec] = O_local[vec] / st_d[0]
                                for vec in T.vectorized(4):
                                    output[batch_idx, by + bz + ty, tx * 4 + vec] = T.Cast("float16", O_local[vec])
                                lse[batch_idx, by + bz + ty] = st_m[0] + T.log2(st_d[0])

    @T.prim_func
    def batch_prefill_paged_kv(_0: T.int32, var_q: T.handle, var_q_indptr: T.handle, var_pages: T.handle, var_page_indptr: T.handle, var_page_values: T.handle, var_length_info: T.handle, var_k_rope_pos_offset: T.handle, var_q_rope_position: T.handle, var_output: T.handle, var_lse: T.handle, causal: T.int32, rotary_mode: T.int32, rope_scale: T.float32, rope_theta: T.float32, attn_score_scaling_factor: T.float32):
        T.func_attr({"target": T.target({"arch": "sm_75", "host": {"keys": ["cpu"], "kind": "llvm", "mcpu": "skylake", "mtriple": "x86_64-conda-linux-gnu", "tag": ""}, "keys": ["cuda", "gpu"], "kind": "cuda", "libs": ["thrust"], "max_num_threads": 1024, "max_shared_memory_per_block": 49152, "max_threads_per_block": 1024, "tag": "", "thread_warp_size": 32}), "tir.is_scheduled": 1})
        total_len = T.int32(is_size_var=True)
        q = T.match_buffer(var_q, (total_len, 32, 80), "float16")
        batch_size = T.int32(is_size_var=True)
        q_indptr = T.match_buffer(var_q_indptr, (batch_size + 1,), "int32", offset_factor=1)
        max_num_pages = T.int32(is_size_var=True)
        pages = T.match_buffer(var_pages, (max_num_pages, 2, 32, 16, 80), "float16")
        page_indptr = T.match_buffer(var_page_indptr, (batch_size + 1,), "int32", offset_factor=1)
        nnz_pages = T.int32(is_size_var=True)
        page_values = T.match_buffer(var_page_values, (nnz_pages,), "int32", offset_factor=1)
        length_info = T.match_buffer(var_length_info, (batch_size,), "int32", offset_factor=1)
        k_rope_pos_offset = T.match_buffer(var_k_rope_pos_offset, (batch_size,), "int32", offset_factor=1)
        q_rope_position = T.match_buffer(var_q_rope_position, (total_len,), "int32", offset_factor=1)
        output = T.match_buffer(var_output, (total_len, 32, 80), "float16")
        lse = T.match_buffer(var_lse, (total_len, 32))
        # with T.block("root"):
        for lbx in T.thread_binding(16, thread="blockIdx.x"):
            for lby in T.thread_binding(32, thread="blockIdx.y"):
                for lty in T.thread_binding(4, thread="threadIdx.y"):
                    for ltx in T.thread_binding(32, thread="threadIdx.x"):
                        with T.block("attn"):
                            bx, by, ty, tx = T.axis.remap("SSSS", [lbx, lby, lty, ltx])
                            T.reads()
                            T.writes()
                            tile_id = T.alloc_buffer((1,), "int32", scope="local")
                            batch_idx = T.alloc_buffer((1,), "int32", scope="local")
                            batch_tiles = T.alloc_buffer((1,), "int32", scope="local")
                            batch_rows = T.alloc_buffer((1,), "int32", scope="local")
                            iterator = T.alloc_buffer((1,), "int32", scope="local")
                            kv_chunk_len = T.alloc_buffer((1,), "int32", scope="local")
                            Q_smem = T.alloc_buffer((32, 80), "float16", scope="shared")
                            K_smem = T.alloc_buffer((16, 80), "float16", scope="shared")
                            V_smem = T.alloc_buffer((16, 80), "float16", scope="shared")
                            S_smem = T.alloc_buffer((32, 16), scope="shared")
                            S_local = T.alloc_buffer((32, 16), scope="local")
                            O_local = T.alloc_buffer((32, 80), scope="local")
                            m_smem = T.alloc_buffer((32,), scope="shared")
                            m_prev_smem = T.alloc_buffer((32,), scope="shared")
                            d_smem = T.alloc_buffer((32,), scope="shared")
                            m_new = T.alloc_buffer((1,), scope="local")
                            m_prev = T.alloc_buffer((1,), scope="local")
                            d_new = T.alloc_buffer((1,), scope="local")
                            tile_id[0] = bx
                            batch_idx[0] = 0
                            batch_rows[0] = q_indptr[1] - q_indptr[0]
                            batch_tiles[0] = (batch_rows[0] + 32 - 1) // 32
                            while T.tvm_thread_invariant(batch_idx[0] < batch_size):
                                while tile_id[0] >= batch_tiles[0] and batch_idx[0] < batch_size:
                                    tile_id[0] = tile_id[0] - batch_tiles[0]
                                    batch_idx[0] = batch_idx[0] + 1
                                    if batch_idx[0] < batch_size:
                                        b_idx: T.int32 = batch_idx[0]
                                        batch_rows[0] = q_indptr[b_idx + 1] - q_indptr[b_idx]
                                        batch_tiles[0] = (batch_rows[0] + 32 - 1) // 32
                                if T.tvm_thread_invariant(batch_idx[0] < batch_size):
                                    b_idx: T.int32 = batch_idx[0]
                                    LH_start: T.int32 = tile_id[0] * 32
                                    q_indptr_val: T.int32 = q_indptr[b_idx]
                                    cur_page_indptr_begin: T.int32 = page_indptr[b_idx]
                                    cur_page_indptr_end: T.int32 = page_indptr[b_idx + 1]
                                    kv_chunk_len[0] = T.if_then_else(cur_page_indptr_begin != cur_page_indptr_end, (cur_page_indptr_end - cur_page_indptr_begin - 1) * 16 + length_info[b_idx], 0)
                                    T.tvm_storage_sync("shared")
                                    for i in range(1):
                                        row: T.int32 = i * 32 * 4 + ty * 32 + tx
                                        if row < 32:
                                            m_smem[row] = T.float32(-50000.0)
                                            d_smem[row] = T.float32(1.0)
                                    for li_0_lj_0_fused_0 in T.thread_binding(4, thread="threadIdx.y"):
                                        for li_0_lj_0_fused_1 in T.thread_binding(32, thread="threadIdx.x"):
                                            for li_1, lj_1 in T.grid(4, 5):
                                                with T.block("O_init"):
                                                    i = T.axis.spatial(32, (li_0_lj_0_fused_0 * 32 + li_0_lj_0_fused_1) // 16 * 4 + li_1)
                                                    j = T.axis.spatial(80, (li_0_lj_0_fused_0 * 32 + li_0_lj_0_fused_1) % 16 * 5 + lj_1)
                                                    T.reads()
                                                    T.writes(O_local[i, j])
                                                    O_local[i, j] = T.float32(0.0)
                                    T.tvm_storage_sync("shared")
                                    for li_lj_fused_0 in range(5):
                                        for li_lj_fused_1 in T.thread_binding(4, thread="threadIdx.y"):
                                            for li_lj_fused_2 in T.thread_binding(32, thread="threadIdx.x"):
                                                for li_lj_fused_3 in T.vectorized(4):
                                                    with T.block("Q_load"):
                                                        i = T.axis.spatial(32, (li_lj_fused_0 * 512 + li_lj_fused_1 * 128 + li_lj_fused_2 * 4 + li_lj_fused_3) // 80)
                                                        j = T.axis.spatial(80, (li_lj_fused_0 * 512 + li_lj_fused_1 * 128 + li_lj_fused_2 * 4 + li_lj_fused_3) % 80)
                                                        T.reads()
                                                        T.writes()
                                                        cur_L: T.int32 = q_indptr_val + (LH_start + i)
                                                        cur_H_qo: T.int32 = by
                                                        if cur_L < q_indptr[b_idx + 1]:
                                                            freq = T.float32()
                                                            Q_smem[i, j] = T.if_then_else(rotary_mode == 1, T.Let(T.Cast("float16", T.cos(freq) * T.Cast("float32", q[cur_L, cur_H_qo, j]) + T.sin(freq) * T.Cast("float32", T.if_then_else(j < 40, q[cur_L, cur_H_qo, j + 40] * T.float16(-1.0), q[cur_L, cur_H_qo, j - 40]))), where={freq: T.Cast("float32", q_rope_position[cur_L]) * rope_scale / T.pow(rope_theta, T.Cast("float32", j * 2 % 80) / T.float32(80.0))}), q[cur_L, cur_H_qo, j])
                                                        else:
                                                            Q_smem[i, j] = T.float16(0.0)
                                    T.tvm_storage_sync("shared")
                                    for iterator_1 in range((kv_chunk_len[0] + 15) // 16):
                                        L_kv_start: T.int32 = iterator_1 * 16
                                        for lz_ly_fused_0 in range(3):
                                            for lz_ly_fused_1 in T.thread_binding(4, thread="threadIdx.y"):
                                                for lz_ly_fused_2 in T.thread_binding(32, thread="threadIdx.x"):
                                                    for lz_ly_fused_3 in T.vectorized(4):
                                                        with T.block("K_load"):
                                                            i = T.axis.spatial(16, (lz_ly_fused_0 * 512 + lz_ly_fused_1 * 128 + lz_ly_fused_2 * 4 + lz_ly_fused_3) // 80)
                                                            j = T.axis.spatial(80, (lz_ly_fused_0 * 512 + lz_ly_fused_1 * 128 + lz_ly_fused_2 * 4 + lz_ly_fused_3) % 80)
                                                            T.where(((lz_ly_fused_0 * 4 + lz_ly_fused_1) * 32 + lz_ly_fused_2) * 4 + lz_ly_fused_3 < 1280)
                                                            T.reads()
                                                            T.writes()
                                                            cur_L: T.int32 = L_kv_start + i
                                                            if cur_L < kv_chunk_len[0]:
                                                                seq_offset: T.int32 = cur_L
                                                                page_no: T.int32 = page_values[cur_page_indptr_begin + seq_offset // 16]
                                                                page_offset: T.int32 = seq_offset % 16
                                                                freq = T.float32()
                                                                K_smem[i, j] = T.if_then_else(rotary_mode == 1, T.Let(T.Cast("float16", T.cos(freq) * T.Cast("float32", pages[page_no, 0, by, page_offset, j]) + T.sin(freq) * T.Cast("float32", T.if_then_else(j < 40, pages[page_no, 0, by, page_offset, j + 40] * T.float16(-1.0), pages[page_no, 0, by, page_offset, j - 40]))), where={freq: T.Cast("float32", k_rope_pos_offset[b_idx] + cur_L) * rope_scale / T.pow(rope_theta, T.Cast("float32", j * 2 % 80) / T.float32(80.0))}), pages[page_no, 0, by, page_offset, j])
                                                            else:
                                                                K_smem[i, j] = T.float16(0.0)
                                        T.tvm_storage_sync("shared")
                                        for lz_ly_fused_0 in range(3):
                                            for lz_ly_fused_1 in T.thread_binding(4, thread="threadIdx.y"):
                                                for lz_ly_fused_2 in T.thread_binding(32, thread="threadIdx.x"):
                                                    for lz_ly_fused_3 in T.vectorized(4):
                                                        with T.block("V_load"):
                                                            i = T.axis.spatial(16, (lz_ly_fused_0 * 512 + lz_ly_fused_1 * 128 + lz_ly_fused_2 * 4 + lz_ly_fused_3) // 80)
                                                            j = T.axis.spatial(80, (lz_ly_fused_0 * 512 + lz_ly_fused_1 * 128 + lz_ly_fused_2 * 4 + lz_ly_fused_3) % 80)
                                                            T.where(((lz_ly_fused_0 * 4 + lz_ly_fused_1) * 32 + lz_ly_fused_2) * 4 + lz_ly_fused_3 < 1280)
                                                            T.reads()
                                                            T.writes()
                                                            cur_L: T.int32 = L_kv_start + i
                                                            if cur_L < kv_chunk_len[0]:
                                                                seq_offset: T.int32 = cur_L
                                                                page_no: T.int32 = page_values[cur_page_indptr_begin + seq_offset // 16]
                                                                page_offset: T.int32 = seq_offset % 16
                                                                V_smem[i, j] = pages[page_no, 1, by, page_offset, j]
                                                            else:
                                                                V_smem[i, j] = T.float16(0.0)
                                        T.tvm_storage_sync("shared")
                                        with T.block(""):
                                            T.reads(Q_smem[0:32, 0:80], K_smem[0:16, 0:80])
                                            T.writes(S_local[0:32, 0:16])
                                            for li_0_lj_0_fused_0_init in T.thread_binding(4, thread="threadIdx.y"):
                                                for li_0_lj_0_fused_1_init in T.thread_binding(32, thread="threadIdx.x"):
                                                    for li_1_init, lj_1_init in T.grid(2, 2):
                                                        with T.block("S_gemm_init"):
                                                            i = T.axis.spatial(32, (li_0_lj_0_fused_0_init * 32 + li_0_lj_0_fused_1_init) // 8 * 2 + li_1_init)
                                                            j = T.axis.spatial(16, (li_0_lj_0_fused_0_init * 32 + li_0_lj_0_fused_1_init) % 8 * 2 + lj_1_init)
                                                            T.reads()
                                                            T.writes(S_local[i, j])
                                                            S_local[i, j] = T.float32(0.0)
                                            for li_0_lj_0_fused_0 in T.thread_binding(4, thread="threadIdx.y"):
                                                for li_0_lj_0_fused_1 in T.thread_binding(32, thread="threadIdx.x"):
                                                    for lk_0, li_1, lj_1, lk_1 in T.grid(10, 2, 2, 8):
                                                        with T.block("S_gemm_update"):
                                                            i = T.axis.spatial(32, (li_0_lj_0_fused_0 * 32 + li_0_lj_0_fused_1) // 8 * 2 + li_1)
                                                            j = T.axis.spatial(16, (li_0_lj_0_fused_0 * 32 + li_0_lj_0_fused_1) % 8 * 2 + lj_1)
                                                            k = T.axis.reduce(80, lk_0 * 8 + lk_1)
                                                            T.reads(S_local[i, j], Q_smem[i, k], K_smem[j, k])
                                                            T.writes(S_local[i, j])
                                                            S_local[i, j] = S_local[i, j] + T.Cast("float32", Q_smem[i, k]) * T.Cast("float32", K_smem[j, k]) * attn_score_scaling_factor * T.float32(0.16129820911147805)
                                        T.tvm_storage_sync("shared")
                                        for li_0_lj_0_fused_0 in T.thread_binding(4, thread="threadIdx.y"):
                                            for li_0_lj_0_fused_1 in T.thread_binding(32, thread="threadIdx.x"):
                                                for li_1, lj_1 in T.grid(2, 2):
                                                    with T.block("S_store"):
                                                        i = T.axis.spatial(32, (li_0_lj_0_fused_0 * 32 + li_0_lj_0_fused_1) // 8 * 2 + li_1)
                                                        j = T.axis.spatial(16, (li_0_lj_0_fused_0 * 32 + li_0_lj_0_fused_1) % 8 * 2 + lj_1)
                                                        T.reads(S_local[i, j])
                                                        T.writes(S_smem[i, j])
                                                        S_smem[i, j] = S_local[i, j]
                                        T.tvm_storage_sync("shared")
                                        for i in range(1):
                                            row: T.int32 = i * 32 * 4 + ty * 32 + tx
                                            if row < 32:
                                                with T.block("update1"):
                                                    T.reads(m_smem[row], kv_chunk_len[0], q_indptr[b_idx:b_idx + 2], m_new[i], S_smem[row, 0:16], d_smem[row], m_prev[i])
                                                    T.writes(m_prev[i], m_new[i], d_new[i])
                                                    m_prev[i] = m_smem[row]
                                                    m_new[i] = m_smem[row]
                                                    row_: T.int32 = LH_start + row
                                                    for j in range(16):
                                                        if T.if_then_else(causal > 0, L_kv_start + j < kv_chunk_len[0] - (q_indptr[b_idx + 1] - q_indptr[b_idx]) + row_ + 1, L_kv_start + j < kv_chunk_len[0]):
                                                            m_new[i] = T.max(m_new[i], S_smem[row, j])
                                                    d_new[i] = d_smem[row] * T.exp2(m_prev[i] - m_new[i])
                                        for i in range(1):
                                            row: T.int32 = i * 32 * 4 + ty * 32 + tx
                                            with T.block("update"):
                                                T.reads(kv_chunk_len[0], q_indptr[b_idx:b_idx + 2], S_smem[row, 0:16], m_new[i])
                                                T.writes(S_smem[row, 0:16])
                                                for j in range(16):
                                                    if row < 32:
                                                        row_: T.int32 = LH_start + row
                                                        if T.if_then_else(causal > 0, L_kv_start + j < kv_chunk_len[0] - (q_indptr[b_idx + 1] - q_indptr[b_idx]) + row_ + 1, L_kv_start + j < kv_chunk_len[0]):
                                                            S_smem[row, j] = T.exp2(S_smem[row, j] - m_new[i])
                                                        else:
                                                            S_smem[row, j] = T.exp2(T.float32(-50000.0) - m_new[i])
                                        for i in range(1):
                                            row: T.int32 = i * 32 * 4 + ty * 32 + tx
                                            if row < 32:
                                                with T.block("update"):
                                                    T.reads(d_new[i], S_smem[row, 0:16], m_new[i], m_prev[i])
                                                    T.writes(d_new[i], m_smem[row], d_smem[row], m_prev_smem[row])
                                                    for j in range(16):
                                                        d_new[i] = d_new[i] + S_smem[row, j]
                                                    m_smem[row] = m_new[i]
                                                    d_smem[row] = d_new[i]
                                                    m_prev_smem[row] = m_prev[i]
                                        T.tvm_storage_sync("shared")
                                        with T.block(""):
                                            T.reads(m_prev_smem[0:32], m_smem[0:32], S_smem[0:32, 0:16], V_smem[0:16, 0:80])
                                            T.writes(O_local[0:32, 0:80])
                                            for li_0_lj_0_fused_0_init in T.thread_binding(4, thread="threadIdx.y"):
                                                for li_0_lj_0_fused_1_init in T.thread_binding(32, thread="threadIdx.x"):
                                                    for li_1_init, lj_1_init in T.grid(4, 5):
                                                        with T.block("O_gemm_init"):
                                                            i = T.axis.spatial(32, (li_0_lj_0_fused_0_init * 32 + li_0_lj_0_fused_1_init) // 16 * 4 + li_1_init)
                                                            j = T.axis.spatial(80, (li_0_lj_0_fused_0_init * 32 + li_0_lj_0_fused_1_init) % 16 * 5 + lj_1_init)
                                                            T.reads()
                                                            T.writes(O_local[i, j])
                                                            O_local[i, j] = O_local[i, j] * T.exp2(m_prev_smem[i] - m_smem[i])
                                            for li_0_lj_0_fused_0 in T.thread_binding(4, thread="threadIdx.y"):
                                                for li_0_lj_0_fused_1 in T.thread_binding(32, thread="threadIdx.x"):
                                                    for lk_0, lk_1, li_1, lj_1 in T.grid(2, 8, 4, 5):
                                                        with T.block("O_gemm_update"):
                                                            i = T.axis.spatial(32, (li_0_lj_0_fused_0 * 32 + li_0_lj_0_fused_1) // 16 * 4 + li_1)
                                                            j = T.axis.spatial(80, (li_0_lj_0_fused_0 * 32 + li_0_lj_0_fused_1) % 16 * 5 + lj_1)
                                                            k = T.axis.reduce(16, lk_0 * 8 + lk_1)
                                                            T.reads(O_local[i, j], m_prev_smem[i], m_smem[i], S_smem[i, k], V_smem[k, j])
                                                            T.writes(O_local[i, j])
                                                            O_local[i, j] = O_local[i, j] + S_smem[i, k] * T.Cast("float32", V_smem[k, j])
                                    for li_0_lj_0_fused_0 in T.thread_binding(4, thread="threadIdx.y"):
                                        for li_0_lj_0_fused_1 in T.thread_binding(32, thread="threadIdx.x"):
                                            for li_1, lj_1 in T.grid(4, 5):
                                                with T.block("O_store"):
                                                    i = T.axis.spatial(32, (li_0_lj_0_fused_0 * 32 + li_0_lj_0_fused_1) // 16 * 4 + li_1)
                                                    j = T.axis.spatial(80, (li_0_lj_0_fused_0 * 32 + li_0_lj_0_fused_1) % 16 * 5 + lj_1)
                                                    T.reads(q_indptr[b_idx:b_idx + 2], O_local[i, j], d_smem[i])
                                                    T.writes(output[q_indptr[b_idx] + (LH_start + i), by, j])
                                                    cur_L: T.int32 = q_indptr[b_idx] + (LH_start + i)
                                                    cur_H_qo: T.int32 = by
                                                    if cur_L < q_indptr[b_idx + 1]:
                                                        output[cur_L, cur_H_qo, j] = T.Cast("float16", O_local[i, j] / d_smem[i])
                                    for li_0 in range(1):
                                        for li_1 in T.thread_binding(4, thread="threadIdx.y"):
                                            for li_2 in T.thread_binding(32, thread="threadIdx.x"):
                                                with T.block("lse_store"):
                                                    i = T.axis.spatial(32, li_0 * 128 + li_1 * 32 + li_2)
                                                    T.where((li_0 * 4 + li_1) * 32 + li_2 < 32)
                                                    T.reads(q_indptr[b_idx:b_idx + 2], m_smem[i], d_smem[i])
                                                    T.writes(lse[q_indptr[b_idx] + (LH_start + i), by])
                                                    cur_L: T.int32 = q_indptr[b_idx] + (LH_start + i)
                                                    cur_H_qo: T.int32 = by
                                                    if cur_L < q_indptr[b_idx + 1]:
                                                        lse[cur_L, cur_H_qo] = m_smem[i] + T.log2(d_smem[i])
                                    tile_id[0] = tile_id[0] + 16

    @T.prim_func
    def batch_prefill_paged_kv_sliding_window(_0: T.int32, var_q: T.handle, var_q_indptr: T.handle, var_pages: T.handle, var_page_indptr: T.handle, var_page_values: T.handle, var_length_info: T.handle, var_k_rope_pos_offset: T.handle, var_q_rope_position: T.handle, var_output: T.handle, var_lse: T.handle, causal: T.int32, rotary_mode: T.int32, rope_scale: T.float32, rope_theta: T.float32, attn_score_scaling_factor: T.float32):
        T.func_attr({"target": T.target({"arch": "sm_75", "host": {"keys": ["cpu"], "kind": "llvm", "mcpu": "skylake", "mtriple": "x86_64-conda-linux-gnu", "tag": ""}, "keys": ["cuda", "gpu"], "kind": "cuda", "libs": ["thrust"], "max_num_threads": 1024, "max_shared_memory_per_block": 49152, "max_threads_per_block": 1024, "tag": "", "thread_warp_size": 32}), "tir.is_scheduled": 1})
        total_len = T.int32(is_size_var=True)
        q = T.match_buffer(var_q, (total_len, 32, 80), "float16")
        batch_size = T.int32(is_size_var=True)
        q_indptr = T.match_buffer(var_q_indptr, (batch_size + 1,), "int32", offset_factor=1)
        max_num_pages = T.int32(is_size_var=True)
        pages = T.match_buffer(var_pages, (max_num_pages, 2, 32, 16, 80), "float16")
        page_indptr = T.match_buffer(var_page_indptr, (batch_size + 1,), "int32", offset_factor=1)
        nnz_pages = T.int32(is_size_var=True)
        page_values = T.match_buffer(var_page_values, (nnz_pages,), "int32", offset_factor=1)
        length_info = T.match_buffer(var_length_info, (3, batch_size), "int32", offset_factor=1)
        k_rope_pos_offset = T.match_buffer(var_k_rope_pos_offset, (batch_size,), "int32", offset_factor=1)
        q_rope_position = T.match_buffer(var_q_rope_position, (total_len,), "int32", offset_factor=1)
        output = T.match_buffer(var_output, (total_len, 32, 80), "float16")
        lse = T.match_buffer(var_lse, (total_len, 32))
        # with T.block("root"):
        for lbx in T.thread_binding(16, thread="blockIdx.x"):
            for lby in T.thread_binding(32, thread="blockIdx.y"):
                for lty in T.thread_binding(4, thread="threadIdx.y"):
                    for ltx in T.thread_binding(32, thread="threadIdx.x"):
                        with T.block("attn"):
                            bx, by, ty, tx = T.axis.remap("SSSS", [lbx, lby, lty, ltx])
                            T.reads()
                            T.writes()
                            tile_id = T.alloc_buffer((1,), "int32", scope="local")
                            batch_idx = T.alloc_buffer((1,), "int32", scope="local")
                            batch_tiles = T.alloc_buffer((1,), "int32", scope="local")
                            batch_rows = T.alloc_buffer((1,), "int32", scope="local")
                            iterator = T.alloc_buffer((1,), "int32", scope="local")
                            kv_chunk_len = T.alloc_buffer((1,), "int32", scope="local")
                            Q_smem = T.alloc_buffer((32, 80), "float16", scope="shared")
                            K_smem = T.alloc_buffer((16, 80), "float16", scope="shared")
                            V_smem = T.alloc_buffer((16, 80), "float16", scope="shared")
                            S_smem = T.alloc_buffer((32, 16), scope="shared")
                            S_local = T.alloc_buffer((32, 16), scope="local")
                            O_local = T.alloc_buffer((32, 80), scope="local")
                            m_smem = T.alloc_buffer((32,), scope="shared")
                            m_prev_smem = T.alloc_buffer((32,), scope="shared")
                            d_smem = T.alloc_buffer((32,), scope="shared")
                            m_new = T.alloc_buffer((1,), scope="local")
                            m_prev = T.alloc_buffer((1,), scope="local")
                            d_new = T.alloc_buffer((1,), scope="local")
                            tile_id[0] = bx
                            batch_idx[0] = 0
                            batch_rows[0] = q_indptr[1] - q_indptr[0]
                            batch_tiles[0] = (batch_rows[0] + 32 - 1) // 32
                            while T.tvm_thread_invariant(batch_idx[0] < batch_size):
                                while tile_id[0] >= batch_tiles[0] and batch_idx[0] < batch_size:
                                    tile_id[0] = tile_id[0] - batch_tiles[0]
                                    batch_idx[0] = batch_idx[0] + 1
                                    if batch_idx[0] < batch_size:
                                        b_idx: T.int32 = batch_idx[0]
                                        batch_rows[0] = q_indptr[b_idx + 1] - q_indptr[b_idx]
                                        batch_tiles[0] = (batch_rows[0] + 32 - 1) // 32
                                if T.tvm_thread_invariant(batch_idx[0] < batch_size):
                                    b_idx: T.int32 = batch_idx[0]
                                    LH_start: T.int32 = tile_id[0] * 32
                                    q_indptr_val: T.int32 = q_indptr[b_idx]
                                    cur_page_indptr_begin: T.int32 = page_indptr[b_idx]
                                    cur_page_indptr_end: T.int32 = page_indptr[b_idx + 1]
                                    kv_chunk_len[0] = T.if_then_else(cur_page_indptr_begin != cur_page_indptr_end, (cur_page_indptr_end - cur_page_indptr_begin - 1) * 16 + length_info[0, b_idx] - length_info[1, b_idx] + length_info[2, b_idx], 0)
                                    T.tvm_storage_sync("shared")
                                    for i in range(1):
                                        row: T.int32 = i * 32 * 4 + ty * 32 + tx
                                        if row < 32:
                                            m_smem[row] = T.float32(-50000.0)
                                            d_smem[row] = T.float32(1.0)
                                    for li_0_lj_0_fused_0 in T.thread_binding(4, thread="threadIdx.y"):
                                        for li_0_lj_0_fused_1 in T.thread_binding(32, thread="threadIdx.x"):
                                            for li_1, lj_1 in T.grid(4, 5):
                                                with T.block("O_init"):
                                                    i = T.axis.spatial(32, (li_0_lj_0_fused_0 * 32 + li_0_lj_0_fused_1) // 16 * 4 + li_1)
                                                    j = T.axis.spatial(80, (li_0_lj_0_fused_0 * 32 + li_0_lj_0_fused_1) % 16 * 5 + lj_1)
                                                    T.reads()
                                                    T.writes(O_local[i, j])
                                                    O_local[i, j] = T.float32(0.0)
                                    T.tvm_storage_sync("shared")
                                    for li_lj_fused_0 in range(5):
                                        for li_lj_fused_1 in T.thread_binding(4, thread="threadIdx.y"):
                                            for li_lj_fused_2 in T.thread_binding(32, thread="threadIdx.x"):
                                                for li_lj_fused_3 in T.vectorized(4):
                                                    with T.block("Q_load"):
                                                        i = T.axis.spatial(32, (li_lj_fused_0 * 512 + li_lj_fused_1 * 128 + li_lj_fused_2 * 4 + li_lj_fused_3) // 80)
                                                        j = T.axis.spatial(80, (li_lj_fused_0 * 512 + li_lj_fused_1 * 128 + li_lj_fused_2 * 4 + li_lj_fused_3) % 80)
                                                        T.reads()
                                                        T.writes()
                                                        cur_L: T.int32 = q_indptr_val + (LH_start + i)
                                                        cur_H_qo: T.int32 = by
                                                        if cur_L < q_indptr[b_idx + 1]:
                                                            freq = T.float32()
                                                            Q_smem[i, j] = T.if_then_else(rotary_mode == 1, T.Let(T.Cast("float16", T.cos(freq) * T.Cast("float32", q[cur_L, cur_H_qo, j]) + T.sin(freq) * T.Cast("float32", T.if_then_else(j < 40, q[cur_L, cur_H_qo, j + 40] * T.float16(-1.0), q[cur_L, cur_H_qo, j - 40]))), where={freq: T.Cast("float32", q_rope_position[cur_L]) * rope_scale / T.pow(rope_theta, T.Cast("float32", j * 2 % 80) / T.float32(80.0))}), q[cur_L, cur_H_qo, j])
                                                        else:
                                                            Q_smem[i, j] = T.float16(0.0)
                                    T.tvm_storage_sync("shared")
                                    for iterator_1 in range((kv_chunk_len[0] + 15) // 16):
                                        L_kv_start: T.int32 = iterator_1 * 16
                                        for lz_ly_fused_0 in range(3):
                                            for lz_ly_fused_1 in T.thread_binding(4, thread="threadIdx.y"):
                                                for lz_ly_fused_2 in T.thread_binding(32, thread="threadIdx.x"):
                                                    for lz_ly_fused_3 in T.vectorized(4):
                                                        with T.block("K_load"):
                                                            i = T.axis.spatial(16, (lz_ly_fused_0 * 512 + lz_ly_fused_1 * 128 + lz_ly_fused_2 * 4 + lz_ly_fused_3) // 80)
                                                            j = T.axis.spatial(80, (lz_ly_fused_0 * 512 + lz_ly_fused_1 * 128 + lz_ly_fused_2 * 4 + lz_ly_fused_3) % 80)
                                                            T.where(((lz_ly_fused_0 * 4 + lz_ly_fused_1) * 32 + lz_ly_fused_2) * 4 + lz_ly_fused_3 < 1280)
                                                            T.reads()
                                                            T.writes()
                                                            cur_L: T.int32 = L_kv_start + i
                                                            if cur_L < kv_chunk_len[0]:
                                                                seq_offset: T.int32 = T.if_then_else(cur_L < length_info[2, b_idx], cur_L, cur_L - length_info[2, b_idx] + length_info[1, b_idx])
                                                                page_no: T.int32 = page_values[cur_page_indptr_begin + seq_offset // 16]
                                                                page_offset: T.int32 = seq_offset % 16
                                                                freq = T.float32()
                                                                K_smem[i, j] = T.if_then_else(rotary_mode == 1, T.Let(T.Cast("float16", T.cos(freq) * T.Cast("float32", pages[page_no, 0, by, page_offset, j]) + T.sin(freq) * T.Cast("float32", T.if_then_else(j < 40, pages[page_no, 0, by, page_offset, j + 40] * T.float16(-1.0), pages[page_no, 0, by, page_offset, j - 40]))), where={freq: T.Cast("float32", k_rope_pos_offset[b_idx] + cur_L) * rope_scale / T.pow(rope_theta, T.Cast("float32", j * 2 % 80) / T.float32(80.0))}), pages[page_no, 0, by, page_offset, j])
                                                            else:
                                                                K_smem[i, j] = T.float16(0.0)
                                        T.tvm_storage_sync("shared")
                                        for lz_ly_fused_0 in range(3):
                                            for lz_ly_fused_1 in T.thread_binding(4, thread="threadIdx.y"):
                                                for lz_ly_fused_2 in T.thread_binding(32, thread="threadIdx.x"):
                                                    for lz_ly_fused_3 in T.vectorized(4):
                                                        with T.block("V_load"):
                                                            i = T.axis.spatial(16, (lz_ly_fused_0 * 512 + lz_ly_fused_1 * 128 + lz_ly_fused_2 * 4 + lz_ly_fused_3) // 80)
                                                            j = T.axis.spatial(80, (lz_ly_fused_0 * 512 + lz_ly_fused_1 * 128 + lz_ly_fused_2 * 4 + lz_ly_fused_3) % 80)
                                                            T.where(((lz_ly_fused_0 * 4 + lz_ly_fused_1) * 32 + lz_ly_fused_2) * 4 + lz_ly_fused_3 < 1280)
                                                            T.reads()
                                                            T.writes()
                                                            cur_L: T.int32 = L_kv_start + i
                                                            if cur_L < kv_chunk_len[0]:
                                                                seq_offset: T.int32 = T.if_then_else(cur_L < length_info[2, b_idx], cur_L, cur_L - length_info[2, b_idx] + length_info[1, b_idx])
                                                                page_no: T.int32 = page_values[cur_page_indptr_begin + seq_offset // 16]
                                                                page_offset: T.int32 = seq_offset % 16
                                                                V_smem[i, j] = pages[page_no, 1, by, page_offset, j]
                                                            else:
                                                                V_smem[i, j] = T.float16(0.0)
                                        T.tvm_storage_sync("shared")
                                        with T.block(""):
                                            T.reads(Q_smem[0:32, 0:80], K_smem[0:16, 0:80])
                                            T.writes(S_local[0:32, 0:16])
                                            for li_0_lj_0_fused_0_init in T.thread_binding(4, thread="threadIdx.y"):
                                                for li_0_lj_0_fused_1_init in T.thread_binding(32, thread="threadIdx.x"):
                                                    for li_1_init, lj_1_init in T.grid(2, 2):
                                                        with T.block("S_gemm_init"):
                                                            i = T.axis.spatial(32, (li_0_lj_0_fused_0_init * 32 + li_0_lj_0_fused_1_init) // 8 * 2 + li_1_init)
                                                            j = T.axis.spatial(16, (li_0_lj_0_fused_0_init * 32 + li_0_lj_0_fused_1_init) % 8 * 2 + lj_1_init)
                                                            T.reads()
                                                            T.writes(S_local[i, j])
                                                            S_local[i, j] = T.float32(0.0)
                                            for li_0_lj_0_fused_0 in T.thread_binding(4, thread="threadIdx.y"):
                                                for li_0_lj_0_fused_1 in T.thread_binding(32, thread="threadIdx.x"):
                                                    for lk_0, li_1, lj_1, lk_1 in T.grid(10, 2, 2, 8):
                                                        with T.block("S_gemm_update"):
                                                            i = T.axis.spatial(32, (li_0_lj_0_fused_0 * 32 + li_0_lj_0_fused_1) // 8 * 2 + li_1)
                                                            j = T.axis.spatial(16, (li_0_lj_0_fused_0 * 32 + li_0_lj_0_fused_1) % 8 * 2 + lj_1)
                                                            k = T.axis.reduce(80, lk_0 * 8 + lk_1)
                                                            T.reads(S_local[i, j], Q_smem[i, k], K_smem[j, k])
                                                            T.writes(S_local[i, j])
                                                            S_local[i, j] = S_local[i, j] + T.Cast("float32", Q_smem[i, k]) * T.Cast("float32", K_smem[j, k]) * attn_score_scaling_factor * T.float32(0.16129820911147805)
                                        T.tvm_storage_sync("shared")
                                        for li_0_lj_0_fused_0 in T.thread_binding(4, thread="threadIdx.y"):
                                            for li_0_lj_0_fused_1 in T.thread_binding(32, thread="threadIdx.x"):
                                                for li_1, lj_1 in T.grid(2, 2):
                                                    with T.block("S_store"):
                                                        i = T.axis.spatial(32, (li_0_lj_0_fused_0 * 32 + li_0_lj_0_fused_1) // 8 * 2 + li_1)
                                                        j = T.axis.spatial(16, (li_0_lj_0_fused_0 * 32 + li_0_lj_0_fused_1) % 8 * 2 + lj_1)
                                                        T.reads(S_local[i, j])
                                                        T.writes(S_smem[i, j])
                                                        S_smem[i, j] = S_local[i, j]
                                        T.tvm_storage_sync("shared")
                                        for i in range(1):
                                            row: T.int32 = i * 32 * 4 + ty * 32 + tx
                                            if row < 32:
                                                with T.block("update1"):
                                                    T.reads(m_smem[row], kv_chunk_len[0], q_indptr[b_idx:b_idx + 2], m_new[i], S_smem[row, 0:16], d_smem[row], m_prev[i])
                                                    T.writes(m_prev[i], m_new[i], d_new[i])
                                                    m_prev[i] = m_smem[row]
                                                    m_new[i] = m_smem[row]
                                                    row_: T.int32 = LH_start + row
                                                    for j in range(16):
                                                        if T.if_then_else(causal > 0, L_kv_start + j < kv_chunk_len[0] - (q_indptr[b_idx + 1] - q_indptr[b_idx]) + row_ + 1, L_kv_start + j < kv_chunk_len[0]):
                                                            m_new[i] = T.max(m_new[i], S_smem[row, j])
                                                    d_new[i] = d_smem[row] * T.exp2(m_prev[i] - m_new[i])
                                        for i in range(1):
                                            row: T.int32 = i * 32 * 4 + ty * 32 + tx
                                            with T.block("update"):
                                                T.reads(kv_chunk_len[0], q_indptr[b_idx:b_idx + 2], S_smem[row, 0:16], m_new[i])
                                                T.writes(S_smem[row, 0:16])
                                                for j in range(16):
                                                    if row < 32:
                                                        row_: T.int32 = LH_start + row
                                                        if T.if_then_else(causal > 0, L_kv_start + j < kv_chunk_len[0] - (q_indptr[b_idx + 1] - q_indptr[b_idx]) + row_ + 1, L_kv_start + j < kv_chunk_len[0]):
                                                            S_smem[row, j] = T.exp2(S_smem[row, j] - m_new[i])
                                                        else:
                                                            S_smem[row, j] = T.exp2(T.float32(-50000.0) - m_new[i])
                                        for i in range(1):
                                            row: T.int32 = i * 32 * 4 + ty * 32 + tx
                                            if row < 32:
                                                with T.block("update"):
                                                    T.reads(d_new[i], S_smem[row, 0:16], m_new[i], m_prev[i])
                                                    T.writes(d_new[i], m_smem[row], d_smem[row], m_prev_smem[row])
                                                    for j in range(16):
                                                        d_new[i] = d_new[i] + S_smem[row, j]
                                                    m_smem[row] = m_new[i]
                                                    d_smem[row] = d_new[i]
                                                    m_prev_smem[row] = m_prev[i]
                                        T.tvm_storage_sync("shared")
                                        with T.block(""):
                                            T.reads(m_prev_smem[0:32], m_smem[0:32], S_smem[0:32, 0:16], V_smem[0:16, 0:80])
                                            T.writes(O_local[0:32, 0:80])
                                            for li_0_lj_0_fused_0_init in T.thread_binding(4, thread="threadIdx.y"):
                                                for li_0_lj_0_fused_1_init in T.thread_binding(32, thread="threadIdx.x"):
                                                    for li_1_init, lj_1_init in T.grid(4, 5):
                                                        with T.block("O_gemm_init"):
                                                            i = T.axis.spatial(32, (li_0_lj_0_fused_0_init * 32 + li_0_lj_0_fused_1_init) // 16 * 4 + li_1_init)
                                                            j = T.axis.spatial(80, (li_0_lj_0_fused_0_init * 32 + li_0_lj_0_fused_1_init) % 16 * 5 + lj_1_init)
                                                            T.reads()
                                                            T.writes(O_local[i, j])
                                                            O_local[i, j] = O_local[i, j] * T.exp2(m_prev_smem[i] - m_smem[i])
                                            for li_0_lj_0_fused_0 in T.thread_binding(4, thread="threadIdx.y"):
                                                for li_0_lj_0_fused_1 in T.thread_binding(32, thread="threadIdx.x"):
                                                    for lk_0, lk_1, li_1, lj_1 in T.grid(2, 8, 4, 5):
                                                        with T.block("O_gemm_update"):
                                                            i = T.axis.spatial(32, (li_0_lj_0_fused_0 * 32 + li_0_lj_0_fused_1) // 16 * 4 + li_1)
                                                            j = T.axis.spatial(80, (li_0_lj_0_fused_0 * 32 + li_0_lj_0_fused_1) % 16 * 5 + lj_1)
                                                            k = T.axis.reduce(16, lk_0 * 8 + lk_1)
                                                            T.reads(O_local[i, j], m_prev_smem[i], m_smem[i], S_smem[i, k], V_smem[k, j])
                                                            T.writes(O_local[i, j])
                                                            O_local[i, j] = O_local[i, j] + S_smem[i, k] * T.Cast("float32", V_smem[k, j])
                                    for li_0_lj_0_fused_0 in T.thread_binding(4, thread="threadIdx.y"):
                                        for li_0_lj_0_fused_1 in T.thread_binding(32, thread="threadIdx.x"):
                                            for li_1, lj_1 in T.grid(4, 5):
                                                with T.block("O_store"):
                                                    i = T.axis.spatial(32, (li_0_lj_0_fused_0 * 32 + li_0_lj_0_fused_1) // 16 * 4 + li_1)
                                                    j = T.axis.spatial(80, (li_0_lj_0_fused_0 * 32 + li_0_lj_0_fused_1) % 16 * 5 + lj_1)
                                                    T.reads(q_indptr[b_idx:b_idx + 2], O_local[i, j], d_smem[i])
                                                    T.writes(output[q_indptr[b_idx] + (LH_start + i), by, j])
                                                    cur_L: T.int32 = q_indptr[b_idx] + (LH_start + i)
                                                    cur_H_qo: T.int32 = by
                                                    if cur_L < q_indptr[b_idx + 1]:
                                                        output[cur_L, cur_H_qo, j] = T.Cast("float16", O_local[i, j] / d_smem[i])
                                    for li_0 in range(1):
                                        for li_1 in T.thread_binding(4, thread="threadIdx.y"):
                                            for li_2 in T.thread_binding(32, thread="threadIdx.x"):
                                                with T.block("lse_store"):
                                                    i = T.axis.spatial(32, li_0 * 128 + li_1 * 32 + li_2)
                                                    T.where((li_0 * 4 + li_1) * 32 + li_2 < 32)
                                                    T.reads(q_indptr[b_idx:b_idx + 2], m_smem[i], d_smem[i])
                                                    T.writes(lse[q_indptr[b_idx] + (LH_start + i), by])
                                                    cur_L: T.int32 = q_indptr[b_idx] + (LH_start + i)
                                                    cur_H_qo: T.int32 = by
                                                    if cur_L < q_indptr[b_idx + 1]:
                                                        lse[cur_L, cur_H_qo] = m_smem[i] + T.log2(d_smem[i])
                                    tile_id[0] = tile_id[0] + 16

    @T.prim_func
    def batch_prefill_ragged_kv(var_q: T.handle, var_q_indptr: T.handle, var_k: T.handle, var_v: T.handle, var_kv_indptr: T.handle, var_q_rope_position: T.handle, var_k_rope_pos_offset: T.handle, var_output: T.handle, var_lse: T.handle, causal: T.int32, rotary_mode: T.int32, rope_scale: T.float32, rope_theta: T.float32, attn_score_scaling_factor: T.float32):
        T.func_attr({"target": T.target({"arch": "sm_75", "host": {"keys": ["cpu"], "kind": "llvm", "mcpu": "skylake", "mtriple": "x86_64-conda-linux-gnu", "tag": ""}, "keys": ["cuda", "gpu"], "kind": "cuda", "libs": ["thrust"], "max_num_threads": 1024, "max_shared_memory_per_block": 49152, "max_threads_per_block": 1024, "tag": "", "thread_warp_size": 32}), "tir.is_scheduled": 1})
        qo_len = T.int32(is_size_var=True)
        q = T.match_buffer(var_q, (qo_len, 32, 80), "float16")
        batch_size = T.int32(is_size_var=True)
        q_indptr = T.match_buffer(var_q_indptr, (batch_size + 1,), "int32", offset_factor=1)
        kv_len = T.int32(is_size_var=True)
        k = T.match_buffer(var_k, (kv_len, 32, 80), "float16")
        v = T.match_buffer(var_v, (kv_len, 32, 80), "float16")
        kv_indptr = T.match_buffer(var_kv_indptr, (batch_size + 1,), "int32", offset_factor=1)
        q_rope_position = T.match_buffer(var_q_rope_position, (qo_len,), "int32", offset_factor=1)
        k_rope_pos_offset = T.match_buffer(var_k_rope_pos_offset, (batch_size,), "int32", offset_factor=1)
        output = T.match_buffer(var_output, (qo_len, 32, 80), "float16")
        lse = T.match_buffer(var_lse, (qo_len, 32))
        # with T.block("root"):
        for lbx in T.thread_binding(16, thread="blockIdx.x"):
            for lby in T.thread_binding(32, thread="blockIdx.y"):
                for lty in T.thread_binding(4, thread="threadIdx.y"):
                    for ltx in T.thread_binding(32, thread="threadIdx.x"):
                        with T.block("attn"):
                            bx, by, ty, tx = T.axis.remap("SSSS", [lbx, lby, lty, ltx])
                            T.reads()
                            T.writes()
                            tile_id = T.alloc_buffer((1,), "int32", scope="local")
                            batch_idx = T.alloc_buffer((1,), "int32", scope="local")
                            batch_tiles = T.alloc_buffer((1,), "int32", scope="local")
                            batch_rows = T.alloc_buffer((1,), "int32", scope="local")
                            iterator = T.alloc_buffer((1,), "int32", scope="local")
                            kv_chunk_len = T.alloc_buffer((1,), "int32", scope="local")
                            Q_smem = T.alloc_buffer((32, 80), "float16", scope="shared")
                            K_smem = T.alloc_buffer((16, 80), "float16", scope="shared")
                            V_smem = T.alloc_buffer((16, 80), "float16", scope="shared")
                            S_smem = T.alloc_buffer((32, 16), scope="shared")
                            S_local = T.alloc_buffer((32, 16), scope="local")
                            O_local = T.alloc_buffer((32, 80), scope="local")
                            m_smem = T.alloc_buffer((32,), scope="shared")
                            m_prev_smem = T.alloc_buffer((32,), scope="shared")
                            d_smem = T.alloc_buffer((32,), scope="shared")
                            m_new = T.alloc_buffer((1,), scope="local")
                            m_prev = T.alloc_buffer((1,), scope="local")
                            d_new = T.alloc_buffer((1,), scope="local")
                            tile_id[0] = bx
                            batch_idx[0] = 0
                            batch_rows[0] = q_indptr[1] - q_indptr[0]
                            batch_tiles[0] = (batch_rows[0] + 32 - 1) // 32
                            while T.tvm_thread_invariant(batch_idx[0] < batch_size):
                                while tile_id[0] >= batch_tiles[0] and batch_idx[0] < batch_size:
                                    tile_id[0] = tile_id[0] - batch_tiles[0]
                                    batch_idx[0] = batch_idx[0] + 1
                                    if batch_idx[0] < batch_size:
                                        b_idx: T.int32 = batch_idx[0]
                                        batch_rows[0] = q_indptr[b_idx + 1] - q_indptr[b_idx]
                                        batch_tiles[0] = (batch_rows[0] + 32 - 1) // 32
                                if T.tvm_thread_invariant(batch_idx[0] < batch_size):
                                    b_idx: T.int32 = batch_idx[0]
                                    q_indptr_val: T.int32 = q_indptr[b_idx]
                                    LH_start: T.int32 = tile_id[0] * 32
                                    kv_chunk_len[0] = kv_indptr[b_idx + 1] - kv_indptr[b_idx]
                                    T.tvm_storage_sync("shared")
                                    for i in range(1):
                                        row: T.int32 = i * 32 * 4 + ty * 32 + tx
                                        if row < 32:
                                            m_smem[row] = T.float32(-50000.0)
                                            d_smem[row] = T.float32(1.0)
                                    for li_0_lj_0_fused_0 in T.thread_binding(4, thread="threadIdx.y"):
                                        for li_0_lj_0_fused_1 in T.thread_binding(32, thread="threadIdx.x"):
                                            for li_1, lj_1 in T.grid(4, 5):
                                                with T.block("O_init"):
                                                    i = T.axis.spatial(32, (li_0_lj_0_fused_0 * 32 + li_0_lj_0_fused_1) // 16 * 4 + li_1)
                                                    j = T.axis.spatial(80, (li_0_lj_0_fused_0 * 32 + li_0_lj_0_fused_1) % 16 * 5 + lj_1)
                                                    T.reads()
                                                    T.writes(O_local[i, j])
                                                    O_local[i, j] = T.float32(0.0)
                                    T.tvm_storage_sync("shared")
                                    for li_lj_fused_0 in range(5):
                                        for li_lj_fused_1 in T.thread_binding(4, thread="threadIdx.y"):
                                            for li_lj_fused_2 in T.thread_binding(32, thread="threadIdx.x"):
                                                for li_lj_fused_3 in T.vectorized(4):
                                                    with T.block("Q_load"):
                                                        i = T.axis.spatial(32, (li_lj_fused_0 * 512 + li_lj_fused_1 * 128 + li_lj_fused_2 * 4 + li_lj_fused_3) // 80)
                                                        j = T.axis.spatial(80, (li_lj_fused_0 * 512 + li_lj_fused_1 * 128 + li_lj_fused_2 * 4 + li_lj_fused_3) % 80)
                                                        T.reads()
                                                        T.writes()
                                                        cur_L: T.int32 = q_indptr_val + (LH_start + i)
                                                        cur_H_qo: T.int32 = by
                                                        if cur_L < q_indptr[b_idx + 1]:
                                                            freq = T.float32()
                                                            Q_smem[i, j] = T.if_then_else(rotary_mode == 1, T.Let(T.Cast("float16", T.cos(freq) * T.Cast("float32", q[cur_L, cur_H_qo, j]) + T.sin(freq) * T.Cast("float32", T.if_then_else(j < 40, q[cur_L, cur_H_qo, j + 40] * T.float16(-1.0), q[cur_L, cur_H_qo, j - 40]))), where={freq: T.Cast("float32", q_rope_position[cur_L]) * rope_scale / T.pow(rope_theta, T.Cast("float32", j * 2 % 80) / T.float32(80.0))}), q[cur_L, cur_H_qo, j])
                                                        else:
                                                            Q_smem[i, j] = T.float16(0.0)
                                    T.tvm_storage_sync("shared")
                                    for iterator_1 in range((kv_chunk_len[0] + 15) // 16):
                                        L_kv_start: T.int32 = iterator_1 * 16
                                        L_kv_base: T.int32 = kv_indptr[b_idx]
                                        for lz_ly_fused_0 in range(3):
                                            for lz_ly_fused_1 in T.thread_binding(4, thread="threadIdx.y"):
                                                for lz_ly_fused_2 in T.thread_binding(32, thread="threadIdx.x"):
                                                    for lz_ly_fused_3 in T.vectorized(4):
                                                        with T.block("K_load"):
                                                            i = T.axis.spatial(16, (lz_ly_fused_0 * 512 + lz_ly_fused_1 * 128 + lz_ly_fused_2 * 4 + lz_ly_fused_3) // 80)
                                                            j = T.axis.spatial(80, (lz_ly_fused_0 * 512 + lz_ly_fused_1 * 128 + lz_ly_fused_2 * 4 + lz_ly_fused_3) % 80)
                                                            T.where(((lz_ly_fused_0 * 4 + lz_ly_fused_1) * 32 + lz_ly_fused_2) * 4 + lz_ly_fused_3 < 1280)
                                                            T.reads()
                                                            T.writes()
                                                            cur_L: T.int32 = L_kv_start + i
                                                            if cur_L < kv_chunk_len[0]:
                                                                freq = T.float32()
                                                                K_smem[i, j] = T.if_then_else(rotary_mode == 1, T.Let(T.Cast("float16", T.cos(freq) * T.Cast("float32", k[L_kv_base + cur_L, by, j]) + T.sin(freq) * T.Cast("float32", T.if_then_else(j < 40, k[L_kv_base + cur_L, by, j + 40] * T.float16(-1.0), k[L_kv_base + cur_L, by, j - 40]))), where={freq: T.Cast("float32", k_rope_pos_offset[b_idx] + cur_L) * rope_scale / T.pow(rope_theta, T.Cast("float32", j * 2 % 80) / T.float32(80.0))}), k[L_kv_base + cur_L, by, j])
                                                            else:
                                                                K_smem[i, j] = T.float16(0.0)
                                        T.tvm_storage_sync("shared")
                                        for lz_ly_fused_0 in range(3):
                                            for lz_ly_fused_1 in T.thread_binding(4, thread="threadIdx.y"):
                                                for lz_ly_fused_2 in T.thread_binding(32, thread="threadIdx.x"):
                                                    for lz_ly_fused_3 in T.vectorized(4):
                                                        with T.block("V_load"):
                                                            i = T.axis.spatial(16, (lz_ly_fused_0 * 512 + lz_ly_fused_1 * 128 + lz_ly_fused_2 * 4 + lz_ly_fused_3) // 80)
                                                            j = T.axis.spatial(80, (lz_ly_fused_0 * 512 + lz_ly_fused_1 * 128 + lz_ly_fused_2 * 4 + lz_ly_fused_3) % 80)
                                                            T.where(((lz_ly_fused_0 * 4 + lz_ly_fused_1) * 32 + lz_ly_fused_2) * 4 + lz_ly_fused_3 < 1280)
                                                            T.reads()
                                                            T.writes()
                                                            cur_L: T.int32 = L_kv_start + i
                                                            if cur_L < kv_chunk_len[0]:
                                                                V_smem[i, j] = v[L_kv_base + cur_L, by, j]
                                                            else:
                                                                V_smem[i, j] = T.float16(0.0)
                                        T.tvm_storage_sync("shared")
                                        with T.block(""):
                                            T.reads(Q_smem[0:32, 0:80], K_smem[0:16, 0:80])
                                            T.writes(S_local[0:32, 0:16])
                                            for li_0_lj_0_fused_0_init in T.thread_binding(4, thread="threadIdx.y"):
                                                for li_0_lj_0_fused_1_init in T.thread_binding(32, thread="threadIdx.x"):
                                                    for li_1_init, lj_1_init in T.grid(2, 2):
                                                        with T.block("S_gemm_init"):
                                                            i = T.axis.spatial(32, (li_0_lj_0_fused_0_init * 32 + li_0_lj_0_fused_1_init) // 8 * 2 + li_1_init)
                                                            j = T.axis.spatial(16, (li_0_lj_0_fused_0_init * 32 + li_0_lj_0_fused_1_init) % 8 * 2 + lj_1_init)
                                                            T.reads()
                                                            T.writes(S_local[i, j])
                                                            S_local[i, j] = T.float32(0.0)
                                            for li_0_lj_0_fused_0 in T.thread_binding(4, thread="threadIdx.y"):
                                                for li_0_lj_0_fused_1 in T.thread_binding(32, thread="threadIdx.x"):
                                                    for lk_0, li_1, lj_1, lk_1 in T.grid(10, 2, 2, 8):
                                                        with T.block("S_gemm_update"):
                                                            i = T.axis.spatial(32, (li_0_lj_0_fused_0 * 32 + li_0_lj_0_fused_1) // 8 * 2 + li_1)
                                                            j = T.axis.spatial(16, (li_0_lj_0_fused_0 * 32 + li_0_lj_0_fused_1) % 8 * 2 + lj_1)
                                                            k_1 = T.axis.reduce(80, lk_0 * 8 + lk_1)
                                                            T.reads(S_local[i, j], Q_smem[i, k_1], K_smem[j, k_1])
                                                            T.writes(S_local[i, j])
                                                            S_local[i, j] = S_local[i, j] + T.Cast("float32", Q_smem[i, k_1]) * T.Cast("float32", K_smem[j, k_1]) * attn_score_scaling_factor * T.float32(0.16129820911147805)
                                        T.tvm_storage_sync("shared")
                                        for li_0_lj_0_fused_0 in T.thread_binding(4, thread="threadIdx.y"):
                                            for li_0_lj_0_fused_1 in T.thread_binding(32, thread="threadIdx.x"):
                                                for li_1, lj_1 in T.grid(2, 2):
                                                    with T.block("S_store"):
                                                        i = T.axis.spatial(32, (li_0_lj_0_fused_0 * 32 + li_0_lj_0_fused_1) // 8 * 2 + li_1)
                                                        j = T.axis.spatial(16, (li_0_lj_0_fused_0 * 32 + li_0_lj_0_fused_1) % 8 * 2 + lj_1)
                                                        T.reads(S_local[i, j])
                                                        T.writes(S_smem[i, j])
                                                        S_smem[i, j] = S_local[i, j]
                                        T.tvm_storage_sync("shared")
                                        for i in range(1):
                                            row: T.int32 = i * 32 * 4 + ty * 32 + tx
                                            if row < 32:
                                                with T.block("update1"):
                                                    T.reads(m_smem[row], kv_chunk_len[0], q_indptr[b_idx:b_idx + 2], m_new[i], S_smem[row, 0:16], d_smem[row], m_prev[i])
                                                    T.writes(m_prev[i], m_new[i], d_new[i])
                                                    m_prev[i] = m_smem[row]
                                                    m_new[i] = m_smem[row]
                                                    row_: T.int32 = LH_start + row
                                                    for j in range(16):
                                                        if T.if_then_else(causal > 0, L_kv_start + j < kv_chunk_len[0] - (q_indptr[b_idx + 1] - q_indptr[b_idx]) + row_ + 1, L_kv_start + j < kv_chunk_len[0]):
                                                            m_new[i] = T.max(m_new[i], S_smem[row, j])
                                                    d_new[i] = d_smem[row] * T.exp2(m_prev[i] - m_new[i])
                                        for i in range(1):
                                            row: T.int32 = i * 32 * 4 + ty * 32 + tx
                                            with T.block("update"):
                                                T.reads(kv_chunk_len[0], q_indptr[b_idx:b_idx + 2], S_smem[row, 0:16], m_new[i])
                                                T.writes(S_smem[row, 0:16])
                                                for j in range(16):
                                                    if row < 32:
                                                        row_: T.int32 = LH_start + row
                                                        if T.if_then_else(causal > 0, L_kv_start + j < kv_chunk_len[0] - (q_indptr[b_idx + 1] - q_indptr[b_idx]) + row_ + 1, L_kv_start + j < kv_chunk_len[0]):
                                                            S_smem[row, j] = T.exp2(S_smem[row, j] - m_new[i])
                                                        else:
                                                            S_smem[row, j] = T.exp2(T.float32(-50000.0) - m_new[i])
                                        for i in range(1):
                                            row: T.int32 = i * 32 * 4 + ty * 32 + tx
                                            if row < 32:
                                                with T.block("update"):
                                                    T.reads(d_new[i], S_smem[row, 0:16], m_new[i], m_prev[i])
                                                    T.writes(d_new[i], m_smem[row], d_smem[row], m_prev_smem[row])
                                                    for j in range(16):
                                                        d_new[i] = d_new[i] + S_smem[row, j]
                                                    m_smem[row] = m_new[i]
                                                    d_smem[row] = d_new[i]
                                                    m_prev_smem[row] = m_prev[i]
                                        T.tvm_storage_sync("shared")
                                        with T.block(""):
                                            T.reads(m_prev_smem[0:32], m_smem[0:32], S_smem[0:32, 0:16], V_smem[0:16, 0:80])
                                            T.writes(O_local[0:32, 0:80])
                                            for li_0_lj_0_fused_0_init in T.thread_binding(4, thread="threadIdx.y"):
                                                for li_0_lj_0_fused_1_init in T.thread_binding(32, thread="threadIdx.x"):
                                                    for li_1_init, lj_1_init in T.grid(4, 5):
                                                        with T.block("O_gemm_init"):
                                                            i = T.axis.spatial(32, (li_0_lj_0_fused_0_init * 32 + li_0_lj_0_fused_1_init) // 16 * 4 + li_1_init)
                                                            j = T.axis.spatial(80, (li_0_lj_0_fused_0_init * 32 + li_0_lj_0_fused_1_init) % 16 * 5 + lj_1_init)
                                                            T.reads()
                                                            T.writes(O_local[i, j])
                                                            O_local[i, j] = O_local[i, j] * T.exp2(m_prev_smem[i] - m_smem[i])
                                            for li_0_lj_0_fused_0 in T.thread_binding(4, thread="threadIdx.y"):
                                                for li_0_lj_0_fused_1 in T.thread_binding(32, thread="threadIdx.x"):
                                                    for lk_0, lk_1, li_1, lj_1 in T.grid(2, 8, 4, 5):
                                                        with T.block("O_gemm_update"):
                                                            i = T.axis.spatial(32, (li_0_lj_0_fused_0 * 32 + li_0_lj_0_fused_1) // 16 * 4 + li_1)
                                                            j = T.axis.spatial(80, (li_0_lj_0_fused_0 * 32 + li_0_lj_0_fused_1) % 16 * 5 + lj_1)
                                                            k_1 = T.axis.reduce(16, lk_0 * 8 + lk_1)
                                                            T.reads(O_local[i, j], m_prev_smem[i], m_smem[i], S_smem[i, k_1], V_smem[k_1, j])
                                                            T.writes(O_local[i, j])
                                                            O_local[i, j] = O_local[i, j] + S_smem[i, k_1] * T.Cast("float32", V_smem[k_1, j])
                                    for li_0_lj_0_fused_0 in T.thread_binding(4, thread="threadIdx.y"):
                                        for li_0_lj_0_fused_1 in T.thread_binding(32, thread="threadIdx.x"):
                                            for li_1, lj_1 in T.grid(4, 5):
                                                with T.block("O_store"):
                                                    i = T.axis.spatial(32, (li_0_lj_0_fused_0 * 32 + li_0_lj_0_fused_1) // 16 * 4 + li_1)
                                                    j = T.axis.spatial(80, (li_0_lj_0_fused_0 * 32 + li_0_lj_0_fused_1) % 16 * 5 + lj_1)
                                                    T.reads(q_indptr[b_idx:b_idx + 2], O_local[i, j], d_smem[i])
                                                    T.writes(output[q_indptr[b_idx] + (LH_start + i), by, j])
                                                    cur_L: T.int32 = q_indptr[b_idx] + (LH_start + i)
                                                    cur_H_qo: T.int32 = by
                                                    if cur_L < q_indptr[b_idx + 1]:
                                                        output[cur_L, cur_H_qo, j] = T.Cast("float16", O_local[i, j] / d_smem[i])
                                    for li_0 in range(1):
                                        for li_1 in T.thread_binding(4, thread="threadIdx.y"):
                                            for li_2 in T.thread_binding(32, thread="threadIdx.x"):
                                                with T.block("lse_store"):
                                                    i = T.axis.spatial(32, li_0 * 128 + li_1 * 32 + li_2)
                                                    T.where((li_0 * 4 + li_1) * 32 + li_2 < 32)
                                                    T.reads(q_indptr[b_idx:b_idx + 2], m_smem[i], d_smem[i])
                                                    T.writes(lse[q_indptr[b_idx] + (LH_start + i), by])
                                                    cur_L: T.int32 = q_indptr[b_idx] + (LH_start + i)
                                                    cur_H_qo: T.int32 = by
                                                    if cur_L < q_indptr[b_idx + 1]:
                                                        lse[cur_L, cur_H_qo] = m_smem[i] + T.log2(d_smem[i])
                                    tile_id[0] = tile_id[0] + 16

    @T.prim_func
    def batch_tree_attn(var_q: T.handle, var_q_indptr: T.handle, var_k: T.handle, var_v: T.handle, var_kv_indptr: T.handle, var_q_rope_position: T.handle, var_mn_indptr: T.handle, var_mask: T.handle, var_output: T.handle, var_lse: T.handle, rotary_mode: T.int32, rope_scale: T.float32, rope_theta: T.float32, attn_score_scaling_factor: T.float32, batch_size: T.int32):
        T.func_attr({"target": T.target({"arch": "sm_75", "host": {"keys": ["cpu"], "kind": "llvm", "mcpu": "skylake", "mtriple": "x86_64-conda-linux-gnu", "tag": ""}, "keys": ["cuda", "gpu"], "kind": "cuda", "libs": ["thrust"], "max_num_threads": 1024, "max_shared_memory_per_block": 49152, "max_threads_per_block": 1024, "tag": "", "thread_warp_size": 32}), "tir.is_scheduled": 1})
        qo_len = T.int32(is_size_var=True)
        q = T.match_buffer(var_q, (qo_len, 32, 80), "float16")
        q_indptr = T.match_buffer(var_q_indptr, (batch_size + 1,), "int32", offset_factor=1)
        kv_len = T.int32(is_size_var=True)
        k = T.match_buffer(var_k, (kv_len, 32, 80), "float16")
        v = T.match_buffer(var_v, (kv_len, 32, 80), "float16")
        kv_indptr = T.match_buffer(var_kv_indptr, (batch_size + 1,), "int32", offset_factor=1)
        q_rope_position = T.match_buffer(var_q_rope_position, (qo_len,), "int32", offset_factor=1)
        mn_indptr = T.match_buffer(var_mn_indptr, (batch_size + 1,), "int32", offset_factor=1)
        tree_size = T.int32(is_size_var=True)
        mask = T.match_buffer(var_mask, (tree_size, 2), "int32", offset_factor=1)
        output = T.match_buffer(var_output, (qo_len, 32, 80), "float16")
        lse = T.match_buffer(var_lse, (qo_len, 32))
        # with T.block("root"):
        for lbx in T.thread_binding(16, thread="blockIdx.x"):
            for lby in T.thread_binding(32, thread="blockIdx.y"):
                for lty in T.thread_binding(4, thread="threadIdx.y"):
                    for ltx in T.thread_binding(32, thread="threadIdx.x"):
                        with T.block("attn"):
                            bx, by, ty, tx = T.axis.remap("SSSS", [lbx, lby, lty, ltx])
                            T.reads()
                            T.writes()
                            tile_id = T.alloc_buffer((1,), "int32", scope="local")
                            batch_idx = T.alloc_buffer((1,), "int32", scope="local")
                            batch_tiles = T.alloc_buffer((1,), "int32", scope="local")
                            batch_rows = T.alloc_buffer((1,), "int32", scope="local")
                            iterator = T.alloc_buffer((1,), "int32", scope="local")
                            kv_chunk_len = T.alloc_buffer((1,), "int32", scope="local")
                            Q_smem = T.alloc_buffer((32, 80), "float16", scope="shared")
                            K_smem = T.alloc_buffer((16, 80), "float16", scope="shared")
                            V_smem = T.alloc_buffer((16, 80), "float16", scope="shared")
                            S_smem = T.alloc_buffer((32, 16), scope="shared")
                            S_local = T.alloc_buffer((32, 16), scope="local")
                            O_local = T.alloc_buffer((32, 80), scope="local")
                            m_smem = T.alloc_buffer((32,), scope="shared")
                            m_prev_smem = T.alloc_buffer((32,), scope="shared")
                            d_smem = T.alloc_buffer((32,), scope="shared")
                            m_new = T.alloc_buffer((1,), scope="local")
                            m_prev = T.alloc_buffer((1,), scope="local")
                            d_new = T.alloc_buffer((1,), scope="local")
                            tile_id[0] = bx
                            batch_idx[0] = 0
                            batch_rows[0] = q_indptr[1] - q_indptr[0]
                            batch_tiles[0] = (batch_rows[0] + 32 - 1) // 32
                            while T.tvm_thread_invariant(batch_idx[0] < batch_size):
                                while tile_id[0] >= batch_tiles[0] and batch_idx[0] < batch_size:
                                    tile_id[0] = tile_id[0] - batch_tiles[0]
                                    batch_idx[0] = batch_idx[0] + 1
                                    if batch_idx[0] < batch_size:
                                        b_idx: T.int32 = batch_idx[0]
                                        batch_rows[0] = q_indptr[b_idx + 1] - q_indptr[b_idx]
                                        batch_tiles[0] = (batch_rows[0] + 32 - 1) // 32
                                if T.tvm_thread_invariant(batch_idx[0] < batch_size):
                                    b_idx: T.int32 = batch_idx[0]
                                    LH_start: T.int32 = tile_id[0] * 32
                                    q_indptr_val: T.int32 = q_indptr[b_idx]
                                    kv_chunk_len[0] = kv_indptr[b_idx + 1] - kv_indptr[b_idx]
                                    T.tvm_storage_sync("shared")
                                    for i in range(1):
                                        row: T.int32 = i * 32 * 4 + ty * 32 + tx
                                        if row < 32:
                                            m_smem[row] = T.float32(-50000.0)
                                            d_smem[row] = T.float32(1.0)
                                    for li_0_lj_0_fused_0 in T.thread_binding(4, thread="threadIdx.y"):
                                        for li_0_lj_0_fused_1 in T.thread_binding(32, thread="threadIdx.x"):
                                            for li_1, lj_1 in T.grid(4, 5):
                                                with T.block("O_init"):
                                                    i = T.axis.spatial(32, (li_0_lj_0_fused_0 * 32 + li_0_lj_0_fused_1) // 16 * 4 + li_1)
                                                    j = T.axis.spatial(80, (li_0_lj_0_fused_0 * 32 + li_0_lj_0_fused_1) % 16 * 5 + lj_1)
                                                    T.reads()
                                                    T.writes(O_local[i, j])
                                                    O_local[i, j] = T.float32(0.0)
                                    T.tvm_storage_sync("shared")
                                    for li_lj_fused_0 in range(5):
                                        for li_lj_fused_1 in T.thread_binding(4, thread="threadIdx.y"):
                                            for li_lj_fused_2 in T.thread_binding(32, thread="threadIdx.x"):
                                                for li_lj_fused_3 in T.vectorized(4):
                                                    with T.block("Q_load"):
                                                        i = T.axis.spatial(32, (li_lj_fused_0 * 512 + li_lj_fused_1 * 128 + li_lj_fused_2 * 4 + li_lj_fused_3) // 80)
                                                        j = T.axis.spatial(80, (li_lj_fused_0 * 512 + li_lj_fused_1 * 128 + li_lj_fused_2 * 4 + li_lj_fused_3) % 80)
                                                        T.reads()
                                                        T.writes()
                                                        cur_L: T.int32 = q_indptr_val + (LH_start + i)
                                                        cur_H_qo: T.int32 = by
                                                        if cur_L < q_indptr[b_idx + 1]:
                                                            freq = T.float32()
                                                            Q_smem[i, j] = T.if_then_else(rotary_mode == 1, T.Let(T.Cast("float16", T.cos(freq) * T.Cast("float32", q[cur_L, cur_H_qo, j]) + T.sin(freq) * T.Cast("float32", T.if_then_else(j < 40, q[cur_L, cur_H_qo, j + 40] * T.float16(-1.0), q[cur_L, cur_H_qo, j - 40]))), where={freq: T.Cast("float32", q_rope_position[cur_L]) * rope_scale / T.pow(rope_theta, T.Cast("float32", j * 2 % 80) / T.float32(80.0))}), q[cur_L, cur_H_qo, j])
                                                        else:
                                                            Q_smem[i, j] = T.float16(0.0)
                                    T.tvm_storage_sync("shared")
                                    for iterator_1 in range((kv_chunk_len[0] + 15) // 16):
                                        L_kv_start: T.int32 = iterator_1 * 16
                                        L_kv_base: T.int32 = kv_indptr[b_idx]
                                        for lz_ly_fused_0 in range(3):
                                            for lz_ly_fused_1 in T.thread_binding(4, thread="threadIdx.y"):
                                                for lz_ly_fused_2 in T.thread_binding(32, thread="threadIdx.x"):
                                                    for lz_ly_fused_3 in T.vectorized(4):
                                                        with T.block("KV_load"):
                                                            i = T.axis.spatial(16, (lz_ly_fused_0 * 512 + lz_ly_fused_1 * 128 + lz_ly_fused_2 * 4 + lz_ly_fused_3) // 80)
                                                            j = T.axis.spatial(80, (lz_ly_fused_0 * 512 + lz_ly_fused_1 * 128 + lz_ly_fused_2 * 4 + lz_ly_fused_3) % 80)
                                                            T.where(((lz_ly_fused_0 * 4 + lz_ly_fused_1) * 32 + lz_ly_fused_2) * 4 + lz_ly_fused_3 < 1280)
                                                            T.reads()
                                                            T.writes()
                                                            cur_L: T.int32 = L_kv_base + L_kv_start + i
                                                            if L_kv_start + i < kv_chunk_len[0]:
                                                                freq = T.float32()
                                                                K_smem[i, j] = T.if_then_else(rotary_mode == 1, T.Let(T.Cast("float16", T.cos(freq) * T.Cast("float32", k[cur_L, by, j]) + T.sin(freq) * T.Cast("float32", T.if_then_else(j < 40, k[cur_L, by, j + 40] * T.float16(-1.0), k[cur_L, by, j - 40]))), where={freq: T.Cast("float32", q_rope_position[cur_L]) * rope_scale / T.pow(rope_theta, T.Cast("float32", j * 2 % 80) / T.float32(80.0))}), k[cur_L, by, j])
                                                                V_smem[i, j] = v[cur_L, by, j]
                                                            else:
                                                                K_smem[i, j] = T.float16(0.0)
                                                                V_smem[i, j] = T.float16(0.0)
                                        T.tvm_storage_sync("shared")
                                        with T.block(""):
                                            T.reads(Q_smem[0:32, 0:80], K_smem[0:16, 0:80])
                                            T.writes(S_local[0:32, 0:16])
                                            for li_0_lj_0_fused_0_init in T.thread_binding(4, thread="threadIdx.y"):
                                                for li_0_lj_0_fused_1_init in T.thread_binding(32, thread="threadIdx.x"):
                                                    for li_1_init, lj_1_init in T.grid(2, 2):
                                                        with T.block("S_gemm_init"):
                                                            i = T.axis.spatial(32, (li_0_lj_0_fused_0_init * 32 + li_0_lj_0_fused_1_init) // 8 * 2 + li_1_init)
                                                            j = T.axis.spatial(16, (li_0_lj_0_fused_0_init * 32 + li_0_lj_0_fused_1_init) % 8 * 2 + lj_1_init)
                                                            T.reads()
                                                            T.writes(S_local[i, j])
                                                            S_local[i, j] = T.float32(0.0)
                                            for li_0_lj_0_fused_0 in T.thread_binding(4, thread="threadIdx.y"):
                                                for li_0_lj_0_fused_1 in T.thread_binding(32, thread="threadIdx.x"):
                                                    for lk_0, li_1, lj_1, lk_1 in T.grid(10, 2, 2, 8):
                                                        with T.block("S_gemm_update"):
                                                            i = T.axis.spatial(32, (li_0_lj_0_fused_0 * 32 + li_0_lj_0_fused_1) // 8 * 2 + li_1)
                                                            j = T.axis.spatial(16, (li_0_lj_0_fused_0 * 32 + li_0_lj_0_fused_1) % 8 * 2 + lj_1)
                                                            k_1 = T.axis.reduce(80, lk_0 * 8 + lk_1)
                                                            T.reads(S_local[i, j], Q_smem[i, k_1], K_smem[j, k_1])
                                                            T.writes(S_local[i, j])
                                                            S_local[i, j] = S_local[i, j] + T.Cast("float32", Q_smem[i, k_1]) * T.Cast("float32", K_smem[j, k_1]) * attn_score_scaling_factor * T.float32(0.16129820911147805)
                                        T.tvm_storage_sync("shared")
                                        for li_0_lj_0_fused_0 in T.thread_binding(4, thread="threadIdx.y"):
                                            for li_0_lj_0_fused_1 in T.thread_binding(32, thread="threadIdx.x"):
                                                for li_1, lj_1 in T.grid(2, 2):
                                                    with T.block("S_store"):
                                                        i = T.axis.spatial(32, (li_0_lj_0_fused_0 * 32 + li_0_lj_0_fused_1) // 8 * 2 + li_1)
                                                        j = T.axis.spatial(16, (li_0_lj_0_fused_0 * 32 + li_0_lj_0_fused_1) % 8 * 2 + lj_1)
                                                        T.reads(S_local[i, j])
                                                        T.writes(S_smem[i, j])
                                                        S_smem[i, j] = S_local[i, j]
                                        T.tvm_storage_sync("shared")
                                        for i in range(1):
                                            row: T.int32 = i * 32 * 4 + ty * 32 + tx
                                            if row < 32:
                                                with T.block("update1"):
                                                    T.reads(m_smem[row], kv_chunk_len[0], mn_indptr[b_idx:b_idx + 2], mask[T.min(LH_start + row + mn_indptr[b_idx + 1] + q_indptr[b_idx] - q_indptr[b_idx + 1], L_kv_start + mn_indptr[b_idx + 1] - kv_chunk_len[0]):T.min(LH_start + row + mn_indptr[b_idx + 1] + q_indptr[b_idx] - q_indptr[b_idx + 1], L_kv_start + mn_indptr[b_idx + 1] - kv_chunk_len[0]) + (T.max(LH_start + row + mn_indptr[b_idx + 1] + q_indptr[b_idx] - q_indptr[b_idx + 1], L_kv_start + mn_indptr[b_idx + 1] + 15 - kv_chunk_len[0]) + 1 - T.min(LH_start + row + mn_indptr[b_idx + 1] + q_indptr[b_idx] - q_indptr[b_idx + 1], L_kv_start + mn_indptr[b_idx + 1] - kv_chunk_len[0])), 0:2], q_indptr[b_idx:b_idx + 2], m_new[i], S_smem[row, 0:16], d_smem[row], m_prev[i])
                                                    T.writes(m_prev[i], m_new[i], d_new[i])
                                                    m_prev[i] = m_smem[row]
                                                    m_new[i] = m_smem[row]
                                                    row_: T.int32 = LH_start + row
                                                    for j in range(16):
                                                        if L_kv_start + j < kv_chunk_len[0] and (L_kv_start + j < kv_chunk_len[0] - (mn_indptr[b_idx + 1] - mn_indptr[b_idx]) or mask[mn_indptr[b_idx] + (row_ + (mn_indptr[b_idx + 1] - mn_indptr[b_idx]) - (q_indptr[b_idx + 1] - q_indptr[b_idx])), 0] >= mask[mn_indptr[b_idx] + (L_kv_start + j - (kv_chunk_len[0] - (mn_indptr[b_idx + 1] - mn_indptr[b_idx]))), 0] and mask[mn_indptr[b_idx] + (row_ + (mn_indptr[b_idx + 1] - mn_indptr[b_idx]) - (q_indptr[b_idx + 1] - q_indptr[b_idx])), 0] < mask[mn_indptr[b_idx] + (L_kv_start + j - (kv_chunk_len[0] - (mn_indptr[b_idx + 1] - mn_indptr[b_idx]))), 1]):
                                                            m_new[i] = T.max(m_new[i], S_smem[row, j])
                                                    d_new[i] = d_smem[row] * T.exp2(m_prev[i] - m_new[i])
                                        for i in range(1):
                                            row: T.int32 = i * 32 * 4 + ty * 32 + tx
                                            with T.block("update"):
                                                T.reads(kv_chunk_len[0], mn_indptr[b_idx:b_idx + 2], mask[T.min(LH_start + row + mn_indptr[b_idx + 1] + q_indptr[b_idx] - q_indptr[b_idx + 1], L_kv_start + mn_indptr[b_idx + 1] - kv_chunk_len[0]):T.min(LH_start + row + mn_indptr[b_idx + 1] + q_indptr[b_idx] - q_indptr[b_idx + 1], L_kv_start + mn_indptr[b_idx + 1] - kv_chunk_len[0]) + (T.max(LH_start + row + mn_indptr[b_idx + 1] + q_indptr[b_idx] - q_indptr[b_idx + 1], L_kv_start + mn_indptr[b_idx + 1] + 15 - kv_chunk_len[0]) + 1 - T.min(LH_start + row + mn_indptr[b_idx + 1] + q_indptr[b_idx] - q_indptr[b_idx + 1], L_kv_start + mn_indptr[b_idx + 1] - kv_chunk_len[0])), 0:2], q_indptr[b_idx:b_idx + 2], S_smem[row, 0:16], m_new[i])
                                                T.writes(S_smem[row, 0:16])
                                                for j in range(16):
                                                    if row < 32:
                                                        row_: T.int32 = LH_start + row
                                                        if L_kv_start + j < kv_chunk_len[0] and (L_kv_start + j < kv_chunk_len[0] - (mn_indptr[b_idx + 1] - mn_indptr[b_idx]) or mask[mn_indptr[b_idx] + (row_ + (mn_indptr[b_idx + 1] - mn_indptr[b_idx]) - (q_indptr[b_idx + 1] - q_indptr[b_idx])), 0] >= mask[mn_indptr[b_idx] + (L_kv_start + j - (kv_chunk_len[0] - (mn_indptr[b_idx + 1] - mn_indptr[b_idx]))), 0] and mask[mn_indptr[b_idx] + (row_ + (mn_indptr[b_idx + 1] - mn_indptr[b_idx]) - (q_indptr[b_idx + 1] - q_indptr[b_idx])), 0] < mask[mn_indptr[b_idx] + (L_kv_start + j - (kv_chunk_len[0] - (mn_indptr[b_idx + 1] - mn_indptr[b_idx]))), 1]):
                                                            S_smem[row, j] = T.exp2(S_smem[row, j] - m_new[i])
                                                        else:
                                                            S_smem[row, j] = T.exp2(T.float32(-50000.0) - m_new[i])
                                        for i in range(1):
                                            row: T.int32 = i * 32 * 4 + ty * 32 + tx
                                            if row < 32:
                                                with T.block("update"):
                                                    T.reads(d_new[i], S_smem[row, 0:16], m_new[i], m_prev[i])
                                                    T.writes(d_new[i], m_smem[row], d_smem[row], m_prev_smem[row])
                                                    for j in range(16):
                                                        d_new[i] = d_new[i] + S_smem[row, j]
                                                    m_smem[row] = m_new[i]
                                                    d_smem[row] = d_new[i]
                                                    m_prev_smem[row] = m_prev[i]
                                        T.tvm_storage_sync("shared")
                                        with T.block(""):
                                            T.reads(m_prev_smem[0:32], m_smem[0:32], S_smem[0:32, 0:16], V_smem[0:16, 0:80])
                                            T.writes(O_local[0:32, 0:80])
                                            for li_0_lj_0_fused_0_init in T.thread_binding(4, thread="threadIdx.y"):
                                                for li_0_lj_0_fused_1_init in T.thread_binding(32, thread="threadIdx.x"):
                                                    for li_1_init, lj_1_init in T.grid(4, 5):
                                                        with T.block("O_gemm_init"):
                                                            i = T.axis.spatial(32, (li_0_lj_0_fused_0_init * 32 + li_0_lj_0_fused_1_init) // 16 * 4 + li_1_init)
                                                            j = T.axis.spatial(80, (li_0_lj_0_fused_0_init * 32 + li_0_lj_0_fused_1_init) % 16 * 5 + lj_1_init)
                                                            T.reads()
                                                            T.writes(O_local[i, j])
                                                            O_local[i, j] = O_local[i, j] * T.exp2(m_prev_smem[i] - m_smem[i])
                                            for li_0_lj_0_fused_0 in T.thread_binding(4, thread="threadIdx.y"):
                                                for li_0_lj_0_fused_1 in T.thread_binding(32, thread="threadIdx.x"):
                                                    for lk_0, lk_1, li_1, lj_1 in T.grid(2, 8, 4, 5):
                                                        with T.block("O_gemm_update"):
                                                            i = T.axis.spatial(32, (li_0_lj_0_fused_0 * 32 + li_0_lj_0_fused_1) // 16 * 4 + li_1)
                                                            j = T.axis.spatial(80, (li_0_lj_0_fused_0 * 32 + li_0_lj_0_fused_1) % 16 * 5 + lj_1)
                                                            k_1 = T.axis.reduce(16, lk_0 * 8 + lk_1)
                                                            T.reads(O_local[i, j], m_prev_smem[i], m_smem[i], S_smem[i, k_1], V_smem[k_1, j])
                                                            T.writes(O_local[i, j])
                                                            O_local[i, j] = O_local[i, j] + S_smem[i, k_1] * T.Cast("float32", V_smem[k_1, j])
                                    for li_0_lj_0_fused_0 in T.thread_binding(4, thread="threadIdx.y"):
                                        for li_0_lj_0_fused_1 in T.thread_binding(32, thread="threadIdx.x"):
                                            for li_1, lj_1 in T.grid(4, 5):
                                                with T.block("O_store"):
                                                    i = T.axis.spatial(32, (li_0_lj_0_fused_0 * 32 + li_0_lj_0_fused_1) // 16 * 4 + li_1)
                                                    j = T.axis.spatial(80, (li_0_lj_0_fused_0 * 32 + li_0_lj_0_fused_1) % 16 * 5 + lj_1)
                                                    T.reads(q_indptr[b_idx:b_idx + 2], O_local[i, j], d_smem[i])
                                                    T.writes(output[q_indptr[b_idx] + (LH_start + i), by, j])
                                                    cur_L: T.int32 = q_indptr[b_idx] + (LH_start + i)
                                                    cur_H_qo: T.int32 = by
                                                    if cur_L < q_indptr[b_idx + 1]:
                                                        output[cur_L, cur_H_qo, j] = T.Cast("float16", O_local[i, j] / d_smem[i])
                                    for li_0 in range(1):
                                        for li_1 in T.thread_binding(4, thread="threadIdx.y"):
                                            for li_2 in T.thread_binding(32, thread="threadIdx.x"):
                                                with T.block("lse_store"):
                                                    i = T.axis.spatial(32, li_0 * 128 + li_1 * 32 + li_2)
                                                    T.where((li_0 * 4 + li_1) * 32 + li_2 < 32)
                                                    T.reads(q_indptr[b_idx:b_idx + 2], m_smem[i], d_smem[i])
                                                    T.writes(lse[q_indptr[b_idx] + (LH_start + i), by])
                                                    cur_L: T.int32 = q_indptr[b_idx] + (LH_start + i)
                                                    cur_H_qo: T.int32 = by
                                                    if cur_L < q_indptr[b_idx + 1]:
                                                        lse[cur_L, cur_H_qo] = m_smem[i] + T.log2(d_smem[i])
                                    tile_id[0] = tile_id[0] + 16

    @T.prim_func(private=True)
    def batch_verify_on_gpu_single_kernel(var_draft_probs: T.handle, var_draft_tokens: T.handle, var_model_probs: T.handle, var_token_tree_first_child: T.handle, var_token_tree_next_sibling: T.handle, var_uniform_samples: T.handle, var_token_tree_parent_ptr: T.handle):
        T.func_attr({"target": T.target({"arch": "sm_75", "keys": ["cuda", "gpu"], "kind": "cuda", "libs": ["thrust"], "max_num_threads": 1024, "max_shared_memory_per_block": 49152, "max_threads_per_block": 1024, "tag": "", "thread_warp_size": 32}), "tir.is_scheduled": 1, "tir.noalias": T.bool(True)})
        num_nodes, vocab_size = T.int32(is_size_var=True), T.int64(is_size_var=True)
        draft_probs = T.match_buffer(var_draft_probs, (num_nodes, vocab_size))
        draft_tokens = T.match_buffer(var_draft_tokens, (num_nodes,), "int32")
        model_probs = T.match_buffer(var_model_probs, (num_nodes, vocab_size))
        token_tree_first_child = T.match_buffer(var_token_tree_first_child, (num_nodes,), "int32")
        token_tree_next_sibling = T.match_buffer(var_token_tree_next_sibling, (num_nodes,), "int32")
        uniform_samples = T.match_buffer(var_uniform_samples, (num_nodes,))
        nbatch = T.int32(is_size_var=True)
        token_tree_parent_ptr = T.match_buffer(var_token_tree_parent_ptr, (nbatch,), "int32")
        # with T.block("root"):
        child_ptr = T.alloc_buffer((1,), "int32", scope="local")
        parent_ptr = T.alloc_buffer((1,), "int32", scope="local")
        child_token = T.alloc_buffer((1,), "int32", scope="local")
        done = T.alloc_buffer((1,), "bool", scope="local")
        psum = T.alloc_buffer((1,), scope="local")
        t0 = T.alloc_buffer((1,), scope="local")
        model_prob_local = T.alloc_buffer((1,), scope="local")
        draft_prob_local = T.alloc_buffer((1,), scope="local")
        p_child = T.alloc_buffer((1,), scope="local")
        q_child = T.alloc_buffer((1,), scope="local")
        uniform_sample = T.alloc_buffer((1,), scope="local")
        pred_shared = T.alloc_buffer((1,), "bool", scope="shared")
        pred_local = T.alloc_buffer((1,), "bool", scope="local")
        for _bx in T.thread_binding(nbatch, thread="blockIdx.x"):
            for _tx in T.thread_binding(1024, thread="threadIdx.x"):
                with T.block("CTA"):
                    b, tx = T.axis.remap("SS", [_bx, _tx])
                    T.reads(token_tree_parent_ptr[b], token_tree_first_child[T.min(parent_ptr[0], child_ptr[0]):T.min(parent_ptr[0], child_ptr[0]) + (T.max(parent_ptr[0], child_ptr[0]) + 1 - T.min(parent_ptr[0], child_ptr[0]))], parent_ptr[0], done[0], child_ptr[0], draft_tokens[child_ptr[0]], model_probs[parent_ptr[0], T.min(T.Cast("int64", child_token[0]), T.Cast("int64", tx)):T.min(T.Cast("int64", child_token[0]), T.Cast("int64", tx)) + (T.max(T.Cast("int64", child_token[0]), (vocab_size + T.int64(1023)) // T.int64(1024) * T.int64(1024) + T.Cast("int64", tx) - T.int64(1024)) + T.int64(1) - T.min(T.Cast("int64", child_token[0]), T.Cast("int64", tx)))], child_token[0], draft_probs[child_ptr[0], T.min(T.Cast("int64", child_token[0]), T.Cast("int64", tx)):T.min(T.Cast("int64", child_token[0]), T.Cast("int64", tx)) + (T.max(T.Cast("int64", child_token[0]), (vocab_size + T.int64(1023)) // T.int64(1024) * T.int64(1024) + T.Cast("int64", tx) - T.int64(1024)) + T.int64(1) - T.min(T.Cast("int64", child_token[0]), T.Cast("int64", tx)))], uniform_samples[child_ptr[0]], p_child[0], uniform_sample[0], q_child[0], pred_shared[0], pred_local[0], model_prob_local[0], draft_prob_local[0], psum[0], t0[0], token_tree_next_sibling[child_ptr[0]])
                    T.writes(parent_ptr[0], child_ptr[0], done[0], child_token[0], p_child[0], q_child[0], uniform_sample[0], pred_shared[0], pred_local[0], psum[0], model_prob_local[0], draft_prob_local[0], t0[0], model_probs[parent_ptr[0], T.Cast("int64", tx):T.Cast("int64", tx) + ((vocab_size + T.int64(1023)) // T.int64(1024) * T.int64(1024) - T.int64(1023))], token_tree_parent_ptr[b])
                    parent_ptr[0] = token_tree_parent_ptr[b]
                    child_ptr[0] = token_tree_first_child[parent_ptr[0]]
                    done[0] = T.bool(False)
                    while not done[0]:
                        T.tvm_storage_sync("shared")
                        if child_ptr[0] == -1:
                            done[0] = T.bool(True)
                            T.tvm_storage_sync("shared")
                        else:
                            if tx == 0:
                                child_token[0] = draft_tokens[child_ptr[0]]
                                p_child[0] = model_probs[parent_ptr[0], child_token[0]]
                                q_child[0] = draft_probs[child_ptr[0], child_token[0]]
                                uniform_sample[0] = uniform_samples[child_ptr[0]]
                                pred_shared[0] = p_child[0] >= uniform_sample[0] * q_child[0]
                            T.tvm_storage_sync("shared")
                            pred_local[0] = pred_shared[0]
                            if pred_local[0]:
                                parent_ptr[0] = child_ptr[0]
                                child_ptr[0] = token_tree_first_child[child_ptr[0]]
                            else:
                                psum[0] = T.float32(0.0)
                                for i in range((vocab_size + T.int64(1023)) // T.int64(1024)):
                                    if i * T.int64(1024) + T.Cast("int64", tx) < vocab_size:
                                        model_prob_local[0] = model_probs[parent_ptr[0], i * T.int64(1024) + T.Cast("int64", tx)]
                                        draft_prob_local[0] = draft_probs[child_ptr[0], i * T.int64(1024) + T.Cast("int64", tx)]
                                        model_prob_local[0] = T.max(model_prob_local[0] - draft_prob_local[0], T.float32(0.0))
                                        psum[0] = psum[0] + model_prob_local[0]
                                with T.block("block_cross_thread"):
                                    T.reads(psum[0])
                                    T.writes(t0[0])
                                    T.attr(T.comm_reducer(lambda x0, y0: x0 + y0, [T.float32(0.0)]), "reduce_scope", T.reinterpret("handle", T.uint64(0)))
                                    T.tvm_thread_allreduce(T.uint32(1), psum[0], T.bool(True), t0[0], tx)
                                if t0[0] < T.float32(9.9999999999999995e-08):
                                    parent_ptr[0] = child_ptr[0]
                                    child_ptr[0] = token_tree_first_child[child_ptr[0]]
                                else:
                                    for i in range((vocab_size + T.int64(1023)) // T.int64(1024)):
                                        if i * T.int64(1024) + T.Cast("int64", tx) < vocab_size:
                                            model_prob_local[0] = model_probs[parent_ptr[0], i * T.int64(1024) + T.Cast("int64", tx)]
                                            draft_prob_local[0] = draft_probs[child_ptr[0], i * T.int64(1024) + T.Cast("int64", tx)]
                                            model_prob_local[0] = T.max(model_prob_local[0] - draft_prob_local[0], T.float32(0.0))
                                            model_probs[parent_ptr[0], i * T.int64(1024) + T.Cast("int64", tx)] = model_prob_local[0] / t0[0]
                                    child_ptr[0] = token_tree_next_sibling[child_ptr[0]]
                    if tx == 0:
                        token_tree_parent_ptr[b] = parent_ptr[0]

    @T.prim_func
    def chunk_lse(var_A: T.handle, var_temperature: T.handle, var_chunked_sum: T.handle, var_chunked_max: T.handle):
        T.func_attr({"target": T.target({"arch": "sm_75", "host": {"keys": ["cpu"], "kind": "llvm", "mcpu": "skylake", "mtriple": "x86_64-conda-linux-gnu", "tag": ""}, "keys": ["cuda", "gpu"], "kind": "cuda", "libs": ["thrust"], "max_num_threads": 1024, "max_shared_memory_per_block": 49152, "max_threads_per_block": 1024, "tag": "", "thread_warp_size": 32}), "tir.noalias": T.bool(True)})
        batch_size, vocab_size = T.int64(is_size_var=True), T.int64(is_size_var=True)
        A = T.match_buffer(var_A, (batch_size, vocab_size))
        temperature = T.match_buffer(var_temperature, (batch_size,))
        num_chunks = T.int64(is_size_var=True)
        chunked_sum = T.match_buffer(var_chunked_sum, (batch_size, num_chunks))
        chunked_max = T.match_buffer(var_chunked_max, (batch_size, num_chunks))
        # with T.block("root"):
        A_pad = T.alloc_buffer((batch_size, num_chunks, T.int64(4096)))
        temp_max = T.alloc_buffer((batch_size, num_chunks))
        temp_sum = T.alloc_buffer((batch_size, num_chunks))
        for l0, l1, l2 in T.grid(batch_size, num_chunks, T.int64(4096)):
            with T.block("pad"):
                v0, v1, v2 = T.axis.remap("SSS", [l0, l1, l2])
                T.reads(temperature[v0], A[v0, v1 * T.int64(4096) + v2])
                T.writes(A_pad[v0, v1, v2])
                A_pad[v0, v1, v2] = T.if_then_else(v1 * T.int64(4096) + v2 < vocab_size, T.if_then_else(temperature[v0] > T.float32(1.0000000000000001e-05), A[v0, v1 * T.int64(4096) + v2] / temperature[v0], A[v0, v1 * T.int64(4096) + v2]), T.float32(-340282346638528859811704183484516925440.0))
        for l0, l1, l2 in T.grid(batch_size, num_chunks, T.int64(4096)):
            with T.block("max"):
                v0, v1, v2 = T.axis.remap("SSR", [l0, l1, l2])
                T.reads(A_pad[v0, v1, v2])
                T.writes(temp_max[v0, v1])
                with T.init():
                    temp_max[v0, v1] = T.float32(-340282346638528859811704183484516925440.0)
                temp_max[v0, v1] = T.max(temp_max[v0, v1], A_pad[v0, v1, v2])
        for l0, l1, l2 in T.grid(batch_size, num_chunks, T.int64(4096)):
            with T.block("sum_exp"):
                v0, v1, v2 = T.axis.remap("SSR", [l0, l1, l2])
                T.reads(temperature[v0], A_pad[v0, v1, v2], temp_max[v0, v1])
                T.writes(temp_sum[v0, v1])
                with T.init():
                    temp_sum[v0, v1] = T.float32(0.0)
                temp_sum[v0, v1] = temp_sum[v0, v1] + T.if_then_else(v1 * T.int64(4096) + v2 < vocab_size, T.Select(temperature[v0] > T.float32(1.0000000000000001e-05), T.exp(A_pad[v0, v1, v2] - temp_max[v0, v1]), T.Cast("float32", A_pad[v0, v1, v2] == temp_max[v0, v1])), T.float32(0.0))
        for l0, l1, l2 in T.grid(batch_size, num_chunks, T.int64(1)):
            with T.block("log"):
                v0, v1, v2 = T.axis.remap("SSS", [l0, l1, l2])
                T.reads(temperature[v0], temp_sum[v0, v1], temp_max[v0, v1])
                T.writes(chunked_sum[v0, v1], chunked_max[v0, v1])
                chunked_sum[v0, v1] = T.Select(temperature[v0] > T.float32(1.0000000000000001e-05), T.log(temp_sum[v0, v1]), temp_sum[v0, v1])
                chunked_max[v0, v1] = temp_max[v0, v1]

    @T.prim_func
    def compact_kv_copy(var_pages: T.handle, var_copy_length_indptr: T.handle, var_copy_src_dst_pos: T.handle, batch_size: T.int32):
        T.func_attr({"target": T.target({"arch": "sm_75", "host": {"keys": ["cpu"], "kind": "llvm", "mcpu": "skylake", "mtriple": "x86_64-conda-linux-gnu", "tag": ""}, "keys": ["cuda", "gpu"], "kind": "cuda", "libs": ["thrust"], "max_num_threads": 1024, "max_shared_memory_per_block": 49152, "max_threads_per_block": 1024, "tag": "", "thread_warp_size": 32}), "tir.is_scheduled": 1})
        num_pages = T.int32()
        pages = T.match_buffer(var_pages, (num_pages, 2, 32, 16, 80), "float16")
        copy_length_indptr = T.match_buffer(var_copy_length_indptr, (batch_size + 1,), "int32", offset_factor=1)
        total_copy_length = T.int32()
        copy_src_dst_pos = T.match_buffer(var_copy_src_dst_pos, (2, total_copy_length), "int32", offset_factor=1)
        with T.block("root"):
            T.reads()
            T.writes()
            for bhd_o in T.thread_binding((batch_size * 2560 + 1023) // 1024, thread="blockIdx.x"):
                for bhd_i in T.thread_binding(1024, thread="threadIdx.x"):
                    b: T.int32 = (bhd_o * 1024 + bhd_i) // 2560
                    h: T.int32 = (bhd_o * 1024 + bhd_i) // 80 % 32
                    d: T.int32 = (bhd_o * 1024 + bhd_i) % 80
                    if bhd_o * 1024 + bhd_i < batch_size * 32 * 80:
                        for i in range(copy_length_indptr[b + 1] - copy_length_indptr[b]):
                            src_pos: T.int32 = copy_src_dst_pos[0, copy_length_indptr[b] + i]
                            dst_pos: T.int32 = copy_src_dst_pos[1, copy_length_indptr[b] + i]
                            pages[dst_pos // 16, 0, h, dst_pos % 16, d] = pages[src_pos // 16, 0, h, src_pos % 16, d]
                            pages[dst_pos // 16, 1, h, dst_pos % 16, d] = pages[src_pos // 16, 1, h, src_pos % 16, d]

    @T.prim_func
    def copy_single_page(var_pages: T.handle, src_page_id: T.int64, tgt_page_id: T.int64, copy_length: T.int64):
        T.func_attr({"target": T.target({"arch": "sm_75", "host": {"keys": ["cpu"], "kind": "llvm", "mcpu": "skylake", "mtriple": "x86_64-conda-linux-gnu", "tag": ""}, "keys": ["cuda", "gpu"], "kind": "cuda", "libs": ["thrust"], "max_num_threads": 1024, "max_shared_memory_per_block": 49152, "max_threads_per_block": 1024, "tag": "", "thread_warp_size": 32}), "tir.is_scheduled": 1})
        num_pages, page_size = T.int32(), T.int64()
        pages = T.match_buffer(var_pages, (num_pages, 2, 32, page_size, 80), "float16")
        # with T.block("root"):
        for b in T.thread_binding((copy_length * T.int64(2560) + T.int64(1023)) // T.int64(1024), thread="blockIdx.x"):
            for t in T.thread_binding(1024, thread="threadIdx.x"):
                with T.block("copy"):
                    vh = T.axis.spatial(32, T.Cast("int32", (b * T.int64(1024) + T.Cast("int64", t)) // (copy_length * T.int64(80))))
                    vp = T.axis.spatial(copy_length, (b * T.int64(1024) + T.Cast("int64", t)) % (copy_length * T.int64(80)) // T.int64(80))
                    vd = T.axis.spatial(80, T.Cast("int32", (b * T.int64(1024) + T.Cast("int64", t)) % T.int64(80)))
                    T.where(b * T.int64(1024) + T.Cast("int64", t) < copy_length * T.int64(32) * T.int64(80))
                    T.reads(pages[src_page_id, 0:2, vh, vp, vd])
                    T.writes(pages[tgt_page_id, 0:2, vh, vp, vd])
                    pages[tgt_page_id, 0, vh, vp, vd] = pages[src_page_id, 0, vh, vp, vd]
                    pages[tgt_page_id, 1, vh, vp, vd] = pages[src_page_id, 1, vh, vp, vd]

    @T.prim_func
    def full(var_result: T.handle, value: T.int32):
        T.func_attr({"target": T.target({"arch": "sm_75", "host": {"keys": ["cpu"], "kind": "llvm", "mcpu": "skylake", "mtriple": "x86_64-conda-linux-gnu", "tag": ""}, "keys": ["cuda", "gpu"], "kind": "cuda", "libs": ["thrust"], "max_num_threads": 1024, "max_shared_memory_per_block": 49152, "max_threads_per_block": 1024, "tag": "", "thread_warp_size": 32})})
        batch_size = T.int32(is_size_var=True)
        result = T.match_buffer(var_result, (batch_size, 1), "int32")
        # with T.block("root"):
        for i in range(batch_size):
            with T.block("block"):
                vi = T.axis.spatial(batch_size, i)
                T.reads()
                T.writes(result[vi, 0])
                result[vi, 0] = value

    @T.prim_func(private=True)
    def fused_dequantize1_fused_NT_matmul10_add10(gpt_neox_layers_0_attention_query_key_value_q_weight2: T.Buffer((T.int64(7680), T.int64(320)), "uint32"), gpt_neox_layers_0_attention_query_key_value_q_scale2: T.Buffer((T.int64(7680), T.int64(80)), "float16"), layer_norm65: T.Buffer((T.int64(1), T.int64(1), T.int64(2560)), "float16"), gpt_neox_layers_0_attention_query_key_value_bias2: T.Buffer((T.int64(7680),), "float16"), T_add_intermediate_intermediate: T.Buffer((T.int64(1), T.int64(1), T.int64(7680)), "float16")):
        T.func_attr({"tir.noalias": T.bool(True)})
        # with T.block("root"):
        compute = T.alloc_buffer((T.int64(7680), T.int64(2560)), "float16")
        dequantize_intermediate = T.alloc_buffer((T.int64(7680), T.int64(2560)), "float16")
        NT_matmul_intermediate = T.alloc_buffer((T.int64(1), T.int64(1), T.int64(7680)), "float16")
        for i0, i1 in T.grid(T.int64(7680), T.int64(2560)):
            with T.block("compute"):
                v_i0, v_i1 = T.axis.remap("SS", [i0, i1])
                T.reads(gpt_neox_layers_0_attention_query_key_value_q_weight2[v_i0, v_i1 // T.int64(8)])
                T.writes(compute[v_i0, v_i1])
                compute[v_i0, v_i1] = T.Cast("float16", T.bitwise_and(T.shift_right(gpt_neox_layers_0_attention_query_key_value_q_weight2[v_i0, v_i1 // T.int64(8)], T.Cast("uint32", v_i1 % T.int64(8) * T.int64(4))), T.uint32(15)))
        for i0, i1 in T.grid(T.int64(7680), T.int64(2560)):
            with T.block("dequantize"):
                v_i0, v_i1 = T.axis.remap("SS", [i0, i1])
                T.reads(compute[v_i0, v_i1], gpt_neox_layers_0_attention_query_key_value_q_scale2[v_i0, v_i1 // T.int64(32)])
                T.writes(dequantize_intermediate[v_i0, v_i1])
                dequantize_intermediate[v_i0, v_i1] = (compute[v_i0, v_i1] - T.float16(7.0)) * gpt_neox_layers_0_attention_query_key_value_q_scale2[v_i0, v_i1 // T.int64(32)]
        for i0, i1, i2, k in T.grid(T.int64(1), T.int64(1), T.int64(7680), T.int64(2560)):
            with T.block("NT_matmul"):
                v_i0, v_i1, v_i2, v_k = T.axis.remap("SSSR", [i0, i1, i2, k])
                T.reads(layer_norm65[v_i0, v_i1, v_k], dequantize_intermediate[v_i2, v_k])
                T.writes(NT_matmul_intermediate[v_i0, v_i1, v_i2])
                with T.init():
                    NT_matmul_intermediate[v_i0, v_i1, v_i2] = T.float16(0.0)
                NT_matmul_intermediate[v_i0, v_i1, v_i2] = NT_matmul_intermediate[v_i0, v_i1, v_i2] + layer_norm65[v_i0, v_i1, v_k] * dequantize_intermediate[v_i2, v_k]
        for ax0, ax1, ax2 in T.grid(T.int64(1), T.int64(1), T.int64(7680)):
            with T.block("T_add"):
                v_ax0, v_ax1, v_ax2 = T.axis.remap("SSS", [ax0, ax1, ax2])
                T.reads(NT_matmul_intermediate[v_ax0, v_ax1, v_ax2], gpt_neox_layers_0_attention_query_key_value_bias2[v_ax2])
                T.writes(T_add_intermediate_intermediate[v_ax0, v_ax1, v_ax2])
                T_add_intermediate_intermediate[v_ax0, v_ax1, v_ax2] = NT_matmul_intermediate[v_ax0, v_ax1, v_ax2] + gpt_neox_layers_0_attention_query_key_value_bias2[v_ax2]

    @T.prim_func(private=True)
    def fused_dequantize1_fused_NT_matmul5_add5(gpt_neox_layers_0_attention_query_key_value_q_weight3: T.Buffer((T.int64(7680), T.int64(320)), "uint32"), gpt_neox_layers_0_attention_query_key_value_q_scale3: T.Buffer((T.int64(7680), T.int64(80)), "float16"), p_layer_norm130: T.handle, gpt_neox_layers_0_attention_query_key_value_bias3: T.Buffer((T.int64(7680),), "float16"), p_output0: T.handle):
        T.func_attr({"tir.noalias": T.bool(True)})
        seq_len = T.int64()
        layer_norm130 = T.match_buffer(p_layer_norm130, (T.int64(1), seq_len, T.int64(2560)), "float16")
        T_add_intermediate_intermediate = T.match_buffer(p_output0, (T.int64(1), seq_len, T.int64(7680)), "float16")
        # with T.block("root"):
        compute = T.alloc_buffer((T.int64(7680), T.int64(2560)), "float16")
        dequantize_intermediate = T.alloc_buffer((T.int64(7680), T.int64(2560)), "float16")
        NT_matmul_intermediate = T.alloc_buffer((T.int64(1), seq_len, T.int64(7680)), "float16")
        for i0, i1 in T.grid(T.int64(7680), T.int64(2560)):
            with T.block("compute"):
                v_i0, v_i1 = T.axis.remap("SS", [i0, i1])
                T.reads(gpt_neox_layers_0_attention_query_key_value_q_weight3[v_i0, v_i1 // T.int64(8)])
                T.writes(compute[v_i0, v_i1])
                compute[v_i0, v_i1] = T.Cast("float16", T.bitwise_and(T.shift_right(gpt_neox_layers_0_attention_query_key_value_q_weight3[v_i0, v_i1 // T.int64(8)], T.Cast("uint32", v_i1 % T.int64(8) * T.int64(4))), T.uint32(15)))
        for i0, i1 in T.grid(T.int64(7680), T.int64(2560)):
            with T.block("dequantize"):
                v_i0, v_i1 = T.axis.remap("SS", [i0, i1])
                T.reads(compute[v_i0, v_i1], gpt_neox_layers_0_attention_query_key_value_q_scale3[v_i0, v_i1 // T.int64(32)])
                T.writes(dequantize_intermediate[v_i0, v_i1])
                dequantize_intermediate[v_i0, v_i1] = (compute[v_i0, v_i1] - T.float16(7.0)) * gpt_neox_layers_0_attention_query_key_value_q_scale3[v_i0, v_i1 // T.int64(32)]
        for i0, i1, i2, k in T.grid(T.int64(1), seq_len, T.int64(7680), T.int64(2560)):
            with T.block("NT_matmul"):
                v_i0, v_i1, v_i2, v_k = T.axis.remap("SSSR", [i0, i1, i2, k])
                T.reads(layer_norm130[v_i0, v_i1, v_k], dequantize_intermediate[v_i2, v_k])
                T.writes(NT_matmul_intermediate[v_i0, v_i1, v_i2])
                with T.init():
                    NT_matmul_intermediate[v_i0, v_i1, v_i2] = T.float16(0.0)
                NT_matmul_intermediate[v_i0, v_i1, v_i2] = NT_matmul_intermediate[v_i0, v_i1, v_i2] + layer_norm130[v_i0, v_i1, v_k] * dequantize_intermediate[v_i2, v_k]
        for ax0, ax1, ax2 in T.grid(T.int64(1), seq_len, T.int64(7680)):
            with T.block("T_add"):
                v_ax0, v_ax1, v_ax2 = T.axis.remap("SSS", [ax0, ax1, ax2])
                T.reads(NT_matmul_intermediate[v_ax0, v_ax1, v_ax2], gpt_neox_layers_0_attention_query_key_value_bias3[v_ax2])
                T.writes(T_add_intermediate_intermediate[v_ax0, v_ax1, v_ax2])
                T_add_intermediate_intermediate[v_ax0, v_ax1, v_ax2] = NT_matmul_intermediate[v_ax0, v_ax1, v_ax2] + gpt_neox_layers_0_attention_query_key_value_bias3[v_ax2]

    @T.prim_func(private=True)
    def fused_dequantize1_fused_NT_matmul_add(gpt_neox_layers_0_attention_query_key_value_q_weight4: T.Buffer((T.int64(7680), T.int64(320)), "uint32"), gpt_neox_layers_0_attention_query_key_value_q_scale4: T.Buffer((T.int64(7680), T.int64(80)), "float16"), p_layer_norm195: T.handle, gpt_neox_layers_0_attention_query_key_value_bias4: T.Buffer((T.int64(7680),), "float16"), p_output0: T.handle):
        T.func_attr({"tir.noalias": T.bool(True)})
        batch_size = T.int64()
        layer_norm195 = T.match_buffer(p_layer_norm195, (batch_size, T.int64(1), T.int64(2560)), "float16")
        T_add_intermediate_intermediate = T.match_buffer(p_output0, (batch_size, T.int64(1), T.int64(7680)), "float16")
        # with T.block("root"):
        compute = T.alloc_buffer((T.int64(7680), T.int64(2560)), "float16")
        dequantize_intermediate = T.alloc_buffer((T.int64(7680), T.int64(2560)), "float16")
        NT_matmul_intermediate = T.alloc_buffer((batch_size, T.int64(1), T.int64(7680)), "float16")
        for i0, i1 in T.grid(T.int64(7680), T.int64(2560)):
            with T.block("compute"):
                v_i0, v_i1 = T.axis.remap("SS", [i0, i1])
                T.reads(gpt_neox_layers_0_attention_query_key_value_q_weight4[v_i0, v_i1 // T.int64(8)])
                T.writes(compute[v_i0, v_i1])
                compute[v_i0, v_i1] = T.Cast("float16", T.bitwise_and(T.shift_right(gpt_neox_layers_0_attention_query_key_value_q_weight4[v_i0, v_i1 // T.int64(8)], T.Cast("uint32", v_i1 % T.int64(8) * T.int64(4))), T.uint32(15)))
        for i0, i1 in T.grid(T.int64(7680), T.int64(2560)):
            with T.block("dequantize"):
                v_i0, v_i1 = T.axis.remap("SS", [i0, i1])
                T.reads(compute[v_i0, v_i1], gpt_neox_layers_0_attention_query_key_value_q_scale4[v_i0, v_i1 // T.int64(32)])
                T.writes(dequantize_intermediate[v_i0, v_i1])
                dequantize_intermediate[v_i0, v_i1] = (compute[v_i0, v_i1] - T.float16(7.0)) * gpt_neox_layers_0_attention_query_key_value_q_scale4[v_i0, v_i1 // T.int64(32)]
        for i0, i1, i2, k in T.grid(batch_size, T.int64(1), T.int64(7680), T.int64(2560)):
            with T.block("NT_matmul"):
                v_i0, v_i1, v_i2, v_k = T.axis.remap("SSSR", [i0, i1, i2, k])
                T.reads(layer_norm195[v_i0, v_i1, v_k], dequantize_intermediate[v_i2, v_k])
                T.writes(NT_matmul_intermediate[v_i0, v_i1, v_i2])
                with T.init():
                    NT_matmul_intermediate[v_i0, v_i1, v_i2] = T.float16(0.0)
                NT_matmul_intermediate[v_i0, v_i1, v_i2] = NT_matmul_intermediate[v_i0, v_i1, v_i2] + layer_norm195[v_i0, v_i1, v_k] * dequantize_intermediate[v_i2, v_k]
        for ax0, ax1, ax2 in T.grid(batch_size, T.int64(1), T.int64(7680)):
            with T.block("T_add"):
                v_ax0, v_ax1, v_ax2 = T.axis.remap("SSS", [ax0, ax1, ax2])
                T.reads(NT_matmul_intermediate[v_ax0, v_ax1, v_ax2], gpt_neox_layers_0_attention_query_key_value_bias4[v_ax2])
                T.writes(T_add_intermediate_intermediate[v_ax0, v_ax1, v_ax2])
                T_add_intermediate_intermediate[v_ax0, v_ax1, v_ax2] = NT_matmul_intermediate[v_ax0, v_ax1, v_ax2] + gpt_neox_layers_0_attention_query_key_value_bias4[v_ax2]

    @T.prim_func(private=True)
    def fused_dequantize2_fused_NT_matmul11_add11_add12(gpt_neox_layers_0_attention_dense_q_weight2: T.Buffer((T.int64(2560), T.int64(320)), "uint32"), gpt_neox_layers_0_attention_dense_q_scale2: T.Buffer((T.int64(2560), T.int64(80)), "float16"), lv389: T.Buffer((T.int64(1), T.int64(1), T.int64(2560)), "float16"), gpt_neox_layers_0_attention_dense_bias2: T.Buffer((T.int64(2560),), "float16"), input_embed: T.Buffer((T.int64(1), T.int64(1), T.int64(2560)), "float16"), T_add_intermediate_1_intermediate: T.Buffer((T.int64(1), T.int64(1), T.int64(2560)), "float16")):
        T.func_attr({"tir.noalias": T.bool(True)})
        # with T.block("root"):
        compute = T.alloc_buffer((T.int64(2560), T.int64(2560)), "float16")
        dequantize_intermediate = T.alloc_buffer((T.int64(2560), T.int64(2560)), "float16")
        NT_matmul_intermediate = T.alloc_buffer((T.int64(1), T.int64(1), T.int64(2560)), "float16")
        T_add_intermediate = T.alloc_buffer((T.int64(1), T.int64(1), T.int64(2560)), "float16")
        for i0, i1 in T.grid(T.int64(2560), T.int64(2560)):
            with T.block("compute"):
                v_i0, v_i1 = T.axis.remap("SS", [i0, i1])
                T.reads(gpt_neox_layers_0_attention_dense_q_weight2[v_i0, v_i1 // T.int64(8)])
                T.writes(compute[v_i0, v_i1])
                compute[v_i0, v_i1] = T.Cast("float16", T.bitwise_and(T.shift_right(gpt_neox_layers_0_attention_dense_q_weight2[v_i0, v_i1 // T.int64(8)], T.Cast("uint32", v_i1 % T.int64(8) * T.int64(4))), T.uint32(15)))
        for i0, i1 in T.grid(T.int64(2560), T.int64(2560)):
            with T.block("dequantize"):
                v_i0, v_i1 = T.axis.remap("SS", [i0, i1])
                T.reads(compute[v_i0, v_i1], gpt_neox_layers_0_attention_dense_q_scale2[v_i0, v_i1 // T.int64(32)])
                T.writes(dequantize_intermediate[v_i0, v_i1])
                dequantize_intermediate[v_i0, v_i1] = (compute[v_i0, v_i1] - T.float16(7.0)) * gpt_neox_layers_0_attention_dense_q_scale2[v_i0, v_i1 // T.int64(32)]
        for i0, i1, i2, k in T.grid(T.int64(1), T.int64(1), T.int64(2560), T.int64(2560)):
            with T.block("NT_matmul"):
                v_i0, v_i1, v_i2, v_k = T.axis.remap("SSSR", [i0, i1, i2, k])
                T.reads(lv389[v_i0, v_i1, v_k], dequantize_intermediate[v_i2, v_k])
                T.writes(NT_matmul_intermediate[v_i0, v_i1, v_i2])
                with T.init():
                    NT_matmul_intermediate[v_i0, v_i1, v_i2] = T.float16(0.0)
                NT_matmul_intermediate[v_i0, v_i1, v_i2] = NT_matmul_intermediate[v_i0, v_i1, v_i2] + lv389[v_i0, v_i1, v_k] * dequantize_intermediate[v_i2, v_k]
        for ax0, ax1, ax2 in T.grid(T.int64(1), T.int64(1), T.int64(2560)):
            with T.block("T_add"):
                v_ax0, v_ax1, v_ax2 = T.axis.remap("SSS", [ax0, ax1, ax2])
                T.reads(NT_matmul_intermediate[v_ax0, v_ax1, v_ax2], gpt_neox_layers_0_attention_dense_bias2[v_ax2])
                T.writes(T_add_intermediate[v_ax0, v_ax1, v_ax2])
                T_add_intermediate[v_ax0, v_ax1, v_ax2] = NT_matmul_intermediate[v_ax0, v_ax1, v_ax2] + gpt_neox_layers_0_attention_dense_bias2[v_ax2]
        for ax0, ax1, ax2 in T.grid(T.int64(1), T.int64(1), T.int64(2560)):
            with T.block("T_add_1"):
                v_ax0, v_ax1, v_ax2 = T.axis.remap("SSS", [ax0, ax1, ax2])
                T.reads(T_add_intermediate[v_ax0, v_ax1, v_ax2], input_embed[v_ax0, v_ax1, v_ax2])
                T.writes(T_add_intermediate_1_intermediate[v_ax0, v_ax1, v_ax2])
                T_add_intermediate_1_intermediate[v_ax0, v_ax1, v_ax2] = T_add_intermediate[v_ax0, v_ax1, v_ax2] + input_embed[v_ax0, v_ax1, v_ax2]

    @T.prim_func(private=True)
    def fused_dequantize2_fused_NT_matmul1_add1_add2(gpt_neox_layers_0_attention_dense_q_weight4: T.Buffer((T.int64(2560), T.int64(320)), "uint32"), gpt_neox_layers_0_attention_dense_q_scale4: T.Buffer((T.int64(2560), T.int64(80)), "float16"), p_reshape387: T.handle, gpt_neox_layers_0_attention_dense_bias4: T.Buffer((T.int64(2560),), "float16"), p_input_embeds: T.handle, p_output0: T.handle):
        T.func_attr({"tir.noalias": T.bool(True)})
        batch_size = T.int64()
        reshape387 = T.match_buffer(p_reshape387, (batch_size, T.int64(1), T.int64(2560)), "float16")
        input_embeds = T.match_buffer(p_input_embeds, (batch_size, T.int64(1), T.int64(2560)), "float16")
        T_add_intermediate_1_intermediate = T.match_buffer(p_output0, (batch_size, T.int64(1), T.int64(2560)), "float16")
        # with T.block("root"):
        compute = T.alloc_buffer((T.int64(2560), T.int64(2560)), "float16")
        dequantize_intermediate = T.alloc_buffer((T.int64(2560), T.int64(2560)), "float16")
        NT_matmul_intermediate = T.alloc_buffer((batch_size, T.int64(1), T.int64(2560)), "float16")
        T_add_intermediate = T.alloc_buffer((batch_size, T.int64(1), T.int64(2560)), "float16")
        for i0, i1 in T.grid(T.int64(2560), T.int64(2560)):
            with T.block("compute"):
                v_i0, v_i1 = T.axis.remap("SS", [i0, i1])
                T.reads(gpt_neox_layers_0_attention_dense_q_weight4[v_i0, v_i1 // T.int64(8)])
                T.writes(compute[v_i0, v_i1])
                compute[v_i0, v_i1] = T.Cast("float16", T.bitwise_and(T.shift_right(gpt_neox_layers_0_attention_dense_q_weight4[v_i0, v_i1 // T.int64(8)], T.Cast("uint32", v_i1 % T.int64(8) * T.int64(4))), T.uint32(15)))
        for i0, i1 in T.grid(T.int64(2560), T.int64(2560)):
            with T.block("dequantize"):
                v_i0, v_i1 = T.axis.remap("SS", [i0, i1])
                T.reads(compute[v_i0, v_i1], gpt_neox_layers_0_attention_dense_q_scale4[v_i0, v_i1 // T.int64(32)])
                T.writes(dequantize_intermediate[v_i0, v_i1])
                dequantize_intermediate[v_i0, v_i1] = (compute[v_i0, v_i1] - T.float16(7.0)) * gpt_neox_layers_0_attention_dense_q_scale4[v_i0, v_i1 // T.int64(32)]
        for i0, i1, i2, k in T.grid(batch_size, T.int64(1), T.int64(2560), T.int64(2560)):
            with T.block("NT_matmul"):
                v_i0, v_i1, v_i2, v_k = T.axis.remap("SSSR", [i0, i1, i2, k])
                T.reads(reshape387[v_i0, v_i1, v_k], dequantize_intermediate[v_i2, v_k])
                T.writes(NT_matmul_intermediate[v_i0, v_i1, v_i2])
                with T.init():
                    NT_matmul_intermediate[v_i0, v_i1, v_i2] = T.float16(0.0)
                NT_matmul_intermediate[v_i0, v_i1, v_i2] = NT_matmul_intermediate[v_i0, v_i1, v_i2] + reshape387[v_i0, v_i1, v_k] * dequantize_intermediate[v_i2, v_k]
        for ax0, ax1, ax2 in T.grid(batch_size, T.int64(1), T.int64(2560)):
            with T.block("T_add"):
                v_ax0, v_ax1, v_ax2 = T.axis.remap("SSS", [ax0, ax1, ax2])
                T.reads(NT_matmul_intermediate[v_ax0, v_ax1, v_ax2], gpt_neox_layers_0_attention_dense_bias4[v_ax2])
                T.writes(T_add_intermediate[v_ax0, v_ax1, v_ax2])
                T_add_intermediate[v_ax0, v_ax1, v_ax2] = NT_matmul_intermediate[v_ax0, v_ax1, v_ax2] + gpt_neox_layers_0_attention_dense_bias4[v_ax2]
        for ax0, ax1, ax2 in T.grid(batch_size, T.int64(1), T.int64(2560)):
            with T.block("T_add_1"):
                v_ax0, v_ax1, v_ax2 = T.axis.remap("SSS", [ax0, ax1, ax2])
                T.reads(T_add_intermediate[v_ax0, v_ax1, v_ax2], input_embeds[v_ax0, v_ax1, v_ax2])
                T.writes(T_add_intermediate_1_intermediate[v_ax0, v_ax1, v_ax2])
                T_add_intermediate_1_intermediate[v_ax0, v_ax1, v_ax2] = T_add_intermediate[v_ax0, v_ax1, v_ax2] + input_embeds[v_ax0, v_ax1, v_ax2]

    @T.prim_func(private=True)
    def fused_dequantize2_fused_NT_matmul6_add6_add7(gpt_neox_layers_0_attention_dense_q_weight3: T.Buffer((T.int64(2560), T.int64(320)), "uint32"), gpt_neox_layers_0_attention_dense_q_scale3: T.Buffer((T.int64(2560), T.int64(80)), "float16"), p_reshape259: T.handle, gpt_neox_layers_0_attention_dense_bias3: T.Buffer((T.int64(2560),), "float16"), p_input_embeds: T.handle, p_output0: T.handle):
        T.func_attr({"tir.noalias": T.bool(True)})
        seq_len = T.int64()
        reshape259 = T.match_buffer(p_reshape259, (T.int64(1), seq_len, T.int64(2560)), "float16")
        input_embeds = T.match_buffer(p_input_embeds, (T.int64(1), seq_len, T.int64(2560)), "float16")
        T_add_intermediate_1_intermediate = T.match_buffer(p_output0, (T.int64(1), seq_len, T.int64(2560)), "float16")
        # with T.block("root"):
        compute = T.alloc_buffer((T.int64(2560), T.int64(2560)), "float16")
        dequantize_intermediate = T.alloc_buffer((T.int64(2560), T.int64(2560)), "float16")
        NT_matmul_intermediate = T.alloc_buffer((T.int64(1), seq_len, T.int64(2560)), "float16")
        T_add_intermediate = T.alloc_buffer((T.int64(1), seq_len, T.int64(2560)), "float16")
        for i0, i1 in T.grid(T.int64(2560), T.int64(2560)):
            with T.block("compute"):
                v_i0, v_i1 = T.axis.remap("SS", [i0, i1])
                T.reads(gpt_neox_layers_0_attention_dense_q_weight3[v_i0, v_i1 // T.int64(8)])
                T.writes(compute[v_i0, v_i1])
                compute[v_i0, v_i1] = T.Cast("float16", T.bitwise_and(T.shift_right(gpt_neox_layers_0_attention_dense_q_weight3[v_i0, v_i1 // T.int64(8)], T.Cast("uint32", v_i1 % T.int64(8) * T.int64(4))), T.uint32(15)))
        for i0, i1 in T.grid(T.int64(2560), T.int64(2560)):
            with T.block("dequantize"):
                v_i0, v_i1 = T.axis.remap("SS", [i0, i1])
                T.reads(compute[v_i0, v_i1], gpt_neox_layers_0_attention_dense_q_scale3[v_i0, v_i1 // T.int64(32)])
                T.writes(dequantize_intermediate[v_i0, v_i1])
                dequantize_intermediate[v_i0, v_i1] = (compute[v_i0, v_i1] - T.float16(7.0)) * gpt_neox_layers_0_attention_dense_q_scale3[v_i0, v_i1 // T.int64(32)]
        for i0, i1, i2, k in T.grid(T.int64(1), seq_len, T.int64(2560), T.int64(2560)):
            with T.block("NT_matmul"):
                v_i0, v_i1, v_i2, v_k = T.axis.remap("SSSR", [i0, i1, i2, k])
                T.reads(reshape259[v_i0, v_i1, v_k], dequantize_intermediate[v_i2, v_k])
                T.writes(NT_matmul_intermediate[v_i0, v_i1, v_i2])
                with T.init():
                    NT_matmul_intermediate[v_i0, v_i1, v_i2] = T.float16(0.0)
                NT_matmul_intermediate[v_i0, v_i1, v_i2] = NT_matmul_intermediate[v_i0, v_i1, v_i2] + reshape259[v_i0, v_i1, v_k] * dequantize_intermediate[v_i2, v_k]
        for ax0, ax1, ax2 in T.grid(T.int64(1), seq_len, T.int64(2560)):
            with T.block("T_add"):
                v_ax0, v_ax1, v_ax2 = T.axis.remap("SSS", [ax0, ax1, ax2])
                T.reads(NT_matmul_intermediate[v_ax0, v_ax1, v_ax2], gpt_neox_layers_0_attention_dense_bias3[v_ax2])
                T.writes(T_add_intermediate[v_ax0, v_ax1, v_ax2])
                T_add_intermediate[v_ax0, v_ax1, v_ax2] = NT_matmul_intermediate[v_ax0, v_ax1, v_ax2] + gpt_neox_layers_0_attention_dense_bias3[v_ax2]
        for ax0, ax1, ax2 in T.grid(T.int64(1), seq_len, T.int64(2560)):
            with T.block("T_add_1"):
                v_ax0, v_ax1, v_ax2 = T.axis.remap("SSS", [ax0, ax1, ax2])
                T.reads(T_add_intermediate[v_ax0, v_ax1, v_ax2], input_embeds[v_ax0, v_ax1, v_ax2])
                T.writes(T_add_intermediate_1_intermediate[v_ax0, v_ax1, v_ax2])
                T_add_intermediate_1_intermediate[v_ax0, v_ax1, v_ax2] = T_add_intermediate[v_ax0, v_ax1, v_ax2] + input_embeds[v_ax0, v_ax1, v_ax2]

    @T.prim_func(private=True)
    def fused_dequantize3_fused_NT_matmul12_add13_gelu2_cast6(gpt_neox_layers_0_mlp_dense_h_to_4h_q_weight2: T.Buffer((T.int64(10240), T.int64(320)), "uint32"), gpt_neox_layers_0_mlp_dense_h_to_4h_q_scale2: T.Buffer((T.int64(10240), T.int64(80)), "float16"), layer_norm66: T.Buffer((T.int64(1), T.int64(1), T.int64(2560)), "float16"), gpt_neox_layers_0_mlp_dense_h_to_4h_bias2: T.Buffer((T.int64(10240),), "float32"), compute_intermediate_intermediate: T.Buffer((T.int64(1), T.int64(1), T.int64(10240)), "float16")):
        T.func_attr({"tir.noalias": T.bool(True)})
        # with T.block("root"):
        compute = T.alloc_buffer((T.int64(10240), T.int64(2560)), "float16")
        dequantize_intermediate = T.alloc_buffer((T.int64(10240), T.int64(2560)), "float16")
        NT_matmul_intermediate = T.alloc_buffer((T.int64(1), T.int64(1), T.int64(10240)))
        T_add_intermediate = T.alloc_buffer((T.int64(1), T.int64(1), T.int64(10240)))
        T_multiply = T.alloc_buffer((T.int64(1), T.int64(1), T.int64(10240)))
        compute_1 = T.alloc_buffer((T.int64(1), T.int64(1), T.int64(10240)))
        T_multiply_1 = T.alloc_buffer((T.int64(1), T.int64(1), T.int64(10240)))
        T_add = T.alloc_buffer((T.int64(1), T.int64(1), T.int64(10240)))
        T_multiply_intermediate = T.alloc_buffer((T.int64(1), T.int64(1), T.int64(10240)))
        for i0, i1 in T.grid(T.int64(10240), T.int64(2560)):
            with T.block("compute"):
                v_i0, v_i1 = T.axis.remap("SS", [i0, i1])
                T.reads(gpt_neox_layers_0_mlp_dense_h_to_4h_q_weight2[v_i0, v_i1 // T.int64(8)])
                T.writes(compute[v_i0, v_i1])
                compute[v_i0, v_i1] = T.Cast("float16", T.bitwise_and(T.shift_right(gpt_neox_layers_0_mlp_dense_h_to_4h_q_weight2[v_i0, v_i1 // T.int64(8)], T.Cast("uint32", v_i1 % T.int64(8) * T.int64(4))), T.uint32(15)))
        for i0, i1 in T.grid(T.int64(10240), T.int64(2560)):
            with T.block("dequantize"):
                v_i0, v_i1 = T.axis.remap("SS", [i0, i1])
                T.reads(compute[v_i0, v_i1], gpt_neox_layers_0_mlp_dense_h_to_4h_q_scale2[v_i0, v_i1 // T.int64(32)])
                T.writes(dequantize_intermediate[v_i0, v_i1])
                dequantize_intermediate[v_i0, v_i1] = (compute[v_i0, v_i1] - T.float16(7.0)) * gpt_neox_layers_0_mlp_dense_h_to_4h_q_scale2[v_i0, v_i1 // T.int64(32)]
        for i0, i1, i2, k in T.grid(T.int64(1), T.int64(1), T.int64(10240), T.int64(2560)):
            with T.block("NT_matmul"):
                v_i0, v_i1, v_i2, v_k = T.axis.remap("SSSR", [i0, i1, i2, k])
                T.reads(layer_norm66[v_i0, v_i1, v_k], dequantize_intermediate[v_i2, v_k])
                T.writes(NT_matmul_intermediate[v_i0, v_i1, v_i2])
                with T.init():
                    NT_matmul_intermediate[v_i0, v_i1, v_i2] = T.float32(0.0)
                NT_matmul_intermediate[v_i0, v_i1, v_i2] = NT_matmul_intermediate[v_i0, v_i1, v_i2] + T.Cast("float32", layer_norm66[v_i0, v_i1, v_k]) * T.Cast("float32", dequantize_intermediate[v_i2, v_k])
        for ax0, ax1, ax2 in T.grid(T.int64(1), T.int64(1), T.int64(10240)):
            with T.block("T_add"):
                v_ax0, v_ax1, v_ax2 = T.axis.remap("SSS", [ax0, ax1, ax2])
                T.reads(NT_matmul_intermediate[v_ax0, v_ax1, v_ax2], gpt_neox_layers_0_mlp_dense_h_to_4h_bias2[v_ax2])
                T.writes(T_add_intermediate[v_ax0, v_ax1, v_ax2])
                T_add_intermediate[v_ax0, v_ax1, v_ax2] = NT_matmul_intermediate[v_ax0, v_ax1, v_ax2] + gpt_neox_layers_0_mlp_dense_h_to_4h_bias2[v_ax2]
        for ax0, ax1, ax2 in T.grid(T.int64(1), T.int64(1), T.int64(10240)):
            with T.block("T_multiply"):
                v_ax0, v_ax1, v_ax2 = T.axis.remap("SSS", [ax0, ax1, ax2])
                T.reads(T_add_intermediate[v_ax0, v_ax1, v_ax2])
                T.writes(T_multiply[v_ax0, v_ax1, v_ax2])
                T_multiply[v_ax0, v_ax1, v_ax2] = T_add_intermediate[v_ax0, v_ax1, v_ax2] * T.float32(0.70710678118654757)
        for i0, i1, i2 in T.grid(T.int64(1), T.int64(1), T.int64(10240)):
            with T.block("compute_1"):
                v_i0, v_i1, v_i2 = T.axis.remap("SSS", [i0, i1, i2])
                T.reads(T_multiply[v_i0, v_i1, v_i2])
                T.writes(compute_1[v_i0, v_i1, v_i2])
                compute_1[v_i0, v_i1, v_i2] = T.erf(T_multiply[v_i0, v_i1, v_i2])
        for ax0, ax1, ax2 in T.grid(T.int64(1), T.int64(1), T.int64(10240)):
            with T.block("T_multiply_1"):
                v_ax0, v_ax1, v_ax2 = T.axis.remap("SSS", [ax0, ax1, ax2])
                T.reads(compute_1[v_ax0, v_ax1, v_ax2])
                T.writes(T_multiply_1[v_ax0, v_ax1, v_ax2])
                T_multiply_1[v_ax0, v_ax1, v_ax2] = compute_1[v_ax0, v_ax1, v_ax2] * T.float32(0.5)
        for ax0, ax1, ax2 in T.grid(T.int64(1), T.int64(1), T.int64(10240)):
            with T.block("T_add_1"):
                v_ax0, v_ax1, v_ax2 = T.axis.remap("SSS", [ax0, ax1, ax2])
                T.reads(T_multiply_1[v_ax0, v_ax1, v_ax2])
                T.writes(T_add[v_ax0, v_ax1, v_ax2])
                T_add[v_ax0, v_ax1, v_ax2] = T.float32(0.5) + T_multiply_1[v_ax0, v_ax1, v_ax2]
        for ax0, ax1, ax2 in T.grid(T.int64(1), T.int64(1), T.int64(10240)):
            with T.block("T_multiply_2"):
                v_ax0, v_ax1, v_ax2 = T.axis.remap("SSS", [ax0, ax1, ax2])
                T.reads(T_add_intermediate[v_ax0, v_ax1, v_ax2], T_add[v_ax0, v_ax1, v_ax2])
                T.writes(T_multiply_intermediate[v_ax0, v_ax1, v_ax2])
                T_multiply_intermediate[v_ax0, v_ax1, v_ax2] = T_add_intermediate[v_ax0, v_ax1, v_ax2] * T_add[v_ax0, v_ax1, v_ax2]
        for i0, i1, i2 in T.grid(T.int64(1), T.int64(1), T.int64(10240)):
            with T.block("compute_1_1"):
                v_i0, v_i1, v_i2 = T.axis.remap("SSS", [i0, i1, i2])
                T.reads(T_multiply_intermediate[v_i0, v_i1, v_i2])
                T.writes(compute_intermediate_intermediate[v_i0, v_i1, v_i2])
                compute_intermediate_intermediate[v_i0, v_i1, v_i2] = T.Cast("float16", T_multiply_intermediate[v_i0, v_i1, v_i2])

    @T.prim_func(private=True)
    def fused_dequantize3_fused_NT_matmul2_add3_gelu_cast(gpt_neox_layers_0_mlp_dense_h_to_4h_q_weight4: T.Buffer((T.int64(10240), T.int64(320)), "uint32"), gpt_neox_layers_0_mlp_dense_h_to_4h_q_scale4: T.Buffer((T.int64(10240), T.int64(80)), "float16"), p_layer_norm196: T.handle, gpt_neox_layers_0_mlp_dense_h_to_4h_bias4: T.Buffer((T.int64(10240),), "float32"), p_output0: T.handle):
        T.func_attr({"tir.noalias": T.bool(True)})
        batch_size = T.int64()
        layer_norm196 = T.match_buffer(p_layer_norm196, (batch_size, T.int64(1), T.int64(2560)), "float16")
        compute_intermediate_intermediate = T.match_buffer(p_output0, (batch_size, T.int64(1), T.int64(10240)), "float16")
        # with T.block("root"):
        compute = T.alloc_buffer((T.int64(10240), T.int64(2560)), "float16")
        dequantize_intermediate = T.alloc_buffer((T.int64(10240), T.int64(2560)), "float16")
        NT_matmul_intermediate = T.alloc_buffer((batch_size, T.int64(1), T.int64(10240)))
        T_add_intermediate = T.alloc_buffer((batch_size, T.int64(1), T.int64(10240)))
        T_multiply = T.alloc_buffer((batch_size, T.int64(1), T.int64(10240)))
        compute_1 = T.alloc_buffer((batch_size, T.int64(1), T.int64(10240)))
        T_multiply_1 = T.alloc_buffer((batch_size, T.int64(1), T.int64(10240)))
        T_add = T.alloc_buffer((batch_size, T.int64(1), T.int64(10240)))
        T_multiply_intermediate = T.alloc_buffer((batch_size, T.int64(1), T.int64(10240)))
        for i0, i1 in T.grid(T.int64(10240), T.int64(2560)):
            with T.block("compute"):
                v_i0, v_i1 = T.axis.remap("SS", [i0, i1])
                T.reads(gpt_neox_layers_0_mlp_dense_h_to_4h_q_weight4[v_i0, v_i1 // T.int64(8)])
                T.writes(compute[v_i0, v_i1])
                compute[v_i0, v_i1] = T.Cast("float16", T.bitwise_and(T.shift_right(gpt_neox_layers_0_mlp_dense_h_to_4h_q_weight4[v_i0, v_i1 // T.int64(8)], T.Cast("uint32", v_i1 % T.int64(8) * T.int64(4))), T.uint32(15)))
        for i0, i1 in T.grid(T.int64(10240), T.int64(2560)):
            with T.block("dequantize"):
                v_i0, v_i1 = T.axis.remap("SS", [i0, i1])
                T.reads(compute[v_i0, v_i1], gpt_neox_layers_0_mlp_dense_h_to_4h_q_scale4[v_i0, v_i1 // T.int64(32)])
                T.writes(dequantize_intermediate[v_i0, v_i1])
                dequantize_intermediate[v_i0, v_i1] = (compute[v_i0, v_i1] - T.float16(7.0)) * gpt_neox_layers_0_mlp_dense_h_to_4h_q_scale4[v_i0, v_i1 // T.int64(32)]
        for i0, i1, i2, k in T.grid(batch_size, T.int64(1), T.int64(10240), T.int64(2560)):
            with T.block("NT_matmul"):
                v_i0, v_i1, v_i2, v_k = T.axis.remap("SSSR", [i0, i1, i2, k])
                T.reads(layer_norm196[v_i0, v_i1, v_k], dequantize_intermediate[v_i2, v_k])
                T.writes(NT_matmul_intermediate[v_i0, v_i1, v_i2])
                with T.init():
                    NT_matmul_intermediate[v_i0, v_i1, v_i2] = T.float32(0.0)
                NT_matmul_intermediate[v_i0, v_i1, v_i2] = NT_matmul_intermediate[v_i0, v_i1, v_i2] + T.Cast("float32", layer_norm196[v_i0, v_i1, v_k]) * T.Cast("float32", dequantize_intermediate[v_i2, v_k])
        for ax0, ax1, ax2 in T.grid(batch_size, T.int64(1), T.int64(10240)):
            with T.block("T_add"):
                v_ax0, v_ax1, v_ax2 = T.axis.remap("SSS", [ax0, ax1, ax2])
                T.reads(NT_matmul_intermediate[v_ax0, v_ax1, v_ax2], gpt_neox_layers_0_mlp_dense_h_to_4h_bias4[v_ax2])
                T.writes(T_add_intermediate[v_ax0, v_ax1, v_ax2])
                T_add_intermediate[v_ax0, v_ax1, v_ax2] = NT_matmul_intermediate[v_ax0, v_ax1, v_ax2] + gpt_neox_layers_0_mlp_dense_h_to_4h_bias4[v_ax2]
        for ax0, ax1, ax2 in T.grid(batch_size, T.int64(1), T.int64(10240)):
            with T.block("T_multiply"):
                v_ax0, v_ax1, v_ax2 = T.axis.remap("SSS", [ax0, ax1, ax2])
                T.reads(T_add_intermediate[v_ax0, v_ax1, v_ax2])
                T.writes(T_multiply[v_ax0, v_ax1, v_ax2])
                T_multiply[v_ax0, v_ax1, v_ax2] = T_add_intermediate[v_ax0, v_ax1, v_ax2] * T.float32(0.70710678118654757)
        for i0, i1, i2 in T.grid(batch_size, T.int64(1), T.int64(10240)):
            with T.block("compute_1"):
                v_i0, v_i1, v_i2 = T.axis.remap("SSS", [i0, i1, i2])
                T.reads(T_multiply[v_i0, v_i1, v_i2])
                T.writes(compute_1[v_i0, v_i1, v_i2])
                compute_1[v_i0, v_i1, v_i2] = T.erf(T_multiply[v_i0, v_i1, v_i2])
        for ax0, ax1, ax2 in T.grid(batch_size, T.int64(1), T.int64(10240)):
            with T.block("T_multiply_1"):
                v_ax0, v_ax1, v_ax2 = T.axis.remap("SSS", [ax0, ax1, ax2])
                T.reads(compute_1[v_ax0, v_ax1, v_ax2])
                T.writes(T_multiply_1[v_ax0, v_ax1, v_ax2])
                T_multiply_1[v_ax0, v_ax1, v_ax2] = compute_1[v_ax0, v_ax1, v_ax2] * T.float32(0.5)
        for ax0, ax1, ax2 in T.grid(batch_size, T.int64(1), T.int64(10240)):
            with T.block("T_add_1"):
                v_ax0, v_ax1, v_ax2 = T.axis.remap("SSS", [ax0, ax1, ax2])
                T.reads(T_multiply_1[v_ax0, v_ax1, v_ax2])
                T.writes(T_add[v_ax0, v_ax1, v_ax2])
                T_add[v_ax0, v_ax1, v_ax2] = T.float32(0.5) + T_multiply_1[v_ax0, v_ax1, v_ax2]
        for ax0, ax1, ax2 in T.grid(batch_size, T.int64(1), T.int64(10240)):
            with T.block("T_multiply_2"):
                v_ax0, v_ax1, v_ax2 = T.axis.remap("SSS", [ax0, ax1, ax2])
                T.reads(T_add_intermediate[v_ax0, v_ax1, v_ax2], T_add[v_ax0, v_ax1, v_ax2])
                T.writes(T_multiply_intermediate[v_ax0, v_ax1, v_ax2])
                T_multiply_intermediate[v_ax0, v_ax1, v_ax2] = T_add_intermediate[v_ax0, v_ax1, v_ax2] * T_add[v_ax0, v_ax1, v_ax2]
        for i0, i1, i2 in T.grid(batch_size, T.int64(1), T.int64(10240)):
            with T.block("compute_1_1"):
                v_i0, v_i1, v_i2 = T.axis.remap("SSS", [i0, i1, i2])
                T.reads(T_multiply_intermediate[v_i0, v_i1, v_i2])
                T.writes(compute_intermediate_intermediate[v_i0, v_i1, v_i2])
                compute_intermediate_intermediate[v_i0, v_i1, v_i2] = T.Cast("float16", T_multiply_intermediate[v_i0, v_i1, v_i2])

    @T.prim_func(private=True)
    def fused_dequantize3_fused_NT_matmul7_add8_gelu1_cast3(gpt_neox_layers_0_mlp_dense_h_to_4h_q_weight3: T.Buffer((T.int64(10240), T.int64(320)), "uint32"), gpt_neox_layers_0_mlp_dense_h_to_4h_q_scale3: T.Buffer((T.int64(10240), T.int64(80)), "float16"), p_layer_norm131: T.handle, gpt_neox_layers_0_mlp_dense_h_to_4h_bias3: T.Buffer((T.int64(10240),), "float32"), p_output0: T.handle):
        T.func_attr({"tir.noalias": T.bool(True)})
        seq_len = T.int64()
        layer_norm131 = T.match_buffer(p_layer_norm131, (T.int64(1), seq_len, T.int64(2560)), "float16")
        compute_intermediate_intermediate = T.match_buffer(p_output0, (T.int64(1), seq_len, T.int64(10240)), "float16")
        # with T.block("root"):
        compute = T.alloc_buffer((T.int64(10240), T.int64(2560)), "float16")
        dequantize_intermediate = T.alloc_buffer((T.int64(10240), T.int64(2560)), "float16")
        NT_matmul_intermediate = T.alloc_buffer((T.int64(1), seq_len, T.int64(10240)))
        T_add_intermediate = T.alloc_buffer((T.int64(1), seq_len, T.int64(10240)))
        T_multiply = T.alloc_buffer((T.int64(1), seq_len, T.int64(10240)))
        compute_1 = T.alloc_buffer((T.int64(1), seq_len, T.int64(10240)))
        T_multiply_1 = T.alloc_buffer((T.int64(1), seq_len, T.int64(10240)))
        T_add = T.alloc_buffer((T.int64(1), seq_len, T.int64(10240)))
        T_multiply_intermediate = T.alloc_buffer((T.int64(1), seq_len, T.int64(10240)))
        for i0, i1 in T.grid(T.int64(10240), T.int64(2560)):
            with T.block("compute"):
                v_i0, v_i1 = T.axis.remap("SS", [i0, i1])
                T.reads(gpt_neox_layers_0_mlp_dense_h_to_4h_q_weight3[v_i0, v_i1 // T.int64(8)])
                T.writes(compute[v_i0, v_i1])
                compute[v_i0, v_i1] = T.Cast("float16", T.bitwise_and(T.shift_right(gpt_neox_layers_0_mlp_dense_h_to_4h_q_weight3[v_i0, v_i1 // T.int64(8)], T.Cast("uint32", v_i1 % T.int64(8) * T.int64(4))), T.uint32(15)))
        for i0, i1 in T.grid(T.int64(10240), T.int64(2560)):
            with T.block("dequantize"):
                v_i0, v_i1 = T.axis.remap("SS", [i0, i1])
                T.reads(compute[v_i0, v_i1], gpt_neox_layers_0_mlp_dense_h_to_4h_q_scale3[v_i0, v_i1 // T.int64(32)])
                T.writes(dequantize_intermediate[v_i0, v_i1])
                dequantize_intermediate[v_i0, v_i1] = (compute[v_i0, v_i1] - T.float16(7.0)) * gpt_neox_layers_0_mlp_dense_h_to_4h_q_scale3[v_i0, v_i1 // T.int64(32)]
        for i0, i1, i2, k in T.grid(T.int64(1), seq_len, T.int64(10240), T.int64(2560)):
            with T.block("NT_matmul"):
                v_i0, v_i1, v_i2, v_k = T.axis.remap("SSSR", [i0, i1, i2, k])
                T.reads(layer_norm131[v_i0, v_i1, v_k], dequantize_intermediate[v_i2, v_k])
                T.writes(NT_matmul_intermediate[v_i0, v_i1, v_i2])
                with T.init():
                    NT_matmul_intermediate[v_i0, v_i1, v_i2] = T.float32(0.0)
                NT_matmul_intermediate[v_i0, v_i1, v_i2] = NT_matmul_intermediate[v_i0, v_i1, v_i2] + T.Cast("float32", layer_norm131[v_i0, v_i1, v_k]) * T.Cast("float32", dequantize_intermediate[v_i2, v_k])
        for ax0, ax1, ax2 in T.grid(T.int64(1), seq_len, T.int64(10240)):
            with T.block("T_add"):
                v_ax0, v_ax1, v_ax2 = T.axis.remap("SSS", [ax0, ax1, ax2])
                T.reads(NT_matmul_intermediate[v_ax0, v_ax1, v_ax2], gpt_neox_layers_0_mlp_dense_h_to_4h_bias3[v_ax2])
                T.writes(T_add_intermediate[v_ax0, v_ax1, v_ax2])
                T_add_intermediate[v_ax0, v_ax1, v_ax2] = NT_matmul_intermediate[v_ax0, v_ax1, v_ax2] + gpt_neox_layers_0_mlp_dense_h_to_4h_bias3[v_ax2]
        for ax0, ax1, ax2 in T.grid(T.int64(1), seq_len, T.int64(10240)):
            with T.block("T_multiply"):
                v_ax0, v_ax1, v_ax2 = T.axis.remap("SSS", [ax0, ax1, ax2])
                T.reads(T_add_intermediate[v_ax0, v_ax1, v_ax2])
                T.writes(T_multiply[v_ax0, v_ax1, v_ax2])
                T_multiply[v_ax0, v_ax1, v_ax2] = T_add_intermediate[v_ax0, v_ax1, v_ax2] * T.float32(0.70710678118654757)
        for i0, i1, i2 in T.grid(T.int64(1), seq_len, T.int64(10240)):
            with T.block("compute_1"):
                v_i0, v_i1, v_i2 = T.axis.remap("SSS", [i0, i1, i2])
                T.reads(T_multiply[v_i0, v_i1, v_i2])
                T.writes(compute_1[v_i0, v_i1, v_i2])
                compute_1[v_i0, v_i1, v_i2] = T.erf(T_multiply[v_i0, v_i1, v_i2])
        for ax0, ax1, ax2 in T.grid(T.int64(1), seq_len, T.int64(10240)):
            with T.block("T_multiply_1"):
                v_ax0, v_ax1, v_ax2 = T.axis.remap("SSS", [ax0, ax1, ax2])
                T.reads(compute_1[v_ax0, v_ax1, v_ax2])
                T.writes(T_multiply_1[v_ax0, v_ax1, v_ax2])
                T_multiply_1[v_ax0, v_ax1, v_ax2] = compute_1[v_ax0, v_ax1, v_ax2] * T.float32(0.5)
        for ax0, ax1, ax2 in T.grid(T.int64(1), seq_len, T.int64(10240)):
            with T.block("T_add_1"):
                v_ax0, v_ax1, v_ax2 = T.axis.remap("SSS", [ax0, ax1, ax2])
                T.reads(T_multiply_1[v_ax0, v_ax1, v_ax2])
                T.writes(T_add[v_ax0, v_ax1, v_ax2])
                T_add[v_ax0, v_ax1, v_ax2] = T.float32(0.5) + T_multiply_1[v_ax0, v_ax1, v_ax2]
        for ax0, ax1, ax2 in T.grid(T.int64(1), seq_len, T.int64(10240)):
            with T.block("T_multiply_2"):
                v_ax0, v_ax1, v_ax2 = T.axis.remap("SSS", [ax0, ax1, ax2])
                T.reads(T_add_intermediate[v_ax0, v_ax1, v_ax2], T_add[v_ax0, v_ax1, v_ax2])
                T.writes(T_multiply_intermediate[v_ax0, v_ax1, v_ax2])
                T_multiply_intermediate[v_ax0, v_ax1, v_ax2] = T_add_intermediate[v_ax0, v_ax1, v_ax2] * T_add[v_ax0, v_ax1, v_ax2]
        for i0, i1, i2 in T.grid(T.int64(1), seq_len, T.int64(10240)):
            with T.block("compute_1_1"):
                v_i0, v_i1, v_i2 = T.axis.remap("SSS", [i0, i1, i2])
                T.reads(T_multiply_intermediate[v_i0, v_i1, v_i2])
                T.writes(compute_intermediate_intermediate[v_i0, v_i1, v_i2])
                compute_intermediate_intermediate[v_i0, v_i1, v_i2] = T.Cast("float16", T_multiply_intermediate[v_i0, v_i1, v_i2])

    @T.prim_func(private=True)
    def fused_dequantize4_fused_NT_matmul13_add14_cast7_add12(gpt_neox_layers_0_mlp_dense_4h_to_h_q_weight2: T.Buffer((T.int64(2560), T.int64(1280)), "uint32"), gpt_neox_layers_0_mlp_dense_4h_to_h_q_scale2: T.Buffer((T.int64(2560), T.int64(320)), "float16"), lv193: T.Buffer((T.int64(1), T.int64(1), T.int64(10240)), "float16"), gpt_neox_layers_0_mlp_dense_4h_to_h_bias2: T.Buffer((T.int64(2560),), "float32"), lv390: T.Buffer((T.int64(1), T.int64(1), T.int64(2560)), "float16"), T_add_intermediate_1_intermediate: T.Buffer((T.int64(1), T.int64(1), T.int64(2560)), "float16")):
        T.func_attr({"tir.noalias": T.bool(True)})
        # with T.block("root"):
        compute = T.alloc_buffer((T.int64(2560), T.int64(10240)), "float16")
        dequantize_intermediate = T.alloc_buffer((T.int64(2560), T.int64(10240)), "float16")
        NT_matmul_intermediate = T.alloc_buffer((T.int64(1), T.int64(1), T.int64(2560)))
        T_add_intermediate = T.alloc_buffer((T.int64(1), T.int64(1), T.int64(2560)))
        compute_intermediate = T.alloc_buffer((T.int64(1), T.int64(1), T.int64(2560)), "float16")
        for i0, i1 in T.grid(T.int64(2560), T.int64(10240)):
            with T.block("compute"):
                v_i0, v_i1 = T.axis.remap("SS", [i0, i1])
                T.reads(gpt_neox_layers_0_mlp_dense_4h_to_h_q_weight2[v_i0, v_i1 // T.int64(8)])
                T.writes(compute[v_i0, v_i1])
                compute[v_i0, v_i1] = T.Cast("float16", T.bitwise_and(T.shift_right(gpt_neox_layers_0_mlp_dense_4h_to_h_q_weight2[v_i0, v_i1 // T.int64(8)], T.Cast("uint32", v_i1 % T.int64(8) * T.int64(4))), T.uint32(15)))
        for i0, i1 in T.grid(T.int64(2560), T.int64(10240)):
            with T.block("dequantize"):
                v_i0, v_i1 = T.axis.remap("SS", [i0, i1])
                T.reads(compute[v_i0, v_i1], gpt_neox_layers_0_mlp_dense_4h_to_h_q_scale2[v_i0, v_i1 // T.int64(32)])
                T.writes(dequantize_intermediate[v_i0, v_i1])
                dequantize_intermediate[v_i0, v_i1] = (compute[v_i0, v_i1] - T.float16(7.0)) * gpt_neox_layers_0_mlp_dense_4h_to_h_q_scale2[v_i0, v_i1 // T.int64(32)]
        for i0, i1, i2, k in T.grid(T.int64(1), T.int64(1), T.int64(2560), T.int64(10240)):
            with T.block("NT_matmul"):
                v_i0, v_i1, v_i2, v_k = T.axis.remap("SSSR", [i0, i1, i2, k])
                T.reads(lv193[v_i0, v_i1, v_k], dequantize_intermediate[v_i2, v_k])
                T.writes(NT_matmul_intermediate[v_i0, v_i1, v_i2])
                with T.init():
                    NT_matmul_intermediate[v_i0, v_i1, v_i2] = T.float32(0.0)
                NT_matmul_intermediate[v_i0, v_i1, v_i2] = NT_matmul_intermediate[v_i0, v_i1, v_i2] + T.Cast("float32", lv193[v_i0, v_i1, v_k]) * T.Cast("float32", dequantize_intermediate[v_i2, v_k])
        for ax0, ax1, ax2 in T.grid(T.int64(1), T.int64(1), T.int64(2560)):
            with T.block("T_add"):
                v_ax0, v_ax1, v_ax2 = T.axis.remap("SSS", [ax0, ax1, ax2])
                T.reads(NT_matmul_intermediate[v_ax0, v_ax1, v_ax2], gpt_neox_layers_0_mlp_dense_4h_to_h_bias2[v_ax2])
                T.writes(T_add_intermediate[v_ax0, v_ax1, v_ax2])
                T_add_intermediate[v_ax0, v_ax1, v_ax2] = NT_matmul_intermediate[v_ax0, v_ax1, v_ax2] + gpt_neox_layers_0_mlp_dense_4h_to_h_bias2[v_ax2]
        for i0, i1, i2 in T.grid(T.int64(1), T.int64(1), T.int64(2560)):
            with T.block("compute_1"):
                v_i0, v_i1, v_i2 = T.axis.remap("SSS", [i0, i1, i2])
                T.reads(T_add_intermediate[v_i0, v_i1, v_i2])
                T.writes(compute_intermediate[v_i0, v_i1, v_i2])
                compute_intermediate[v_i0, v_i1, v_i2] = T.Cast("float16", T_add_intermediate[v_i0, v_i1, v_i2])
        for ax0, ax1, ax2 in T.grid(T.int64(1), T.int64(1), T.int64(2560)):
            with T.block("T_add_1"):
                v_ax0, v_ax1, v_ax2 = T.axis.remap("SSS", [ax0, ax1, ax2])
                T.reads(compute_intermediate[v_ax0, v_ax1, v_ax2], lv390[v_ax0, v_ax1, v_ax2])
                T.writes(T_add_intermediate_1_intermediate[v_ax0, v_ax1, v_ax2])
                T_add_intermediate_1_intermediate[v_ax0, v_ax1, v_ax2] = compute_intermediate[v_ax0, v_ax1, v_ax2] + lv390[v_ax0, v_ax1, v_ax2]

    @T.prim_func(private=True)
    def fused_dequantize4_fused_NT_matmul3_add4_cast1_add2(gpt_neox_layers_0_mlp_dense_4h_to_h_q_weight4: T.Buffer((T.int64(2560), T.int64(1280)), "uint32"), gpt_neox_layers_0_mlp_dense_4h_to_h_q_scale4: T.Buffer((T.int64(2560), T.int64(320)), "float16"), p_lv1: T.handle, gpt_neox_layers_0_mlp_dense_4h_to_h_bias4: T.Buffer((T.int64(2560),), "float32"), p_lv1_1: T.handle, p_output0: T.handle):
        T.func_attr({"tir.noalias": T.bool(True)})
        batch_size = T.int64()
        lv1 = T.match_buffer(p_lv1, (batch_size, T.int64(1), T.int64(10240)), "float16")
        lv1_1 = T.match_buffer(p_lv1_1, (batch_size, T.int64(1), T.int64(2560)), "float16")
        T_add_intermediate_1_intermediate = T.match_buffer(p_output0, (batch_size, T.int64(1), T.int64(2560)), "float16")
        # with T.block("root"):
        compute = T.alloc_buffer((T.int64(2560), T.int64(10240)), "float16")
        dequantize_intermediate = T.alloc_buffer((T.int64(2560), T.int64(10240)), "float16")
        NT_matmul_intermediate = T.alloc_buffer((batch_size, T.int64(1), T.int64(2560)))
        T_add_intermediate = T.alloc_buffer((batch_size, T.int64(1), T.int64(2560)))
        compute_intermediate = T.alloc_buffer((batch_size, T.int64(1), T.int64(2560)), "float16")
        for i0, i1 in T.grid(T.int64(2560), T.int64(10240)):
            with T.block("compute"):
                v_i0, v_i1 = T.axis.remap("SS", [i0, i1])
                T.reads(gpt_neox_layers_0_mlp_dense_4h_to_h_q_weight4[v_i0, v_i1 // T.int64(8)])
                T.writes(compute[v_i0, v_i1])
                compute[v_i0, v_i1] = T.Cast("float16", T.bitwise_and(T.shift_right(gpt_neox_layers_0_mlp_dense_4h_to_h_q_weight4[v_i0, v_i1 // T.int64(8)], T.Cast("uint32", v_i1 % T.int64(8) * T.int64(4))), T.uint32(15)))
        for i0, i1 in T.grid(T.int64(2560), T.int64(10240)):
            with T.block("dequantize"):
                v_i0, v_i1 = T.axis.remap("SS", [i0, i1])
                T.reads(compute[v_i0, v_i1], gpt_neox_layers_0_mlp_dense_4h_to_h_q_scale4[v_i0, v_i1 // T.int64(32)])
                T.writes(dequantize_intermediate[v_i0, v_i1])
                dequantize_intermediate[v_i0, v_i1] = (compute[v_i0, v_i1] - T.float16(7.0)) * gpt_neox_layers_0_mlp_dense_4h_to_h_q_scale4[v_i0, v_i1 // T.int64(32)]
        for i0, i1, i2, k in T.grid(batch_size, T.int64(1), T.int64(2560), T.int64(10240)):
            with T.block("NT_matmul"):
                v_i0, v_i1, v_i2, v_k = T.axis.remap("SSSR", [i0, i1, i2, k])
                T.reads(lv1[v_i0, v_i1, v_k], dequantize_intermediate[v_i2, v_k])
                T.writes(NT_matmul_intermediate[v_i0, v_i1, v_i2])
                with T.init():
                    NT_matmul_intermediate[v_i0, v_i1, v_i2] = T.float32(0.0)
                NT_matmul_intermediate[v_i0, v_i1, v_i2] = NT_matmul_intermediate[v_i0, v_i1, v_i2] + T.Cast("float32", lv1[v_i0, v_i1, v_k]) * T.Cast("float32", dequantize_intermediate[v_i2, v_k])
        for ax0, ax1, ax2 in T.grid(batch_size, T.int64(1), T.int64(2560)):
            with T.block("T_add"):
                v_ax0, v_ax1, v_ax2 = T.axis.remap("SSS", [ax0, ax1, ax2])
                T.reads(NT_matmul_intermediate[v_ax0, v_ax1, v_ax2], gpt_neox_layers_0_mlp_dense_4h_to_h_bias4[v_ax2])
                T.writes(T_add_intermediate[v_ax0, v_ax1, v_ax2])
                T_add_intermediate[v_ax0, v_ax1, v_ax2] = NT_matmul_intermediate[v_ax0, v_ax1, v_ax2] + gpt_neox_layers_0_mlp_dense_4h_to_h_bias4[v_ax2]
        for i0, i1, i2 in T.grid(batch_size, T.int64(1), T.int64(2560)):
            with T.block("compute_1"):
                v_i0, v_i1, v_i2 = T.axis.remap("SSS", [i0, i1, i2])
                T.reads(T_add_intermediate[v_i0, v_i1, v_i2])
                T.writes(compute_intermediate[v_i0, v_i1, v_i2])
                compute_intermediate[v_i0, v_i1, v_i2] = T.Cast("float16", T_add_intermediate[v_i0, v_i1, v_i2])
        for ax0, ax1, ax2 in T.grid(batch_size, T.int64(1), T.int64(2560)):
            with T.block("T_add_1"):
                v_ax0, v_ax1, v_ax2 = T.axis.remap("SSS", [ax0, ax1, ax2])
                T.reads(compute_intermediate[v_ax0, v_ax1, v_ax2], lv1_1[v_ax0, v_ax1, v_ax2])
                T.writes(T_add_intermediate_1_intermediate[v_ax0, v_ax1, v_ax2])
                T_add_intermediate_1_intermediate[v_ax0, v_ax1, v_ax2] = compute_intermediate[v_ax0, v_ax1, v_ax2] + lv1_1[v_ax0, v_ax1, v_ax2]

    @T.prim_func(private=True)
    def fused_dequantize4_fused_NT_matmul8_add9_cast4_add7(gpt_neox_layers_0_mlp_dense_4h_to_h_q_weight3: T.Buffer((T.int64(2560), T.int64(1280)), "uint32"), gpt_neox_layers_0_mlp_dense_4h_to_h_q_scale3: T.Buffer((T.int64(2560), T.int64(320)), "float16"), p_lv65: T.handle, gpt_neox_layers_0_mlp_dense_4h_to_h_bias3: T.Buffer((T.int64(2560),), "float32"), p_lv130: T.handle, p_output0: T.handle):
        T.func_attr({"tir.noalias": T.bool(True)})
        seq_len = T.int64()
        lv65 = T.match_buffer(p_lv65, (T.int64(1), seq_len, T.int64(10240)), "float16")
        lv130 = T.match_buffer(p_lv130, (T.int64(1), seq_len, T.int64(2560)), "float16")
        T_add_intermediate_1_intermediate = T.match_buffer(p_output0, (T.int64(1), seq_len, T.int64(2560)), "float16")
        # with T.block("root"):
        compute = T.alloc_buffer((T.int64(2560), T.int64(10240)), "float16")
        dequantize_intermediate = T.alloc_buffer((T.int64(2560), T.int64(10240)), "float16")
        NT_matmul_intermediate = T.alloc_buffer((T.int64(1), seq_len, T.int64(2560)))
        T_add_intermediate = T.alloc_buffer((T.int64(1), seq_len, T.int64(2560)))
        compute_intermediate = T.alloc_buffer((T.int64(1), seq_len, T.int64(2560)), "float16")
        for i0, i1 in T.grid(T.int64(2560), T.int64(10240)):
            with T.block("compute"):
                v_i0, v_i1 = T.axis.remap("SS", [i0, i1])
                T.reads(gpt_neox_layers_0_mlp_dense_4h_to_h_q_weight3[v_i0, v_i1 // T.int64(8)])
                T.writes(compute[v_i0, v_i1])
                compute[v_i0, v_i1] = T.Cast("float16", T.bitwise_and(T.shift_right(gpt_neox_layers_0_mlp_dense_4h_to_h_q_weight3[v_i0, v_i1 // T.int64(8)], T.Cast("uint32", v_i1 % T.int64(8) * T.int64(4))), T.uint32(15)))
        for i0, i1 in T.grid(T.int64(2560), T.int64(10240)):
            with T.block("dequantize"):
                v_i0, v_i1 = T.axis.remap("SS", [i0, i1])
                T.reads(compute[v_i0, v_i1], gpt_neox_layers_0_mlp_dense_4h_to_h_q_scale3[v_i0, v_i1 // T.int64(32)])
                T.writes(dequantize_intermediate[v_i0, v_i1])
                dequantize_intermediate[v_i0, v_i1] = (compute[v_i0, v_i1] - T.float16(7.0)) * gpt_neox_layers_0_mlp_dense_4h_to_h_q_scale3[v_i0, v_i1 // T.int64(32)]
        for i0, i1, i2, k in T.grid(T.int64(1), seq_len, T.int64(2560), T.int64(10240)):
            with T.block("NT_matmul"):
                v_i0, v_i1, v_i2, v_k = T.axis.remap("SSSR", [i0, i1, i2, k])
                T.reads(lv65[v_i0, v_i1, v_k], dequantize_intermediate[v_i2, v_k])
                T.writes(NT_matmul_intermediate[v_i0, v_i1, v_i2])
                with T.init():
                    NT_matmul_intermediate[v_i0, v_i1, v_i2] = T.float32(0.0)
                NT_matmul_intermediate[v_i0, v_i1, v_i2] = NT_matmul_intermediate[v_i0, v_i1, v_i2] + T.Cast("float32", lv65[v_i0, v_i1, v_k]) * T.Cast("float32", dequantize_intermediate[v_i2, v_k])
        for ax0, ax1, ax2 in T.grid(T.int64(1), seq_len, T.int64(2560)):
            with T.block("T_add"):
                v_ax0, v_ax1, v_ax2 = T.axis.remap("SSS", [ax0, ax1, ax2])
                T.reads(NT_matmul_intermediate[v_ax0, v_ax1, v_ax2], gpt_neox_layers_0_mlp_dense_4h_to_h_bias3[v_ax2])
                T.writes(T_add_intermediate[v_ax0, v_ax1, v_ax2])
                T_add_intermediate[v_ax0, v_ax1, v_ax2] = NT_matmul_intermediate[v_ax0, v_ax1, v_ax2] + gpt_neox_layers_0_mlp_dense_4h_to_h_bias3[v_ax2]
        for i0, i1, i2 in T.grid(T.int64(1), seq_len, T.int64(2560)):
            with T.block("compute_1"):
                v_i0, v_i1, v_i2 = T.axis.remap("SSS", [i0, i1, i2])
                T.reads(T_add_intermediate[v_i0, v_i1, v_i2])
                T.writes(compute_intermediate[v_i0, v_i1, v_i2])
                compute_intermediate[v_i0, v_i1, v_i2] = T.Cast("float16", T_add_intermediate[v_i0, v_i1, v_i2])
        for ax0, ax1, ax2 in T.grid(T.int64(1), seq_len, T.int64(2560)):
            with T.block("T_add_1"):
                v_ax0, v_ax1, v_ax2 = T.axis.remap("SSS", [ax0, ax1, ax2])
                T.reads(compute_intermediate[v_ax0, v_ax1, v_ax2], lv130[v_ax0, v_ax1, v_ax2])
                T.writes(T_add_intermediate_1_intermediate[v_ax0, v_ax1, v_ax2])
                T_add_intermediate_1_intermediate[v_ax0, v_ax1, v_ax2] = compute_intermediate[v_ax0, v_ax1, v_ax2] + lv130[v_ax0, v_ax1, v_ax2]

    @T.prim_func(private=True)
    def fused_dequantize_fused_NT_matmul14_cast8(p_embed_out_q_weight2: T.handle, p_embed_out_q_scale2: T.handle, layer_norm129: T.Buffer((T.int64(1), T.int64(1), T.int64(2560)), "float16"), p_output0: T.handle):
        T.func_attr({"tir.noalias": T.bool(True)})
        vocab_size = T.int64()
        embed_out_q_weight2 = T.match_buffer(p_embed_out_q_weight2, (vocab_size, T.int64(320)), "uint32")
        embed_out_q_scale2 = T.match_buffer(p_embed_out_q_scale2, (vocab_size, T.int64(80)), "float16")
        compute_intermediate_intermediate = T.match_buffer(p_output0, (T.int64(1), T.int64(1), vocab_size))
        # with T.block("root"):
        compute = T.alloc_buffer((vocab_size, T.int64(2560)), "float16")
        dequantize_intermediate = T.alloc_buffer((vocab_size, T.int64(2560)), "float16")
        NT_matmul_intermediate = T.alloc_buffer((T.int64(1), T.int64(1), vocab_size), "float16")
        for i0, i1 in T.grid(vocab_size, T.int64(2560)):
            with T.block("compute"):
                v_i0, v_i1 = T.axis.remap("SS", [i0, i1])
                T.reads(embed_out_q_weight2[v_i0, v_i1 // T.int64(8)])
                T.writes(compute[v_i0, v_i1])
                compute[v_i0, v_i1] = T.Cast("float16", T.bitwise_and(T.shift_right(embed_out_q_weight2[v_i0, v_i1 // T.int64(8)], T.Cast("uint32", v_i1 % T.int64(8) * T.int64(4))), T.uint32(15)))
        for i0, i1 in T.grid(vocab_size, T.int64(2560)):
            with T.block("dequantize"):
                v_i0, v_i1 = T.axis.remap("SS", [i0, i1])
                T.reads(compute[v_i0, v_i1], embed_out_q_scale2[v_i0, v_i1 // T.int64(32)])
                T.writes(dequantize_intermediate[v_i0, v_i1])
                dequantize_intermediate[v_i0, v_i1] = (compute[v_i0, v_i1] - T.float16(7.0)) * embed_out_q_scale2[v_i0, v_i1 // T.int64(32)]
        for i0, i1, i2, k in T.grid(T.int64(1), T.int64(1), vocab_size, T.int64(2560)):
            with T.block("NT_matmul"):
                v_i0, v_i1, v_i2, v_k = T.axis.remap("SSSR", [i0, i1, i2, k])
                T.reads(layer_norm129[v_i0, v_i1, v_k], dequantize_intermediate[v_i2, v_k])
                T.writes(NT_matmul_intermediate[v_i0, v_i1, v_i2])
                with T.init():
                    NT_matmul_intermediate[v_i0, v_i1, v_i2] = T.float16(0.0)
                NT_matmul_intermediate[v_i0, v_i1, v_i2] = NT_matmul_intermediate[v_i0, v_i1, v_i2] + layer_norm129[v_i0, v_i1, v_k] * dequantize_intermediate[v_i2, v_k]
        for i0, i1, i2 in T.grid(T.int64(1), T.int64(1), vocab_size):
            with T.block("compute_1"):
                v_i0, v_i1, v_i2 = T.axis.remap("SSS", [i0, i1, i2])
                T.reads(NT_matmul_intermediate[v_i0, v_i1, v_i2])
                T.writes(compute_intermediate_intermediate[v_i0, v_i1, v_i2])
                compute_intermediate_intermediate[v_i0, v_i1, v_i2] = T.Cast("float32", NT_matmul_intermediate[v_i0, v_i1, v_i2])

    @T.prim_func(private=True)
    def fused_dequantize_fused_NT_matmul4_cast2(p_embed_out_q_weight4: T.handle, p_embed_out_q_scale4: T.handle, p_layer_norm259: T.handle, p_output0: T.handle):
        T.func_attr({"tir.noalias": T.bool(True)})
        vocab_size = T.int64()
        embed_out_q_weight4 = T.match_buffer(p_embed_out_q_weight4, (vocab_size, T.int64(320)), "uint32")
        embed_out_q_scale4 = T.match_buffer(p_embed_out_q_scale4, (vocab_size, T.int64(80)), "float16")
        batch_size = T.int64()
        layer_norm259 = T.match_buffer(p_layer_norm259, (batch_size, T.int64(1), T.int64(2560)), "float16")
        compute_intermediate_intermediate = T.match_buffer(p_output0, (batch_size, T.int64(1), vocab_size))
        # with T.block("root"):
        compute = T.alloc_buffer((vocab_size, T.int64(2560)), "float16")
        dequantize_intermediate = T.alloc_buffer((vocab_size, T.int64(2560)), "float16")
        NT_matmul_intermediate = T.alloc_buffer((batch_size, T.int64(1), vocab_size), "float16")
        for i0, i1 in T.grid(vocab_size, T.int64(2560)):
            with T.block("compute"):
                v_i0, v_i1 = T.axis.remap("SS", [i0, i1])
                T.reads(embed_out_q_weight4[v_i0, v_i1 // T.int64(8)])
                T.writes(compute[v_i0, v_i1])
                compute[v_i0, v_i1] = T.Cast("float16", T.bitwise_and(T.shift_right(embed_out_q_weight4[v_i0, v_i1 // T.int64(8)], T.Cast("uint32", v_i1 % T.int64(8) * T.int64(4))), T.uint32(15)))
        for i0, i1 in T.grid(vocab_size, T.int64(2560)):
            with T.block("dequantize"):
                v_i0, v_i1 = T.axis.remap("SS", [i0, i1])
                T.reads(compute[v_i0, v_i1], embed_out_q_scale4[v_i0, v_i1 // T.int64(32)])
                T.writes(dequantize_intermediate[v_i0, v_i1])
                dequantize_intermediate[v_i0, v_i1] = (compute[v_i0, v_i1] - T.float16(7.0)) * embed_out_q_scale4[v_i0, v_i1 // T.int64(32)]
        for i0, i1, i2, k in T.grid(batch_size, T.int64(1), vocab_size, T.int64(2560)):
            with T.block("NT_matmul"):
                v_i0, v_i1, v_i2, v_k = T.axis.remap("SSSR", [i0, i1, i2, k])
                T.reads(layer_norm259[v_i0, v_i1, v_k], dequantize_intermediate[v_i2, v_k])
                T.writes(NT_matmul_intermediate[v_i0, v_i1, v_i2])
                with T.init():
                    NT_matmul_intermediate[v_i0, v_i1, v_i2] = T.float16(0.0)
                NT_matmul_intermediate[v_i0, v_i1, v_i2] = NT_matmul_intermediate[v_i0, v_i1, v_i2] + layer_norm259[v_i0, v_i1, v_k] * dequantize_intermediate[v_i2, v_k]
        for i0, i1, i2 in T.grid(batch_size, T.int64(1), vocab_size):
            with T.block("compute_1"):
                v_i0, v_i1, v_i2 = T.axis.remap("SSS", [i0, i1, i2])
                T.reads(NT_matmul_intermediate[v_i0, v_i1, v_i2])
                T.writes(compute_intermediate_intermediate[v_i0, v_i1, v_i2])
                compute_intermediate_intermediate[v_i0, v_i1, v_i2] = T.Cast("float32", NT_matmul_intermediate[v_i0, v_i1, v_i2])

    @T.prim_func(private=True)
    def fused_dequantize_fused_NT_matmul9_cast5(p_embed_out_q_weight3: T.handle, p_embed_out_q_scale3: T.handle, p_take1: T.handle, p_output0: T.handle):
        T.func_attr({"tir.noalias": T.bool(True)})
        vocab_size = T.int64()
        embed_out_q_weight3 = T.match_buffer(p_embed_out_q_weight3, (vocab_size, T.int64(320)), "uint32")
        embed_out_q_scale3 = T.match_buffer(p_embed_out_q_scale3, (vocab_size, T.int64(80)), "float16")
        batch_size = T.int64()
        take1 = T.match_buffer(p_take1, (T.int64(1), batch_size, T.int64(2560)), "float16")
        compute_intermediate_intermediate = T.match_buffer(p_output0, (T.int64(1), batch_size, vocab_size))
        # with T.block("root"):
        compute = T.alloc_buffer((vocab_size, T.int64(2560)), "float16")
        dequantize_intermediate = T.alloc_buffer((vocab_size, T.int64(2560)), "float16")
        NT_matmul_intermediate = T.alloc_buffer((T.int64(1), batch_size, vocab_size), "float16")
        for i0, i1 in T.grid(vocab_size, T.int64(2560)):
            with T.block("compute"):
                v_i0, v_i1 = T.axis.remap("SS", [i0, i1])
                T.reads(embed_out_q_weight3[v_i0, v_i1 // T.int64(8)])
                T.writes(compute[v_i0, v_i1])
                compute[v_i0, v_i1] = T.Cast("float16", T.bitwise_and(T.shift_right(embed_out_q_weight3[v_i0, v_i1 // T.int64(8)], T.Cast("uint32", v_i1 % T.int64(8) * T.int64(4))), T.uint32(15)))
        for i0, i1 in T.grid(vocab_size, T.int64(2560)):
            with T.block("dequantize"):
                v_i0, v_i1 = T.axis.remap("SS", [i0, i1])
                T.reads(compute[v_i0, v_i1], embed_out_q_scale3[v_i0, v_i1 // T.int64(32)])
                T.writes(dequantize_intermediate[v_i0, v_i1])
                dequantize_intermediate[v_i0, v_i1] = (compute[v_i0, v_i1] - T.float16(7.0)) * embed_out_q_scale3[v_i0, v_i1 // T.int64(32)]
        for i0, i1, i2, k in T.grid(T.int64(1), batch_size, vocab_size, T.int64(2560)):
            with T.block("NT_matmul"):
                v_i0, v_i1, v_i2, v_k = T.axis.remap("SSSR", [i0, i1, i2, k])
                T.reads(take1[v_i0, v_i1, v_k], dequantize_intermediate[v_i2, v_k])
                T.writes(NT_matmul_intermediate[v_i0, v_i1, v_i2])
                with T.init():
                    NT_matmul_intermediate[v_i0, v_i1, v_i2] = T.float16(0.0)
                NT_matmul_intermediate[v_i0, v_i1, v_i2] = NT_matmul_intermediate[v_i0, v_i1, v_i2] + take1[v_i0, v_i1, v_k] * dequantize_intermediate[v_i2, v_k]
        for i0, i1, i2 in T.grid(T.int64(1), batch_size, vocab_size):
            with T.block("compute_1"):
                v_i0, v_i1, v_i2 = T.axis.remap("SSS", [i0, i1, i2])
                T.reads(NT_matmul_intermediate[v_i0, v_i1, v_i2])
                T.writes(compute_intermediate_intermediate[v_i0, v_i1, v_i2])
                compute_intermediate_intermediate[v_i0, v_i1, v_i2] = T.Cast("float32", NT_matmul_intermediate[v_i0, v_i1, v_i2])

    @T.prim_func(private=True)
    def fused_dequantize_take1(p_gpt_neox_embed_in_q_weight: T.handle, p_gpt_neox_embed_in_q_scale: T.handle, p_input_ids: T.handle, p_output0: T.handle):
        T.func_attr({"tir.noalias": T.bool(True)})
        vocab_size = T.int32()
        gpt_neox_embed_in_q_weight = T.match_buffer(p_gpt_neox_embed_in_q_weight, (vocab_size, 320), "uint32")
        gpt_neox_embed_in_q_scale = T.match_buffer(p_gpt_neox_embed_in_q_scale, (vocab_size, 80), "float16")
        seq_len = T.int32()
        input_ids = T.match_buffer(p_input_ids, (seq_len,), "int32")
        T_take_intermediate = T.match_buffer(p_output0, (seq_len, 2560), "float16")
        # with T.block("root"):
        compute = T.alloc_buffer((vocab_size, 2560), "float16")
        for i0, i1 in T.grid(vocab_size, 2560):
            with T.block("compute"):
                v_i0, v_i1 = T.axis.remap("SS", [i0, i1])
                T.reads(gpt_neox_embed_in_q_weight[v_i0, v_i1 // 8])
                T.writes(compute[v_i0, v_i1])
                compute[v_i0, v_i1] = T.Cast("float16", T.bitwise_and(T.shift_right(gpt_neox_embed_in_q_weight[v_i0, v_i1 // 8], T.Cast("uint32", v_i1 % 8 * 4)), T.uint32(15)))
        for ax0, ax1 in T.grid(seq_len, 2560):
            with T.block("T_take"):
                v_ax0, v_ax1 = T.axis.remap("SS", [ax0, ax1])
                T.reads(input_ids[v_ax0], compute[input_ids[v_ax0], v_ax1], gpt_neox_embed_in_q_scale[input_ids[v_ax0], v_ax1 // 32])
                T.writes(T_take_intermediate[v_ax0, v_ax1])
                T_take_intermediate[v_ax0, v_ax1] = (compute[input_ids[v_ax0], v_ax1] - T.float16(7.0)) * gpt_neox_embed_in_q_scale[input_ids[v_ax0], v_ax1 // 32]

    @T.prim_func(private=True)
    def fused_reshape10_reshape11(lv164: T.Buffer((T.int64(1), T.int64(32), T.int64(80)), "float16"), T_reshape_intermediate_1: T.Buffer((T.int64(1), T.int64(1), T.int64(2560)), "float16")):
        T.func_attr({"tir.noalias": T.bool(True)})
        # with T.block("root"):
        T_reshape_intermediate = T.alloc_buffer((T.int64(1), T.int64(1), T.int64(32), T.int64(80)), "float16")
        for ax0, ax1, ax2, ax3 in T.grid(T.int64(1), T.int64(1), T.int64(32), T.int64(80)):
            with T.block("T_reshape"):
                v_ax0, v_ax1, v_ax2, v_ax3 = T.axis.remap("SSSS", [ax0, ax1, ax2, ax3])
                T.reads(lv164[T.int64(0), (v_ax3 // T.int64(80) + v_ax2) % T.int64(32), v_ax3 % T.int64(80)])
                T.writes(T_reshape_intermediate[v_ax0, v_ax1, v_ax2, v_ax3])
                T_reshape_intermediate[v_ax0, v_ax1, v_ax2, v_ax3] = lv164[T.int64(0), (v_ax3 // T.int64(80) + v_ax2) % T.int64(32), v_ax3 % T.int64(80)]
        for ax0, ax1, ax2 in T.grid(T.int64(1), T.int64(1), T.int64(2560)):
            with T.block("T_reshape_1"):
                v_ax0, v_ax1, v_ax2 = T.axis.remap("SSS", [ax0, ax1, ax2])
                T.reads(T_reshape_intermediate[T.int64(0), T.int64(0), v_ax2 % T.int64(2560) // T.int64(80), v_ax2 % T.int64(80)])
                T.writes(T_reshape_intermediate_1[v_ax0, v_ax1, v_ax2])
                T_reshape_intermediate_1[v_ax0, v_ax1, v_ax2] = T_reshape_intermediate[T.int64(0), T.int64(0), v_ax2 % T.int64(2560) // T.int64(80), v_ax2 % T.int64(80)]

    @T.prim_func(private=True)
    def fused_reshape8_reshape9(add192: T.Buffer((T.int64(1), T.int64(1), T.int64(7680)), "float16"), T_reshape_intermediate_1: T.Buffer((T.int64(1), T.int64(96), T.int64(80)), "float16")):
        T.func_attr({"tir.noalias": T.bool(True)})
        # with T.block("root"):
        T_reshape_intermediate = T.alloc_buffer((T.int64(1), T.int64(1), T.int64(96), T.int64(80)), "float16")
        for ax0, ax1, ax2, ax3 in T.grid(T.int64(1), T.int64(1), T.int64(96), T.int64(80)):
            with T.block("T_reshape"):
                v_ax0, v_ax1, v_ax2, v_ax3 = T.axis.remap("SSSS", [ax0, ax1, ax2, ax3])
                T.reads(add192[T.int64(0), T.int64(0), (v_ax2 * T.int64(80) + v_ax3) % T.int64(7680)])
                T.writes(T_reshape_intermediate[v_ax0, v_ax1, v_ax2, v_ax3])
                T_reshape_intermediate[v_ax0, v_ax1, v_ax2, v_ax3] = add192[T.int64(0), T.int64(0), (v_ax2 * T.int64(80) + v_ax3) % T.int64(7680)]
        for ax0, ax1, ax2 in T.grid(T.int64(1), T.int64(96), T.int64(80)):
            with T.block("T_reshape_1"):
                v_ax0, v_ax1, v_ax2 = T.axis.remap("SSS", [ax0, ax1, ax2])
                T.reads(T_reshape_intermediate[T.int64(0), T.int64(0), (v_ax2 // T.int64(80) + v_ax1) % T.int64(96), v_ax2 % T.int64(80)])
                T.writes(T_reshape_intermediate_1[v_ax0, v_ax1, v_ax2])
                T_reshape_intermediate_1[v_ax0, v_ax1, v_ax2] = T_reshape_intermediate[T.int64(0), T.int64(0), (v_ax2 // T.int64(80) + v_ax1) % T.int64(96), v_ax2 % T.int64(80)]

    @T.prim_func
    def fused_rope(var_qkv: T.handle, var_position_map: T.handle, var_q: T.handle, var_k: T.handle, var_v: T.handle, apply_rope: T.int32):
        T.func_attr({"target": T.target({"arch": "sm_75", "host": {"keys": ["cpu"], "kind": "llvm", "mcpu": "skylake", "mtriple": "x86_64-conda-linux-gnu", "tag": ""}, "keys": ["cuda", "gpu"], "kind": "cuda", "libs": ["thrust"], "max_num_threads": 1024, "max_shared_memory_per_block": 49152, "max_threads_per_block": 1024, "tag": "", "thread_warp_size": 32}), "tir.noalias": T.bool(True)})
        seq_len = T.int32()
        qkv = T.match_buffer(var_qkv, (seq_len, 96, 80), "float16")
        position_map = T.match_buffer(var_position_map, (seq_len,), "int32", offset_factor=1)
        q = T.match_buffer(var_q, (seq_len, 32, 80), "float16")
        k = T.match_buffer(var_k, (seq_len, 32, 80), "float16")
        v = T.match_buffer(var_v, (seq_len, 32, 80), "float16")
        # with T.block("root"):
        for iters_0, iters_1, iters_2 in T.grid(seq_len, 96, 80):
            with T.block("llama_fused_rope"):
                s, h, d = T.axis.remap("SSS", [iters_0, iters_1, iters_2])
                T.reads(position_map[s], qkv[s, h, d - 40:d - 40 + 81])
                T.writes(q[s, h, d], k[s, h - 32, d], v[s, h - 64, d])
                if h < 32:
                    freq = T.float32()
                    q[s, h, d] = T.if_then_else(apply_rope > 0 and d < 80, T.Let(T.Cast("float16", T.cos(freq) * T.Cast("float32", qkv[s, h, d]) + T.sin(freq) * T.Cast("float32", T.if_then_else(d < 40, qkv[s, h, d + 40] * T.float16(-1.0), qkv[s, h, d - 40]))), where={freq: T.Cast("float32", position_map[s]) / T.pow(T.float32(10000.0), T.Cast("float32", d * 2 % 80) / T.float32(80.0))}), qkv[s, h, d])
                else:
                    if h < 64:
                        freq = T.float32()
                        k[s, h - 32, d] = T.if_then_else(apply_rope > 0 and d < 80, T.Let(T.Cast("float16", T.cos(freq) * T.Cast("float32", qkv[s, h, d]) + T.sin(freq) * T.Cast("float32", T.if_then_else(d < 40, qkv[s, h, d + 40] * T.float16(-1.0), qkv[s, h, d - 40]))), where={freq: T.Cast("float32", position_map[s]) / T.pow(T.float32(10000.0), T.Cast("float32", d * 2 % 80) / T.float32(80.0))}), qkv[s, h, d])
                    else:
                        v[s, h - 64, d] = qkv[s, h, d]

    @T.prim_func
    def gather_probs(var_src: T.handle, var_indices: T.handle, var_dst: T.handle):
        T.func_attr({"target": T.target({"arch": "sm_75", "host": {"keys": ["cpu"], "kind": "llvm", "mcpu": "skylake", "mtriple": "x86_64-conda-linux-gnu", "tag": ""}, "keys": ["cuda", "gpu"], "kind": "cuda", "libs": ["thrust"], "max_num_threads": 1024, "max_shared_memory_per_block": 49152, "max_threads_per_block": 1024, "tag": "", "thread_warp_size": 32}), "tir.noalias": T.bool(True)})
        m, n = T.int32(is_size_var=True), T.int32(is_size_var=True)
        src = T.match_buffer(var_src, (m, n))
        batch_size = T.int32(is_size_var=True)
        indices = T.match_buffer(var_indices, (batch_size,), "int32")
        dst = T.match_buffer(var_dst, (batch_size, n))
        # with T.block("root"):
        for b, j in T.grid(batch_size, n):
            with T.block("gather_2d"):
                vb, vj = T.axis.remap("SS", [b, j])
                T.reads(src[indices[vb], vj], indices[vb])
                T.writes(dst[vb, vj])
                dst[vb, vj] = src[indices[vb], vj]

    @T.prim_func(private=True)
    def get_index_from_sorted(A: T.handle, B: T.handle, C: T.handle, D: T.handle, E: T.handle, F: T.handle):
        T.func_attr({"target": T.target({"arch": "sm_75", "keys": ["cuda", "gpu"], "kind": "cuda", "libs": ["thrust"], "max_num_threads": 1024, "max_shared_memory_per_block": 49152, "max_threads_per_block": 1024, "tag": "", "thread_warp_size": 32})})
        batch, vocab_size = T.int64(is_size_var=True), T.int64(is_size_var=True)
        cumsum_sorted = T.match_buffer(A, (batch, vocab_size))
        indices = T.match_buffer(B, (batch, vocab_size), "int32")
        renorm_prob = T.match_buffer(C, (batch, 1))
        out_batch = T.int64(is_size_var=True)
        usample = T.match_buffer(D, (out_batch, 1))
        sample_indices = T.match_buffer(E, (out_batch, 1), "int32")
        output_index = T.match_buffer(F, (out_batch, 1), "int32")
        # with T.block("root"):
        for ax0, ax1 in T.grid(out_batch, vocab_size):
            with T.block("T_get_index_from_sorted"):
                v_ax0, v_ax1 = T.axis.remap("SS", [ax0, ax1])
                T.reads(usample[v_ax0, T.int64(0)], cumsum_sorted[sample_indices[v_ax0, T.int64(0)], v_ax1 - T.int64(1):v_ax1 - T.int64(1) + T.int64(2)], sample_indices[v_ax0, T.int64(0)], renorm_prob[sample_indices[v_ax0, T.int64(0)], 0], indices[sample_indices[v_ax0, T.int64(0)], T.min(T.int64(0), v_ax1):T.min(T.int64(0), v_ax1) + (T.max(T.int64(0), v_ax1) + T.int64(1) - T.min(T.int64(0), v_ax1))])
                T.writes(output_index[v_ax0, 0])
                if usample[v_ax0, T.int64(0)] < cumsum_sorted[sample_indices[v_ax0, T.int64(0)], v_ax1] / renorm_prob[sample_indices[v_ax0, T.int64(0)], 0] or v_ax1 + T.int64(1) == vocab_size:
                    if v_ax1 == T.int64(0):
                        output_index[v_ax0, 0] = indices[sample_indices[v_ax0, T.int64(0)], 0]
                    else:
                        if usample[v_ax0, T.int64(0)] >= cumsum_sorted[sample_indices[v_ax0, T.int64(0)], v_ax1 - T.int64(1)] / renorm_prob[sample_indices[v_ax0, T.int64(0)], 0]:
                            output_index[v_ax0, 0] = indices[sample_indices[v_ax0, T.int64(0)], v_ax1]

    @T.prim_func(private=True)
    def get_renorm_prob(A: T.handle, B: T.handle, C: T.handle, D: T.handle):
        T.func_attr({"target": T.target({"arch": "sm_75", "keys": ["cuda", "gpu"], "kind": "cuda", "libs": ["thrust"], "max_num_threads": 1024, "max_shared_memory_per_block": 49152, "max_threads_per_block": 1024, "tag": "", "thread_warp_size": 32})})
        batch, vocab_size = T.int64(is_size_var=True), T.int64(is_size_var=True)
        cumsum_sorted = T.match_buffer(A, (batch, vocab_size))
        top_p = T.match_buffer(B, (batch, 1))
        top_k = T.match_buffer(C, (batch, 1), "int32")
        renorm_prob = T.match_buffer(D, (batch, 1))
        # with T.block("root"):
        for ax0, ax1 in T.grid(batch, vocab_size):
            with T.block("T_get_renorm_prob"):
                v_ax0, v_ax1 = T.axis.remap("SS", [ax0, ax1])
                T.reads(cumsum_sorted[v_ax0, T.min(T.min(T.int64(0), v_ax1), v_ax1 + T.int64(1)):T.min(T.min(T.int64(0), v_ax1), v_ax1 + T.int64(1)) + (T.max(T.max(T.int64(0), v_ax1), v_ax1 + T.int64(1)) + T.int64(1) - T.min(T.min(T.int64(0), v_ax1), v_ax1 + T.int64(1)))], top_p[v_ax0, 0], top_k[v_ax0, 0])
                T.writes(renorm_prob[v_ax0, 0])
                if not (cumsum_sorted[v_ax0, 0] < top_p[v_ax0, 0] and top_k[v_ax0, 0] > 1):
                    renorm_prob[v_ax0, 0] = cumsum_sorted[v_ax0, 0]
                else:
                    if cumsum_sorted[v_ax0, v_ax1] < top_p[v_ax0, 0] and v_ax1 + T.int64(1) < T.Cast("int64", top_k[v_ax0, 0]):
                        if v_ax1 + T.int64(1) == vocab_size:
                            renorm_prob[v_ax0, 0] = cumsum_sorted[v_ax0, v_ax1]
                        else:
                            if not (cumsum_sorted[v_ax0, v_ax1 + T.int64(1)] < top_p[v_ax0, 0] and v_ax1 + T.int64(1) + T.int64(1) < T.Cast("int64", top_k[v_ax0, 0])):
                                renorm_prob[v_ax0, 0] = cumsum_sorted[v_ax0, v_ax1 + T.int64(1)]

    @T.prim_func(private=True)
    def gpu_2d_continuous_cumsum(var_a: T.handle, var_out: T.handle):
        T.func_attr({"tir.is_scheduled": 1})
        m, n = T.int64(), T.int64()
        A = T.match_buffer(var_a, (m, n))
        Out = T.match_buffer(var_out, (m, n))
        # with T.block("root"):
        Tmp = T.alloc_buffer((m, n))
        ceil_log2: T.int64 = T.Cast("int64", T.ceil(T.log2(T.Cast("float32", n))))
        total_rounds: T.int64 = ceil_log2 // T.int64(9)
        for by in T.thread_binding(m, thread="blockIdx.y"):
            for bx in T.thread_binding((n + T.int64(511)) // T.int64(512), thread="blockIdx.x"):
                with T.block(""):
                    T.reads(A[by, bx * T.int64(512):bx * T.int64(512) + T.int64(512)])
                    T.writes(Out[by, bx * T.int64(512):bx * T.int64(512) + T.int64(512)], Tmp[by, bx])
                    local_buf = T.alloc_buffer((4,), scope="local")
                    shared_buf = T.alloc_buffer((T.int64(512),), scope="shared")
                    for ty in T.thread_binding(T.int64(4), thread="threadIdx.y"):
                        for tx in T.thread_binding(T.int64(32), thread="threadIdx.x"):
                            tx_idx: T.int64 = bx * T.int64(512) + ty * T.int64(128) + tx * T.int64(4)
                            for i in T.vectorized(T.int64(4)):
                                local_buf[i] = T.if_then_else(tx_idx + i < n, T.Cast("float32", A[by, tx_idx + i]), T.Cast("float32", 0))
                            for i in T.unroll(T.int64(1), T.int64(4)):
                                local_buf[i] = local_buf[i] + local_buf[i - T.int64(1)]
                            for i in T.vectorized(T.int64(4)):
                                shared_buf[ty * T.int64(128) + tx * T.int64(4) + i] = local_buf[i]
                            for i in T.unroll(T.int64(5)):
                                for j in T.vectorized(T.int64(4)):
                                    idx: T.int64 = ty * T.int64(128) + tx * T.int64(4)
                                    if tx >= T.shift_left(T.int64(1), i):
                                        shared_buf[idx + j] = shared_buf[idx + j] + shared_buf[idx - T.shift_left(T.int64(1), i) * T.int64(4) + T.int64(4) - T.int64(1)]
                            for i in T.unroll(T.int64(1), T.int64(4)):
                                for j in T.vectorized(T.int64(4)):
                                    if ty == T.int64(0):
                                        idx: T.int64 = i * T.int64(128) + tx * T.int64(4)
                                        shared_buf[idx + j] = shared_buf[idx + j] + shared_buf[i * T.int64(128) - T.int64(1)]
                            for i in T.vectorized(T.int64(4)):
                                idx: T.int64 = ty * T.int64(128) + tx * T.int64(4) + i
                                if bx * T.int64(512) + idx < n:
                                    Out[by, bx * T.int64(512) + idx] = shared_buf[idx]
                            if tx == T.int64(0) and ty == T.int64(0):
                                for i in T.vectorized(T.int64(4)):
                                    Tmp[by, bx] = shared_buf[T.int64(511)]
        for i in range(total_rounds):
            cur_len: T.int64 = (n + T.shift_left(T.int64(1), T.int64(9) * (i + T.int64(1))) - T.int64(1)) // T.shift_left(T.int64(1), T.int64(9) * (i + T.int64(1)))
            for by in T.thread_binding(m, thread="blockIdx.y"):
                for bx in T.thread_binding((cur_len + T.int64(511)) // T.int64(512), thread="blockIdx.x"):
                    with T.block(""):
                        T.reads(Tmp[by, bx * T.int64(512) + i * ((n + T.int64(511)) // T.int64(512)):bx * T.int64(512) + i * ((n + T.int64(511)) // T.int64(512)) + T.int64(512)])
                        T.writes(Tmp[by, T.min(bx * T.int64(512) + i * ((n + T.int64(511)) // T.int64(512)), (i + T.int64(1)) * ((n + T.int64(511)) // T.int64(512)) + bx):T.min(bx * T.int64(512) + i * ((n + T.int64(511)) // T.int64(512)), (i + T.int64(1)) * ((n + T.int64(511)) // T.int64(512)) + bx) + (T.max(bx * T.int64(512) + i * ((n + T.int64(511)) // T.int64(512)) + T.int64(511), (i + T.int64(1)) * ((n + T.int64(511)) // T.int64(512)) + bx) + T.int64(1) - T.min(bx * T.int64(512) + i * ((n + T.int64(511)) // T.int64(512)), (i + T.int64(1)) * ((n + T.int64(511)) // T.int64(512)) + bx))])
                        local_buf = T.alloc_buffer((4,), scope="local")
                        shared_buf = T.alloc_buffer((T.int64(512),), scope="shared")
                        for ty in T.thread_binding(T.int64(4), thread="threadIdx.y"):
                            for tx in T.thread_binding(T.int64(32), thread="threadIdx.x"):
                                tx_idx: T.int64 = bx * T.int64(512) + ty * T.int64(128) + tx * T.int64(4)
                                for i_1 in T.vectorized(T.int64(4)):
                                    local_buf[i_1] = T.if_then_else(tx_idx + i_1 < cur_len, T.Cast("float32", Tmp[by, i * ((n + T.int64(512) - T.int64(1)) // T.int64(512)) + tx_idx + i_1]), T.Cast("float32", 0))
                                for i_1 in T.unroll(T.int64(1), T.int64(4)):
                                    local_buf[i_1] = local_buf[i_1] + local_buf[i_1 - T.int64(1)]
                                for i_1 in T.vectorized(T.int64(4)):
                                    shared_buf[ty * T.int64(128) + tx * T.int64(4) + i_1] = local_buf[i_1]
                                for i_1 in T.unroll(T.int64(5)):
                                    for j in T.vectorized(T.int64(4)):
                                        idx: T.int64 = ty * T.int64(128) + tx * T.int64(4)
                                        if tx >= T.shift_left(T.int64(1), i_1):
                                            shared_buf[idx + j] = shared_buf[idx + j] + shared_buf[idx - T.shift_left(T.int64(1), i_1) * T.int64(4) + T.int64(4) - T.int64(1)]
                                for i_1 in T.unroll(T.int64(1), T.int64(4)):
                                    for j in T.vectorized(T.int64(4)):
                                        if ty == T.int64(0):
                                            idx: T.int64 = i_1 * T.int64(128) + tx * T.int64(4)
                                            shared_buf[idx + j] = shared_buf[idx + j] + shared_buf[i_1 * T.int64(128) - T.int64(1)]
                                for i_1 in T.vectorized(T.int64(4)):
                                    idx: T.int64 = ty * T.int64(128) + tx * T.int64(4) + i_1
                                    if bx * T.int64(512) + idx < cur_len:
                                        Tmp[by, i * ((n + T.int64(512) - T.int64(1)) // T.int64(512)) + bx * T.int64(512) + idx] = shared_buf[idx]
                                if tx == T.int64(0) and ty == T.int64(0):
                                    for i_1 in T.vectorized(T.int64(4)):
                                        Tmp[by, (i + T.int64(1)) * ((n + T.int64(512) - T.int64(1)) // T.int64(512)) + bx] = shared_buf[T.int64(511)]
        for i in range(total_rounds - T.int64(1)):
            real_idx: T.int64 = total_rounds - T.int64(1) - i - T.int64(1)
            cur_len: T.int64 = (n + T.shift_left(T.int64(1), T.int64(9) * (real_idx + T.int64(1))) - T.int64(1)) // T.shift_left(T.int64(1), T.int64(9) * (real_idx + T.int64(1)))
            for by in T.thread_binding(m, thread="blockIdx.y"):
                for bx in T.thread_binding((cur_len + T.int64(511)) // T.int64(512), thread="blockIdx.x"):
                    for ty in T.thread_binding(T.int64(4), thread="threadIdx.y"):
                        for tx in T.thread_binding(T.int64(32), thread="threadIdx.x"):
                            for i_1 in range(T.int64(4)):
                                idx: T.int64 = bx * T.int64(512) + ty * T.int64(128) + i_1 * T.int64(32) + tx
                                if idx < cur_len:
                                    Tmp[by, real_idx * ((n + T.int64(512) - T.int64(1)) // T.int64(512)) + idx] = Tmp[by, real_idx * ((n + T.int64(512) - T.int64(1)) // T.int64(512)) + idx] + T.if_then_else(bx > T.int64(0), Tmp[by, (real_idx + T.int64(1)) * ((n + T.int64(512) - T.int64(1)) // T.int64(512)) + bx - T.int64(1)], T.float32(0.0))
        for by in T.thread_binding(m, thread="blockIdx.y"):
            for bx in T.thread_binding((n + T.int64(511)) // T.int64(512), thread="blockIdx.x"):
                for ty in T.thread_binding(T.int64(4), thread="threadIdx.y"):
                    for tx in T.thread_binding(T.int64(32), thread="threadIdx.x"):
                        for i in range(T.int64(4)):
                            idx: T.int64 = bx * T.int64(512) + ty * T.int64(128) + i * T.int64(32) + tx
                            if idx < n:
                                Out[by, idx] = Out[by, idx] + T.if_then_else(bx > T.int64(0), Tmp[by, bx - T.int64(1)], T.float32(0.0))

    @T.prim_func(private=True)
    def index(var_layer_norm64: T.handle, index: T.Buffer((T.int64(1), T.int64(1), T.int64(2560)), "float16")):
        T.func_attr({"target": T.target({"arch": "sm_75", "keys": ["cuda", "gpu"], "kind": "cuda", "libs": ["thrust"], "max_num_threads": 1024, "max_shared_memory_per_block": 49152, "max_threads_per_block": 1024, "tag": "", "thread_warp_size": 32}), "tir.noalias": T.bool(True)})
        seq_len = T.int64()
        layer_norm64 = T.match_buffer(var_layer_norm64, (T.int64(1), seq_len, T.int64(2560)), "float16")
        # with T.block("root"):
        for i, _, k in T.grid(T.int64(1), T.int64(1), T.int64(2560)):
            with T.block("index"):
                v_i, v__, v_k = T.axis.remap("SSS", [i, _, k])
                T.reads(layer_norm64[v_i, seq_len - T.int64(1), v_k])
                T.writes(index[v_i, v__, v_k])
                index[v_i, v__, v_k] = layer_norm64[v_i, seq_len - T.int64(1), v_k]

    @T.prim_func(private=True)
    def layer_norm(var_input_embeds: T.handle, gpt_neox_layers_0_input_layernorm_weight4: T.Buffer((T.int64(2560),), "float16"), gpt_neox_layers_0_input_layernorm_bias4: T.Buffer((T.int64(2560),), "float16"), var_T_layer_norm: T.handle):
        T.func_attr({"tir.noalias": T.bool(True)})
        batch_size = T.int64()
        input_embeds = T.match_buffer(var_input_embeds, (batch_size, T.int64(1), T.int64(2560)), "float16")
        T_layer_norm = T.match_buffer(var_T_layer_norm, (batch_size, T.int64(1), T.int64(2560)), "float16")
        # with T.block("root"):
        input_embeds_red_temp_v0 = T.alloc_buffer((batch_size, T.int64(1)))
        input_embeds_red_temp_v1 = T.alloc_buffer((batch_size, T.int64(1)))
        for ax0, ax1, k2 in T.grid(batch_size, T.int64(1), T.int64(2560)):
            with T.block("input_embeds_red_temp"):
                v_ax0, v_ax1, v_k2 = T.axis.remap("SSR", [ax0, ax1, k2])
                T.reads(input_embeds[v_ax0, v_ax1, v_k2])
                T.writes(input_embeds_red_temp_v0[v_ax0, v_ax1], input_embeds_red_temp_v1[v_ax0, v_ax1])
                with T.init():
                    input_embeds_red_temp_v0[v_ax0, v_ax1] = T.float32(0.0)
                    input_embeds_red_temp_v1[v_ax0, v_ax1] = T.float32(0.0)
                v_input_embeds_red_temp_v0: T.float32 = input_embeds_red_temp_v0[v_ax0, v_ax1] + T.Cast("float32", input_embeds[v_ax0, v_ax1, v_k2])
                v_input_embeds_red_temp_v1: T.float32 = input_embeds_red_temp_v1[v_ax0, v_ax1] + T.Cast("float32", input_embeds[v_ax0, v_ax1, v_k2]) * T.Cast("float32", input_embeds[v_ax0, v_ax1, v_k2])
                input_embeds_red_temp_v0[v_ax0, v_ax1] = v_input_embeds_red_temp_v0
                input_embeds_red_temp_v1[v_ax0, v_ax1] = v_input_embeds_red_temp_v1
        for ax0, ax1, ax2 in T.grid(batch_size, T.int64(1), T.int64(2560)):
            with T.block("T_layer_norm"):
                v_ax0, v_ax1, v_ax2 = T.axis.remap("SSS", [ax0, ax1, ax2])
                T.reads(input_embeds[v_ax0, v_ax1, v_ax2], input_embeds_red_temp_v0[v_ax0, v_ax1], input_embeds_red_temp_v1[v_ax0, v_ax1], gpt_neox_layers_0_input_layernorm_weight4[v_ax2], gpt_neox_layers_0_input_layernorm_bias4[v_ax2])
                T.writes(T_layer_norm[v_ax0, v_ax1, v_ax2])
                T_layer_norm[v_ax0, v_ax1, v_ax2] = T.Cast("float16", (T.Cast("float32", input_embeds[v_ax0, v_ax1, v_ax2]) - input_embeds_red_temp_v0[v_ax0, v_ax1] * T.float32(0.00039062500000000002)) * T.rsqrt(input_embeds_red_temp_v1[v_ax0, v_ax1] * T.float32(0.00039062500000000002) - input_embeds_red_temp_v0[v_ax0, v_ax1] * T.float32(0.00039062500000000002) * (input_embeds_red_temp_v0[v_ax0, v_ax1] * T.float32(0.00039062500000000002)) + T.float32(1.0000000000000001e-05))) * gpt_neox_layers_0_input_layernorm_weight4[v_ax2] + gpt_neox_layers_0_input_layernorm_bias4[v_ax2]

    @T.prim_func(private=True)
    def layer_norm1(var_input_embeds: T.handle, gpt_neox_layers_0_input_layernorm_weight3: T.Buffer((T.int64(2560),), "float16"), gpt_neox_layers_0_input_layernorm_bias3: T.Buffer((T.int64(2560),), "float16"), var_T_layer_norm: T.handle):
        T.func_attr({"tir.noalias": T.bool(True)})
        seq_len = T.int64()
        input_embeds = T.match_buffer(var_input_embeds, (T.int64(1), seq_len, T.int64(2560)), "float16")
        T_layer_norm = T.match_buffer(var_T_layer_norm, (T.int64(1), seq_len, T.int64(2560)), "float16")
        # with T.block("root"):
        input_embeds_red_temp_v0 = T.alloc_buffer((T.int64(1), seq_len))
        input_embeds_red_temp_v1 = T.alloc_buffer((T.int64(1), seq_len))
        for ax0, ax1, k2 in T.grid(T.int64(1), seq_len, T.int64(2560)):
            with T.block("input_embeds_red_temp"):
                v_ax0, v_ax1, v_k2 = T.axis.remap("SSR", [ax0, ax1, k2])
                T.reads(input_embeds[v_ax0, v_ax1, v_k2])
                T.writes(input_embeds_red_temp_v0[v_ax0, v_ax1], input_embeds_red_temp_v1[v_ax0, v_ax1])
                with T.init():
                    input_embeds_red_temp_v0[v_ax0, v_ax1] = T.float32(0.0)
                    input_embeds_red_temp_v1[v_ax0, v_ax1] = T.float32(0.0)
                v_input_embeds_red_temp_v0: T.float32 = input_embeds_red_temp_v0[v_ax0, v_ax1] + T.Cast("float32", input_embeds[v_ax0, v_ax1, v_k2])
                v_input_embeds_red_temp_v1: T.float32 = input_embeds_red_temp_v1[v_ax0, v_ax1] + T.Cast("float32", input_embeds[v_ax0, v_ax1, v_k2]) * T.Cast("float32", input_embeds[v_ax0, v_ax1, v_k2])
                input_embeds_red_temp_v0[v_ax0, v_ax1] = v_input_embeds_red_temp_v0
                input_embeds_red_temp_v1[v_ax0, v_ax1] = v_input_embeds_red_temp_v1
        for ax0, ax1, ax2 in T.grid(T.int64(1), seq_len, T.int64(2560)):
            with T.block("T_layer_norm"):
                v_ax0, v_ax1, v_ax2 = T.axis.remap("SSS", [ax0, ax1, ax2])
                T.reads(input_embeds[v_ax0, v_ax1, v_ax2], input_embeds_red_temp_v0[v_ax0, v_ax1], input_embeds_red_temp_v1[v_ax0, v_ax1], gpt_neox_layers_0_input_layernorm_weight3[v_ax2], gpt_neox_layers_0_input_layernorm_bias3[v_ax2])
                T.writes(T_layer_norm[v_ax0, v_ax1, v_ax2])
                T_layer_norm[v_ax0, v_ax1, v_ax2] = T.Cast("float16", (T.Cast("float32", input_embeds[v_ax0, v_ax1, v_ax2]) - input_embeds_red_temp_v0[v_ax0, v_ax1] * T.float32(0.00039062500000000002)) * T.rsqrt(input_embeds_red_temp_v1[v_ax0, v_ax1] * T.float32(0.00039062500000000002) - input_embeds_red_temp_v0[v_ax0, v_ax1] * T.float32(0.00039062500000000002) * (input_embeds_red_temp_v0[v_ax0, v_ax1] * T.float32(0.00039062500000000002)) + T.float32(1.0000000000000001e-05))) * gpt_neox_layers_0_input_layernorm_weight3[v_ax2] + gpt_neox_layers_0_input_layernorm_bias3[v_ax2]

    @T.prim_func(private=True)
    def layer_norm2(input_embed: T.Buffer((T.int64(1), T.int64(1), T.int64(2560)), "float16"), gpt_neox_layers_0_input_layernorm_weight2: T.Buffer((T.int64(2560),), "float16"), gpt_neox_layers_0_input_layernorm_bias2: T.Buffer((T.int64(2560),), "float16"), T_layer_norm: T.Buffer((T.int64(1), T.int64(1), T.int64(2560)), "float16")):
        T.func_attr({"tir.noalias": T.bool(True)})
        # with T.block("root"):
        input_embed_red_temp_v0 = T.alloc_buffer((T.int64(1), T.int64(1)))
        input_embed_red_temp_v1 = T.alloc_buffer((T.int64(1), T.int64(1)))
        for ax0, ax1, k2 in T.grid(T.int64(1), T.int64(1), T.int64(2560)):
            with T.block("input_embed_red_temp"):
                v_ax0, v_ax1, v_k2 = T.axis.remap("SSR", [ax0, ax1, k2])
                T.reads(input_embed[v_ax0, v_ax1, v_k2])
                T.writes(input_embed_red_temp_v0[v_ax0, v_ax1], input_embed_red_temp_v1[v_ax0, v_ax1])
                with T.init():
                    input_embed_red_temp_v0[v_ax0, v_ax1] = T.float32(0.0)
                    input_embed_red_temp_v1[v_ax0, v_ax1] = T.float32(0.0)
                v_input_embed_red_temp_v0: T.float32 = input_embed_red_temp_v0[v_ax0, v_ax1] + T.Cast("float32", input_embed[v_ax0, v_ax1, v_k2])
                v_input_embed_red_temp_v1: T.float32 = input_embed_red_temp_v1[v_ax0, v_ax1] + T.Cast("float32", input_embed[v_ax0, v_ax1, v_k2]) * T.Cast("float32", input_embed[v_ax0, v_ax1, v_k2])
                input_embed_red_temp_v0[v_ax0, v_ax1] = v_input_embed_red_temp_v0
                input_embed_red_temp_v1[v_ax0, v_ax1] = v_input_embed_red_temp_v1
        for ax0, ax1, ax2 in T.grid(T.int64(1), T.int64(1), T.int64(2560)):
            with T.block("T_layer_norm"):
                v_ax0, v_ax1, v_ax2 = T.axis.remap("SSS", [ax0, ax1, ax2])
                T.reads(input_embed[v_ax0, v_ax1, v_ax2], input_embed_red_temp_v0[v_ax0, v_ax1], input_embed_red_temp_v1[v_ax0, v_ax1], gpt_neox_layers_0_input_layernorm_weight2[v_ax2], gpt_neox_layers_0_input_layernorm_bias2[v_ax2])
                T.writes(T_layer_norm[v_ax0, v_ax1, v_ax2])
                T_layer_norm[v_ax0, v_ax1, v_ax2] = T.Cast("float16", (T.Cast("float32", input_embed[v_ax0, v_ax1, v_ax2]) - input_embed_red_temp_v0[v_ax0, v_ax1] * T.float32(0.00039062500000000002)) * T.rsqrt(input_embed_red_temp_v1[v_ax0, v_ax1] * T.float32(0.00039062500000000002) - input_embed_red_temp_v0[v_ax0, v_ax1] * T.float32(0.00039062500000000002) * (input_embed_red_temp_v0[v_ax0, v_ax1] * T.float32(0.00039062500000000002)) + T.float32(1.0000000000000001e-05))) * gpt_neox_layers_0_input_layernorm_weight2[v_ax2] + gpt_neox_layers_0_input_layernorm_bias2[v_ax2]

    @T.prim_func
    def merge_state_inplace(v: T.handle, s: T.handle, v_other: T.handle, s_other: T.handle):
        T.func_attr({"target": T.target({"arch": "sm_75", "host": {"keys": ["cpu"], "kind": "llvm", "mcpu": "skylake", "mtriple": "x86_64-conda-linux-gnu", "tag": ""}, "keys": ["cuda", "gpu"], "kind": "cuda", "libs": ["thrust"], "max_num_threads": 1024, "max_shared_memory_per_block": 49152, "max_threads_per_block": 1024, "tag": "", "thread_warp_size": 32}), "tir.is_scheduled": 1})
        N, H, D = T.int32(is_size_var=True), T.int32(is_size_var=True), T.int32(is_size_var=True)
        V = T.match_buffer(v, (N, H, D), "float16")
        S = T.match_buffer(s, (N, H))
        V_other = T.match_buffer(v_other, (N, H, D), "float16")
        S_other = T.match_buffer(s_other, (N, H))
        # with T.block("root"):
        for bx in T.thread_binding(N, thread="blockIdx.x"):
            for by in T.thread_binding(1, thread="blockIdx.y"):
                for ty in T.thread_binding(32, thread="threadIdx.y"):
                    for tx in T.thread_binding(20, thread="threadIdx.x"):
                        with T.block("merge"):
                            T.reads(S[bx, ty + by * 32], S_other[bx, ty + by * 32], V[bx, ty + by * 32, tx * 4:tx * 4 + 4], V_other[bx, ty + by * 32, tx * 4:tx * 4 + 4])
                            T.writes(V[bx, ty + by * 32, tx * 4:tx * 4 + 4], S[bx, ty + by * 32])
                            s_val = T.alloc_buffer((1,), scope="local")
                            s_other_val = T.alloc_buffer((1,), scope="local")
                            s_max = T.alloc_buffer((1,), scope="local")
                            scale = T.alloc_buffer((1,), scope="local")
                            other_scale = T.alloc_buffer((1,), scope="local")
                            v_vec = T.alloc_buffer((4,), "float16", scope="local")
                            v_other_vec = T.alloc_buffer((4,), "float16", scope="local")
                            s_val[0] = S[bx, ty + by * 32]
                            s_other_val[0] = S_other[bx, ty + by * 32]
                            s_max[0] = T.max(s_val[0], s_other_val[0])
                            s_val[0] = T.exp2(s_val[0] - s_max[0])
                            s_other_val[0] = T.exp2(s_other_val[0] - s_max[0])
                            scale[0] = s_val[0] / (s_val[0] + s_other_val[0])
                            other_scale[0] = s_other_val[0] / (s_val[0] + s_other_val[0])
                            for vec in T.vectorized(4):
                                v_vec[vec] = V[bx, ty + by * 32, tx * 4 + vec]
                            for vec in T.vectorized(4):
                                v_other_vec[vec] = V_other[bx, ty + by * 32, tx * 4 + vec]
                            for vec in range(4):
                                v_vec[vec] = T.Cast("float16", T.Cast("float32", v_vec[vec]) * scale[0] + T.Cast("float32", v_other_vec[vec]) * other_scale[0])
                            for vec in T.vectorized(4):
                                V[bx, ty + by * 32, tx * 4 + vec] = v_vec[vec]
                            S[bx, ty + by * 32] = T.log2(s_val[0] + s_other_val[0]) + s_max[0]

    @T.prim_func
    def parallel_sampling_from_prob(var_prob: T.handle, var_uniform_samples: T.handle, var_row_indices: T.handle, var_sampled_token_ids: T.handle):
        T.func_attr({"tir.is_scheduled": 1})
        n, vocab_size = T.int64(), T.int64()
        prob = T.match_buffer(var_prob, (n, vocab_size))
        batch_size = T.int64()
        uniform_samples = T.match_buffer(var_uniform_samples, (batch_size, 1))
        row_indices = T.match_buffer(var_row_indices, (batch_size, 1), "int32")
        token_ids = T.match_buffer(var_sampled_token_ids, (batch_size, 1), "int32")
        # with T.block("root"):
        aggregate = T.alloc_buffer((), scope="local")
        sample_id_local = T.alloc_buffer((), "int32", scope="local")
        step_iter = T.alloc_buffer((), "int32", scope="local")
        for bx in T.thread_binding(batch_size, thread="blockIdx.x"):
            row_idx: T.int32 = row_indices[bx, 0]
            for ty in T.thread_binding(T.int64(4), thread="threadIdx.y"):
                for tx in T.thread_binding(T.int64(32), thread="threadIdx.x"):
                    u: T.float32 = uniform_samples[bx, 0]
                    aggregate[()] = T.Cast("float32", 0)
                    step_iter[()] = 0
                    while T.tvm_thread_invariant((step_iter[()] == 0 or aggregate[()] < u - T.float32(9.9999999999999995e-07)) and T.Cast("int64", step_iter[()]) < (vocab_size + T.int64(512) - T.int64(1)) // T.int64(512)):
                        with T.block(""):
                            T.reads(step_iter[()], prob[row_idx, T.Cast("int64", step_iter[()]) * T.int64(512) + ty * T.int64(128) + tx * T.int64(4):T.Cast("int64", step_iter[()]) * T.int64(512) + ty * T.int64(128) + tx * T.int64(4) + T.int64(4)], aggregate[()])
                            T.writes(sample_id_local[()], aggregate[()])
                            prob_gt_threshold = T.alloc_buffer((T.int64(4),), scope="local")
                            cumsum = T.alloc_buffer((T.int64(512),), scope="shared")
                            greater_than_u = T.alloc_buffer((T.int64(4),), "bool", scope="local")
                            mask = T.alloc_buffer((T.int64(4),), "bool", scope="local")
                            valid = T.alloc_buffer((T.int64(4),), "bool", scope="local")
                            indices = T.alloc_buffer((T.int64(4),), "int32", scope="local")
                            step_aggregate = T.alloc_buffer((), scope="local")
                            for v in T.unroll(T.int64(4)):
                                idx: T.int64 = T.Cast("int64", step_iter[()]) * T.int64(512) + ty * T.int64(128) + tx * T.int64(4) + v
                                prob_local: T.float32 = T.if_then_else(idx < vocab_size, prob[row_idx, idx], T.Cast("float32", 0))
                                prob_gt_threshold[v] = T.if_then_else(prob_local > T.float32(0.0), prob_local, T.Cast("float32", 0))
                                valid[v] = prob_local > T.float32(0.0) and idx < vocab_size
                            with T.block(""):
                                T.reads(prob_gt_threshold[T.int64(0):T.int64(4)])
                                T.writes(step_aggregate[()])
                                local_sum = T.alloc_buffer((), scope="local")
                                shared_buf = T.alloc_buffer((T.int64(128),), scope="shared")
                                idx: T.int64 = ty * T.int64(32) + tx
                                local_sum[()] = T.Cast("float32", 0)
                                for i in T.unroll(T.int64(4)):
                                    local_sum[()] = local_sum[()] + prob_gt_threshold[i]
                                shared_buf[idx] = local_sum[()]
                                for i in T.unroll(T.int64(7)):
                                    if idx % T.shift_left(T.int64(1), i + T.int64(1)) == T.int64(0):
                                        shared_buf[idx] = shared_buf[idx] + shared_buf[idx + T.shift_left(T.int64(1), i)]
                                step_aggregate[()] = shared_buf[0]
                            if T.tvm_thread_invariant(aggregate[()] + step_aggregate[()] >= u - T.float32(9.9999999999999995e-07)):
                                for i in T.unroll(T.int64(1), T.int64(4)):
                                    prob_gt_threshold[i] = prob_gt_threshold[i] + prob_gt_threshold[i - T.int64(1)]
                                for i in T.vectorized(T.int64(4)):
                                    cumsum[ty * T.int64(128) + tx * T.int64(4) + i] = prob_gt_threshold[i]
                                for i in T.unroll(T.int64(5)):
                                    for j in T.vectorized(T.int64(4)):
                                        idx: T.int64 = ty * T.int64(128) + tx * T.int64(4)
                                        if tx >= T.shift_left(T.int64(1), i):
                                            cumsum[idx + j] = cumsum[idx + j] + cumsum[idx - T.shift_left(T.int64(1), i) * T.int64(4) + T.int64(4) - T.int64(1)]
                                for i in T.unroll(T.int64(1), T.int64(4)):
                                    for j in T.vectorized(T.int64(4)):
                                        if ty == T.int64(0):
                                            idx: T.int64 = i * T.int64(128) + tx * T.int64(4)
                                            cumsum[idx + j] = cumsum[idx + j] + cumsum[i * T.int64(128) - T.int64(1)]
                                for v in T.unroll(T.int64(4)):
                                    greater_than_u[v] = cumsum[ty * T.int64(128) + tx * T.int64(4) + v] + aggregate[()] >= u - T.float32(9.9999999999999995e-07)
                                with T.block(""):
                                    T.reads(greater_than_u[T.int64(0):T.int64(4)])
                                    T.writes(mask[T.int64(0):T.int64(4)])
                                    shared_buf = T.alloc_buffer((T.int64(128),), "bool", scope="shared")
                                    tx_idx: T.int64 = ty * T.int64(32) + tx
                                    shared_buf[tx_idx] = greater_than_u[T.int64(3)]
                                    mask[0] = T.if_then_else(tx_idx != T.int64(0), T.Cast("int8", greater_than_u[0]) != T.Cast("int8", shared_buf[tx_idx - T.int64(1)]), greater_than_u[0])
                                    for i in T.unroll(T.int64(1), T.int64(4)):
                                        mask[i] = T.Cast("int8", greater_than_u[i]) != T.Cast("int8", greater_than_u[i - T.int64(1)])
                                for v in T.unroll(T.int64(4)):
                                    mask[v] = mask[v] and valid[v]
                                    indices[v] = T.Cast("int32", T.Cast("int64", step_iter[()]) * T.int64(512) + ty * T.int64(128) + tx * T.int64(4) + v)
                                with T.block(""):
                                    T.reads(mask[T.int64(0):T.int64(4)], indices[T.int64(0):T.int64(4)])
                                    T.writes(sample_id_local[()])
                                    local_sum = T.alloc_buffer((), "int32", scope="local")
                                    shared_buf = T.alloc_buffer((T.int64(128),), "int32", scope="shared")
                                    idx: T.int64 = ty * T.int64(32) + tx
                                    local_sum[()] = T.Cast("int32", vocab_size - T.int64(1))
                                    for i in T.unroll(T.int64(4)):
                                        if mask[i]:
                                            local_sum[()] = T.min(local_sum[()], indices[i])
                                    shared_buf[idx] = local_sum[()]
                                    for i in T.unroll(T.int64(7)):
                                        if idx % T.shift_left(T.int64(1), i + T.int64(1)) == T.int64(0):
                                            shared_buf[idx] = T.min(shared_buf[idx], shared_buf[idx + T.shift_left(T.int64(1), i)])
                                    sample_id_local[()] = shared_buf[0]
                            aggregate[()] = aggregate[()] + step_aggregate[()]
                        step_iter[()] = step_iter[()] + 1
                    if tx == T.int64(0) and ty == T.int64(0):
                        token_ids[bx, 0] = sample_id_local[()]

    @T.prim_func(private=True)
    def reshape(var_add576: T.handle, var_T_reshape: T.handle):
        T.func_attr({"tir.noalias": T.bool(True)})
        batch_size = T.int64()
        add576 = T.match_buffer(var_add576, (batch_size, T.int64(1), T.int64(7680)), "float16")
        T_reshape = T.match_buffer(var_T_reshape, (batch_size, T.int64(1), T.int64(96), T.int64(80)), "float16")
        # with T.block("root"):
        for ax0, ax1, ax2, ax3 in T.grid(batch_size, T.int64(1), T.int64(96), T.int64(80)):
            with T.block("T_reshape"):
                v_ax0, v_ax1, v_ax2, v_ax3 = T.axis.remap("SSSS", [ax0, ax1, ax2, ax3])
                T.reads(add576[((v_ax2 * T.int64(80) + v_ax3) // T.int64(7680) + v_ax0 + v_ax1) % batch_size, T.int64(0), (v_ax2 * T.int64(80) + v_ax3) % T.int64(7680)])
                T.writes(T_reshape[v_ax0, v_ax1, v_ax2, v_ax3])
                T_reshape[v_ax0, v_ax1, v_ax2, v_ax3] = add576[((v_ax2 * T.int64(80) + v_ax3) // T.int64(7680) + v_ax0 + v_ax1) % batch_size, T.int64(0), (v_ax2 * T.int64(80) + v_ax3) % T.int64(7680)]

    @T.prim_func(private=True)
    def reshape1(var_reshape384: T.handle, var_T_reshape: T.handle):
        T.func_attr({"tir.noalias": T.bool(True)})
        batch_size = T.int64()
        reshape384 = T.match_buffer(var_reshape384, (batch_size, T.int64(1), T.int64(96), T.int64(80)), "float16")
        T_reshape = T.match_buffer(var_T_reshape, (batch_size, T.int64(96), T.int64(80)), "float16")
        # with T.block("root"):
        for ax0, ax1, ax2 in T.grid(batch_size, T.int64(96), T.int64(80)):
            with T.block("T_reshape"):
                v_ax0, v_ax1, v_ax2 = T.axis.remap("SSS", [ax0, ax1, ax2])
                T.reads(reshape384[((v_ax2 // T.int64(80) + v_ax1) // T.int64(96) + v_ax0) % batch_size, T.int64(0), (v_ax2 // T.int64(80) + v_ax1) % T.int64(96), v_ax2 % T.int64(80)])
                T.writes(T_reshape[v_ax0, v_ax1, v_ax2])
                T_reshape[v_ax0, v_ax1, v_ax2] = reshape384[((v_ax2 // T.int64(80) + v_ax1) // T.int64(96) + v_ax0) % batch_size, T.int64(0), (v_ax2 // T.int64(80) + v_ax1) % T.int64(96), v_ax2 % T.int64(80)]

    @T.prim_func(private=True)
    def reshape2(var_lv486: T.handle, var_T_reshape: T.handle):
        T.func_attr({"tir.noalias": T.bool(True)})
        batch_size = T.int64()
        lv486 = T.match_buffer(var_lv486, (batch_size, T.int64(32), T.int64(80)), "float16")
        T_reshape = T.match_buffer(var_T_reshape, (batch_size, T.int64(1), T.int64(32), T.int64(80)), "float16")
        # with T.block("root"):
        for ax0, ax1, ax2, ax3 in T.grid(batch_size, T.int64(1), T.int64(32), T.int64(80)):
            with T.block("T_reshape"):
                v_ax0, v_ax1, v_ax2, v_ax3 = T.axis.remap("SSSS", [ax0, ax1, ax2, ax3])
                T.reads(lv486[((v_ax3 // T.int64(80) + v_ax2) // T.int64(32) + v_ax0 + v_ax1) % batch_size, (v_ax3 // T.int64(80) + v_ax2) % T.int64(32), v_ax3 % T.int64(80)])
                T.writes(T_reshape[v_ax0, v_ax1, v_ax2, v_ax3])
                T_reshape[v_ax0, v_ax1, v_ax2, v_ax3] = lv486[((v_ax3 // T.int64(80) + v_ax2) // T.int64(32) + v_ax0 + v_ax1) % batch_size, (v_ax3 // T.int64(80) + v_ax2) % T.int64(32), v_ax3 % T.int64(80)]

    @T.prim_func(private=True)
    def reshape3(var_reshape386: T.handle, var_T_reshape: T.handle):
        T.func_attr({"tir.noalias": T.bool(True)})
        batch_size = T.int64()
        reshape386 = T.match_buffer(var_reshape386, (batch_size, T.int64(1), T.int64(32), T.int64(80)), "float16")
        T_reshape = T.match_buffer(var_T_reshape, (batch_size, T.int64(1), T.int64(2560)), "float16")
        # with T.block("root"):
        for ax0, ax1, ax2 in T.grid(batch_size, T.int64(1), T.int64(2560)):
            with T.block("T_reshape"):
                v_ax0, v_ax1, v_ax2 = T.axis.remap("SSS", [ax0, ax1, ax2])
                T.reads(reshape386[(v_ax2 // T.int64(2560) + v_ax0 + v_ax1) % batch_size, T.int64(0), v_ax2 % T.int64(2560) // T.int64(80), v_ax2 % T.int64(80)])
                T.writes(T_reshape[v_ax0, v_ax1, v_ax2])
                T_reshape[v_ax0, v_ax1, v_ax2] = reshape386[(v_ax2 // T.int64(2560) + v_ax0 + v_ax1) % batch_size, T.int64(0), v_ax2 % T.int64(2560) // T.int64(80), v_ax2 % T.int64(80)]

    @T.prim_func(private=True)
    def reshape4(var_add384: T.handle, var_T_reshape: T.handle):
        T.func_attr({"tir.noalias": T.bool(True)})
        seq_len = T.int64()
        add384 = T.match_buffer(var_add384, (T.int64(1), seq_len, T.int64(7680)), "float16")
        T_reshape = T.match_buffer(var_T_reshape, (T.int64(1), seq_len, T.int64(96), T.int64(80)), "float16")
        # with T.block("root"):
        for ax0, ax1, ax2, ax3 in T.grid(T.int64(1), seq_len, T.int64(96), T.int64(80)):
            with T.block("T_reshape"):
                v_ax0, v_ax1, v_ax2, v_ax3 = T.axis.remap("SSSS", [ax0, ax1, ax2, ax3])
                T.reads(add384[T.int64(0), ((v_ax2 * T.int64(80) + v_ax3) // T.int64(7680) + v_ax0 * seq_len + v_ax1) % seq_len, (v_ax2 * T.int64(80) + v_ax3) % T.int64(7680)])
                T.writes(T_reshape[v_ax0, v_ax1, v_ax2, v_ax3])
                T_reshape[v_ax0, v_ax1, v_ax2, v_ax3] = add384[T.int64(0), ((v_ax2 * T.int64(80) + v_ax3) // T.int64(7680) + v_ax0 * seq_len + v_ax1) % seq_len, (v_ax2 * T.int64(80) + v_ax3) % T.int64(7680)]

    @T.prim_func(private=True)
    def reshape5(var_reshape256: T.handle, var_T_reshape: T.handle):
        T.func_attr({"tir.noalias": T.bool(True)})
        seq_len = T.int64()
        reshape256 = T.match_buffer(var_reshape256, (T.int64(1), seq_len, T.int64(96), T.int64(80)), "float16")
        T_reshape = T.match_buffer(var_T_reshape, (seq_len, T.int64(96), T.int64(80)), "float16")
        # with T.block("root"):
        for ax0, ax1, ax2 in T.grid(seq_len, T.int64(96), T.int64(80)):
            with T.block("T_reshape"):
                v_ax0, v_ax1, v_ax2 = T.axis.remap("SSS", [ax0, ax1, ax2])
                T.reads(reshape256[T.int64(0), ((v_ax2 // T.int64(80) + v_ax1) // T.int64(96) + v_ax0) % seq_len, (v_ax2 // T.int64(80) + v_ax1) % T.int64(96), v_ax2 % T.int64(80)])
                T.writes(T_reshape[v_ax0, v_ax1, v_ax2])
                T_reshape[v_ax0, v_ax1, v_ax2] = reshape256[T.int64(0), ((v_ax2 // T.int64(80) + v_ax1) // T.int64(96) + v_ax0) % seq_len, (v_ax2 // T.int64(80) + v_ax1) % T.int64(96), v_ax2 % T.int64(80)]

    @T.prim_func(private=True)
    def reshape6(var_lv325: T.handle, var_T_reshape: T.handle):
        T.func_attr({"tir.noalias": T.bool(True)})
        seq_len = T.int64()
        lv325 = T.match_buffer(var_lv325, (seq_len, T.int64(32), T.int64(80)), "float16")
        T_reshape = T.match_buffer(var_T_reshape, (T.int64(1), seq_len, T.int64(32), T.int64(80)), "float16")
        # with T.block("root"):
        for ax0, ax1, ax2, ax3 in T.grid(T.int64(1), seq_len, T.int64(32), T.int64(80)):
            with T.block("T_reshape"):
                v_ax0, v_ax1, v_ax2, v_ax3 = T.axis.remap("SSSS", [ax0, ax1, ax2, ax3])
                T.reads(lv325[((v_ax3 // T.int64(80) + v_ax2) // T.int64(32) + v_ax0 * seq_len + v_ax1) % seq_len, (v_ax3 // T.int64(80) + v_ax2) % T.int64(32), v_ax3 % T.int64(80)])
                T.writes(T_reshape[v_ax0, v_ax1, v_ax2, v_ax3])
                T_reshape[v_ax0, v_ax1, v_ax2, v_ax3] = lv325[((v_ax3 // T.int64(80) + v_ax2) // T.int64(32) + v_ax0 * seq_len + v_ax1) % seq_len, (v_ax3 // T.int64(80) + v_ax2) % T.int64(32), v_ax3 % T.int64(80)]

    @T.prim_func(private=True)
    def reshape7(var_reshape258: T.handle, var_T_reshape: T.handle):
        T.func_attr({"tir.noalias": T.bool(True)})
        seq_len = T.int64()
        reshape258 = T.match_buffer(var_reshape258, (T.int64(1), seq_len, T.int64(32), T.int64(80)), "float16")
        T_reshape = T.match_buffer(var_T_reshape, (T.int64(1), seq_len, T.int64(2560)), "float16")
        # with T.block("root"):
        for ax0, ax1, ax2 in T.grid(T.int64(1), seq_len, T.int64(2560)):
            with T.block("T_reshape"):
                v_ax0, v_ax1, v_ax2 = T.axis.remap("SSS", [ax0, ax1, ax2])
                T.reads(reshape258[T.int64(0), (v_ax2 // T.int64(2560) + v_ax0 * seq_len + v_ax1) % seq_len, v_ax2 % T.int64(2560) // T.int64(80), v_ax2 % T.int64(80)])
                T.writes(T_reshape[v_ax0, v_ax1, v_ax2])
                T_reshape[v_ax0, v_ax1, v_ax2] = reshape258[T.int64(0), (v_ax2 // T.int64(2560) + v_ax0 * seq_len + v_ax1) % seq_len, v_ax2 % T.int64(2560) // T.int64(80), v_ax2 % T.int64(80)]

    @T.prim_func
    def sampler_take_probs_tir(var_unsorted_probs: T.handle, var_sorted_indices: T.handle, var_sample_indices: T.handle, var_sampling_results: T.handle, var_top_prob_offsets: T.handle, var_sampled_values: T.handle, var_top_prob_probs: T.handle, var_top_prob_indices: T.handle):
        T.func_attr({"target": T.target({"arch": "sm_75", "host": {"keys": ["cpu"], "kind": "llvm", "mcpu": "skylake", "mtriple": "x86_64-conda-linux-gnu", "tag": ""}, "keys": ["cuda", "gpu"], "kind": "cuda", "libs": ["thrust"], "max_num_threads": 1024, "max_shared_memory_per_block": 49152, "max_threads_per_block": 1024, "tag": "", "thread_warp_size": 32})})
        batch_size, vocab_size = T.int32(is_size_var=True), T.int32(is_size_var=True)
        unsorted_probs = T.match_buffer(var_unsorted_probs, (batch_size, vocab_size))
        sorted_indices = T.match_buffer(var_sorted_indices, (batch_size, vocab_size), "int32")
        num_samples = T.int32(is_size_var=True)
        sample_indices = T.match_buffer(var_sample_indices, (num_samples,), "int32")
        sampling_results = T.match_buffer(var_sampling_results, (num_samples,), "int32")
        num_positions = T.int32(is_size_var=True)
        top_prob_offsets = T.match_buffer(var_top_prob_offsets, (num_positions,), "int32")
        sampled_values = T.match_buffer(var_sampled_values, (num_samples,))
        top_prob_probs = T.match_buffer(var_top_prob_probs, (num_positions,))
        top_prob_indices = T.match_buffer(var_top_prob_indices, (num_positions,), "int32")
        # with T.block("root"):
        for i in range(num_positions + num_samples):
            with T.block("block"):
                vi = T.axis.spatial(num_positions + num_samples, i)
                T.reads(top_prob_offsets[vi], sorted_indices[top_prob_offsets[vi] // vocab_size, top_prob_offsets[vi] % vocab_size], unsorted_probs[T.min(top_prob_offsets[vi] // vocab_size, sample_indices[vi - num_positions]):T.min(top_prob_offsets[vi] // vocab_size, sample_indices[vi - num_positions]) + (T.max(top_prob_offsets[vi] // vocab_size, sample_indices[vi - num_positions]) + 1 - T.min(top_prob_offsets[vi] // vocab_size, sample_indices[vi - num_positions])), T.min(sorted_indices[top_prob_offsets[vi] // vocab_size, top_prob_offsets[vi] % vocab_size], sampling_results[vi - num_positions]):T.min(sorted_indices[top_prob_offsets[vi] // vocab_size, top_prob_offsets[vi] % vocab_size], sampling_results[vi - num_positions]) + (T.max(sorted_indices[top_prob_offsets[vi] // vocab_size, top_prob_offsets[vi] % vocab_size], sampling_results[vi - num_positions]) + 1 - T.min(sorted_indices[top_prob_offsets[vi] // vocab_size, top_prob_offsets[vi] % vocab_size], sampling_results[vi - num_positions]))], sample_indices[vi - num_positions], sampling_results[vi - num_positions])
                T.writes(top_prob_indices[vi], top_prob_probs[vi], sampled_values[vi - num_positions])
                if vi < num_positions:
                    row: T.int32 = top_prob_offsets[vi] // vocab_size
                    col: T.int32 = top_prob_offsets[vi] % vocab_size
                    top_prob_indices[vi] = sorted_indices[row, col]
                    top_prob_probs[vi] = unsorted_probs[row, sorted_indices[row, col]]
                else:
                    vj: T.int32 = vi - num_positions
                    sampled_values[vj] = unsorted_probs[sample_indices[vj], sampling_results[vj]]

    @T.prim_func
    def scatter_probs(var_src: T.handle, var_indices: T.handle, var_dst: T.handle):
        T.func_attr({"target": T.target({"arch": "sm_75", "host": {"keys": ["cpu"], "kind": "llvm", "mcpu": "skylake", "mtriple": "x86_64-conda-linux-gnu", "tag": ""}, "keys": ["cuda", "gpu"], "kind": "cuda", "libs": ["thrust"], "max_num_threads": 1024, "max_shared_memory_per_block": 49152, "max_threads_per_block": 1024, "tag": "", "thread_warp_size": 32}), "tir.noalias": T.bool(True)})
        batch_size, n = T.int32(is_size_var=True), T.int32(is_size_var=True)
        src = T.match_buffer(var_src, (batch_size, n))
        indices = T.match_buffer(var_indices, (batch_size,), "int32")
        m = T.int32(is_size_var=True)
        dst = T.match_buffer(var_dst, (m, n))
        # with T.block("root"):
        for b, j in T.grid(batch_size, n):
            with T.block("scatter_2d"):
                vb, vj = T.axis.remap("SS", [b, j])
                T.reads(src[vb, vj], indices[vb])
                T.writes(dst[indices[vb], vj])
                dst[indices[vb], vj] = src[vb, vj]

    @T.prim_func
    def softmax_with_chunked_sum(var_A: T.handle, var_temperature: T.handle, var_chunked_sum: T.handle, var_chunked_max: T.handle, var_softmax: T.handle):
        T.func_attr({"target": T.target({"arch": "sm_75", "host": {"keys": ["cpu"], "kind": "llvm", "mcpu": "skylake", "mtriple": "x86_64-conda-linux-gnu", "tag": ""}, "keys": ["cuda", "gpu"], "kind": "cuda", "libs": ["thrust"], "max_num_threads": 1024, "max_shared_memory_per_block": 49152, "max_threads_per_block": 1024, "tag": "", "thread_warp_size": 32}), "tir.is_scheduled": 1, "tir.noalias": T.bool(True)})
        batch_size, vocab_size = T.int64(is_size_var=True), T.int64(is_size_var=True)
        A = T.match_buffer(var_A, (batch_size, vocab_size))
        temperature = T.match_buffer(var_temperature, (batch_size,))
        num_chunks = T.int64(is_size_var=True)
        chunked_sum = T.match_buffer(var_chunked_sum, (batch_size, num_chunks))
        chunked_max = T.match_buffer(var_chunked_max, (batch_size, num_chunks))
        softmax = T.match_buffer(var_softmax, (batch_size, vocab_size))
        # with T.block("root"):
        temp_max_shared = T.alloc_buffer((batch_size,), scope="shared")
        temp_sum_shared = T.alloc_buffer((batch_size,), scope="shared")
        for l0_l1_fused in T.thread_binding(batch_size * num_chunks, thread="blockIdx.x"):
            for ax0_1 in T.thread_binding(T.int64(32), thread="threadIdx.x"):
                for ax0_0 in T.serial((num_chunks + T.int64(31)) // T.int64(32), annotations={"pragma_auto_unroll_max_step": 64, "pragma_unroll_explicit": 1}):
                    with T.block("max"):
                        v0 = T.axis.spatial(batch_size, l0_l1_fused % (num_chunks * batch_size) // num_chunks)
                        v1 = T.axis.reduce(num_chunks, ax0_0 * T.int64(32) + ax0_1)
                        T.where(ax0_0 * T.int64(32) + ax0_1 < num_chunks)
                        T.reads(chunked_max[v0, v1])
                        T.writes(temp_max_shared[v0])
                        with T.init():
                            temp_max_shared[v0] = T.float32(-340282346638528859811704183484516925440.0)
                        temp_max_shared[v0] = T.max(temp_max_shared[v0], chunked_max[v0, v1])
            for ax0_1 in T.thread_binding(T.int64(32), thread="threadIdx.x"):
                for ax0_0 in T.serial((num_chunks + T.int64(31)) // T.int64(32), annotations={"pragma_auto_unroll_max_step": 64, "pragma_unroll_explicit": 1}):
                    with T.block("sum_exp"):
                        v0 = T.axis.spatial(batch_size, l0_l1_fused % (num_chunks * batch_size) // num_chunks)
                        v1 = T.axis.reduce(num_chunks, ax0_0 * T.int64(32) + ax0_1)
                        T.where(ax0_0 * T.int64(32) + ax0_1 < num_chunks)
                        T.reads(temperature[v0], chunked_sum[v0, v1], chunked_max[v0, v1], temp_max_shared[v0])
                        T.writes(temp_sum_shared[v0])
                        with T.init():
                            temp_sum_shared[v0] = T.float32(0.0)
                        temp_sum_shared[v0] = temp_sum_shared[v0] + T.Select(temperature[v0] > T.float32(1.0000000000000001e-05), T.exp(chunked_sum[v0, v1] + chunked_max[v0, v1] - temp_max_shared[v0]), T.Cast("float32", chunked_max[v0, v1] == temp_max_shared[v0]) * chunked_sum[v0, v1])
            for l2_0 in T.serial(T.int64(4), annotations={"pragma_auto_unroll_max_step": 64, "pragma_unroll_explicit": 1}):
                for l2_1 in T.thread_binding(T.int64(32), thread="threadIdx.y"):
                    for l2_2 in T.thread_binding(T.int64(32), thread="threadIdx.x"):
                        with T.block("log_pad"):
                            v0 = T.axis.spatial(batch_size, l0_l1_fused % (num_chunks * batch_size) // num_chunks)
                            v1 = T.axis.spatial(num_chunks, l0_l1_fused % num_chunks)
                            v2 = T.axis.spatial(T.int64(4096), l2_0 * T.int64(1024) + l2_1 * T.int64(32) + l2_2)
                            T.reads(temperature[v0], A[v0, v1 * T.int64(4096) + v2], temp_sum_shared[v0], temp_max_shared[v0])
                            T.writes(softmax[v0, v1 * T.int64(4096) + v2])
                            if v1 * T.int64(4096) + v2 < vocab_size:
                                softmax[v0, v1 * T.int64(4096) + v2] = T.if_then_else(temperature[v0] > T.float32(1.0000000000000001e-05), T.exp(A[v0, v1 * T.int64(4096) + v2] / temperature[v0] - (T.log(temp_sum_shared[v0]) + temp_max_shared[v0])), T.Cast("float32", A[v0, v1 * T.int64(4096) + v2] == temp_max_shared[v0]) / temp_sum_shared[v0])

    @T.prim_func(private=True)
    def take(var_layer_norm194: T.handle, var_logit_positions: T.handle, var_T_take: T.handle):
        T.func_attr({"tir.noalias": T.bool(True)})
        seq_len = T.int64()
        layer_norm194 = T.match_buffer(var_layer_norm194, (T.int64(1), seq_len, T.int64(2560)), "float16")
        batch_size = T.int64()
        logit_positions = T.match_buffer(var_logit_positions, (batch_size,), "int32")
        T_take = T.match_buffer(var_T_take, (T.int64(1), batch_size, T.int64(2560)), "float16")
        # with T.block("root"):
        for ax0, ax1, ax2 in T.grid(T.int64(1), batch_size, T.int64(2560)):
            with T.block("T_take"):
                v_ax0, v_ax1, v_ax2 = T.axis.remap("SSS", [ax0, ax1, ax2])
                T.reads(layer_norm194[v_ax0, logit_positions[v_ax1], v_ax2], logit_positions[v_ax1])
                T.writes(T_take[v_ax0, v_ax1, v_ax2])
                T_take[v_ax0, v_ax1, v_ax2] = layer_norm194[v_ax0, logit_positions[v_ax1], v_ax2]

    @T.prim_func(private=True)
    def take_sorted_probs(var_probs: T.handle, var_lv1: T.handle, var_take_sorted_probs: T.handle):
        T.func_attr({"target": T.target({"arch": "sm_75", "keys": ["cuda", "gpu"], "kind": "cuda", "libs": ["thrust"], "max_num_threads": 1024, "max_shared_memory_per_block": 49152, "max_threads_per_block": 1024, "tag": "", "thread_warp_size": 32}), "tir.noalias": T.bool(True)})
        batch_size, vocab_size = T.int64(), T.int64()
        probs = T.match_buffer(var_probs, (batch_size, vocab_size))
        lv1 = T.match_buffer(var_lv1, (batch_size, vocab_size), "int32")
        batch_size_1, vocab_size_1 = T.int64(is_size_var=True), T.int64(is_size_var=True)
        take_sorted_probs = T.match_buffer(var_take_sorted_probs, (batch_size_1, vocab_size_1))
        # with T.block("root"):
        for i, j in T.grid(batch_size_1, vocab_size_1):
            with T.block("take_sorted_probs"):
                v_i, v_j = T.axis.remap("SS", [i, j])
                T.reads(probs[v_i, lv1[v_i, v_j]], lv1[v_i, v_j])
                T.writes(take_sorted_probs[v_i, v_j])
                take_sorted_probs[v_i, v_j] = probs[v_i, lv1[v_i, v_j]]

    @T.prim_func
    def tir_kv_cache_debug_get_kv(var_pages: T.handle, var_position_map: T.handle, var_k_data: T.handle, var_v_data: T.handle, layer_id: T.int64):
        T.func_attr({"target": T.target({"arch": "sm_75", "host": {"keys": ["cpu"], "kind": "llvm", "mcpu": "skylake", "mtriple": "x86_64-conda-linux-gnu", "tag": ""}, "keys": ["cuda", "gpu"], "kind": "cuda", "libs": ["thrust"], "max_num_threads": 1024, "max_shared_memory_per_block": 49152, "max_threads_per_block": 1024, "tag": "", "thread_warp_size": 32}), "tir.noalias": T.bool(True)})
        num_pages, page_size = T.int64(), T.int64(is_size_var=True)
        pages = T.match_buffer(var_pages, (num_pages, 2, 32, page_size, 80), "float16")
        seqlen = T.int64(is_size_var=True)
        position_map = T.match_buffer(var_position_map, (seqlen,), "int32", offset_factor=1)
        k_data = T.match_buffer(var_k_data, (32, seqlen, 32, 80), "float16")
        v_data = T.match_buffer(var_v_data, (32, seqlen, 32, 80), "float16")
        # with T.block("root"):
        for p, h, d in T.grid(seqlen, 32, 80):
            with T.block("copy0"):
                vp, vh, vd = T.axis.remap("SSS", [p, h, d])
                T.reads(position_map[vp], pages[T.Cast("int64", position_map[vp]) // page_size, 0:2, vh, T.Cast("int64", position_map[vp]) % page_size, vd])
                T.writes(k_data[layer_id, vp, vh, vd], v_data[layer_id, vp, vh, vd])
                position: T.int32 = position_map[vp]
                k_data[layer_id, vp, vh, vd] = pages[T.Cast("int64", position) // page_size, 0, vh, T.Cast("int64", position) % page_size, vd]
                v_data[layer_id, vp, vh, vd] = pages[T.Cast("int64", position) // page_size, 1, vh, T.Cast("int64", position) % page_size, vd]

    @T.prim_func
    def tir_kv_cache_transpose_append(var_pages: T.handle, var_k_data: T.handle, var_v_data: T.handle, var_position_map: T.handle):
        T.func_attr({"target": T.target({"arch": "sm_75", "host": {"keys": ["cpu"], "kind": "llvm", "mcpu": "skylake", "mtriple": "x86_64-conda-linux-gnu", "tag": ""}, "keys": ["cuda", "gpu"], "kind": "cuda", "libs": ["thrust"], "max_num_threads": 1024, "max_shared_memory_per_block": 49152, "max_threads_per_block": 1024, "tag": "", "thread_warp_size": 32}), "tir.noalias": T.bool(True)})
        num_pages = T.int64()
        pages = T.match_buffer(var_pages, (num_pages, 2, 32, 16, 80), "float16")
        ntoken = T.int64(is_size_var=True)
        k_data = T.match_buffer(var_k_data, (ntoken, 32, 80), "float16")
        v_data = T.match_buffer(var_v_data, (ntoken, 32, 80), "float16")
        position_map = T.match_buffer(var_position_map, (ntoken,), "int32", offset_factor=1)
        # with T.block("root"):
        for global_pos, h, f in T.grid(ntoken, 32, 80):
            if position_map[global_pos] != -1:
                with T.block("k_transpose_append"):
                    vgpos, vh, vf = T.axis.remap("SSS", [global_pos, h, f])
                    T.reads(position_map[vgpos], k_data[vgpos, vh, vf])
                    T.writes(pages[position_map[vgpos] // 16, 0, vh, position_map[vgpos] % 16, vf])
                    position: T.int32 = position_map[vgpos]
                    pages[position // 16, 0, vh, position % 16, vf] = k_data[vgpos, vh, vf]
                with T.block("v_transpose_append"):
                    vgpos, vh, vf = T.axis.remap("SSS", [global_pos, h, f])
                    T.reads(position_map[vgpos], v_data[vgpos, vh, vf])
                    T.writes(pages[position_map[vgpos] // 16, 1, vh, position_map[vgpos] % 16, vf])
                    position: T.int32 = position_map[vgpos]
                    pages[position // 16, 1, vh, position % 16, vf] = v_data[vgpos, vh, vf]

    @T.prim_func(private=True)
    def top_p_pivot_cutoff(var_prob: T.handle, var_top_p_arr: T.handle, var_init_pivots: T.handle, var_final_pivot: T.handle, var_final_lsum: T.handle):
        T.func_attr({"target": T.target({"arch": "sm_75", "keys": ["cuda", "gpu"], "kind": "cuda", "libs": ["thrust"], "max_num_threads": 1024, "max_shared_memory_per_block": 49152, "max_threads_per_block": 1024, "tag": "", "thread_warp_size": 32}), "tir.is_scheduled": 1, "tir.noalias": T.bool(True)})
        B, N = T.int32(is_size_var=True), T.int32(is_size_var=True)
        prob = T.match_buffer(var_prob, (B, N))
        top_p_arr = T.match_buffer(var_top_p_arr, (B,))
        init_pivots = T.match_buffer(var_init_pivots, (B, 3))
        final_pivot = T.match_buffer(var_final_pivot, (B,))
        final_lsum = T.match_buffer(var_final_lsum, (B,))
        # with T.block("root"):
        pivot = T.alloc_buffer((3,), scope="local")
        top_p = T.alloc_buffer((1,), scope="local")
        L = T.alloc_buffer((1,), scope="shared")
        R_1 = T.alloc_buffer((1,), scope="shared")
        L_local = T.alloc_buffer((1,), scope="local")
        R_local = T.alloc_buffer((1,), scope="local")
        q = T.alloc_buffer((1,), scope="local")
        lsum = T.alloc_buffer((3,), scope="local")
        lmin_broadcast = T.alloc_buffer((1,), scope="shared")
        lmin_broadcast_local = T.alloc_buffer((1,), scope="local")
        lmin = T.alloc_buffer((3,), scope="local")
        cmin = T.alloc_buffer((3,), "int32", scope="local")
        total_sum = T.alloc_buffer((1,), scope="local")
        it = T.alloc_buffer((1,), "int32", scope="local")
        es_local = T.alloc_buffer((1,), "bool", scope="local")
        es = T.alloc_buffer((1,), "bool", scope="shared")
        find_pivot_local = T.alloc_buffer((1,), "bool", scope="local")
        find_pivot = T.alloc_buffer((1,), "bool", scope="shared")
        total_sum_reduce = T.alloc_buffer((1,), scope="local")
        lsum_reduce = T.alloc_buffer((1,), scope="local")
        lmin_reduce = T.alloc_buffer((1,), scope="local")
        cmin_reduce = T.alloc_buffer((1,), "int32", scope="local")
        for _bx in T.thread_binding(B, thread="blockIdx.x"):
            for _tx in T.thread_binding(1024, thread="threadIdx.x"):
                with T.block("CTA"):
                    b, tx = T.axis.remap("SS", [_bx, _tx])
                    T.reads(top_p_arr[b], top_p[0], L[0], R_1[0], init_pivots[b, 0:3], L_local[0], R_local[0], find_pivot_local[0], it[0], es_local[0], prob[b, it[0] * 1024 + tx], total_sum[0], q[0], pivot[T.min(0, it[0]):T.min(0, it[0]) + (T.max(2, it[0]) + 1 - T.min(0, it[0]))], lsum[T.min(0, it[0]):T.min(0, it[0]) + (T.max(2, it[0]) + 1 - T.min(0, it[0]))], lmin[T.min(0, it[0]):T.min(0, it[0]) + (T.max(2, it[0]) + 1 - T.min(0, it[0]))], cmin[T.min(0, it[0]):T.min(0, it[0]) + (T.max(2, it[0]) + 1 - T.min(0, it[0]))], total_sum_reduce[0], es[0], lmin_reduce[0], lmin_broadcast[0], lmin_broadcast_local[0], lsum_reduce[0], cmin_reduce[0], find_pivot[0])
                    T.writes(top_p[0], L[0], R_1[0], find_pivot[0], L_local[0], R_local[0], pivot[0:3], find_pivot_local[0], final_lsum[b], final_pivot[b], lsum[0:3], lmin[0:3], cmin[0:3], total_sum[0], it[0], es_local[0], q[0], total_sum_reduce[0], es[0], lsum_reduce[0], lmin_reduce[0], lmin_broadcast[0], lmin_broadcast_local[0], cmin_reduce[0])
                    top_p[0] = top_p_arr[b]
                    if tx == 0:
                        L[0] = T.float32(1.0) - top_p[0]
                        R_1[0] = T.float32(9.9999999999999995e-08)
                        find_pivot[0] = T.bool(False)
                    T.tvm_storage_sync("shared")
                    L_local[0] = L[0]
                    R_local[0] = R_1[0]
                    for i in T.unroll(3):
                        pivot[i] = init_pivots[b, i]
                    find_pivot_local[0] = T.bool(False)
                    if L_local[0] - R_local[0] <= T.float32(9.9999999999999995e-08):
                        if tx == 0:
                            final_lsum[b] = T.float32(1.0)
                            final_pivot[b] = T.float32(0.0)
                        find_pivot_local[0] = T.bool(True)
                    while T.tvm_thread_invariant(L_local[0] - R_local[0] > T.float32(9.9999999999999995e-08) and not find_pivot_local[0]):
                        T.tvm_storage_sync("shared")
                        for pidx in T.unroll(3):
                            lsum[pidx] = T.float32(0.0)
                            lmin[pidx] = T.float32(340282346638528859811704183484516925440.0)
                            cmin[pidx] = 0
                        total_sum[0] = T.float32(0.0)
                        it[0] = 0
                        es_local[0] = T.bool(False)
                        while it[0] < (N + 1024 - 1) // 1024 and not es_local[0]:
                            q[0] = T.if_then_else(it[0] * 1024 + tx < N, prob[b, it[0] * 1024 + tx], T.float32(0.0))
                            total_sum[0] = total_sum[0] + q[0]
                            for pidx in T.unroll(3):
                                if q[0] >= pivot[pidx]:
                                    lsum[pidx] = lsum[pidx] + q[0]
                                    if lmin[pidx] > q[0]:
                                        lmin[pidx] = q[0]
                                        cmin[pidx] = 1
                                    else:
                                        if lmin[pidx] == q[0]:
                                            cmin[pidx] = cmin[pidx] + 1
                            it[0] = it[0] + 1
                            if it[0] % 32 == 0:
                                with T.block("block_cross_thread"):
                                    T.reads(total_sum[0])
                                    T.writes(total_sum_reduce[0])
                                    T.attr(T.comm_reducer(lambda x0, y0: x0 + y0, [T.float32(0.0)]), "reduce_scope", T.reinterpret("handle", T.uint64(0)))
                                    T.tvm_thread_allreduce(T.uint32(1), total_sum[0], T.bool(True), total_sum_reduce[0], tx)
                                if tx == 0:
                                    es[0] = T.float32(1.0) - total_sum_reduce[0] < pivot[2]
                                T.tvm_storage_sync("shared")
                                es_local[0] = es[0]
                        T.tvm_storage_sync("shared")
                        for pidx in range(3):
                            with T.block("block_cross_thread"):
                                T.reads(lsum[pidx])
                                T.writes(lsum_reduce[0])
                                T.attr(T.comm_reducer(lambda x0, y0: x0 + y0, [T.float32(0.0)]), "reduce_scope", T.reinterpret("handle", T.uint64(0)))
                                T.tvm_thread_allreduce(T.uint32(1), lsum[pidx], T.bool(True), lsum_reduce[0], tx)
                            with T.block("block_cross_thread"):
                                T.reads(lmin[pidx])
                                T.writes(lmin_reduce[0])
                                T.attr(T.comm_reducer(lambda x0, y0: T.min(x0, y0), [T.float32(0.0)]), "reduce_scope", T.reinterpret("handle", T.uint64(0)))
                                T.tvm_thread_allreduce(T.uint32(1), lmin[pidx], T.bool(True), lmin_reduce[0], tx)
                            if tx == 0:
                                lmin_broadcast[0] = lmin_reduce[0]
                            T.tvm_storage_sync("shared")
                            lmin_broadcast_local[0] = lmin_broadcast[0]
                            if lmin[pidx] > lmin_broadcast_local[0]:
                                cmin[pidx] = 0
                            if tx == 0:
                                lsum[pidx] = lsum_reduce[0]
                                lmin[pidx] = lmin_reduce[0]
                            with T.block("block_cross_thread"):
                                T.reads(cmin[pidx])
                                T.writes(cmin_reduce[0])
                                T.attr(T.comm_reducer(lambda x0, y0: x0 + y0, [0]), "reduce_scope", T.reinterpret("handle", T.uint64(0)))
                                T.tvm_thread_allreduce(T.uint32(1), cmin[pidx], T.bool(True), cmin_reduce[0], tx)
                            if tx == 0:
                                cmin[pidx] = cmin_reduce[0]
                        T.tvm_storage_sync("shared")
                        if tx == 0:
                            it[0] = 0
                            while it[0] < 3 and not find_pivot_local[0]:
                                if lsum[it[0]] >= top_p[0] and top_p[0] > lsum[it[0]] - T.Cast("float32", cmin[it[0]]) * lmin[it[0]]:
                                    find_pivot[0] = T.bool(True)
                                    find_pivot_local[0] = T.bool(True)
                                    final_pivot[b] = pivot[it[0]]
                                    final_lsum[b] = lsum[it[0]]
                                else:
                                    if lsum[it[0]] - lmin[it[0]] * T.Cast("float32", cmin[it[0]]) >= top_p[0]:
                                        R_1[0] = pivot[it[0]]
                                        final_lsum[b] = lsum[it[0]]
                                    else:
                                        if lsum[it[0]] < top_p[0]:
                                            L[0] = pivot[it[0]]
                                it[0] = it[0] + 1
                        T.tvm_storage_sync("shared")
                        L_local[0] = L[0]
                        R_local[0] = R_1[0]
                        find_pivot_local[0] = find_pivot[0]
                        for pidx in T.unroll(3):
                            pivot[pidx] = L[0] - T.Cast("float32", pidx + 1) * (L_local[0] - R_local[0]) / T.float32(4.0)
                    if tx == 0:
                        if not find_pivot_local[0]:
                            final_pivot[b] = R_local[0]
                            if R_local[0] == T.float32(9.9999999999999995e-08):
                                final_lsum[b] = lsum[2]

    @T.prim_func(private=True)
    def top_p_renorm_after_cutoff(var_prob: T.handle, var_final_pivot: T.handle, var_final_lsum: T.handle, var_renorm_prob: T.handle):
        T.func_attr({"target": T.target({"arch": "sm_75", "keys": ["cuda", "gpu"], "kind": "cuda", "libs": ["thrust"], "max_num_threads": 1024, "max_shared_memory_per_block": 49152, "max_threads_per_block": 1024, "tag": "", "thread_warp_size": 32}), "tir.is_scheduled": 1, "tir.noalias": T.bool(True)})
        B, N = T.int32(is_size_var=True), T.int32(is_size_var=True)
        prob = T.match_buffer(var_prob, (B, N))
        final_pivot = T.match_buffer(var_final_pivot, (B,))
        final_lsum = T.match_buffer(var_final_lsum, (B,))
        renorm_prob = T.match_buffer(var_renorm_prob, (B, N))
        # with T.block("root"):
        pivot = T.alloc_buffer((1,), scope="local")
        lsum = T.alloc_buffer((1,), scope="local")
        for _by in T.thread_binding(B, thread="blockIdx.y"):
            for _bx in T.thread_binding(511 // B + 1, thread="blockIdx.x"):
                for _tx in T.thread_binding(1024, thread="threadIdx.x"):
                    with T.block("CTA"):
                        by, bx, tx = T.axis.remap("SSS", [_by, _bx, _tx])
                        T.reads(final_pivot[by], final_lsum[by], prob[by, bx * 1024 + tx:bx * 1024 + tx + (((511 // B * 1024 + N + 1023) // (511 // B * 1024 + 1024) - 1) * (511 // B + 1) * 1024 + 1)], pivot[0], lsum[0])
                        T.writes(pivot[0], lsum[0], renorm_prob[by, bx * 1024 + tx:bx * 1024 + tx + (((511 // B * 1024 + N + 1023) // (511 // B * 1024 + 1024) - 1) * (511 // B + 1) * 1024 + 1)])
                        pivot[0] = final_pivot[by]
                        lsum[0] = final_lsum[by]
                        for i in range((511 // B * 1024 + N + 1023) // (511 // B * 1024 + 1024)):
                            if i * ((512 + B - 1) // B) * 1024 + bx * 1024 + tx < N:
                                renorm_prob[by, i * ((512 + B - 1) // B) * 1024 + bx * 1024 + tx] = T.if_then_else(prob[by, i * ((512 + B - 1) // B) * 1024 + bx * 1024 + tx] >= pivot[0], prob[by, i * ((512 + B - 1) // B) * 1024 + bx * 1024 + tx] / lsum[0], T.float32(0.0))

    @T.prim_func
    def tree_attn_paged_kv(_0: T.int32, var_q: T.handle, var_q_indptr: T.handle, var_pages: T.handle, var_page_indptr: T.handle, var_page_values: T.handle, var_length_info: T.handle, var_k_rope_pos_offset: T.handle, var_q_rope_position: T.handle, var_output: T.handle, var_lse: T.handle, rotary_mode: T.int32, rope_scale: T.float32, rope_theta: T.float32, attn_score_scaling_factor: T.float32, tree_order_indptr_handle: T.handle, tree_order_handle: T.handle):
        T.func_attr({"target": T.target({"arch": "sm_75", "host": {"keys": ["cpu"], "kind": "llvm", "mcpu": "skylake", "mtriple": "x86_64-conda-linux-gnu", "tag": ""}, "keys": ["cuda", "gpu"], "kind": "cuda", "libs": ["thrust"], "max_num_threads": 1024, "max_shared_memory_per_block": 49152, "max_threads_per_block": 1024, "tag": "", "thread_warp_size": 32}), "tir.is_scheduled": 1})
        total_len = T.int32(is_size_var=True)
        q = T.match_buffer(var_q, (total_len, 32, 80), "float16")
        batch_size = T.int32(is_size_var=True)
        q_indptr = T.match_buffer(var_q_indptr, (batch_size + 1,), "int32", offset_factor=1)
        max_num_pages = T.int32(is_size_var=True)
        pages = T.match_buffer(var_pages, (max_num_pages, 2, 32, 16, 80), "float16")
        page_indptr = T.match_buffer(var_page_indptr, (batch_size + 1,), "int32", offset_factor=1)
        nnz_pages = T.int32(is_size_var=True)
        page_values = T.match_buffer(var_page_values, (nnz_pages,), "int32", offset_factor=1)
        length_info = T.match_buffer(var_length_info, (batch_size,), "int32", offset_factor=1)
        k_rope_pos_offset = T.match_buffer(var_k_rope_pos_offset, (batch_size,), "int32", offset_factor=1)
        q_rope_position = T.match_buffer(var_q_rope_position, (total_len,), "int32", offset_factor=1)
        output = T.match_buffer(var_output, (total_len, 32, 80), "float16")
        lse = T.match_buffer(var_lse, (total_len, 32))
        tree_order_indptr = T.match_buffer(tree_order_indptr_handle, (batch_size + 1,), "int32", offset_factor=1)
        total_tree_order_len = T.int32(is_size_var=True)
        tree_order = T.match_buffer(tree_order_handle, (total_tree_order_len, 2), "int32", offset_factor=1)
        # with T.block("root"):
        assert rotary_mode == 0, "Inline rotary mode is not supported in tree attention."
        for lbx in T.thread_binding(16, thread="blockIdx.x"):
            for lby in T.thread_binding(32, thread="blockIdx.y"):
                for lty in T.thread_binding(4, thread="threadIdx.y"):
                    for ltx in T.thread_binding(32, thread="threadIdx.x"):
                        with T.block("attn"):
                            bx, by, ty, tx = T.axis.remap("SSSS", [lbx, lby, lty, ltx])
                            T.reads()
                            T.writes()
                            tile_id = T.alloc_buffer((1,), "int32", scope="local")
                            batch_idx = T.alloc_buffer((1,), "int32", scope="local")
                            batch_tiles = T.alloc_buffer((1,), "int32", scope="local")
                            batch_rows = T.alloc_buffer((1,), "int32", scope="local")
                            iterator = T.alloc_buffer((1,), "int32", scope="local")
                            kv_chunk_len = T.alloc_buffer((1,), "int32", scope="local")
                            Q_smem = T.alloc_buffer((32, 80), "float16", scope="shared")
                            K_smem = T.alloc_buffer((16, 80), "float16", scope="shared")
                            V_smem = T.alloc_buffer((16, 80), "float16", scope="shared")
                            S_smem = T.alloc_buffer((32, 16), scope="shared")
                            S_local = T.alloc_buffer((32, 16), scope="local")
                            O_local = T.alloc_buffer((32, 80), scope="local")
                            m_smem = T.alloc_buffer((32,), scope="shared")
                            m_prev_smem = T.alloc_buffer((32,), scope="shared")
                            d_smem = T.alloc_buffer((32,), scope="shared")
                            m_new = T.alloc_buffer((1,), scope="local")
                            m_prev = T.alloc_buffer((1,), scope="local")
                            d_new = T.alloc_buffer((1,), scope="local")
                            tile_id[0] = bx
                            batch_idx[0] = 0
                            batch_rows[0] = q_indptr[1] - q_indptr[0]
                            batch_tiles[0] = (batch_rows[0] + 32 - 1) // 32
                            while T.tvm_thread_invariant(batch_idx[0] < batch_size):
                                while tile_id[0] >= batch_tiles[0] and batch_idx[0] < batch_size:
                                    tile_id[0] = tile_id[0] - batch_tiles[0]
                                    batch_idx[0] = batch_idx[0] + 1
                                    if batch_idx[0] < batch_size:
                                        b_idx: T.int32 = batch_idx[0]
                                        batch_rows[0] = q_indptr[b_idx + 1] - q_indptr[b_idx]
                                        batch_tiles[0] = (batch_rows[0] + 32 - 1) // 32
                                if T.tvm_thread_invariant(batch_idx[0] < batch_size):
                                    b_idx: T.int32 = batch_idx[0]
                                    LH_start: T.int32 = tile_id[0] * 32
                                    q_indptr_val: T.int32 = q_indptr[b_idx]
                                    cur_page_indptr_begin: T.int32 = page_indptr[b_idx]
                                    cur_page_indptr_end: T.int32 = page_indptr[b_idx + 1]
                                    kv_chunk_len[0] = T.if_then_else(cur_page_indptr_begin != cur_page_indptr_end, (cur_page_indptr_end - cur_page_indptr_begin - 1) * 16 + length_info[b_idx], 0)
                                    T.tvm_storage_sync("shared")
                                    for i in range(1):
                                        row: T.int32 = i * 32 * 4 + ty * 32 + tx
                                        if row < 32:
                                            m_smem[row] = T.float32(-50000.0)
                                            d_smem[row] = T.float32(1.0)
                                    for li_0_lj_0_fused_0 in T.thread_binding(4, thread="threadIdx.y"):
                                        for li_0_lj_0_fused_1 in T.thread_binding(32, thread="threadIdx.x"):
                                            for li_1, lj_1 in T.grid(4, 5):
                                                with T.block("O_init"):
                                                    i = T.axis.spatial(32, (li_0_lj_0_fused_0 * 32 + li_0_lj_0_fused_1) // 16 * 4 + li_1)
                                                    j = T.axis.spatial(80, (li_0_lj_0_fused_0 * 32 + li_0_lj_0_fused_1) % 16 * 5 + lj_1)
                                                    T.reads()
                                                    T.writes(O_local[i, j])
                                                    O_local[i, j] = T.float32(0.0)
                                    T.tvm_storage_sync("shared")
                                    for li_lj_fused_0 in range(5):
                                        for li_lj_fused_1 in T.thread_binding(4, thread="threadIdx.y"):
                                            for li_lj_fused_2 in T.thread_binding(32, thread="threadIdx.x"):
                                                for li_lj_fused_3 in T.vectorized(4):
                                                    with T.block("Q_load"):
                                                        i = T.axis.spatial(32, (li_lj_fused_0 * 512 + li_lj_fused_1 * 128 + li_lj_fused_2 * 4 + li_lj_fused_3) // 80)
                                                        j = T.axis.spatial(80, (li_lj_fused_0 * 512 + li_lj_fused_1 * 128 + li_lj_fused_2 * 4 + li_lj_fused_3) % 80)
                                                        T.reads()
                                                        T.writes()
                                                        cur_L: T.int32 = q_indptr_val + (LH_start + i)
                                                        cur_H_qo: T.int32 = by
                                                        if cur_L < q_indptr[b_idx + 1]:
                                                            freq = T.float32()
                                                            Q_smem[i, j] = T.if_then_else(rotary_mode == 1, T.Let(T.Cast("float16", T.cos(freq) * T.Cast("float32", q[cur_L, cur_H_qo, j]) + T.sin(freq) * T.Cast("float32", T.if_then_else(j < 40, q[cur_L, cur_H_qo, j + 40] * T.float16(-1.0), q[cur_L, cur_H_qo, j - 40]))), where={freq: T.Cast("float32", q_rope_position[cur_L]) * rope_scale / T.pow(rope_theta, T.Cast("float32", j * 2 % 80) / T.float32(80.0))}), q[cur_L, cur_H_qo, j])
                                                        else:
                                                            Q_smem[i, j] = T.float16(0.0)
                                    T.tvm_storage_sync("shared")
                                    for iterator_1 in range((kv_chunk_len[0] + 15) // 16):
                                        L_kv_start: T.int32 = iterator_1 * 16
                                        for lz_ly_fused_0 in range(3):
                                            for lz_ly_fused_1 in T.thread_binding(4, thread="threadIdx.y"):
                                                for lz_ly_fused_2 in T.thread_binding(32, thread="threadIdx.x"):
                                                    for lz_ly_fused_3 in T.vectorized(4):
                                                        with T.block("K_load"):
                                                            i = T.axis.spatial(16, (lz_ly_fused_0 * 512 + lz_ly_fused_1 * 128 + lz_ly_fused_2 * 4 + lz_ly_fused_3) // 80)
                                                            j = T.axis.spatial(80, (lz_ly_fused_0 * 512 + lz_ly_fused_1 * 128 + lz_ly_fused_2 * 4 + lz_ly_fused_3) % 80)
                                                            T.where(((lz_ly_fused_0 * 4 + lz_ly_fused_1) * 32 + lz_ly_fused_2) * 4 + lz_ly_fused_3 < 1280)
                                                            T.reads()
                                                            T.writes()
                                                            cur_L: T.int32 = L_kv_start + i
                                                            if cur_L < kv_chunk_len[0]:
                                                                seq_offset: T.int32 = cur_L
                                                                page_no: T.int32 = page_values[cur_page_indptr_begin + seq_offset // 16]
                                                                page_offset: T.int32 = seq_offset % 16
                                                                K_smem[i, j] = pages[page_no, 0, by, page_offset, j]
                                                            else:
                                                                K_smem[i, j] = T.float16(0.0)
                                        T.tvm_storage_sync("shared")
                                        for lz_ly_fused_0 in range(3):
                                            for lz_ly_fused_1 in T.thread_binding(4, thread="threadIdx.y"):
                                                for lz_ly_fused_2 in T.thread_binding(32, thread="threadIdx.x"):
                                                    for lz_ly_fused_3 in T.vectorized(4):
                                                        with T.block("V_load"):
                                                            i = T.axis.spatial(16, (lz_ly_fused_0 * 512 + lz_ly_fused_1 * 128 + lz_ly_fused_2 * 4 + lz_ly_fused_3) // 80)
                                                            j = T.axis.spatial(80, (lz_ly_fused_0 * 512 + lz_ly_fused_1 * 128 + lz_ly_fused_2 * 4 + lz_ly_fused_3) % 80)
                                                            T.where(((lz_ly_fused_0 * 4 + lz_ly_fused_1) * 32 + lz_ly_fused_2) * 4 + lz_ly_fused_3 < 1280)
                                                            T.reads()
                                                            T.writes()
                                                            cur_L: T.int32 = L_kv_start + i
                                                            if cur_L < kv_chunk_len[0]:
                                                                seq_offset: T.int32 = cur_L
                                                                page_no: T.int32 = page_values[cur_page_indptr_begin + seq_offset // 16]
                                                                page_offset: T.int32 = seq_offset % 16
                                                                V_smem[i, j] = pages[page_no, 1, by, page_offset, j]
                                                            else:
                                                                V_smem[i, j] = T.float16(0.0)
                                        T.tvm_storage_sync("shared")
                                        with T.block(""):
                                            T.reads(Q_smem[0:32, 0:80], K_smem[0:16, 0:80])
                                            T.writes(S_local[0:32, 0:16])
                                            for li_0_lj_0_fused_0_init in T.thread_binding(4, thread="threadIdx.y"):
                                                for li_0_lj_0_fused_1_init in T.thread_binding(32, thread="threadIdx.x"):
                                                    for li_1_init, lj_1_init in T.grid(2, 2):
                                                        with T.block("S_gemm_init"):
                                                            i = T.axis.spatial(32, (li_0_lj_0_fused_0_init * 32 + li_0_lj_0_fused_1_init) // 8 * 2 + li_1_init)
                                                            j = T.axis.spatial(16, (li_0_lj_0_fused_0_init * 32 + li_0_lj_0_fused_1_init) % 8 * 2 + lj_1_init)
                                                            T.reads()
                                                            T.writes(S_local[i, j])
                                                            S_local[i, j] = T.float32(0.0)
                                            for li_0_lj_0_fused_0 in T.thread_binding(4, thread="threadIdx.y"):
                                                for li_0_lj_0_fused_1 in T.thread_binding(32, thread="threadIdx.x"):
                                                    for lk_0, li_1, lj_1, lk_1 in T.grid(10, 2, 2, 8):
                                                        with T.block("S_gemm_update"):
                                                            i = T.axis.spatial(32, (li_0_lj_0_fused_0 * 32 + li_0_lj_0_fused_1) // 8 * 2 + li_1)
                                                            j = T.axis.spatial(16, (li_0_lj_0_fused_0 * 32 + li_0_lj_0_fused_1) % 8 * 2 + lj_1)
                                                            k = T.axis.reduce(80, lk_0 * 8 + lk_1)
                                                            T.reads(S_local[i, j], Q_smem[i, k], K_smem[j, k])
                                                            T.writes(S_local[i, j])
                                                            S_local[i, j] = S_local[i, j] + T.Cast("float32", Q_smem[i, k]) * T.Cast("float32", K_smem[j, k]) * attn_score_scaling_factor * T.float32(0.16129820911147805)
                                        T.tvm_storage_sync("shared")
                                        for li_0_lj_0_fused_0 in T.thread_binding(4, thread="threadIdx.y"):
                                            for li_0_lj_0_fused_1 in T.thread_binding(32, thread="threadIdx.x"):
                                                for li_1, lj_1 in T.grid(2, 2):
                                                    with T.block("S_store"):
                                                        i = T.axis.spatial(32, (li_0_lj_0_fused_0 * 32 + li_0_lj_0_fused_1) // 8 * 2 + li_1)
                                                        j = T.axis.spatial(16, (li_0_lj_0_fused_0 * 32 + li_0_lj_0_fused_1) % 8 * 2 + lj_1)
                                                        T.reads(S_local[i, j])
                                                        T.writes(S_smem[i, j])
                                                        S_smem[i, j] = S_local[i, j]
                                        T.tvm_storage_sync("shared")
                                        for i in range(1):
                                            row: T.int32 = i * 32 * 4 + ty * 32 + tx
                                            if row < 32:
                                                with T.block("update1"):
                                                    T.reads(m_smem[row], kv_chunk_len[0], tree_order_indptr[b_idx:b_idx + 2], tree_order[T.min(LH_start + row + tree_order_indptr[b_idx + 1] + q_indptr[b_idx] - q_indptr[b_idx + 1], L_kv_start + tree_order_indptr[b_idx + 1] - kv_chunk_len[0]):T.min(LH_start + row + tree_order_indptr[b_idx + 1] + q_indptr[b_idx] - q_indptr[b_idx + 1], L_kv_start + tree_order_indptr[b_idx + 1] - kv_chunk_len[0]) + (T.max(LH_start + row + tree_order_indptr[b_idx + 1] + q_indptr[b_idx] - q_indptr[b_idx + 1], L_kv_start + tree_order_indptr[b_idx + 1] + 15 - kv_chunk_len[0]) + 1 - T.min(LH_start + row + tree_order_indptr[b_idx + 1] + q_indptr[b_idx] - q_indptr[b_idx + 1], L_kv_start + tree_order_indptr[b_idx + 1] - kv_chunk_len[0])), 0:2], q_indptr[b_idx:b_idx + 2], m_new[i], S_smem[row, 0:16], d_smem[row], m_prev[i])
                                                    T.writes(m_prev[i], m_new[i], d_new[i])
                                                    m_prev[i] = m_smem[row]
                                                    m_new[i] = m_smem[row]
                                                    row_: T.int32 = LH_start + row
                                                    for j in range(16):
                                                        if L_kv_start + j < kv_chunk_len[0] and (L_kv_start + j < kv_chunk_len[0] - (tree_order_indptr[b_idx + 1] - tree_order_indptr[b_idx]) or tree_order[tree_order_indptr[b_idx] + (row_ + (tree_order_indptr[b_idx + 1] - tree_order_indptr[b_idx]) - (q_indptr[b_idx + 1] - q_indptr[b_idx])), 0] >= tree_order[tree_order_indptr[b_idx] + (L_kv_start + j - (kv_chunk_len[0] - (tree_order_indptr[b_idx + 1] - tree_order_indptr[b_idx]))), 0] and tree_order[tree_order_indptr[b_idx] + (row_ + (tree_order_indptr[b_idx + 1] - tree_order_indptr[b_idx]) - (q_indptr[b_idx + 1] - q_indptr[b_idx])), 0] < tree_order[tree_order_indptr[b_idx] + (L_kv_start + j - (kv_chunk_len[0] - (tree_order_indptr[b_idx + 1] - tree_order_indptr[b_idx]))), 1]):
                                                            m_new[i] = T.max(m_new[i], S_smem[row, j])
                                                    d_new[i] = d_smem[row] * T.exp2(m_prev[i] - m_new[i])
                                        for i in range(1):
                                            row: T.int32 = i * 32 * 4 + ty * 32 + tx
                                            with T.block("update"):
                                                T.reads(kv_chunk_len[0], tree_order_indptr[b_idx:b_idx + 2], tree_order[T.min(LH_start + row + tree_order_indptr[b_idx + 1] + q_indptr[b_idx] - q_indptr[b_idx + 1], L_kv_start + tree_order_indptr[b_idx + 1] - kv_chunk_len[0]):T.min(LH_start + row + tree_order_indptr[b_idx + 1] + q_indptr[b_idx] - q_indptr[b_idx + 1], L_kv_start + tree_order_indptr[b_idx + 1] - kv_chunk_len[0]) + (T.max(LH_start + row + tree_order_indptr[b_idx + 1] + q_indptr[b_idx] - q_indptr[b_idx + 1], L_kv_start + tree_order_indptr[b_idx + 1] + 15 - kv_chunk_len[0]) + 1 - T.min(LH_start + row + tree_order_indptr[b_idx + 1] + q_indptr[b_idx] - q_indptr[b_idx + 1], L_kv_start + tree_order_indptr[b_idx + 1] - kv_chunk_len[0])), 0:2], q_indptr[b_idx:b_idx + 2], S_smem[row, 0:16], m_new[i])
                                                T.writes(S_smem[row, 0:16])
                                                for j in range(16):
                                                    if row < 32:
                                                        row_: T.int32 = LH_start + row
                                                        if L_kv_start + j < kv_chunk_len[0] and (L_kv_start + j < kv_chunk_len[0] - (tree_order_indptr[b_idx + 1] - tree_order_indptr[b_idx]) or tree_order[tree_order_indptr[b_idx] + (row_ + (tree_order_indptr[b_idx + 1] - tree_order_indptr[b_idx]) - (q_indptr[b_idx + 1] - q_indptr[b_idx])), 0] >= tree_order[tree_order_indptr[b_idx] + (L_kv_start + j - (kv_chunk_len[0] - (tree_order_indptr[b_idx + 1] - tree_order_indptr[b_idx]))), 0] and tree_order[tree_order_indptr[b_idx] + (row_ + (tree_order_indptr[b_idx + 1] - tree_order_indptr[b_idx]) - (q_indptr[b_idx + 1] - q_indptr[b_idx])), 0] < tree_order[tree_order_indptr[b_idx] + (L_kv_start + j - (kv_chunk_len[0] - (tree_order_indptr[b_idx + 1] - tree_order_indptr[b_idx]))), 1]):
                                                            S_smem[row, j] = T.exp2(S_smem[row, j] - m_new[i])
                                                        else:
                                                            S_smem[row, j] = T.exp2(T.float32(-50000.0) - m_new[i])
                                        for i in range(1):
                                            row: T.int32 = i * 32 * 4 + ty * 32 + tx
                                            if row < 32:
                                                with T.block("update"):
                                                    T.reads(d_new[i], S_smem[row, 0:16], m_new[i], m_prev[i])
                                                    T.writes(d_new[i], m_smem[row], d_smem[row], m_prev_smem[row])
                                                    for j in range(16):
                                                        d_new[i] = d_new[i] + S_smem[row, j]
                                                    m_smem[row] = m_new[i]
                                                    d_smem[row] = d_new[i]
                                                    m_prev_smem[row] = m_prev[i]
                                        T.tvm_storage_sync("shared")
                                        with T.block(""):
                                            T.reads(m_prev_smem[0:32], m_smem[0:32], S_smem[0:32, 0:16], V_smem[0:16, 0:80])
                                            T.writes(O_local[0:32, 0:80])
                                            for li_0_lj_0_fused_0_init in T.thread_binding(4, thread="threadIdx.y"):
                                                for li_0_lj_0_fused_1_init in T.thread_binding(32, thread="threadIdx.x"):
                                                    for li_1_init, lj_1_init in T.grid(4, 5):
                                                        with T.block("O_gemm_init"):
                                                            i = T.axis.spatial(32, (li_0_lj_0_fused_0_init * 32 + li_0_lj_0_fused_1_init) // 16 * 4 + li_1_init)
                                                            j = T.axis.spatial(80, (li_0_lj_0_fused_0_init * 32 + li_0_lj_0_fused_1_init) % 16 * 5 + lj_1_init)
                                                            T.reads()
                                                            T.writes(O_local[i, j])
                                                            O_local[i, j] = O_local[i, j] * T.exp2(m_prev_smem[i] - m_smem[i])
                                            for li_0_lj_0_fused_0 in T.thread_binding(4, thread="threadIdx.y"):
                                                for li_0_lj_0_fused_1 in T.thread_binding(32, thread="threadIdx.x"):
                                                    for lk_0, lk_1, li_1, lj_1 in T.grid(2, 8, 4, 5):
                                                        with T.block("O_gemm_update"):
                                                            i = T.axis.spatial(32, (li_0_lj_0_fused_0 * 32 + li_0_lj_0_fused_1) // 16 * 4 + li_1)
                                                            j = T.axis.spatial(80, (li_0_lj_0_fused_0 * 32 + li_0_lj_0_fused_1) % 16 * 5 + lj_1)
                                                            k = T.axis.reduce(16, lk_0 * 8 + lk_1)
                                                            T.reads(O_local[i, j], m_prev_smem[i], m_smem[i], S_smem[i, k], V_smem[k, j])
                                                            T.writes(O_local[i, j])
                                                            O_local[i, j] = O_local[i, j] + S_smem[i, k] * T.Cast("float32", V_smem[k, j])
                                    for li_0_lj_0_fused_0 in T.thread_binding(4, thread="threadIdx.y"):
                                        for li_0_lj_0_fused_1 in T.thread_binding(32, thread="threadIdx.x"):
                                            for li_1, lj_1 in T.grid(4, 5):
                                                with T.block("O_store"):
                                                    i = T.axis.spatial(32, (li_0_lj_0_fused_0 * 32 + li_0_lj_0_fused_1) // 16 * 4 + li_1)
                                                    j = T.axis.spatial(80, (li_0_lj_0_fused_0 * 32 + li_0_lj_0_fused_1) % 16 * 5 + lj_1)
                                                    T.reads(q_indptr[b_idx:b_idx + 2], O_local[i, j], d_smem[i])
                                                    T.writes(output[q_indptr[b_idx] + (LH_start + i), by, j])
                                                    cur_L: T.int32 = q_indptr[b_idx] + (LH_start + i)
                                                    cur_H_qo: T.int32 = by
                                                    if cur_L < q_indptr[b_idx + 1]:
                                                        output[cur_L, cur_H_qo, j] = T.Cast("float16", O_local[i, j] / d_smem[i])
                                    for li_0 in range(1):
                                        for li_1 in T.thread_binding(4, thread="threadIdx.y"):
                                            for li_2 in T.thread_binding(32, thread="threadIdx.x"):
                                                with T.block("lse_store"):
                                                    i = T.axis.spatial(32, li_0 * 128 + li_1 * 32 + li_2)
                                                    T.where((li_0 * 4 + li_1) * 32 + li_2 < 32)
                                                    T.reads(q_indptr[b_idx:b_idx + 2], m_smem[i], d_smem[i])
                                                    T.writes(lse[q_indptr[b_idx] + (LH_start + i), by])
                                                    cur_L: T.int32 = q_indptr[b_idx] + (LH_start + i)
                                                    cur_H_qo: T.int32 = by
                                                    if cur_L < q_indptr[b_idx + 1]:
                                                        lse[cur_L, cur_H_qo] = m_smem[i] + T.log2(d_smem[i])
                                    tile_id[0] = tile_id[0] + 16

    @R.function
    def alloc_embedding_tensor() -> R.Tensor((2048, 2560), dtype="float16"):
        R.func_attr({"relax.memory_plan_dynamic_func_output": True})
        gv: R.Tensor((2048, 2560), dtype="float16") = R.builtin.alloc_tensor(R.shape([2048, 2560]), R.dtype("float16"), R.prim_value(0), R.str("global"))
        return gv

    @R.function
    def argsort_probs(probs: R.Tensor(("batch_size", "vocab_size"), dtype="float32")) -> R.Tuple(R.Tensor(("batch_size", "vocab_size"), dtype="float32"), R.Tensor(("batch_size", "vocab_size"), dtype="int32")):
        batch_size = T.int64(is_size_var=True)
        vocab_size = T.int64(is_size_var=True)
        R.func_attr({"relax.memory_plan_dynamic_func_output": True, "tir_non_negative_var": ["vocab_size"], "tir_var_upper_bound": {"batch_size": 128, "num_positions": 768, "num_samples": 128}})
        cls = Module
        with R.dataflow():
            lv1 = R.call_tir(cls.argsort, (probs,), out_sinfo=R.Tensor((batch_size, vocab_size), dtype="int32"))
            lv2 = R.call_tir(cls.take_sorted_probs, (probs, lv1), out_sinfo=R.Tensor((batch_size, vocab_size), dtype="float32"))
            gv1: R.Tuple(R.Tensor((batch_size, vocab_size), dtype="float32"), R.Tensor((batch_size, vocab_size), dtype="int32")) = lv2, lv1
            R.output(gv1)
        return gv1

    @R.function
    def batch_decode(input_embeds: R.Tensor(("batch_size", 1, 2560), dtype="float16"), paged_kv_cache: R.Object, packed_params: R.Tuple(R.Tensor(("vocab_size", 320), dtype="uint32"), R.Tensor(("vocab_size", 80), dtype="float16"), R.Tensor((2560,), dtype="float16"), R.Tensor((2560,), dtype="float16"), R.Tensor((2560,), dtype="float16"), R.Tensor((2560,), dtype="float16"), R.Tensor((7680, 320), dtype="uint32"), R.Tensor((7680, 80), dtype="float16"), R.Tensor((7680,), dtype="float16"), R.Tensor((2560, 320), dtype="uint32"), R.Tensor((2560, 80), dtype="float16"), R.Tensor((2560,), dtype="float16"), R.Tensor((10240, 320), dtype="uint32"), R.Tensor((10240, 80), dtype="float16"), R.Tensor((10240,), dtype="float32"), R.Tensor((2560, 1280), dtype="uint32"), R.Tensor((2560, 320), dtype="float16"), R.Tensor((2560,), dtype="float32"), R.Tensor((2560,), dtype="float16"), R.Tensor((2560,), dtype="float16"), R.Tensor((2560,), dtype="float16"), R.Tensor((2560,), dtype="float16"), R.Tensor((7680, 320), dtype="uint32"), R.Tensor((7680, 80), dtype="float16"), R.Tensor((7680,), dtype="float16"), R.Tensor((2560, 320), dtype="uint32"), R.Tensor((2560, 80), dtype="float16"), R.Tensor((2560,), dtype="float16"), R.Tensor((10240, 320), dtype="uint32"), R.Tensor((10240, 80), dtype="float16"), R.Tensor((10240,), dtype="float32"), R.Tensor((2560, 1280), dtype="uint32"), R.Tensor((2560, 320), dtype="float16"), R.Tensor((2560,), dtype="float32"), R.Tensor((2560,), dtype="float16"), R.Tensor((2560,), dtype="float16"), R.Tensor((2560,), dtype="float16"), R.Tensor((2560,), dtype="float16"), R.Tensor((7680, 320), dtype="uint32"), R.Tensor((7680, 80), dtype="float16"), R.Tensor((7680,), dtype="float16"), R.Tensor((2560, 320), dtype="uint32"), R.Tensor((2560, 80), dtype="float16"), R.Tensor((2560,), dtype="float16"), R.Tensor((10240, 320), dtype="uint32"), R.Tensor((10240, 80), dtype="float16"), R.Tensor((10240,), dtype="float32"), R.Tensor((2560, 1280), dtype="uint32"), R.Tensor((2560, 320), dtype="float16"), R.Tensor((2560,), dtype="float32"), R.Tensor((2560,), dtype="float16"), R.Tensor((2560,), dtype="float16"), R.Tensor((2560,), dtype="float16"), R.Tensor((2560,), dtype="float16"), R.Tensor((7680, 320), dtype="uint32"), R.Tensor((7680, 80), dtype="float16"), R.Tensor((7680,), dtype="float16"), R.Tensor((2560, 320), dtype="uint32"), R.Tensor((2560, 80), dtype="float16"), R.Tensor((2560,), dtype="float16"), R.Tensor((10240, 320), dtype="uint32"), R.Tensor((10240, 80), dtype="float16"), R.Tensor((10240,), dtype="float32"), R.Tensor((2560, 1280), dtype="uint32"), R.Tensor((2560, 320), dtype="float16"), R.Tensor((2560,), dtype="float32"), R.Tensor((2560,), dtype="float16"), R.Tensor((2560,), dtype="float16"), R.Tensor((2560,), dtype="float16"), R.Tensor((2560,), dtype="float16"), R.Tensor((7680, 320), dtype="uint32"), R.Tensor((7680, 80), dtype="float16"), R.Tensor((7680,), dtype="float16"), R.Tensor((2560, 320), dtype="uint32"), R.Tensor((2560, 80), dtype="float16"), R.Tensor((2560,), dtype="float16"), R.Tensor((10240, 320), dtype="uint32"), R.Tensor((10240, 80), dtype="float16"), R.Tensor((10240,), dtype="float32"), R.Tensor((2560, 1280), dtype="uint32"), R.Tensor((2560, 320), dtype="float16"), R.Tensor((2560,), dtype="float32"), R.Tensor((2560,), dtype="float16"), R.Tensor((2560,), dtype="float16"), R.Tensor((2560,), dtype="float16"), R.Tensor((2560,), dtype="float16"), R.Tensor((7680, 320), dtype="uint32"), R.Tensor((7680, 80), dtype="float16"), R.Tensor((7680,), dtype="float16"), R.Tensor((2560, 320), dtype="uint32"), R.Tensor((2560, 80), dtype="float16"), R.Tensor((2560,), dtype="float16"), R.Tensor((10240, 320), dtype="uint32"), R.Tensor((10240, 80), dtype="float16"), R.Tensor((10240,), dtype="float32"), R.Tensor((2560, 1280), dtype="uint32"), R.Tensor((2560, 320), dtype="float16"), R.Tensor((2560,), dtype="float32"), R.Tensor((2560,), dtype="float16"), R.Tensor((2560,), dtype="float16"), R.Tensor((2560,), dtype="float16"), R.Tensor((2560,), dtype="float16"), R.Tensor((7680, 320), dtype="uint32"), R.Tensor((7680, 80), dtype="float16"), R.Tensor((7680,), dtype="float16"), R.Tensor((2560, 320), dtype="uint32"), R.Tensor((2560, 80), dtype="float16"), R.Tensor((2560,), dtype="float16"), R.Tensor((10240, 320), dtype="uint32"), R.Tensor((10240, 80), dtype="float16"), R.Tensor((10240,), dtype="float32"), R.Tensor((2560, 1280), dtype="uint32"), R.Tensor((2560, 320), dtype="float16"), R.Tensor((2560,), dtype="float32"), R.Tensor((2560,), dtype="float16"), R.Tensor((2560,), dtype="float16"), R.Tensor((2560,), dtype="float16"), R.Tensor((2560,), dtype="float16"), R.Tensor((7680, 320), dtype="uint32"), R.Tensor((7680, 80), dtype="float16"), R.Tensor((7680,), dtype="float16"), R.Tensor((2560, 320), dtype="uint32"), R.Tensor((2560, 80), dtype="float16"), R.Tensor((2560,), dtype="float16"), R.Tensor((10240, 320), dtype="uint32"), R.Tensor((10240, 80), dtype="float16"), R.Tensor((10240,), dtype="float32"), R.Tensor((2560, 1280), dtype="uint32"), R.Tensor((2560, 320), dtype="float16"), R.Tensor((2560,), dtype="float32"), R.Tensor((2560,), dtype="float16"), R.Tensor((2560,), dtype="float16"), R.Tensor((2560,), dtype="float16"), R.Tensor((2560,), dtype="float16"), R.Tensor((7680, 320), dtype="uint32"), R.Tensor((7680, 80), dtype="float16"), R.Tensor((7680,), dtype="float16"), R.Tensor((2560, 320), dtype="uint32"), R.Tensor((2560, 80), dtype="float16"), R.Tensor((2560,), dtype="float16"), R.Tensor((10240, 320), dtype="uint32"), R.Tensor((10240, 80), dtype="float16"), R.Tensor((10240,), dtype="float32"), R.Tensor((2560, 1280), dtype="uint32"), R.Tensor((2560, 320), dtype="float16"), R.Tensor((2560,), dtype="float32"), R.Tensor((2560,), dtype="float16"), R.Tensor((2560,), dtype="float16"), R.Tensor((2560,), dtype="float16"), R.Tensor((2560,), dtype="float16"), R.Tensor((7680, 320), dtype="uint32"), R.Tensor((7680, 80), dtype="float16"), R.Tensor((7680,), dtype="float16"), R.Tensor((2560, 320), dtype="uint32"), R.Tensor((2560, 80), dtype="float16"), R.Tensor((2560,), dtype="float16"), R.Tensor((10240, 320), dtype="uint32"), R.Tensor((10240, 80), dtype="float16"), R.Tensor((10240,), dtype="float32"), R.Tensor((2560, 1280), dtype="uint32"), R.Tensor((2560, 320), dtype="float16"), R.Tensor((2560,), dtype="float32"), R.Tensor((2560,), dtype="float16"), R.Tensor((2560,), dtype="float16"), R.Tensor((2560,), dtype="float16"), R.Tensor((2560,), dtype="float16"), R.Tensor((7680, 320), dtype="uint32"), R.Tensor((7680, 80), dtype="float16"), R.Tensor((7680,), dtype="float16"), R.Tensor((2560, 320), dtype="uint32"), R.Tensor((2560, 80), dtype="float16"), R.Tensor((2560,), dtype="float16"), R.Tensor((10240, 320), dtype="uint32"), R.Tensor((10240, 80), dtype="float16"), R.Tensor((10240,), dtype="float32"), R.Tensor((2560, 1280), dtype="uint32"), R.Tensor((2560, 320), dtype="float16"), R.Tensor((2560,), dtype="float32"), R.Tensor((2560,), dtype="float16"), R.Tensor((2560,), dtype="float16"), R.Tensor((2560,), dtype="float16"), R.Tensor((2560,), dtype="float16"), R.Tensor((7680, 320), dtype="uint32"), R.Tensor((7680, 80), dtype="float16"), R.Tensor((7680,), dtype="float16"), R.Tensor((2560, 320), dtype="uint32"), R.Tensor((2560, 80), dtype="float16"), R.Tensor((2560,), dtype="float16"), R.Tensor((10240, 320), dtype="uint32"), R.Tensor((10240, 80), dtype="float16"), R.Tensor((10240,), dtype="float32"), R.Tensor((2560, 1280), dtype="uint32"), R.Tensor((2560, 320), dtype="float16"), R.Tensor((2560,), dtype="float32"), R.Tensor((2560,), dtype="float16"), R.Tensor((2560,), dtype="float16"), R.Tensor((2560,), dtype="float16"), R.Tensor((2560,), dtype="float16"), R.Tensor((7680, 320), dtype="uint32"), R.Tensor((7680, 80), dtype="float16"), R.Tensor((7680,), dtype="float16"), R.Tensor((2560, 320), dtype="uint32"), R.Tensor((2560, 80), dtype="float16"), R.Tensor((2560,), dtype="float16"), R.Tensor((10240, 320), dtype="uint32"), R.Tensor((10240, 80), dtype="float16"), R.Tensor((10240,), dtype="float32"), R.Tensor((2560, 1280), dtype="uint32"), R.Tensor((2560, 320), dtype="float16"), R.Tensor((2560,), dtype="float32"), R.Tensor((2560,), dtype="float16"), R.Tensor((2560,), dtype="float16"), R.Tensor((2560,), dtype="float16"), R.Tensor((2560,), dtype="float16"), R.Tensor((7680, 320), dtype="uint32"), R.Tensor((7680, 80), dtype="float16"), R.Tensor((7680,), dtype="float16"), R.Tensor((2560, 320), dtype="uint32"), R.Tensor((2560, 80), dtype="float16"), R.Tensor((2560,), dtype="float16"), R.Tensor((10240, 320), dtype="uint32"), R.Tensor((10240, 80), dtype="float16"), R.Tensor((10240,), dtype="float32"), R.Tensor((2560, 1280), dtype="uint32"), R.Tensor((2560, 320), dtype="float16"), R.Tensor((2560,), dtype="float32"), R.Tensor((2560,), dtype="float16"), R.Tensor((2560,), dtype="float16"), R.Tensor((2560,), dtype="float16"), R.Tensor((2560,), dtype="float16"), R.Tensor((7680, 320), dtype="uint32"), R.Tensor((7680, 80), dtype="float16"), R.Tensor((7680,), dtype="float16"), R.Tensor((2560, 320), dtype="uint32"), R.Tensor((2560, 80), dtype="float16"), R.Tensor((2560,), dtype="float16"), R.Tensor((10240, 320), dtype="uint32"), R.Tensor((10240, 80), dtype="float16"), R.Tensor((10240,), dtype="float32"), R.Tensor((2560, 1280), dtype="uint32"), R.Tensor((2560, 320), dtype="float16"), R.Tensor((2560,), dtype="float32"), R.Tensor((2560,), dtype="float16"), R.Tensor((2560,), dtype="float16"), R.Tensor((2560,), dtype="float16"), R.Tensor((2560,), dtype="float16"), R.Tensor((7680, 320), dtype="uint32"), R.Tensor((7680, 80), dtype="float16"), R.Tensor((7680,), dtype="float16"), R.Tensor((2560, 320), dtype="uint32"), R.Tensor((2560, 80), dtype="float16"), R.Tensor((2560,), dtype="float16"), R.Tensor((10240, 320), dtype="uint32"), R.Tensor((10240, 80), dtype="float16"), R.Tensor((10240,), dtype="float32"), R.Tensor((2560, 1280), dtype="uint32"), R.Tensor((2560, 320), dtype="float16"), R.Tensor((2560,), dtype="float32"), R.Tensor((2560,), dtype="float16"), R.Tensor((2560,), dtype="float16"), R.Tensor((2560,), dtype="float16"), R.Tensor((2560,), dtype="float16"), R.Tensor((7680, 320), dtype="uint32"), R.Tensor((7680, 80), dtype="float16"), R.Tensor((7680,), dtype="float16"), R.Tensor((2560, 320), dtype="uint32"), R.Tensor((2560, 80), dtype="float16"), R.Tensor((2560,), dtype="float16"), R.Tensor((10240, 320), dtype="uint32"), R.Tensor((10240, 80), dtype="float16"), R.Tensor((10240,), dtype="float32"), R.Tensor((2560, 1280), dtype="uint32"), R.Tensor((2560, 320), dtype="float16"), R.Tensor((2560,), dtype="float32"), R.Tensor((2560,), dtype="float16"), R.Tensor((2560,), dtype="float16"), R.Tensor((2560,), dtype="float16"), R.Tensor((2560,), dtype="float16"), R.Tensor((7680, 320), dtype="uint32"), R.Tensor((7680, 80), dtype="float16"), R.Tensor((7680,), dtype="float16"), R.Tensor((2560, 320), dtype="uint32"), R.Tensor((2560, 80), dtype="float16"), R.Tensor((2560,), dtype="float16"), R.Tensor((10240, 320), dtype="uint32"), R.Tensor((10240, 80), dtype="float16"), R.Tensor((10240,), dtype="float32"), R.Tensor((2560, 1280), dtype="uint32"), R.Tensor((2560, 320), dtype="float16"), R.Tensor((2560,), dtype="float32"), R.Tensor((2560,), dtype="float16"), R.Tensor((2560,), dtype="float16"), R.Tensor((2560,), dtype="float16"), R.Tensor((2560,), dtype="float16"), R.Tensor((7680, 320), dtype="uint32"), R.Tensor((7680, 80), dtype="float16"), R.Tensor((7680,), dtype="float16"), R.Tensor((2560, 320), dtype="uint32"), R.Tensor((2560, 80), dtype="float16"), R.Tensor((2560,), dtype="float16"), R.Tensor((10240, 320), dtype="uint32"), R.Tensor((10240, 80), dtype="float16"), R.Tensor((10240,), dtype="float32"), R.Tensor((2560, 1280), dtype="uint32"), R.Tensor((2560, 320), dtype="float16"), R.Tensor((2560,), dtype="float32"), R.Tensor((2560,), dtype="float16"), R.Tensor((2560,), dtype="float16"), R.Tensor((2560,), dtype="float16"), R.Tensor((2560,), dtype="float16"), R.Tensor((7680, 320), dtype="uint32"), R.Tensor((7680, 80), dtype="float16"), R.Tensor((7680,), dtype="float16"), R.Tensor((2560, 320), dtype="uint32"), R.Tensor((2560, 80), dtype="float16"), R.Tensor((2560,), dtype="float16"), R.Tensor((10240, 320), dtype="uint32"), R.Tensor((10240, 80), dtype="float16"), R.Tensor((10240,), dtype="float32"), R.Tensor((2560, 1280), dtype="uint32"), R.Tensor((2560, 320), dtype="float16"), R.Tensor((2560,), dtype="float32"), R.Tensor((2560,), dtype="float16"), R.Tensor((2560,), dtype="float16"), R.Tensor((2560,), dtype="float16"), R.Tensor((2560,), dtype="float16"), R.Tensor((7680, 320), dtype="uint32"), R.Tensor((7680, 80), dtype="float16"), R.Tensor((7680,), dtype="float16"), R.Tensor((2560, 320), dtype="uint32"), R.Tensor((2560, 80), dtype="float16"), R.Tensor((2560,), dtype="float16"), R.Tensor((10240, 320), dtype="uint32"), R.Tensor((10240, 80), dtype="float16"), R.Tensor((10240,), dtype="float32"), R.Tensor((2560, 1280), dtype="uint32"), R.Tensor((2560, 320), dtype="float16"), R.Tensor((2560,), dtype="float32"), R.Tensor((2560,), dtype="float16"), R.Tensor((2560,), dtype="float16"), R.Tensor((2560,), dtype="float16"), R.Tensor((2560,), dtype="float16"), R.Tensor((7680, 320), dtype="uint32"), R.Tensor((7680, 80), dtype="float16"), R.Tensor((7680,), dtype="float16"), R.Tensor((2560, 320), dtype="uint32"), R.Tensor((2560, 80), dtype="float16"), R.Tensor((2560,), dtype="float16"), R.Tensor((10240, 320), dtype="uint32"), R.Tensor((10240, 80), dtype="float16"), R.Tensor((10240,), dtype="float32"), R.Tensor((2560, 1280), dtype="uint32"), R.Tensor((2560, 320), dtype="float16"), R.Tensor((2560,), dtype="float32"), R.Tensor((2560,), dtype="float16"), R.Tensor((2560,), dtype="float16"), R.Tensor((2560,), dtype="float16"), R.Tensor((2560,), dtype="float16"), R.Tensor((7680, 320), dtype="uint32"), R.Tensor((7680, 80), dtype="float16"), R.Tensor((7680,), dtype="float16"), R.Tensor((2560, 320), dtype="uint32"), R.Tensor((2560, 80), dtype="float16"), R.Tensor((2560,), dtype="float16"), R.Tensor((10240, 320), dtype="uint32"), R.Tensor((10240, 80), dtype="float16"), R.Tensor((10240,), dtype="float32"), R.Tensor((2560, 1280), dtype="uint32"), R.Tensor((2560, 320), dtype="float16"), R.Tensor((2560,), dtype="float32"), R.Tensor((2560,), dtype="float16"), R.Tensor((2560,), dtype="float16"), R.Tensor((2560,), dtype="float16"), R.Tensor((2560,), dtype="float16"), R.Tensor((7680, 320), dtype="uint32"), R.Tensor((7680, 80), dtype="float16"), R.Tensor((7680,), dtype="float16"), R.Tensor((2560, 320), dtype="uint32"), R.Tensor((2560, 80), dtype="float16"), R.Tensor((2560,), dtype="float16"), R.Tensor((10240, 320), dtype="uint32"), R.Tensor((10240, 80), dtype="float16"), R.Tensor((10240,), dtype="float32"), R.Tensor((2560, 1280), dtype="uint32"), R.Tensor((2560, 320), dtype="float16"), R.Tensor((2560,), dtype="float32"), R.Tensor((2560,), dtype="float16"), R.Tensor((2560,), dtype="float16"), R.Tensor((2560,), dtype="float16"), R.Tensor((2560,), dtype="float16"), R.Tensor((7680, 320), dtype="uint32"), R.Tensor((7680, 80), dtype="float16"), R.Tensor((7680,), dtype="float16"), R.Tensor((2560, 320), dtype="uint32"), R.Tensor((2560, 80), dtype="float16"), R.Tensor((2560,), dtype="float16"), R.Tensor((10240, 320), dtype="uint32"), R.Tensor((10240, 80), dtype="float16"), R.Tensor((10240,), dtype="float32"), R.Tensor((2560, 1280), dtype="uint32"), R.Tensor((2560, 320), dtype="float16"), R.Tensor((2560,), dtype="float32"), R.Tensor((2560,), dtype="float16"), R.Tensor((2560,), dtype="float16"), R.Tensor((2560,), dtype="float16"), R.Tensor((2560,), dtype="float16"), R.Tensor((7680, 320), dtype="uint32"), R.Tensor((7680, 80), dtype="float16"), R.Tensor((7680,), dtype="float16"), R.Tensor((2560, 320), dtype="uint32"), R.Tensor((2560, 80), dtype="float16"), R.Tensor((2560,), dtype="float16"), R.Tensor((10240, 320), dtype="uint32"), R.Tensor((10240, 80), dtype="float16"), R.Tensor((10240,), dtype="float32"), R.Tensor((2560, 1280), dtype="uint32"), R.Tensor((2560, 320), dtype="float16"), R.Tensor((2560,), dtype="float32"), R.Tensor((2560,), dtype="float16"), R.Tensor((2560,), dtype="float16"), R.Tensor((2560,), dtype="float16"), R.Tensor((2560,), dtype="float16"), R.Tensor((7680, 320), dtype="uint32"), R.Tensor((7680, 80), dtype="float16"), R.Tensor((7680,), dtype="float16"), R.Tensor((2560, 320), dtype="uint32"), R.Tensor((2560, 80), dtype="float16"), R.Tensor((2560,), dtype="float16"), R.Tensor((10240, 320), dtype="uint32"), R.Tensor((10240, 80), dtype="float16"), R.Tensor((10240,), dtype="float32"), R.Tensor((2560, 1280), dtype="uint32"), R.Tensor((2560, 320), dtype="float16"), R.Tensor((2560,), dtype="float32"), R.Tensor((2560,), dtype="float16"), R.Tensor((2560,), dtype="float16"), R.Tensor((2560,), dtype="float16"), R.Tensor((2560,), dtype="float16"), R.Tensor((7680, 320), dtype="uint32"), R.Tensor((7680, 80), dtype="float16"), R.Tensor((7680,), dtype="float16"), R.Tensor((2560, 320), dtype="uint32"), R.Tensor((2560, 80), dtype="float16"), R.Tensor((2560,), dtype="float16"), R.Tensor((10240, 320), dtype="uint32"), R.Tensor((10240, 80), dtype="float16"), R.Tensor((10240,), dtype="float32"), R.Tensor((2560, 1280), dtype="uint32"), R.Tensor((2560, 320), dtype="float16"), R.Tensor((2560,), dtype="float32"), R.Tensor((2560,), dtype="float16"), R.Tensor((2560,), dtype="float16"), R.Tensor((2560,), dtype="float16"), R.Tensor((2560,), dtype="float16"), R.Tensor((7680, 320), dtype="uint32"), R.Tensor((7680, 80), dtype="float16"), R.Tensor((7680,), dtype="float16"), R.Tensor((2560, 320), dtype="uint32"), R.Tensor((2560, 80), dtype="float16"), R.Tensor((2560,), dtype="float16"), R.Tensor((10240, 320), dtype="uint32"), R.Tensor((10240, 80), dtype="float16"), R.Tensor((10240,), dtype="float32"), R.Tensor((2560, 1280), dtype="uint32"), R.Tensor((2560, 320), dtype="float16"), R.Tensor((2560,), dtype="float32"), R.Tensor((2560,), dtype="float16"), R.Tensor((2560,), dtype="float16"), R.Tensor((2560,), dtype="float16"), R.Tensor((2560,), dtype="float16"), R.Tensor((7680, 320), dtype="uint32"), R.Tensor((7680, 80), dtype="float16"), R.Tensor((7680,), dtype="float16"), R.Tensor((2560, 320), dtype="uint32"), R.Tensor((2560, 80), dtype="float16"), R.Tensor((2560,), dtype="float16"), R.Tensor((10240, 320), dtype="uint32"), R.Tensor((10240, 80), dtype="float16"), R.Tensor((10240,), dtype="float32"), R.Tensor((2560, 1280), dtype="uint32"), R.Tensor((2560, 320), dtype="float16"), R.Tensor((2560,), dtype="float32"), R.Tensor((2560,), dtype="float16"), R.Tensor((2560,), dtype="float16"), R.Tensor((2560,), dtype="float16"), R.Tensor((2560,), dtype="float16"), R.Tensor((7680, 320), dtype="uint32"), R.Tensor((7680, 80), dtype="float16"), R.Tensor((7680,), dtype="float16"), R.Tensor((2560, 320), dtype="uint32"), R.Tensor((2560, 80), dtype="float16"), R.Tensor((2560,), dtype="float16"), R.Tensor((10240, 320), dtype="uint32"), R.Tensor((10240, 80), dtype="float16"), R.Tensor((10240,), dtype="float32"), R.Tensor((2560, 1280), dtype="uint32"), R.Tensor((2560, 320), dtype="float16"), R.Tensor((2560,), dtype="float32"), R.Tensor((2560,), dtype="float16"), R.Tensor((2560,), dtype="float16"), R.Tensor((2560,), dtype="float16"), R.Tensor((2560,), dtype="float16"), R.Tensor((7680, 320), dtype="uint32"), R.Tensor((7680, 80), dtype="float16"), R.Tensor((7680,), dtype="float16"), R.Tensor((2560, 320), dtype="uint32"), R.Tensor((2560, 80), dtype="float16"), R.Tensor((2560,), dtype="float16"), R.Tensor((10240, 320), dtype="uint32"), R.Tensor((10240, 80), dtype="float16"), R.Tensor((10240,), dtype="float32"), R.Tensor((2560, 1280), dtype="uint32"), R.Tensor((2560, 320), dtype="float16"), R.Tensor((2560,), dtype="float32"), R.Tensor((2560,), dtype="float16"), R.Tensor((2560,), dtype="float16"), R.Tensor(("vocab_size", 320), dtype="uint32"), R.Tensor(("vocab_size", 80), dtype="float16"))) -> R.Tuple(R.Tensor(("batch_size", 1, "vocab_size"), dtype="float32"), R.Object):
        batch_size = T.int64()
        vocab_size = T.int64()
        R.func_attr({"num_input": 2, "pipeline_parallel_stages": 1, "relax.memory_plan_dynamic_func_output": True, "relax.rewrite_cuda_graph.capture_symbolic_vars": ["batch_size"], "tir_non_negative_var": ["vocab_size"], "tir_var_upper_bound": {"batch_size": 128, "seq_len": 2048, "total_seq_len": 2048}})
        cls = Module
        with R.dataflow():
            gpt_neox_layers_0_input_layernorm_weight4: R.Tensor((2560,), dtype="float16") = packed_params[2]
            gpt_neox_layers_0_input_layernorm_bias4: R.Tensor((2560,), dtype="float16") = packed_params[3]
            gpt_neox_layers_0_post_attention_layernorm_weight4: R.Tensor((2560,), dtype="float16") = packed_params[4]
            gpt_neox_layers_0_post_attention_layernorm_bias4: R.Tensor((2560,), dtype="float16") = packed_params[5]
            gpt_neox_layers_0_attention_query_key_value_q_weight4: R.Tensor((7680, 320), dtype="uint32") = packed_params[6]
            gpt_neox_layers_0_attention_query_key_value_q_scale4: R.Tensor((7680, 80), dtype="float16") = packed_params[7]
            gpt_neox_layers_0_attention_query_key_value_bias4: R.Tensor((7680,), dtype="float16") = packed_params[8]
            gpt_neox_layers_0_attention_dense_q_weight4: R.Tensor((2560, 320), dtype="uint32") = packed_params[9]
            gpt_neox_layers_0_attention_dense_q_scale4: R.Tensor((2560, 80), dtype="float16") = packed_params[10]
            gpt_neox_layers_0_attention_dense_bias4: R.Tensor((2560,), dtype="float16") = packed_params[11]
            gpt_neox_layers_0_mlp_dense_h_to_4h_q_weight4: R.Tensor((10240, 320), dtype="uint32") = packed_params[12]
            gpt_neox_layers_0_mlp_dense_h_to_4h_q_scale4: R.Tensor((10240, 80), dtype="float16") = packed_params[13]
            gpt_neox_layers_0_mlp_dense_h_to_4h_bias4: R.Tensor((10240,), dtype="float32") = packed_params[14]
            gpt_neox_layers_0_mlp_dense_4h_to_h_q_weight4: R.Tensor((2560, 1280), dtype="uint32") = packed_params[15]
            gpt_neox_layers_0_mlp_dense_4h_to_h_q_scale4: R.Tensor((2560, 320), dtype="float16") = packed_params[16]
            gpt_neox_layers_0_mlp_dense_4h_to_h_bias4: R.Tensor((2560,), dtype="float32") = packed_params[17]
            gpt_neox_layers_1_input_layernorm_weight4: R.Tensor((2560,), dtype="float16") = packed_params[18]
            gpt_neox_layers_1_input_layernorm_bias4: R.Tensor((2560,), dtype="float16") = packed_params[19]
            gpt_neox_layers_1_post_attention_layernorm_weight4: R.Tensor((2560,), dtype="float16") = packed_params[20]
            gpt_neox_layers_1_post_attention_layernorm_bias4: R.Tensor((2560,), dtype="float16") = packed_params[21]
            gpt_neox_layers_1_attention_query_key_value_q_weight4: R.Tensor((7680, 320), dtype="uint32") = packed_params[22]
            gpt_neox_layers_1_attention_query_key_value_q_scale4: R.Tensor((7680, 80), dtype="float16") = packed_params[23]
            gpt_neox_layers_1_attention_query_key_value_bias4: R.Tensor((7680,), dtype="float16") = packed_params[24]
            gpt_neox_layers_1_attention_dense_q_weight4: R.Tensor((2560, 320), dtype="uint32") = packed_params[25]
            gpt_neox_layers_1_attention_dense_q_scale4: R.Tensor((2560, 80), dtype="float16") = packed_params[26]
            gpt_neox_layers_1_attention_dense_bias4: R.Tensor((2560,), dtype="float16") = packed_params[27]
            gpt_neox_layers_1_mlp_dense_h_to_4h_q_weight4: R.Tensor((10240, 320), dtype="uint32") = packed_params[28]
            gpt_neox_layers_1_mlp_dense_h_to_4h_q_scale4: R.Tensor((10240, 80), dtype="float16") = packed_params[29]
            gpt_neox_layers_1_mlp_dense_h_to_4h_bias4: R.Tensor((10240,), dtype="float32") = packed_params[30]
            gpt_neox_layers_1_mlp_dense_4h_to_h_q_weight4: R.Tensor((2560, 1280), dtype="uint32") = packed_params[31]
            gpt_neox_layers_1_mlp_dense_4h_to_h_q_scale4: R.Tensor((2560, 320), dtype="float16") = packed_params[32]
            gpt_neox_layers_1_mlp_dense_4h_to_h_bias4: R.Tensor((2560,), dtype="float32") = packed_params[33]
            gpt_neox_layers_2_input_layernorm_weight4: R.Tensor((2560,), dtype="float16") = packed_params[34]
            gpt_neox_layers_2_input_layernorm_bias4: R.Tensor((2560,), dtype="float16") = packed_params[35]
            gpt_neox_layers_2_post_attention_layernorm_weight4: R.Tensor((2560,), dtype="float16") = packed_params[36]
            gpt_neox_layers_2_post_attention_layernorm_bias4: R.Tensor((2560,), dtype="float16") = packed_params[37]
            gpt_neox_layers_2_attention_query_key_value_q_weight4: R.Tensor((7680, 320), dtype="uint32") = packed_params[38]
            gpt_neox_layers_2_attention_query_key_value_q_scale4: R.Tensor((7680, 80), dtype="float16") = packed_params[39]
            gpt_neox_layers_2_attention_query_key_value_bias4: R.Tensor((7680,), dtype="float16") = packed_params[40]
            gpt_neox_layers_2_attention_dense_q_weight4: R.Tensor((2560, 320), dtype="uint32") = packed_params[41]
            gpt_neox_layers_2_attention_dense_q_scale4: R.Tensor((2560, 80), dtype="float16") = packed_params[42]
            gpt_neox_layers_2_attention_dense_bias4: R.Tensor((2560,), dtype="float16") = packed_params[43]
            gpt_neox_layers_2_mlp_dense_h_to_4h_q_weight4: R.Tensor((10240, 320), dtype="uint32") = packed_params[44]
            gpt_neox_layers_2_mlp_dense_h_to_4h_q_scale4: R.Tensor((10240, 80), dtype="float16") = packed_params[45]
            gpt_neox_layers_2_mlp_dense_h_to_4h_bias4: R.Tensor((10240,), dtype="float32") = packed_params[46]
            gpt_neox_layers_2_mlp_dense_4h_to_h_q_weight4: R.Tensor((2560, 1280), dtype="uint32") = packed_params[47]
            gpt_neox_layers_2_mlp_dense_4h_to_h_q_scale4: R.Tensor((2560, 320), dtype="float16") = packed_params[48]
            gpt_neox_layers_2_mlp_dense_4h_to_h_bias4: R.Tensor((2560,), dtype="float32") = packed_params[49]
            gpt_neox_layers_3_input_layernorm_weight4: R.Tensor((2560,), dtype="float16") = packed_params[50]
            gpt_neox_layers_3_input_layernorm_bias4: R.Tensor((2560,), dtype="float16") = packed_params[51]
            gpt_neox_layers_3_post_attention_layernorm_weight4: R.Tensor((2560,), dtype="float16") = packed_params[52]
            gpt_neox_layers_3_post_attention_layernorm_bias4: R.Tensor((2560,), dtype="float16") = packed_params[53]
            gpt_neox_layers_3_attention_query_key_value_q_weight4: R.Tensor((7680, 320), dtype="uint32") = packed_params[54]
            gpt_neox_layers_3_attention_query_key_value_q_scale4: R.Tensor((7680, 80), dtype="float16") = packed_params[55]
            gpt_neox_layers_3_attention_query_key_value_bias4: R.Tensor((7680,), dtype="float16") = packed_params[56]
            gpt_neox_layers_3_attention_dense_q_weight4: R.Tensor((2560, 320), dtype="uint32") = packed_params[57]
            gpt_neox_layers_3_attention_dense_q_scale4: R.Tensor((2560, 80), dtype="float16") = packed_params[58]
            gpt_neox_layers_3_attention_dense_bias4: R.Tensor((2560,), dtype="float16") = packed_params[59]
            gpt_neox_layers_3_mlp_dense_h_to_4h_q_weight4: R.Tensor((10240, 320), dtype="uint32") = packed_params[60]
            gpt_neox_layers_3_mlp_dense_h_to_4h_q_scale4: R.Tensor((10240, 80), dtype="float16") = packed_params[61]
            gpt_neox_layers_3_mlp_dense_h_to_4h_bias4: R.Tensor((10240,), dtype="float32") = packed_params[62]
            gpt_neox_layers_3_mlp_dense_4h_to_h_q_weight4: R.Tensor((2560, 1280), dtype="uint32") = packed_params[63]
            gpt_neox_layers_3_mlp_dense_4h_to_h_q_scale4: R.Tensor((2560, 320), dtype="float16") = packed_params[64]
            gpt_neox_layers_3_mlp_dense_4h_to_h_bias4: R.Tensor((2560,), dtype="float32") = packed_params[65]
            gpt_neox_layers_4_input_layernorm_weight4: R.Tensor((2560,), dtype="float16") = packed_params[66]
            gpt_neox_layers_4_input_layernorm_bias4: R.Tensor((2560,), dtype="float16") = packed_params[67]
            gpt_neox_layers_4_post_attention_layernorm_weight4: R.Tensor((2560,), dtype="float16") = packed_params[68]
            gpt_neox_layers_4_post_attention_layernorm_bias4: R.Tensor((2560,), dtype="float16") = packed_params[69]
            gpt_neox_layers_4_attention_query_key_value_q_weight4: R.Tensor((7680, 320), dtype="uint32") = packed_params[70]
            gpt_neox_layers_4_attention_query_key_value_q_scale4: R.Tensor((7680, 80), dtype="float16") = packed_params[71]
            gpt_neox_layers_4_attention_query_key_value_bias4: R.Tensor((7680,), dtype="float16") = packed_params[72]
            gpt_neox_layers_4_attention_dense_q_weight4: R.Tensor((2560, 320), dtype="uint32") = packed_params[73]
            gpt_neox_layers_4_attention_dense_q_scale4: R.Tensor((2560, 80), dtype="float16") = packed_params[74]
            gpt_neox_layers_4_attention_dense_bias4: R.Tensor((2560,), dtype="float16") = packed_params[75]
            gpt_neox_layers_4_mlp_dense_h_to_4h_q_weight4: R.Tensor((10240, 320), dtype="uint32") = packed_params[76]
            gpt_neox_layers_4_mlp_dense_h_to_4h_q_scale4: R.Tensor((10240, 80), dtype="float16") = packed_params[77]
            gpt_neox_layers_4_mlp_dense_h_to_4h_bias4: R.Tensor((10240,), dtype="float32") = packed_params[78]
            gpt_neox_layers_4_mlp_dense_4h_to_h_q_weight4: R.Tensor((2560, 1280), dtype="uint32") = packed_params[79]
            gpt_neox_layers_4_mlp_dense_4h_to_h_q_scale4: R.Tensor((2560, 320), dtype="float16") = packed_params[80]
            gpt_neox_layers_4_mlp_dense_4h_to_h_bias4: R.Tensor((2560,), dtype="float32") = packed_params[81]
            gpt_neox_layers_5_input_layernorm_weight4: R.Tensor((2560,), dtype="float16") = packed_params[82]
            gpt_neox_layers_5_input_layernorm_bias4: R.Tensor((2560,), dtype="float16") = packed_params[83]
            gpt_neox_layers_5_post_attention_layernorm_weight4: R.Tensor((2560,), dtype="float16") = packed_params[84]
            gpt_neox_layers_5_post_attention_layernorm_bias4: R.Tensor((2560,), dtype="float16") = packed_params[85]
            gpt_neox_layers_5_attention_query_key_value_q_weight4: R.Tensor((7680, 320), dtype="uint32") = packed_params[86]
            gpt_neox_layers_5_attention_query_key_value_q_scale4: R.Tensor((7680, 80), dtype="float16") = packed_params[87]
            gpt_neox_layers_5_attention_query_key_value_bias4: R.Tensor((7680,), dtype="float16") = packed_params[88]
            gpt_neox_layers_5_attention_dense_q_weight4: R.Tensor((2560, 320), dtype="uint32") = packed_params[89]
            gpt_neox_layers_5_attention_dense_q_scale4: R.Tensor((2560, 80), dtype="float16") = packed_params[90]
            gpt_neox_layers_5_attention_dense_bias4: R.Tensor((2560,), dtype="float16") = packed_params[91]
            gpt_neox_layers_5_mlp_dense_h_to_4h_q_weight4: R.Tensor((10240, 320), dtype="uint32") = packed_params[92]
            gpt_neox_layers_5_mlp_dense_h_to_4h_q_scale4: R.Tensor((10240, 80), dtype="float16") = packed_params[93]
            gpt_neox_layers_5_mlp_dense_h_to_4h_bias4: R.Tensor((10240,), dtype="float32") = packed_params[94]
            gpt_neox_layers_5_mlp_dense_4h_to_h_q_weight4: R.Tensor((2560, 1280), dtype="uint32") = packed_params[95]
            gpt_neox_layers_5_mlp_dense_4h_to_h_q_scale4: R.Tensor((2560, 320), dtype="float16") = packed_params[96]
            gpt_neox_layers_5_mlp_dense_4h_to_h_bias4: R.Tensor((2560,), dtype="float32") = packed_params[97]
            gpt_neox_layers_6_input_layernorm_weight4: R.Tensor((2560,), dtype="float16") = packed_params[98]
            gpt_neox_layers_6_input_layernorm_bias4: R.Tensor((2560,), dtype="float16") = packed_params[99]
            gpt_neox_layers_6_post_attention_layernorm_weight4: R.Tensor((2560,), dtype="float16") = packed_params[100]
            gpt_neox_layers_6_post_attention_layernorm_bias4: R.Tensor((2560,), dtype="float16") = packed_params[101]
            gpt_neox_layers_6_attention_query_key_value_q_weight4: R.Tensor((7680, 320), dtype="uint32") = packed_params[102]
            gpt_neox_layers_6_attention_query_key_value_q_scale4: R.Tensor((7680, 80), dtype="float16") = packed_params[103]
            gpt_neox_layers_6_attention_query_key_value_bias4: R.Tensor((7680,), dtype="float16") = packed_params[104]
            gpt_neox_layers_6_attention_dense_q_weight4: R.Tensor((2560, 320), dtype="uint32") = packed_params[105]
            gpt_neox_layers_6_attention_dense_q_scale4: R.Tensor((2560, 80), dtype="float16") = packed_params[106]
            gpt_neox_layers_6_attention_dense_bias4: R.Tensor((2560,), dtype="float16") = packed_params[107]
            gpt_neox_layers_6_mlp_dense_h_to_4h_q_weight4: R.Tensor((10240, 320), dtype="uint32") = packed_params[108]
            gpt_neox_layers_6_mlp_dense_h_to_4h_q_scale4: R.Tensor((10240, 80), dtype="float16") = packed_params[109]
            gpt_neox_layers_6_mlp_dense_h_to_4h_bias4: R.Tensor((10240,), dtype="float32") = packed_params[110]
            gpt_neox_layers_6_mlp_dense_4h_to_h_q_weight4: R.Tensor((2560, 1280), dtype="uint32") = packed_params[111]
            gpt_neox_layers_6_mlp_dense_4h_to_h_q_scale4: R.Tensor((2560, 320), dtype="float16") = packed_params[112]
            gpt_neox_layers_6_mlp_dense_4h_to_h_bias4: R.Tensor((2560,), dtype="float32") = packed_params[113]
            gpt_neox_layers_7_input_layernorm_weight4: R.Tensor((2560,), dtype="float16") = packed_params[114]
            gpt_neox_layers_7_input_layernorm_bias4: R.Tensor((2560,), dtype="float16") = packed_params[115]
            gpt_neox_layers_7_post_attention_layernorm_weight4: R.Tensor((2560,), dtype="float16") = packed_params[116]
            gpt_neox_layers_7_post_attention_layernorm_bias4: R.Tensor((2560,), dtype="float16") = packed_params[117]
            gpt_neox_layers_7_attention_query_key_value_q_weight4: R.Tensor((7680, 320), dtype="uint32") = packed_params[118]
            gpt_neox_layers_7_attention_query_key_value_q_scale4: R.Tensor((7680, 80), dtype="float16") = packed_params[119]
            gpt_neox_layers_7_attention_query_key_value_bias4: R.Tensor((7680,), dtype="float16") = packed_params[120]
            gpt_neox_layers_7_attention_dense_q_weight4: R.Tensor((2560, 320), dtype="uint32") = packed_params[121]
            gpt_neox_layers_7_attention_dense_q_scale4: R.Tensor((2560, 80), dtype="float16") = packed_params[122]
            gpt_neox_layers_7_attention_dense_bias4: R.Tensor((2560,), dtype="float16") = packed_params[123]
            gpt_neox_layers_7_mlp_dense_h_to_4h_q_weight4: R.Tensor((10240, 320), dtype="uint32") = packed_params[124]
            gpt_neox_layers_7_mlp_dense_h_to_4h_q_scale4: R.Tensor((10240, 80), dtype="float16") = packed_params[125]
            gpt_neox_layers_7_mlp_dense_h_to_4h_bias4: R.Tensor((10240,), dtype="float32") = packed_params[126]
            gpt_neox_layers_7_mlp_dense_4h_to_h_q_weight4: R.Tensor((2560, 1280), dtype="uint32") = packed_params[127]
            gpt_neox_layers_7_mlp_dense_4h_to_h_q_scale4: R.Tensor((2560, 320), dtype="float16") = packed_params[128]
            gpt_neox_layers_7_mlp_dense_4h_to_h_bias4: R.Tensor((2560,), dtype="float32") = packed_params[129]
            gpt_neox_layers_8_input_layernorm_weight4: R.Tensor((2560,), dtype="float16") = packed_params[130]
            gpt_neox_layers_8_input_layernorm_bias4: R.Tensor((2560,), dtype="float16") = packed_params[131]
            gpt_neox_layers_8_post_attention_layernorm_weight4: R.Tensor((2560,), dtype="float16") = packed_params[132]
            gpt_neox_layers_8_post_attention_layernorm_bias4: R.Tensor((2560,), dtype="float16") = packed_params[133]
            gpt_neox_layers_8_attention_query_key_value_q_weight4: R.Tensor((7680, 320), dtype="uint32") = packed_params[134]
            gpt_neox_layers_8_attention_query_key_value_q_scale4: R.Tensor((7680, 80), dtype="float16") = packed_params[135]
            gpt_neox_layers_8_attention_query_key_value_bias4: R.Tensor((7680,), dtype="float16") = packed_params[136]
            gpt_neox_layers_8_attention_dense_q_weight4: R.Tensor((2560, 320), dtype="uint32") = packed_params[137]
            gpt_neox_layers_8_attention_dense_q_scale4: R.Tensor((2560, 80), dtype="float16") = packed_params[138]
            gpt_neox_layers_8_attention_dense_bias4: R.Tensor((2560,), dtype="float16") = packed_params[139]
            gpt_neox_layers_8_mlp_dense_h_to_4h_q_weight4: R.Tensor((10240, 320), dtype="uint32") = packed_params[140]
            gpt_neox_layers_8_mlp_dense_h_to_4h_q_scale4: R.Tensor((10240, 80), dtype="float16") = packed_params[141]
            gpt_neox_layers_8_mlp_dense_h_to_4h_bias4: R.Tensor((10240,), dtype="float32") = packed_params[142]
            gpt_neox_layers_8_mlp_dense_4h_to_h_q_weight4: R.Tensor((2560, 1280), dtype="uint32") = packed_params[143]
            gpt_neox_layers_8_mlp_dense_4h_to_h_q_scale4: R.Tensor((2560, 320), dtype="float16") = packed_params[144]
            gpt_neox_layers_8_mlp_dense_4h_to_h_bias4: R.Tensor((2560,), dtype="float32") = packed_params[145]
            gpt_neox_layers_9_input_layernorm_weight4: R.Tensor((2560,), dtype="float16") = packed_params[146]
            gpt_neox_layers_9_input_layernorm_bias4: R.Tensor((2560,), dtype="float16") = packed_params[147]
            gpt_neox_layers_9_post_attention_layernorm_weight4: R.Tensor((2560,), dtype="float16") = packed_params[148]
            gpt_neox_layers_9_post_attention_layernorm_bias4: R.Tensor((2560,), dtype="float16") = packed_params[149]
            gpt_neox_layers_9_attention_query_key_value_q_weight4: R.Tensor((7680, 320), dtype="uint32") = packed_params[150]
            gpt_neox_layers_9_attention_query_key_value_q_scale4: R.Tensor((7680, 80), dtype="float16") = packed_params[151]
            gpt_neox_layers_9_attention_query_key_value_bias4: R.Tensor((7680,), dtype="float16") = packed_params[152]
            gpt_neox_layers_9_attention_dense_q_weight4: R.Tensor((2560, 320), dtype="uint32") = packed_params[153]
            gpt_neox_layers_9_attention_dense_q_scale4: R.Tensor((2560, 80), dtype="float16") = packed_params[154]
            gpt_neox_layers_9_attention_dense_bias4: R.Tensor((2560,), dtype="float16") = packed_params[155]
            gpt_neox_layers_9_mlp_dense_h_to_4h_q_weight4: R.Tensor((10240, 320), dtype="uint32") = packed_params[156]
            gpt_neox_layers_9_mlp_dense_h_to_4h_q_scale4: R.Tensor((10240, 80), dtype="float16") = packed_params[157]
            gpt_neox_layers_9_mlp_dense_h_to_4h_bias4: R.Tensor((10240,), dtype="float32") = packed_params[158]
            gpt_neox_layers_9_mlp_dense_4h_to_h_q_weight4: R.Tensor((2560, 1280), dtype="uint32") = packed_params[159]
            gpt_neox_layers_9_mlp_dense_4h_to_h_q_scale4: R.Tensor((2560, 320), dtype="float16") = packed_params[160]
            gpt_neox_layers_9_mlp_dense_4h_to_h_bias4: R.Tensor((2560,), dtype="float32") = packed_params[161]
            gpt_neox_layers_10_input_layernorm_weight4: R.Tensor((2560,), dtype="float16") = packed_params[162]
            gpt_neox_layers_10_input_layernorm_bias4: R.Tensor((2560,), dtype="float16") = packed_params[163]
            gpt_neox_layers_10_post_attention_layernorm_weight4: R.Tensor((2560,), dtype="float16") = packed_params[164]
            gpt_neox_layers_10_post_attention_layernorm_bias4: R.Tensor((2560,), dtype="float16") = packed_params[165]
            gpt_neox_layers_10_attention_query_key_value_q_weight4: R.Tensor((7680, 320), dtype="uint32") = packed_params[166]
            gpt_neox_layers_10_attention_query_key_value_q_scale4: R.Tensor((7680, 80), dtype="float16") = packed_params[167]
            gpt_neox_layers_10_attention_query_key_value_bias4: R.Tensor((7680,), dtype="float16") = packed_params[168]
            gpt_neox_layers_10_attention_dense_q_weight4: R.Tensor((2560, 320), dtype="uint32") = packed_params[169]
            gpt_neox_layers_10_attention_dense_q_scale4: R.Tensor((2560, 80), dtype="float16") = packed_params[170]
            gpt_neox_layers_10_attention_dense_bias4: R.Tensor((2560,), dtype="float16") = packed_params[171]
            gpt_neox_layers_10_mlp_dense_h_to_4h_q_weight4: R.Tensor((10240, 320), dtype="uint32") = packed_params[172]
            gpt_neox_layers_10_mlp_dense_h_to_4h_q_scale4: R.Tensor((10240, 80), dtype="float16") = packed_params[173]
            gpt_neox_layers_10_mlp_dense_h_to_4h_bias4: R.Tensor((10240,), dtype="float32") = packed_params[174]
            gpt_neox_layers_10_mlp_dense_4h_to_h_q_weight4: R.Tensor((2560, 1280), dtype="uint32") = packed_params[175]
            gpt_neox_layers_10_mlp_dense_4h_to_h_q_scale4: R.Tensor((2560, 320), dtype="float16") = packed_params[176]
            gpt_neox_layers_10_mlp_dense_4h_to_h_bias4: R.Tensor((2560,), dtype="float32") = packed_params[177]
            gpt_neox_layers_11_input_layernorm_weight4: R.Tensor((2560,), dtype="float16") = packed_params[178]
            gpt_neox_layers_11_input_layernorm_bias4: R.Tensor((2560,), dtype="float16") = packed_params[179]
            gpt_neox_layers_11_post_attention_layernorm_weight4: R.Tensor((2560,), dtype="float16") = packed_params[180]
            gpt_neox_layers_11_post_attention_layernorm_bias4: R.Tensor((2560,), dtype="float16") = packed_params[181]
            gpt_neox_layers_11_attention_query_key_value_q_weight4: R.Tensor((7680, 320), dtype="uint32") = packed_params[182]
            gpt_neox_layers_11_attention_query_key_value_q_scale4: R.Tensor((7680, 80), dtype="float16") = packed_params[183]
            gpt_neox_layers_11_attention_query_key_value_bias4: R.Tensor((7680,), dtype="float16") = packed_params[184]
            gpt_neox_layers_11_attention_dense_q_weight4: R.Tensor((2560, 320), dtype="uint32") = packed_params[185]
            gpt_neox_layers_11_attention_dense_q_scale4: R.Tensor((2560, 80), dtype="float16") = packed_params[186]
            gpt_neox_layers_11_attention_dense_bias4: R.Tensor((2560,), dtype="float16") = packed_params[187]
            gpt_neox_layers_11_mlp_dense_h_to_4h_q_weight4: R.Tensor((10240, 320), dtype="uint32") = packed_params[188]
            gpt_neox_layers_11_mlp_dense_h_to_4h_q_scale4: R.Tensor((10240, 80), dtype="float16") = packed_params[189]
            gpt_neox_layers_11_mlp_dense_h_to_4h_bias4: R.Tensor((10240,), dtype="float32") = packed_params[190]
            gpt_neox_layers_11_mlp_dense_4h_to_h_q_weight4: R.Tensor((2560, 1280), dtype="uint32") = packed_params[191]
            gpt_neox_layers_11_mlp_dense_4h_to_h_q_scale4: R.Tensor((2560, 320), dtype="float16") = packed_params[192]
            gpt_neox_layers_11_mlp_dense_4h_to_h_bias4: R.Tensor((2560,), dtype="float32") = packed_params[193]
            gpt_neox_layers_12_input_layernorm_weight4: R.Tensor((2560,), dtype="float16") = packed_params[194]
            gpt_neox_layers_12_input_layernorm_bias4: R.Tensor((2560,), dtype="float16") = packed_params[195]
            gpt_neox_layers_12_post_attention_layernorm_weight4: R.Tensor((2560,), dtype="float16") = packed_params[196]
            gpt_neox_layers_12_post_attention_layernorm_bias4: R.Tensor((2560,), dtype="float16") = packed_params[197]
            gpt_neox_layers_12_attention_query_key_value_q_weight4: R.Tensor((7680, 320), dtype="uint32") = packed_params[198]
            gpt_neox_layers_12_attention_query_key_value_q_scale4: R.Tensor((7680, 80), dtype="float16") = packed_params[199]
            gpt_neox_layers_12_attention_query_key_value_bias4: R.Tensor((7680,), dtype="float16") = packed_params[200]
            gpt_neox_layers_12_attention_dense_q_weight4: R.Tensor((2560, 320), dtype="uint32") = packed_params[201]
            gpt_neox_layers_12_attention_dense_q_scale4: R.Tensor((2560, 80), dtype="float16") = packed_params[202]
            gpt_neox_layers_12_attention_dense_bias4: R.Tensor((2560,), dtype="float16") = packed_params[203]
            gpt_neox_layers_12_mlp_dense_h_to_4h_q_weight4: R.Tensor((10240, 320), dtype="uint32") = packed_params[204]
            gpt_neox_layers_12_mlp_dense_h_to_4h_q_scale4: R.Tensor((10240, 80), dtype="float16") = packed_params[205]
            gpt_neox_layers_12_mlp_dense_h_to_4h_bias4: R.Tensor((10240,), dtype="float32") = packed_params[206]
            gpt_neox_layers_12_mlp_dense_4h_to_h_q_weight4: R.Tensor((2560, 1280), dtype="uint32") = packed_params[207]
            gpt_neox_layers_12_mlp_dense_4h_to_h_q_scale4: R.Tensor((2560, 320), dtype="float16") = packed_params[208]
            gpt_neox_layers_12_mlp_dense_4h_to_h_bias4: R.Tensor((2560,), dtype="float32") = packed_params[209]
            gpt_neox_layers_13_input_layernorm_weight4: R.Tensor((2560,), dtype="float16") = packed_params[210]
            gpt_neox_layers_13_input_layernorm_bias4: R.Tensor((2560,), dtype="float16") = packed_params[211]
            gpt_neox_layers_13_post_attention_layernorm_weight4: R.Tensor((2560,), dtype="float16") = packed_params[212]
            gpt_neox_layers_13_post_attention_layernorm_bias4: R.Tensor((2560,), dtype="float16") = packed_params[213]
            gpt_neox_layers_13_attention_query_key_value_q_weight4: R.Tensor((7680, 320), dtype="uint32") = packed_params[214]
            gpt_neox_layers_13_attention_query_key_value_q_scale4: R.Tensor((7680, 80), dtype="float16") = packed_params[215]
            gpt_neox_layers_13_attention_query_key_value_bias4: R.Tensor((7680,), dtype="float16") = packed_params[216]
            gpt_neox_layers_13_attention_dense_q_weight4: R.Tensor((2560, 320), dtype="uint32") = packed_params[217]
            gpt_neox_layers_13_attention_dense_q_scale4: R.Tensor((2560, 80), dtype="float16") = packed_params[218]
            gpt_neox_layers_13_attention_dense_bias4: R.Tensor((2560,), dtype="float16") = packed_params[219]
            gpt_neox_layers_13_mlp_dense_h_to_4h_q_weight4: R.Tensor((10240, 320), dtype="uint32") = packed_params[220]
            gpt_neox_layers_13_mlp_dense_h_to_4h_q_scale4: R.Tensor((10240, 80), dtype="float16") = packed_params[221]
            gpt_neox_layers_13_mlp_dense_h_to_4h_bias4: R.Tensor((10240,), dtype="float32") = packed_params[222]
            gpt_neox_layers_13_mlp_dense_4h_to_h_q_weight4: R.Tensor((2560, 1280), dtype="uint32") = packed_params[223]
            gpt_neox_layers_13_mlp_dense_4h_to_h_q_scale4: R.Tensor((2560, 320), dtype="float16") = packed_params[224]
            gpt_neox_layers_13_mlp_dense_4h_to_h_bias4: R.Tensor((2560,), dtype="float32") = packed_params[225]
            gpt_neox_layers_14_input_layernorm_weight4: R.Tensor((2560,), dtype="float16") = packed_params[226]
            gpt_neox_layers_14_input_layernorm_bias4: R.Tensor((2560,), dtype="float16") = packed_params[227]
            gpt_neox_layers_14_post_attention_layernorm_weight4: R.Tensor((2560,), dtype="float16") = packed_params[228]
            gpt_neox_layers_14_post_attention_layernorm_bias4: R.Tensor((2560,), dtype="float16") = packed_params[229]
            gpt_neox_layers_14_attention_query_key_value_q_weight4: R.Tensor((7680, 320), dtype="uint32") = packed_params[230]
            gpt_neox_layers_14_attention_query_key_value_q_scale4: R.Tensor((7680, 80), dtype="float16") = packed_params[231]
            gpt_neox_layers_14_attention_query_key_value_bias4: R.Tensor((7680,), dtype="float16") = packed_params[232]
            gpt_neox_layers_14_attention_dense_q_weight4: R.Tensor((2560, 320), dtype="uint32") = packed_params[233]
            gpt_neox_layers_14_attention_dense_q_scale4: R.Tensor((2560, 80), dtype="float16") = packed_params[234]
            gpt_neox_layers_14_attention_dense_bias4: R.Tensor((2560,), dtype="float16") = packed_params[235]
            gpt_neox_layers_14_mlp_dense_h_to_4h_q_weight4: R.Tensor((10240, 320), dtype="uint32") = packed_params[236]
            gpt_neox_layers_14_mlp_dense_h_to_4h_q_scale4: R.Tensor((10240, 80), dtype="float16") = packed_params[237]
            gpt_neox_layers_14_mlp_dense_h_to_4h_bias4: R.Tensor((10240,), dtype="float32") = packed_params[238]
            gpt_neox_layers_14_mlp_dense_4h_to_h_q_weight4: R.Tensor((2560, 1280), dtype="uint32") = packed_params[239]
            gpt_neox_layers_14_mlp_dense_4h_to_h_q_scale4: R.Tensor((2560, 320), dtype="float16") = packed_params[240]
            gpt_neox_layers_14_mlp_dense_4h_to_h_bias4: R.Tensor((2560,), dtype="float32") = packed_params[241]
            gpt_neox_layers_15_input_layernorm_weight4: R.Tensor((2560,), dtype="float16") = packed_params[242]
            gpt_neox_layers_15_input_layernorm_bias4: R.Tensor((2560,), dtype="float16") = packed_params[243]
            gpt_neox_layers_15_post_attention_layernorm_weight4: R.Tensor((2560,), dtype="float16") = packed_params[244]
            gpt_neox_layers_15_post_attention_layernorm_bias4: R.Tensor((2560,), dtype="float16") = packed_params[245]
            gpt_neox_layers_15_attention_query_key_value_q_weight4: R.Tensor((7680, 320), dtype="uint32") = packed_params[246]
            gpt_neox_layers_15_attention_query_key_value_q_scale4: R.Tensor((7680, 80), dtype="float16") = packed_params[247]
            gpt_neox_layers_15_attention_query_key_value_bias4: R.Tensor((7680,), dtype="float16") = packed_params[248]
            gpt_neox_layers_15_attention_dense_q_weight4: R.Tensor((2560, 320), dtype="uint32") = packed_params[249]
            gpt_neox_layers_15_attention_dense_q_scale4: R.Tensor((2560, 80), dtype="float16") = packed_params[250]
            gpt_neox_layers_15_attention_dense_bias4: R.Tensor((2560,), dtype="float16") = packed_params[251]
            gpt_neox_layers_15_mlp_dense_h_to_4h_q_weight4: R.Tensor((10240, 320), dtype="uint32") = packed_params[252]
            gpt_neox_layers_15_mlp_dense_h_to_4h_q_scale4: R.Tensor((10240, 80), dtype="float16") = packed_params[253]
            gpt_neox_layers_15_mlp_dense_h_to_4h_bias4: R.Tensor((10240,), dtype="float32") = packed_params[254]
            gpt_neox_layers_15_mlp_dense_4h_to_h_q_weight4: R.Tensor((2560, 1280), dtype="uint32") = packed_params[255]
            gpt_neox_layers_15_mlp_dense_4h_to_h_q_scale4: R.Tensor((2560, 320), dtype="float16") = packed_params[256]
            gpt_neox_layers_15_mlp_dense_4h_to_h_bias4: R.Tensor((2560,), dtype="float32") = packed_params[257]
            gpt_neox_layers_16_input_layernorm_weight4: R.Tensor((2560,), dtype="float16") = packed_params[258]
            gpt_neox_layers_16_input_layernorm_bias4: R.Tensor((2560,), dtype="float16") = packed_params[259]
            gpt_neox_layers_16_post_attention_layernorm_weight4: R.Tensor((2560,), dtype="float16") = packed_params[260]
            gpt_neox_layers_16_post_attention_layernorm_bias4: R.Tensor((2560,), dtype="float16") = packed_params[261]
            gpt_neox_layers_16_attention_query_key_value_q_weight4: R.Tensor((7680, 320), dtype="uint32") = packed_params[262]
            gpt_neox_layers_16_attention_query_key_value_q_scale4: R.Tensor((7680, 80), dtype="float16") = packed_params[263]
            gpt_neox_layers_16_attention_query_key_value_bias4: R.Tensor((7680,), dtype="float16") = packed_params[264]
            gpt_neox_layers_16_attention_dense_q_weight4: R.Tensor((2560, 320), dtype="uint32") = packed_params[265]
            gpt_neox_layers_16_attention_dense_q_scale4: R.Tensor((2560, 80), dtype="float16") = packed_params[266]
            gpt_neox_layers_16_attention_dense_bias4: R.Tensor((2560,), dtype="float16") = packed_params[267]
            gpt_neox_layers_16_mlp_dense_h_to_4h_q_weight4: R.Tensor((10240, 320), dtype="uint32") = packed_params[268]
            gpt_neox_layers_16_mlp_dense_h_to_4h_q_scale4: R.Tensor((10240, 80), dtype="float16") = packed_params[269]
            gpt_neox_layers_16_mlp_dense_h_to_4h_bias4: R.Tensor((10240,), dtype="float32") = packed_params[270]
            gpt_neox_layers_16_mlp_dense_4h_to_h_q_weight4: R.Tensor((2560, 1280), dtype="uint32") = packed_params[271]
            gpt_neox_layers_16_mlp_dense_4h_to_h_q_scale4: R.Tensor((2560, 320), dtype="float16") = packed_params[272]
            gpt_neox_layers_16_mlp_dense_4h_to_h_bias4: R.Tensor((2560,), dtype="float32") = packed_params[273]
            gpt_neox_layers_17_input_layernorm_weight4: R.Tensor((2560,), dtype="float16") = packed_params[274]
            gpt_neox_layers_17_input_layernorm_bias4: R.Tensor((2560,), dtype="float16") = packed_params[275]
            gpt_neox_layers_17_post_attention_layernorm_weight4: R.Tensor((2560,), dtype="float16") = packed_params[276]
            gpt_neox_layers_17_post_attention_layernorm_bias4: R.Tensor((2560,), dtype="float16") = packed_params[277]
            gpt_neox_layers_17_attention_query_key_value_q_weight4: R.Tensor((7680, 320), dtype="uint32") = packed_params[278]
            gpt_neox_layers_17_attention_query_key_value_q_scale4: R.Tensor((7680, 80), dtype="float16") = packed_params[279]
            gpt_neox_layers_17_attention_query_key_value_bias4: R.Tensor((7680,), dtype="float16") = packed_params[280]
            gpt_neox_layers_17_attention_dense_q_weight4: R.Tensor((2560, 320), dtype="uint32") = packed_params[281]
            gpt_neox_layers_17_attention_dense_q_scale4: R.Tensor((2560, 80), dtype="float16") = packed_params[282]
            gpt_neox_layers_17_attention_dense_bias4: R.Tensor((2560,), dtype="float16") = packed_params[283]
            gpt_neox_layers_17_mlp_dense_h_to_4h_q_weight4: R.Tensor((10240, 320), dtype="uint32") = packed_params[284]
            gpt_neox_layers_17_mlp_dense_h_to_4h_q_scale4: R.Tensor((10240, 80), dtype="float16") = packed_params[285]
            gpt_neox_layers_17_mlp_dense_h_to_4h_bias4: R.Tensor((10240,), dtype="float32") = packed_params[286]
            gpt_neox_layers_17_mlp_dense_4h_to_h_q_weight4: R.Tensor((2560, 1280), dtype="uint32") = packed_params[287]
            gpt_neox_layers_17_mlp_dense_4h_to_h_q_scale4: R.Tensor((2560, 320), dtype="float16") = packed_params[288]
            gpt_neox_layers_17_mlp_dense_4h_to_h_bias4: R.Tensor((2560,), dtype="float32") = packed_params[289]
            gpt_neox_layers_18_input_layernorm_weight4: R.Tensor((2560,), dtype="float16") = packed_params[290]
            gpt_neox_layers_18_input_layernorm_bias4: R.Tensor((2560,), dtype="float16") = packed_params[291]
            gpt_neox_layers_18_post_attention_layernorm_weight4: R.Tensor((2560,), dtype="float16") = packed_params[292]
            gpt_neox_layers_18_post_attention_layernorm_bias4: R.Tensor((2560,), dtype="float16") = packed_params[293]
            gpt_neox_layers_18_attention_query_key_value_q_weight4: R.Tensor((7680, 320), dtype="uint32") = packed_params[294]
            gpt_neox_layers_18_attention_query_key_value_q_scale4: R.Tensor((7680, 80), dtype="float16") = packed_params[295]
            gpt_neox_layers_18_attention_query_key_value_bias4: R.Tensor((7680,), dtype="float16") = packed_params[296]
            gpt_neox_layers_18_attention_dense_q_weight4: R.Tensor((2560, 320), dtype="uint32") = packed_params[297]
            gpt_neox_layers_18_attention_dense_q_scale4: R.Tensor((2560, 80), dtype="float16") = packed_params[298]
            gpt_neox_layers_18_attention_dense_bias4: R.Tensor((2560,), dtype="float16") = packed_params[299]
            gpt_neox_layers_18_mlp_dense_h_to_4h_q_weight4: R.Tensor((10240, 320), dtype="uint32") = packed_params[300]
            gpt_neox_layers_18_mlp_dense_h_to_4h_q_scale4: R.Tensor((10240, 80), dtype="float16") = packed_params[301]
            gpt_neox_layers_18_mlp_dense_h_to_4h_bias4: R.Tensor((10240,), dtype="float32") = packed_params[302]
            gpt_neox_layers_18_mlp_dense_4h_to_h_q_weight4: R.Tensor((2560, 1280), dtype="uint32") = packed_params[303]
            gpt_neox_layers_18_mlp_dense_4h_to_h_q_scale4: R.Tensor((2560, 320), dtype="float16") = packed_params[304]
            gpt_neox_layers_18_mlp_dense_4h_to_h_bias4: R.Tensor((2560,), dtype="float32") = packed_params[305]
            gpt_neox_layers_19_input_layernorm_weight4: R.Tensor((2560,), dtype="float16") = packed_params[306]
            gpt_neox_layers_19_input_layernorm_bias4: R.Tensor((2560,), dtype="float16") = packed_params[307]
            gpt_neox_layers_19_post_attention_layernorm_weight4: R.Tensor((2560,), dtype="float16") = packed_params[308]
            gpt_neox_layers_19_post_attention_layernorm_bias4: R.Tensor((2560,), dtype="float16") = packed_params[309]
            gpt_neox_layers_19_attention_query_key_value_q_weight4: R.Tensor((7680, 320), dtype="uint32") = packed_params[310]
            gpt_neox_layers_19_attention_query_key_value_q_scale4: R.Tensor((7680, 80), dtype="float16") = packed_params[311]
            gpt_neox_layers_19_attention_query_key_value_bias4: R.Tensor((7680,), dtype="float16") = packed_params[312]
            gpt_neox_layers_19_attention_dense_q_weight4: R.Tensor((2560, 320), dtype="uint32") = packed_params[313]
            gpt_neox_layers_19_attention_dense_q_scale4: R.Tensor((2560, 80), dtype="float16") = packed_params[314]
            gpt_neox_layers_19_attention_dense_bias4: R.Tensor((2560,), dtype="float16") = packed_params[315]
            gpt_neox_layers_19_mlp_dense_h_to_4h_q_weight4: R.Tensor((10240, 320), dtype="uint32") = packed_params[316]
            gpt_neox_layers_19_mlp_dense_h_to_4h_q_scale4: R.Tensor((10240, 80), dtype="float16") = packed_params[317]
            gpt_neox_layers_19_mlp_dense_h_to_4h_bias4: R.Tensor((10240,), dtype="float32") = packed_params[318]
            gpt_neox_layers_19_mlp_dense_4h_to_h_q_weight4: R.Tensor((2560, 1280), dtype="uint32") = packed_params[319]
            gpt_neox_layers_19_mlp_dense_4h_to_h_q_scale4: R.Tensor((2560, 320), dtype="float16") = packed_params[320]
            gpt_neox_layers_19_mlp_dense_4h_to_h_bias4: R.Tensor((2560,), dtype="float32") = packed_params[321]
            gpt_neox_layers_20_input_layernorm_weight4: R.Tensor((2560,), dtype="float16") = packed_params[322]
            gpt_neox_layers_20_input_layernorm_bias4: R.Tensor((2560,), dtype="float16") = packed_params[323]
            gpt_neox_layers_20_post_attention_layernorm_weight4: R.Tensor((2560,), dtype="float16") = packed_params[324]
            gpt_neox_layers_20_post_attention_layernorm_bias4: R.Tensor((2560,), dtype="float16") = packed_params[325]
            gpt_neox_layers_20_attention_query_key_value_q_weight4: R.Tensor((7680, 320), dtype="uint32") = packed_params[326]
            gpt_neox_layers_20_attention_query_key_value_q_scale4: R.Tensor((7680, 80), dtype="float16") = packed_params[327]
            gpt_neox_layers_20_attention_query_key_value_bias4: R.Tensor((7680,), dtype="float16") = packed_params[328]
            gpt_neox_layers_20_attention_dense_q_weight4: R.Tensor((2560, 320), dtype="uint32") = packed_params[329]
            gpt_neox_layers_20_attention_dense_q_scale4: R.Tensor((2560, 80), dtype="float16") = packed_params[330]
            gpt_neox_layers_20_attention_dense_bias4: R.Tensor((2560,), dtype="float16") = packed_params[331]
            gpt_neox_layers_20_mlp_dense_h_to_4h_q_weight4: R.Tensor((10240, 320), dtype="uint32") = packed_params[332]
            gpt_neox_layers_20_mlp_dense_h_to_4h_q_scale4: R.Tensor((10240, 80), dtype="float16") = packed_params[333]
            gpt_neox_layers_20_mlp_dense_h_to_4h_bias4: R.Tensor((10240,), dtype="float32") = packed_params[334]
            gpt_neox_layers_20_mlp_dense_4h_to_h_q_weight4: R.Tensor((2560, 1280), dtype="uint32") = packed_params[335]
            gpt_neox_layers_20_mlp_dense_4h_to_h_q_scale4: R.Tensor((2560, 320), dtype="float16") = packed_params[336]
            gpt_neox_layers_20_mlp_dense_4h_to_h_bias4: R.Tensor((2560,), dtype="float32") = packed_params[337]
            gpt_neox_layers_21_input_layernorm_weight4: R.Tensor((2560,), dtype="float16") = packed_params[338]
            gpt_neox_layers_21_input_layernorm_bias4: R.Tensor((2560,), dtype="float16") = packed_params[339]
            gpt_neox_layers_21_post_attention_layernorm_weight4: R.Tensor((2560,), dtype="float16") = packed_params[340]
            gpt_neox_layers_21_post_attention_layernorm_bias4: R.Tensor((2560,), dtype="float16") = packed_params[341]
            gpt_neox_layers_21_attention_query_key_value_q_weight4: R.Tensor((7680, 320), dtype="uint32") = packed_params[342]
            gpt_neox_layers_21_attention_query_key_value_q_scale4: R.Tensor((7680, 80), dtype="float16") = packed_params[343]
            gpt_neox_layers_21_attention_query_key_value_bias4: R.Tensor((7680,), dtype="float16") = packed_params[344]
            gpt_neox_layers_21_attention_dense_q_weight4: R.Tensor((2560, 320), dtype="uint32") = packed_params[345]
            gpt_neox_layers_21_attention_dense_q_scale4: R.Tensor((2560, 80), dtype="float16") = packed_params[346]
            gpt_neox_layers_21_attention_dense_bias4: R.Tensor((2560,), dtype="float16") = packed_params[347]
            gpt_neox_layers_21_mlp_dense_h_to_4h_q_weight4: R.Tensor((10240, 320), dtype="uint32") = packed_params[348]
            gpt_neox_layers_21_mlp_dense_h_to_4h_q_scale4: R.Tensor((10240, 80), dtype="float16") = packed_params[349]
            gpt_neox_layers_21_mlp_dense_h_to_4h_bias4: R.Tensor((10240,), dtype="float32") = packed_params[350]
            gpt_neox_layers_21_mlp_dense_4h_to_h_q_weight4: R.Tensor((2560, 1280), dtype="uint32") = packed_params[351]
            gpt_neox_layers_21_mlp_dense_4h_to_h_q_scale4: R.Tensor((2560, 320), dtype="float16") = packed_params[352]
            gpt_neox_layers_21_mlp_dense_4h_to_h_bias4: R.Tensor((2560,), dtype="float32") = packed_params[353]
            gpt_neox_layers_22_input_layernorm_weight4: R.Tensor((2560,), dtype="float16") = packed_params[354]
            gpt_neox_layers_22_input_layernorm_bias4: R.Tensor((2560,), dtype="float16") = packed_params[355]
            gpt_neox_layers_22_post_attention_layernorm_weight4: R.Tensor((2560,), dtype="float16") = packed_params[356]
            gpt_neox_layers_22_post_attention_layernorm_bias4: R.Tensor((2560,), dtype="float16") = packed_params[357]
            gpt_neox_layers_22_attention_query_key_value_q_weight4: R.Tensor((7680, 320), dtype="uint32") = packed_params[358]
            gpt_neox_layers_22_attention_query_key_value_q_scale4: R.Tensor((7680, 80), dtype="float16") = packed_params[359]
            gpt_neox_layers_22_attention_query_key_value_bias4: R.Tensor((7680,), dtype="float16") = packed_params[360]
            gpt_neox_layers_22_attention_dense_q_weight4: R.Tensor((2560, 320), dtype="uint32") = packed_params[361]
            gpt_neox_layers_22_attention_dense_q_scale4: R.Tensor((2560, 80), dtype="float16") = packed_params[362]
            gpt_neox_layers_22_attention_dense_bias4: R.Tensor((2560,), dtype="float16") = packed_params[363]
            gpt_neox_layers_22_mlp_dense_h_to_4h_q_weight4: R.Tensor((10240, 320), dtype="uint32") = packed_params[364]
            gpt_neox_layers_22_mlp_dense_h_to_4h_q_scale4: R.Tensor((10240, 80), dtype="float16") = packed_params[365]
            gpt_neox_layers_22_mlp_dense_h_to_4h_bias4: R.Tensor((10240,), dtype="float32") = packed_params[366]
            gpt_neox_layers_22_mlp_dense_4h_to_h_q_weight4: R.Tensor((2560, 1280), dtype="uint32") = packed_params[367]
            gpt_neox_layers_22_mlp_dense_4h_to_h_q_scale4: R.Tensor((2560, 320), dtype="float16") = packed_params[368]
            gpt_neox_layers_22_mlp_dense_4h_to_h_bias4: R.Tensor((2560,), dtype="float32") = packed_params[369]
            gpt_neox_layers_23_input_layernorm_weight4: R.Tensor((2560,), dtype="float16") = packed_params[370]
            gpt_neox_layers_23_input_layernorm_bias4: R.Tensor((2560,), dtype="float16") = packed_params[371]
            gpt_neox_layers_23_post_attention_layernorm_weight4: R.Tensor((2560,), dtype="float16") = packed_params[372]
            gpt_neox_layers_23_post_attention_layernorm_bias4: R.Tensor((2560,), dtype="float16") = packed_params[373]
            gpt_neox_layers_23_attention_query_key_value_q_weight4: R.Tensor((7680, 320), dtype="uint32") = packed_params[374]
            gpt_neox_layers_23_attention_query_key_value_q_scale4: R.Tensor((7680, 80), dtype="float16") = packed_params[375]
            gpt_neox_layers_23_attention_query_key_value_bias4: R.Tensor((7680,), dtype="float16") = packed_params[376]
            gpt_neox_layers_23_attention_dense_q_weight4: R.Tensor((2560, 320), dtype="uint32") = packed_params[377]
            gpt_neox_layers_23_attention_dense_q_scale4: R.Tensor((2560, 80), dtype="float16") = packed_params[378]
            gpt_neox_layers_23_attention_dense_bias4: R.Tensor((2560,), dtype="float16") = packed_params[379]
            gpt_neox_layers_23_mlp_dense_h_to_4h_q_weight4: R.Tensor((10240, 320), dtype="uint32") = packed_params[380]
            gpt_neox_layers_23_mlp_dense_h_to_4h_q_scale4: R.Tensor((10240, 80), dtype="float16") = packed_params[381]
            gpt_neox_layers_23_mlp_dense_h_to_4h_bias4: R.Tensor((10240,), dtype="float32") = packed_params[382]
            gpt_neox_layers_23_mlp_dense_4h_to_h_q_weight4: R.Tensor((2560, 1280), dtype="uint32") = packed_params[383]
            gpt_neox_layers_23_mlp_dense_4h_to_h_q_scale4: R.Tensor((2560, 320), dtype="float16") = packed_params[384]
            gpt_neox_layers_23_mlp_dense_4h_to_h_bias4: R.Tensor((2560,), dtype="float32") = packed_params[385]
            gpt_neox_layers_24_input_layernorm_weight4: R.Tensor((2560,), dtype="float16") = packed_params[386]
            gpt_neox_layers_24_input_layernorm_bias4: R.Tensor((2560,), dtype="float16") = packed_params[387]
            gpt_neox_layers_24_post_attention_layernorm_weight4: R.Tensor((2560,), dtype="float16") = packed_params[388]
            gpt_neox_layers_24_post_attention_layernorm_bias4: R.Tensor((2560,), dtype="float16") = packed_params[389]
            gpt_neox_layers_24_attention_query_key_value_q_weight4: R.Tensor((7680, 320), dtype="uint32") = packed_params[390]
            gpt_neox_layers_24_attention_query_key_value_q_scale4: R.Tensor((7680, 80), dtype="float16") = packed_params[391]
            gpt_neox_layers_24_attention_query_key_value_bias4: R.Tensor((7680,), dtype="float16") = packed_params[392]
            gpt_neox_layers_24_attention_dense_q_weight4: R.Tensor((2560, 320), dtype="uint32") = packed_params[393]
            gpt_neox_layers_24_attention_dense_q_scale4: R.Tensor((2560, 80), dtype="float16") = packed_params[394]
            gpt_neox_layers_24_attention_dense_bias4: R.Tensor((2560,), dtype="float16") = packed_params[395]
            gpt_neox_layers_24_mlp_dense_h_to_4h_q_weight4: R.Tensor((10240, 320), dtype="uint32") = packed_params[396]
            gpt_neox_layers_24_mlp_dense_h_to_4h_q_scale4: R.Tensor((10240, 80), dtype="float16") = packed_params[397]
            gpt_neox_layers_24_mlp_dense_h_to_4h_bias4: R.Tensor((10240,), dtype="float32") = packed_params[398]
            gpt_neox_layers_24_mlp_dense_4h_to_h_q_weight4: R.Tensor((2560, 1280), dtype="uint32") = packed_params[399]
            gpt_neox_layers_24_mlp_dense_4h_to_h_q_scale4: R.Tensor((2560, 320), dtype="float16") = packed_params[400]
            gpt_neox_layers_24_mlp_dense_4h_to_h_bias4: R.Tensor((2560,), dtype="float32") = packed_params[401]
            gpt_neox_layers_25_input_layernorm_weight4: R.Tensor((2560,), dtype="float16") = packed_params[402]
            gpt_neox_layers_25_input_layernorm_bias4: R.Tensor((2560,), dtype="float16") = packed_params[403]
            gpt_neox_layers_25_post_attention_layernorm_weight4: R.Tensor((2560,), dtype="float16") = packed_params[404]
            gpt_neox_layers_25_post_attention_layernorm_bias4: R.Tensor((2560,), dtype="float16") = packed_params[405]
            gpt_neox_layers_25_attention_query_key_value_q_weight4: R.Tensor((7680, 320), dtype="uint32") = packed_params[406]
            gpt_neox_layers_25_attention_query_key_value_q_scale4: R.Tensor((7680, 80), dtype="float16") = packed_params[407]
            gpt_neox_layers_25_attention_query_key_value_bias4: R.Tensor((7680,), dtype="float16") = packed_params[408]
            gpt_neox_layers_25_attention_dense_q_weight4: R.Tensor((2560, 320), dtype="uint32") = packed_params[409]
            gpt_neox_layers_25_attention_dense_q_scale4: R.Tensor((2560, 80), dtype="float16") = packed_params[410]
            gpt_neox_layers_25_attention_dense_bias4: R.Tensor((2560,), dtype="float16") = packed_params[411]
            gpt_neox_layers_25_mlp_dense_h_to_4h_q_weight4: R.Tensor((10240, 320), dtype="uint32") = packed_params[412]
            gpt_neox_layers_25_mlp_dense_h_to_4h_q_scale4: R.Tensor((10240, 80), dtype="float16") = packed_params[413]
            gpt_neox_layers_25_mlp_dense_h_to_4h_bias4: R.Tensor((10240,), dtype="float32") = packed_params[414]
            gpt_neox_layers_25_mlp_dense_4h_to_h_q_weight4: R.Tensor((2560, 1280), dtype="uint32") = packed_params[415]
            gpt_neox_layers_25_mlp_dense_4h_to_h_q_scale4: R.Tensor((2560, 320), dtype="float16") = packed_params[416]
            gpt_neox_layers_25_mlp_dense_4h_to_h_bias4: R.Tensor((2560,), dtype="float32") = packed_params[417]
            gpt_neox_layers_26_input_layernorm_weight4: R.Tensor((2560,), dtype="float16") = packed_params[418]
            gpt_neox_layers_26_input_layernorm_bias4: R.Tensor((2560,), dtype="float16") = packed_params[419]
            gpt_neox_layers_26_post_attention_layernorm_weight4: R.Tensor((2560,), dtype="float16") = packed_params[420]
            gpt_neox_layers_26_post_attention_layernorm_bias4: R.Tensor((2560,), dtype="float16") = packed_params[421]
            gpt_neox_layers_26_attention_query_key_value_q_weight4: R.Tensor((7680, 320), dtype="uint32") = packed_params[422]
            gpt_neox_layers_26_attention_query_key_value_q_scale4: R.Tensor((7680, 80), dtype="float16") = packed_params[423]
            gpt_neox_layers_26_attention_query_key_value_bias4: R.Tensor((7680,), dtype="float16") = packed_params[424]
            gpt_neox_layers_26_attention_dense_q_weight4: R.Tensor((2560, 320), dtype="uint32") = packed_params[425]
            gpt_neox_layers_26_attention_dense_q_scale4: R.Tensor((2560, 80), dtype="float16") = packed_params[426]
            gpt_neox_layers_26_attention_dense_bias4: R.Tensor((2560,), dtype="float16") = packed_params[427]
            gpt_neox_layers_26_mlp_dense_h_to_4h_q_weight4: R.Tensor((10240, 320), dtype="uint32") = packed_params[428]
            gpt_neox_layers_26_mlp_dense_h_to_4h_q_scale4: R.Tensor((10240, 80), dtype="float16") = packed_params[429]
            gpt_neox_layers_26_mlp_dense_h_to_4h_bias4: R.Tensor((10240,), dtype="float32") = packed_params[430]
            gpt_neox_layers_26_mlp_dense_4h_to_h_q_weight4: R.Tensor((2560, 1280), dtype="uint32") = packed_params[431]
            gpt_neox_layers_26_mlp_dense_4h_to_h_q_scale4: R.Tensor((2560, 320), dtype="float16") = packed_params[432]
            gpt_neox_layers_26_mlp_dense_4h_to_h_bias4: R.Tensor((2560,), dtype="float32") = packed_params[433]
            gpt_neox_layers_27_input_layernorm_weight4: R.Tensor((2560,), dtype="float16") = packed_params[434]
            gpt_neox_layers_27_input_layernorm_bias4: R.Tensor((2560,), dtype="float16") = packed_params[435]
            gpt_neox_layers_27_post_attention_layernorm_weight4: R.Tensor((2560,), dtype="float16") = packed_params[436]
            gpt_neox_layers_27_post_attention_layernorm_bias4: R.Tensor((2560,), dtype="float16") = packed_params[437]
            gpt_neox_layers_27_attention_query_key_value_q_weight4: R.Tensor((7680, 320), dtype="uint32") = packed_params[438]
            gpt_neox_layers_27_attention_query_key_value_q_scale4: R.Tensor((7680, 80), dtype="float16") = packed_params[439]
            gpt_neox_layers_27_attention_query_key_value_bias4: R.Tensor((7680,), dtype="float16") = packed_params[440]
            gpt_neox_layers_27_attention_dense_q_weight4: R.Tensor((2560, 320), dtype="uint32") = packed_params[441]
            gpt_neox_layers_27_attention_dense_q_scale4: R.Tensor((2560, 80), dtype="float16") = packed_params[442]
            gpt_neox_layers_27_attention_dense_bias4: R.Tensor((2560,), dtype="float16") = packed_params[443]
            gpt_neox_layers_27_mlp_dense_h_to_4h_q_weight4: R.Tensor((10240, 320), dtype="uint32") = packed_params[444]
            gpt_neox_layers_27_mlp_dense_h_to_4h_q_scale4: R.Tensor((10240, 80), dtype="float16") = packed_params[445]
            gpt_neox_layers_27_mlp_dense_h_to_4h_bias4: R.Tensor((10240,), dtype="float32") = packed_params[446]
            gpt_neox_layers_27_mlp_dense_4h_to_h_q_weight4: R.Tensor((2560, 1280), dtype="uint32") = packed_params[447]
            gpt_neox_layers_27_mlp_dense_4h_to_h_q_scale4: R.Tensor((2560, 320), dtype="float16") = packed_params[448]
            gpt_neox_layers_27_mlp_dense_4h_to_h_bias4: R.Tensor((2560,), dtype="float32") = packed_params[449]
            gpt_neox_layers_28_input_layernorm_weight4: R.Tensor((2560,), dtype="float16") = packed_params[450]
            gpt_neox_layers_28_input_layernorm_bias4: R.Tensor((2560,), dtype="float16") = packed_params[451]
            gpt_neox_layers_28_post_attention_layernorm_weight4: R.Tensor((2560,), dtype="float16") = packed_params[452]
            gpt_neox_layers_28_post_attention_layernorm_bias4: R.Tensor((2560,), dtype="float16") = packed_params[453]
            gpt_neox_layers_28_attention_query_key_value_q_weight4: R.Tensor((7680, 320), dtype="uint32") = packed_params[454]
            gpt_neox_layers_28_attention_query_key_value_q_scale4: R.Tensor((7680, 80), dtype="float16") = packed_params[455]
            gpt_neox_layers_28_attention_query_key_value_bias4: R.Tensor((7680,), dtype="float16") = packed_params[456]
            gpt_neox_layers_28_attention_dense_q_weight4: R.Tensor((2560, 320), dtype="uint32") = packed_params[457]
            gpt_neox_layers_28_attention_dense_q_scale4: R.Tensor((2560, 80), dtype="float16") = packed_params[458]
            gpt_neox_layers_28_attention_dense_bias4: R.Tensor((2560,), dtype="float16") = packed_params[459]
            gpt_neox_layers_28_mlp_dense_h_to_4h_q_weight4: R.Tensor((10240, 320), dtype="uint32") = packed_params[460]
            gpt_neox_layers_28_mlp_dense_h_to_4h_q_scale4: R.Tensor((10240, 80), dtype="float16") = packed_params[461]
            gpt_neox_layers_28_mlp_dense_h_to_4h_bias4: R.Tensor((10240,), dtype="float32") = packed_params[462]
            gpt_neox_layers_28_mlp_dense_4h_to_h_q_weight4: R.Tensor((2560, 1280), dtype="uint32") = packed_params[463]
            gpt_neox_layers_28_mlp_dense_4h_to_h_q_scale4: R.Tensor((2560, 320), dtype="float16") = packed_params[464]
            gpt_neox_layers_28_mlp_dense_4h_to_h_bias4: R.Tensor((2560,), dtype="float32") = packed_params[465]
            gpt_neox_layers_29_input_layernorm_weight4: R.Tensor((2560,), dtype="float16") = packed_params[466]
            gpt_neox_layers_29_input_layernorm_bias4: R.Tensor((2560,), dtype="float16") = packed_params[467]
            gpt_neox_layers_29_post_attention_layernorm_weight4: R.Tensor((2560,), dtype="float16") = packed_params[468]
            gpt_neox_layers_29_post_attention_layernorm_bias4: R.Tensor((2560,), dtype="float16") = packed_params[469]
            gpt_neox_layers_29_attention_query_key_value_q_weight4: R.Tensor((7680, 320), dtype="uint32") = packed_params[470]
            gpt_neox_layers_29_attention_query_key_value_q_scale4: R.Tensor((7680, 80), dtype="float16") = packed_params[471]
            gpt_neox_layers_29_attention_query_key_value_bias4: R.Tensor((7680,), dtype="float16") = packed_params[472]
            gpt_neox_layers_29_attention_dense_q_weight4: R.Tensor((2560, 320), dtype="uint32") = packed_params[473]
            gpt_neox_layers_29_attention_dense_q_scale4: R.Tensor((2560, 80), dtype="float16") = packed_params[474]
            gpt_neox_layers_29_attention_dense_bias4: R.Tensor((2560,), dtype="float16") = packed_params[475]
            gpt_neox_layers_29_mlp_dense_h_to_4h_q_weight4: R.Tensor((10240, 320), dtype="uint32") = packed_params[476]
            gpt_neox_layers_29_mlp_dense_h_to_4h_q_scale4: R.Tensor((10240, 80), dtype="float16") = packed_params[477]
            gpt_neox_layers_29_mlp_dense_h_to_4h_bias4: R.Tensor((10240,), dtype="float32") = packed_params[478]
            gpt_neox_layers_29_mlp_dense_4h_to_h_q_weight4: R.Tensor((2560, 1280), dtype="uint32") = packed_params[479]
            gpt_neox_layers_29_mlp_dense_4h_to_h_q_scale4: R.Tensor((2560, 320), dtype="float16") = packed_params[480]
            gpt_neox_layers_29_mlp_dense_4h_to_h_bias4: R.Tensor((2560,), dtype="float32") = packed_params[481]
            gpt_neox_layers_30_input_layernorm_weight4: R.Tensor((2560,), dtype="float16") = packed_params[482]
            gpt_neox_layers_30_input_layernorm_bias4: R.Tensor((2560,), dtype="float16") = packed_params[483]
            gpt_neox_layers_30_post_attention_layernorm_weight4: R.Tensor((2560,), dtype="float16") = packed_params[484]
            gpt_neox_layers_30_post_attention_layernorm_bias4: R.Tensor((2560,), dtype="float16") = packed_params[485]
            gpt_neox_layers_30_attention_query_key_value_q_weight4: R.Tensor((7680, 320), dtype="uint32") = packed_params[486]
            gpt_neox_layers_30_attention_query_key_value_q_scale4: R.Tensor((7680, 80), dtype="float16") = packed_params[487]
            gpt_neox_layers_30_attention_query_key_value_bias4: R.Tensor((7680,), dtype="float16") = packed_params[488]
            gpt_neox_layers_30_attention_dense_q_weight4: R.Tensor((2560, 320), dtype="uint32") = packed_params[489]
            gpt_neox_layers_30_attention_dense_q_scale4: R.Tensor((2560, 80), dtype="float16") = packed_params[490]
            gpt_neox_layers_30_attention_dense_bias4: R.Tensor((2560,), dtype="float16") = packed_params[491]
            gpt_neox_layers_30_mlp_dense_h_to_4h_q_weight4: R.Tensor((10240, 320), dtype="uint32") = packed_params[492]
            gpt_neox_layers_30_mlp_dense_h_to_4h_q_scale4: R.Tensor((10240, 80), dtype="float16") = packed_params[493]
            gpt_neox_layers_30_mlp_dense_h_to_4h_bias4: R.Tensor((10240,), dtype="float32") = packed_params[494]
            gpt_neox_layers_30_mlp_dense_4h_to_h_q_weight4: R.Tensor((2560, 1280), dtype="uint32") = packed_params[495]
            gpt_neox_layers_30_mlp_dense_4h_to_h_q_scale4: R.Tensor((2560, 320), dtype="float16") = packed_params[496]
            gpt_neox_layers_30_mlp_dense_4h_to_h_bias4: R.Tensor((2560,), dtype="float32") = packed_params[497]
            gpt_neox_layers_31_input_layernorm_weight4: R.Tensor((2560,), dtype="float16") = packed_params[498]
            gpt_neox_layers_31_input_layernorm_bias4: R.Tensor((2560,), dtype="float16") = packed_params[499]
            gpt_neox_layers_31_post_attention_layernorm_weight4: R.Tensor((2560,), dtype="float16") = packed_params[500]
            gpt_neox_layers_31_post_attention_layernorm_bias4: R.Tensor((2560,), dtype="float16") = packed_params[501]
            gpt_neox_layers_31_attention_query_key_value_q_weight4: R.Tensor((7680, 320), dtype="uint32") = packed_params[502]
            gpt_neox_layers_31_attention_query_key_value_q_scale4: R.Tensor((7680, 80), dtype="float16") = packed_params[503]
            gpt_neox_layers_31_attention_query_key_value_bias4: R.Tensor((7680,), dtype="float16") = packed_params[504]
            gpt_neox_layers_31_attention_dense_q_weight4: R.Tensor((2560, 320), dtype="uint32") = packed_params[505]
            gpt_neox_layers_31_attention_dense_q_scale4: R.Tensor((2560, 80), dtype="float16") = packed_params[506]
            gpt_neox_layers_31_attention_dense_bias4: R.Tensor((2560,), dtype="float16") = packed_params[507]
            gpt_neox_layers_31_mlp_dense_h_to_4h_q_weight4: R.Tensor((10240, 320), dtype="uint32") = packed_params[508]
            gpt_neox_layers_31_mlp_dense_h_to_4h_q_scale4: R.Tensor((10240, 80), dtype="float16") = packed_params[509]
            gpt_neox_layers_31_mlp_dense_h_to_4h_bias4: R.Tensor((10240,), dtype="float32") = packed_params[510]
            gpt_neox_layers_31_mlp_dense_4h_to_h_q_weight4: R.Tensor((2560, 1280), dtype="uint32") = packed_params[511]
            gpt_neox_layers_31_mlp_dense_4h_to_h_q_scale4: R.Tensor((2560, 320), dtype="float16") = packed_params[512]
            gpt_neox_layers_31_mlp_dense_4h_to_h_bias4: R.Tensor((2560,), dtype="float32") = packed_params[513]
            gpt_neox_final_layer_norm_weight4: R.Tensor((2560,), dtype="float16") = packed_params[514]
            gpt_neox_final_layer_norm_bias4: R.Tensor((2560,), dtype="float16") = packed_params[515]
            embed_out_q_weight4: R.Tensor((vocab_size, 320), dtype="uint32") = packed_params[516]
            embed_out_q_scale4: R.Tensor((vocab_size, 80), dtype="float16") = packed_params[517]
            layer_norm195 = R.call_tir(cls.layer_norm, (input_embeds, gpt_neox_layers_0_input_layernorm_weight4, gpt_neox_layers_0_input_layernorm_bias4), out_sinfo=R.Tensor((batch_size, 1, 2560), dtype="float16"))
            lv = R.call_tir(cls.fused_dequantize1_fused_NT_matmul_add, (gpt_neox_layers_0_attention_query_key_value_q_weight4, gpt_neox_layers_0_attention_query_key_value_q_scale4, layer_norm195, gpt_neox_layers_0_attention_query_key_value_bias4), out_sinfo=R.Tensor((batch_size, 1, 7680), dtype="float16"))
            reshape384 = R.call_tir(cls.reshape, (lv,), out_sinfo=R.Tensor((batch_size, 1, 96, 80), dtype="float16"))
            reshape385 = R.call_tir(cls.reshape1, (reshape384,), out_sinfo=R.Tensor((batch_size, 96, 80), dtype="float16"))
            lv486 = R.call_dps_packed("vm.builtin.attention_kv_cache_attention_with_fused_qkv", (paged_kv_cache, R.prim_value(0), R.prim_value(T.float32(1.0)), reshape385), out_sinfo=R.Tensor((batch_size, 32, 80), dtype="float16"))
            reshape386 = R.call_tir(cls.reshape2, (lv486,), out_sinfo=R.Tensor((batch_size, 1, 32, 80), dtype="float16"))
            reshape387 = R.call_tir(cls.reshape3, (reshape386,), out_sinfo=R.Tensor((batch_size, 1, 2560), dtype="float16"))
            lv_1 = R.call_tir(cls.fused_dequantize2_fused_NT_matmul1_add1_add2, (gpt_neox_layers_0_attention_dense_q_weight4, gpt_neox_layers_0_attention_dense_q_scale4, reshape387, gpt_neox_layers_0_attention_dense_bias4, input_embeds), out_sinfo=R.Tensor((batch_size, 1, 2560), dtype="float16"))
            layer_norm196 = R.call_tir(cls.layer_norm, (lv_1, gpt_neox_layers_0_post_attention_layernorm_weight4, gpt_neox_layers_0_post_attention_layernorm_bias4), out_sinfo=R.Tensor((batch_size, 1, 2560), dtype="float16"))
            lv1 = R.call_tir(cls.fused_dequantize3_fused_NT_matmul2_add3_gelu_cast, (gpt_neox_layers_0_mlp_dense_h_to_4h_q_weight4, gpt_neox_layers_0_mlp_dense_h_to_4h_q_scale4, layer_norm196, gpt_neox_layers_0_mlp_dense_h_to_4h_bias4), out_sinfo=R.Tensor((batch_size, 1, 10240), dtype="float16"))
            lv1_1 = R.call_tir(cls.fused_dequantize4_fused_NT_matmul3_add4_cast1_add2, (gpt_neox_layers_0_mlp_dense_4h_to_h_q_weight4, gpt_neox_layers_0_mlp_dense_4h_to_h_q_scale4, lv1, gpt_neox_layers_0_mlp_dense_4h_to_h_bias4, lv_1), out_sinfo=R.Tensor((batch_size, 1, 2560), dtype="float16"))
            layer_norm197 = R.call_tir(cls.layer_norm, (lv1_1, gpt_neox_layers_1_input_layernorm_weight4, gpt_neox_layers_1_input_layernorm_bias4), out_sinfo=R.Tensor((batch_size, 1, 2560), dtype="float16"))
            lv2 = R.call_tir(cls.fused_dequantize1_fused_NT_matmul_add, (gpt_neox_layers_1_attention_query_key_value_q_weight4, gpt_neox_layers_1_attention_query_key_value_q_scale4, layer_norm197, gpt_neox_layers_1_attention_query_key_value_bias4), out_sinfo=R.Tensor((batch_size, 1, 7680), dtype="float16"))
            reshape388 = R.call_tir(cls.reshape, (lv2,), out_sinfo=R.Tensor((batch_size, 1, 96, 80), dtype="float16"))
            reshape389 = R.call_tir(cls.reshape1, (reshape388,), out_sinfo=R.Tensor((batch_size, 96, 80), dtype="float16"))
            lv491 = R.call_dps_packed("vm.builtin.attention_kv_cache_attention_with_fused_qkv", (paged_kv_cache, R.prim_value(1), R.prim_value(T.float32(1.0)), reshape389), out_sinfo=R.Tensor((batch_size, 32, 80), dtype="float16"))
            reshape390 = R.call_tir(cls.reshape2, (lv491,), out_sinfo=R.Tensor((batch_size, 1, 32, 80), dtype="float16"))
            reshape391 = R.call_tir(cls.reshape3, (reshape390,), out_sinfo=R.Tensor((batch_size, 1, 2560), dtype="float16"))
            lv2_1 = R.call_tir(cls.fused_dequantize2_fused_NT_matmul1_add1_add2, (gpt_neox_layers_1_attention_dense_q_weight4, gpt_neox_layers_1_attention_dense_q_scale4, reshape391, gpt_neox_layers_1_attention_dense_bias4, lv1_1), out_sinfo=R.Tensor((batch_size, 1, 2560), dtype="float16"))
            layer_norm198 = R.call_tir(cls.layer_norm, (lv2_1, gpt_neox_layers_1_post_attention_layernorm_weight4, gpt_neox_layers_1_post_attention_layernorm_bias4), out_sinfo=R.Tensor((batch_size, 1, 2560), dtype="float16"))
            lv3 = R.call_tir(cls.fused_dequantize3_fused_NT_matmul2_add3_gelu_cast, (gpt_neox_layers_1_mlp_dense_h_to_4h_q_weight4, gpt_neox_layers_1_mlp_dense_h_to_4h_q_scale4, layer_norm198, gpt_neox_layers_1_mlp_dense_h_to_4h_bias4), out_sinfo=R.Tensor((batch_size, 1, 10240), dtype="float16"))
            lv3_1 = R.call_tir(cls.fused_dequantize4_fused_NT_matmul3_add4_cast1_add2, (gpt_neox_layers_1_mlp_dense_4h_to_h_q_weight4, gpt_neox_layers_1_mlp_dense_4h_to_h_q_scale4, lv3, gpt_neox_layers_1_mlp_dense_4h_to_h_bias4, lv2_1), out_sinfo=R.Tensor((batch_size, 1, 2560), dtype="float16"))
            layer_norm199 = R.call_tir(cls.layer_norm, (lv3_1, gpt_neox_layers_2_input_layernorm_weight4, gpt_neox_layers_2_input_layernorm_bias4), out_sinfo=R.Tensor((batch_size, 1, 2560), dtype="float16"))
            lv4 = R.call_tir(cls.fused_dequantize1_fused_NT_matmul_add, (gpt_neox_layers_2_attention_query_key_value_q_weight4, gpt_neox_layers_2_attention_query_key_value_q_scale4, layer_norm199, gpt_neox_layers_2_attention_query_key_value_bias4), out_sinfo=R.Tensor((batch_size, 1, 7680), dtype="float16"))
            reshape392 = R.call_tir(cls.reshape, (lv4,), out_sinfo=R.Tensor((batch_size, 1, 96, 80), dtype="float16"))
            reshape393 = R.call_tir(cls.reshape1, (reshape392,), out_sinfo=R.Tensor((batch_size, 96, 80), dtype="float16"))
            lv496 = R.call_dps_packed("vm.builtin.attention_kv_cache_attention_with_fused_qkv", (paged_kv_cache, R.prim_value(2), R.prim_value(T.float32(1.0)), reshape393), out_sinfo=R.Tensor((batch_size, 32, 80), dtype="float16"))
            reshape394 = R.call_tir(cls.reshape2, (lv496,), out_sinfo=R.Tensor((batch_size, 1, 32, 80), dtype="float16"))
            reshape395 = R.call_tir(cls.reshape3, (reshape394,), out_sinfo=R.Tensor((batch_size, 1, 2560), dtype="float16"))
            lv4_1 = R.call_tir(cls.fused_dequantize2_fused_NT_matmul1_add1_add2, (gpt_neox_layers_2_attention_dense_q_weight4, gpt_neox_layers_2_attention_dense_q_scale4, reshape395, gpt_neox_layers_2_attention_dense_bias4, lv3_1), out_sinfo=R.Tensor((batch_size, 1, 2560), dtype="float16"))
            layer_norm200 = R.call_tir(cls.layer_norm, (lv4_1, gpt_neox_layers_2_post_attention_layernorm_weight4, gpt_neox_layers_2_post_attention_layernorm_bias4), out_sinfo=R.Tensor((batch_size, 1, 2560), dtype="float16"))
            lv5 = R.call_tir(cls.fused_dequantize3_fused_NT_matmul2_add3_gelu_cast, (gpt_neox_layers_2_mlp_dense_h_to_4h_q_weight4, gpt_neox_layers_2_mlp_dense_h_to_4h_q_scale4, layer_norm200, gpt_neox_layers_2_mlp_dense_h_to_4h_bias4), out_sinfo=R.Tensor((batch_size, 1, 10240), dtype="float16"))
            lv5_1 = R.call_tir(cls.fused_dequantize4_fused_NT_matmul3_add4_cast1_add2, (gpt_neox_layers_2_mlp_dense_4h_to_h_q_weight4, gpt_neox_layers_2_mlp_dense_4h_to_h_q_scale4, lv5, gpt_neox_layers_2_mlp_dense_4h_to_h_bias4, lv4_1), out_sinfo=R.Tensor((batch_size, 1, 2560), dtype="float16"))
            layer_norm201 = R.call_tir(cls.layer_norm, (lv5_1, gpt_neox_layers_3_input_layernorm_weight4, gpt_neox_layers_3_input_layernorm_bias4), out_sinfo=R.Tensor((batch_size, 1, 2560), dtype="float16"))
            lv6 = R.call_tir(cls.fused_dequantize1_fused_NT_matmul_add, (gpt_neox_layers_3_attention_query_key_value_q_weight4, gpt_neox_layers_3_attention_query_key_value_q_scale4, layer_norm201, gpt_neox_layers_3_attention_query_key_value_bias4), out_sinfo=R.Tensor((batch_size, 1, 7680), dtype="float16"))
            reshape396 = R.call_tir(cls.reshape, (lv6,), out_sinfo=R.Tensor((batch_size, 1, 96, 80), dtype="float16"))
            reshape397 = R.call_tir(cls.reshape1, (reshape396,), out_sinfo=R.Tensor((batch_size, 96, 80), dtype="float16"))
            lv501 = R.call_dps_packed("vm.builtin.attention_kv_cache_attention_with_fused_qkv", (paged_kv_cache, R.prim_value(3), R.prim_value(T.float32(1.0)), reshape397), out_sinfo=R.Tensor((batch_size, 32, 80), dtype="float16"))
            reshape398 = R.call_tir(cls.reshape2, (lv501,), out_sinfo=R.Tensor((batch_size, 1, 32, 80), dtype="float16"))
            reshape399 = R.call_tir(cls.reshape3, (reshape398,), out_sinfo=R.Tensor((batch_size, 1, 2560), dtype="float16"))
            lv6_1 = R.call_tir(cls.fused_dequantize2_fused_NT_matmul1_add1_add2, (gpt_neox_layers_3_attention_dense_q_weight4, gpt_neox_layers_3_attention_dense_q_scale4, reshape399, gpt_neox_layers_3_attention_dense_bias4, lv5_1), out_sinfo=R.Tensor((batch_size, 1, 2560), dtype="float16"))
            layer_norm202 = R.call_tir(cls.layer_norm, (lv6_1, gpt_neox_layers_3_post_attention_layernorm_weight4, gpt_neox_layers_3_post_attention_layernorm_bias4), out_sinfo=R.Tensor((batch_size, 1, 2560), dtype="float16"))
            lv7 = R.call_tir(cls.fused_dequantize3_fused_NT_matmul2_add3_gelu_cast, (gpt_neox_layers_3_mlp_dense_h_to_4h_q_weight4, gpt_neox_layers_3_mlp_dense_h_to_4h_q_scale4, layer_norm202, gpt_neox_layers_3_mlp_dense_h_to_4h_bias4), out_sinfo=R.Tensor((batch_size, 1, 10240), dtype="float16"))
            lv7_1 = R.call_tir(cls.fused_dequantize4_fused_NT_matmul3_add4_cast1_add2, (gpt_neox_layers_3_mlp_dense_4h_to_h_q_weight4, gpt_neox_layers_3_mlp_dense_4h_to_h_q_scale4, lv7, gpt_neox_layers_3_mlp_dense_4h_to_h_bias4, lv6_1), out_sinfo=R.Tensor((batch_size, 1, 2560), dtype="float16"))
            layer_norm203 = R.call_tir(cls.layer_norm, (lv7_1, gpt_neox_layers_4_input_layernorm_weight4, gpt_neox_layers_4_input_layernorm_bias4), out_sinfo=R.Tensor((batch_size, 1, 2560), dtype="float16"))
            lv8 = R.call_tir(cls.fused_dequantize1_fused_NT_matmul_add, (gpt_neox_layers_4_attention_query_key_value_q_weight4, gpt_neox_layers_4_attention_query_key_value_q_scale4, layer_norm203, gpt_neox_layers_4_attention_query_key_value_bias4), out_sinfo=R.Tensor((batch_size, 1, 7680), dtype="float16"))
            reshape400 = R.call_tir(cls.reshape, (lv8,), out_sinfo=R.Tensor((batch_size, 1, 96, 80), dtype="float16"))
            reshape401 = R.call_tir(cls.reshape1, (reshape400,), out_sinfo=R.Tensor((batch_size, 96, 80), dtype="float16"))
            lv506 = R.call_dps_packed("vm.builtin.attention_kv_cache_attention_with_fused_qkv", (paged_kv_cache, R.prim_value(4), R.prim_value(T.float32(1.0)), reshape401), out_sinfo=R.Tensor((batch_size, 32, 80), dtype="float16"))
            reshape402 = R.call_tir(cls.reshape2, (lv506,), out_sinfo=R.Tensor((batch_size, 1, 32, 80), dtype="float16"))
            reshape403 = R.call_tir(cls.reshape3, (reshape402,), out_sinfo=R.Tensor((batch_size, 1, 2560), dtype="float16"))
            lv8_1 = R.call_tir(cls.fused_dequantize2_fused_NT_matmul1_add1_add2, (gpt_neox_layers_4_attention_dense_q_weight4, gpt_neox_layers_4_attention_dense_q_scale4, reshape403, gpt_neox_layers_4_attention_dense_bias4, lv7_1), out_sinfo=R.Tensor((batch_size, 1, 2560), dtype="float16"))
            layer_norm204 = R.call_tir(cls.layer_norm, (lv8_1, gpt_neox_layers_4_post_attention_layernorm_weight4, gpt_neox_layers_4_post_attention_layernorm_bias4), out_sinfo=R.Tensor((batch_size, 1, 2560), dtype="float16"))
            lv9 = R.call_tir(cls.fused_dequantize3_fused_NT_matmul2_add3_gelu_cast, (gpt_neox_layers_4_mlp_dense_h_to_4h_q_weight4, gpt_neox_layers_4_mlp_dense_h_to_4h_q_scale4, layer_norm204, gpt_neox_layers_4_mlp_dense_h_to_4h_bias4), out_sinfo=R.Tensor((batch_size, 1, 10240), dtype="float16"))
            lv9_1 = R.call_tir(cls.fused_dequantize4_fused_NT_matmul3_add4_cast1_add2, (gpt_neox_layers_4_mlp_dense_4h_to_h_q_weight4, gpt_neox_layers_4_mlp_dense_4h_to_h_q_scale4, lv9, gpt_neox_layers_4_mlp_dense_4h_to_h_bias4, lv8_1), out_sinfo=R.Tensor((batch_size, 1, 2560), dtype="float16"))
            layer_norm205 = R.call_tir(cls.layer_norm, (lv9_1, gpt_neox_layers_5_input_layernorm_weight4, gpt_neox_layers_5_input_layernorm_bias4), out_sinfo=R.Tensor((batch_size, 1, 2560), dtype="float16"))
            lv10 = R.call_tir(cls.fused_dequantize1_fused_NT_matmul_add, (gpt_neox_layers_5_attention_query_key_value_q_weight4, gpt_neox_layers_5_attention_query_key_value_q_scale4, layer_norm205, gpt_neox_layers_5_attention_query_key_value_bias4), out_sinfo=R.Tensor((batch_size, 1, 7680), dtype="float16"))
            reshape404 = R.call_tir(cls.reshape, (lv10,), out_sinfo=R.Tensor((batch_size, 1, 96, 80), dtype="float16"))
            reshape405 = R.call_tir(cls.reshape1, (reshape404,), out_sinfo=R.Tensor((batch_size, 96, 80), dtype="float16"))
            lv511 = R.call_dps_packed("vm.builtin.attention_kv_cache_attention_with_fused_qkv", (paged_kv_cache, R.prim_value(5), R.prim_value(T.float32(1.0)), reshape405), out_sinfo=R.Tensor((batch_size, 32, 80), dtype="float16"))
            reshape406 = R.call_tir(cls.reshape2, (lv511,), out_sinfo=R.Tensor((batch_size, 1, 32, 80), dtype="float16"))
            reshape407 = R.call_tir(cls.reshape3, (reshape406,), out_sinfo=R.Tensor((batch_size, 1, 2560), dtype="float16"))
            lv10_1 = R.call_tir(cls.fused_dequantize2_fused_NT_matmul1_add1_add2, (gpt_neox_layers_5_attention_dense_q_weight4, gpt_neox_layers_5_attention_dense_q_scale4, reshape407, gpt_neox_layers_5_attention_dense_bias4, lv9_1), out_sinfo=R.Tensor((batch_size, 1, 2560), dtype="float16"))
            layer_norm206 = R.call_tir(cls.layer_norm, (lv10_1, gpt_neox_layers_5_post_attention_layernorm_weight4, gpt_neox_layers_5_post_attention_layernorm_bias4), out_sinfo=R.Tensor((batch_size, 1, 2560), dtype="float16"))
            lv11 = R.call_tir(cls.fused_dequantize3_fused_NT_matmul2_add3_gelu_cast, (gpt_neox_layers_5_mlp_dense_h_to_4h_q_weight4, gpt_neox_layers_5_mlp_dense_h_to_4h_q_scale4, layer_norm206, gpt_neox_layers_5_mlp_dense_h_to_4h_bias4), out_sinfo=R.Tensor((batch_size, 1, 10240), dtype="float16"))
            lv11_1 = R.call_tir(cls.fused_dequantize4_fused_NT_matmul3_add4_cast1_add2, (gpt_neox_layers_5_mlp_dense_4h_to_h_q_weight4, gpt_neox_layers_5_mlp_dense_4h_to_h_q_scale4, lv11, gpt_neox_layers_5_mlp_dense_4h_to_h_bias4, lv10_1), out_sinfo=R.Tensor((batch_size, 1, 2560), dtype="float16"))
            layer_norm207 = R.call_tir(cls.layer_norm, (lv11_1, gpt_neox_layers_6_input_layernorm_weight4, gpt_neox_layers_6_input_layernorm_bias4), out_sinfo=R.Tensor((batch_size, 1, 2560), dtype="float16"))
            lv12 = R.call_tir(cls.fused_dequantize1_fused_NT_matmul_add, (gpt_neox_layers_6_attention_query_key_value_q_weight4, gpt_neox_layers_6_attention_query_key_value_q_scale4, layer_norm207, gpt_neox_layers_6_attention_query_key_value_bias4), out_sinfo=R.Tensor((batch_size, 1, 7680), dtype="float16"))
            reshape408 = R.call_tir(cls.reshape, (lv12,), out_sinfo=R.Tensor((batch_size, 1, 96, 80), dtype="float16"))
            reshape409 = R.call_tir(cls.reshape1, (reshape408,), out_sinfo=R.Tensor((batch_size, 96, 80), dtype="float16"))
            lv516 = R.call_dps_packed("vm.builtin.attention_kv_cache_attention_with_fused_qkv", (paged_kv_cache, R.prim_value(6), R.prim_value(T.float32(1.0)), reshape409), out_sinfo=R.Tensor((batch_size, 32, 80), dtype="float16"))
            reshape410 = R.call_tir(cls.reshape2, (lv516,), out_sinfo=R.Tensor((batch_size, 1, 32, 80), dtype="float16"))
            reshape411 = R.call_tir(cls.reshape3, (reshape410,), out_sinfo=R.Tensor((batch_size, 1, 2560), dtype="float16"))
            lv12_1 = R.call_tir(cls.fused_dequantize2_fused_NT_matmul1_add1_add2, (gpt_neox_layers_6_attention_dense_q_weight4, gpt_neox_layers_6_attention_dense_q_scale4, reshape411, gpt_neox_layers_6_attention_dense_bias4, lv11_1), out_sinfo=R.Tensor((batch_size, 1, 2560), dtype="float16"))
            layer_norm208 = R.call_tir(cls.layer_norm, (lv12_1, gpt_neox_layers_6_post_attention_layernorm_weight4, gpt_neox_layers_6_post_attention_layernorm_bias4), out_sinfo=R.Tensor((batch_size, 1, 2560), dtype="float16"))
            lv13 = R.call_tir(cls.fused_dequantize3_fused_NT_matmul2_add3_gelu_cast, (gpt_neox_layers_6_mlp_dense_h_to_4h_q_weight4, gpt_neox_layers_6_mlp_dense_h_to_4h_q_scale4, layer_norm208, gpt_neox_layers_6_mlp_dense_h_to_4h_bias4), out_sinfo=R.Tensor((batch_size, 1, 10240), dtype="float16"))
            lv13_1 = R.call_tir(cls.fused_dequantize4_fused_NT_matmul3_add4_cast1_add2, (gpt_neox_layers_6_mlp_dense_4h_to_h_q_weight4, gpt_neox_layers_6_mlp_dense_4h_to_h_q_scale4, lv13, gpt_neox_layers_6_mlp_dense_4h_to_h_bias4, lv12_1), out_sinfo=R.Tensor((batch_size, 1, 2560), dtype="float16"))
            layer_norm209 = R.call_tir(cls.layer_norm, (lv13_1, gpt_neox_layers_7_input_layernorm_weight4, gpt_neox_layers_7_input_layernorm_bias4), out_sinfo=R.Tensor((batch_size, 1, 2560), dtype="float16"))
            lv14 = R.call_tir(cls.fused_dequantize1_fused_NT_matmul_add, (gpt_neox_layers_7_attention_query_key_value_q_weight4, gpt_neox_layers_7_attention_query_key_value_q_scale4, layer_norm209, gpt_neox_layers_7_attention_query_key_value_bias4), out_sinfo=R.Tensor((batch_size, 1, 7680), dtype="float16"))
            reshape412 = R.call_tir(cls.reshape, (lv14,), out_sinfo=R.Tensor((batch_size, 1, 96, 80), dtype="float16"))
            reshape413 = R.call_tir(cls.reshape1, (reshape412,), out_sinfo=R.Tensor((batch_size, 96, 80), dtype="float16"))
            lv521 = R.call_dps_packed("vm.builtin.attention_kv_cache_attention_with_fused_qkv", (paged_kv_cache, R.prim_value(7), R.prim_value(T.float32(1.0)), reshape413), out_sinfo=R.Tensor((batch_size, 32, 80), dtype="float16"))
            reshape414 = R.call_tir(cls.reshape2, (lv521,), out_sinfo=R.Tensor((batch_size, 1, 32, 80), dtype="float16"))
            reshape415 = R.call_tir(cls.reshape3, (reshape414,), out_sinfo=R.Tensor((batch_size, 1, 2560), dtype="float16"))
            lv14_1 = R.call_tir(cls.fused_dequantize2_fused_NT_matmul1_add1_add2, (gpt_neox_layers_7_attention_dense_q_weight4, gpt_neox_layers_7_attention_dense_q_scale4, reshape415, gpt_neox_layers_7_attention_dense_bias4, lv13_1), out_sinfo=R.Tensor((batch_size, 1, 2560), dtype="float16"))
            layer_norm210 = R.call_tir(cls.layer_norm, (lv14_1, gpt_neox_layers_7_post_attention_layernorm_weight4, gpt_neox_layers_7_post_attention_layernorm_bias4), out_sinfo=R.Tensor((batch_size, 1, 2560), dtype="float16"))
            lv15 = R.call_tir(cls.fused_dequantize3_fused_NT_matmul2_add3_gelu_cast, (gpt_neox_layers_7_mlp_dense_h_to_4h_q_weight4, gpt_neox_layers_7_mlp_dense_h_to_4h_q_scale4, layer_norm210, gpt_neox_layers_7_mlp_dense_h_to_4h_bias4), out_sinfo=R.Tensor((batch_size, 1, 10240), dtype="float16"))
            lv15_1 = R.call_tir(cls.fused_dequantize4_fused_NT_matmul3_add4_cast1_add2, (gpt_neox_layers_7_mlp_dense_4h_to_h_q_weight4, gpt_neox_layers_7_mlp_dense_4h_to_h_q_scale4, lv15, gpt_neox_layers_7_mlp_dense_4h_to_h_bias4, lv14_1), out_sinfo=R.Tensor((batch_size, 1, 2560), dtype="float16"))
            layer_norm211 = R.call_tir(cls.layer_norm, (lv15_1, gpt_neox_layers_8_input_layernorm_weight4, gpt_neox_layers_8_input_layernorm_bias4), out_sinfo=R.Tensor((batch_size, 1, 2560), dtype="float16"))
            lv16 = R.call_tir(cls.fused_dequantize1_fused_NT_matmul_add, (gpt_neox_layers_8_attention_query_key_value_q_weight4, gpt_neox_layers_8_attention_query_key_value_q_scale4, layer_norm211, gpt_neox_layers_8_attention_query_key_value_bias4), out_sinfo=R.Tensor((batch_size, 1, 7680), dtype="float16"))
            reshape416 = R.call_tir(cls.reshape, (lv16,), out_sinfo=R.Tensor((batch_size, 1, 96, 80), dtype="float16"))
            reshape417 = R.call_tir(cls.reshape1, (reshape416,), out_sinfo=R.Tensor((batch_size, 96, 80), dtype="float16"))
            lv526 = R.call_dps_packed("vm.builtin.attention_kv_cache_attention_with_fused_qkv", (paged_kv_cache, R.prim_value(8), R.prim_value(T.float32(1.0)), reshape417), out_sinfo=R.Tensor((batch_size, 32, 80), dtype="float16"))
            reshape418 = R.call_tir(cls.reshape2, (lv526,), out_sinfo=R.Tensor((batch_size, 1, 32, 80), dtype="float16"))
            reshape419 = R.call_tir(cls.reshape3, (reshape418,), out_sinfo=R.Tensor((batch_size, 1, 2560), dtype="float16"))
            lv16_1 = R.call_tir(cls.fused_dequantize2_fused_NT_matmul1_add1_add2, (gpt_neox_layers_8_attention_dense_q_weight4, gpt_neox_layers_8_attention_dense_q_scale4, reshape419, gpt_neox_layers_8_attention_dense_bias4, lv15_1), out_sinfo=R.Tensor((batch_size, 1, 2560), dtype="float16"))
            layer_norm212 = R.call_tir(cls.layer_norm, (lv16_1, gpt_neox_layers_8_post_attention_layernorm_weight4, gpt_neox_layers_8_post_attention_layernorm_bias4), out_sinfo=R.Tensor((batch_size, 1, 2560), dtype="float16"))
            lv17 = R.call_tir(cls.fused_dequantize3_fused_NT_matmul2_add3_gelu_cast, (gpt_neox_layers_8_mlp_dense_h_to_4h_q_weight4, gpt_neox_layers_8_mlp_dense_h_to_4h_q_scale4, layer_norm212, gpt_neox_layers_8_mlp_dense_h_to_4h_bias4), out_sinfo=R.Tensor((batch_size, 1, 10240), dtype="float16"))
            lv17_1 = R.call_tir(cls.fused_dequantize4_fused_NT_matmul3_add4_cast1_add2, (gpt_neox_layers_8_mlp_dense_4h_to_h_q_weight4, gpt_neox_layers_8_mlp_dense_4h_to_h_q_scale4, lv17, gpt_neox_layers_8_mlp_dense_4h_to_h_bias4, lv16_1), out_sinfo=R.Tensor((batch_size, 1, 2560), dtype="float16"))
            layer_norm213 = R.call_tir(cls.layer_norm, (lv17_1, gpt_neox_layers_9_input_layernorm_weight4, gpt_neox_layers_9_input_layernorm_bias4), out_sinfo=R.Tensor((batch_size, 1, 2560), dtype="float16"))
            lv18 = R.call_tir(cls.fused_dequantize1_fused_NT_matmul_add, (gpt_neox_layers_9_attention_query_key_value_q_weight4, gpt_neox_layers_9_attention_query_key_value_q_scale4, layer_norm213, gpt_neox_layers_9_attention_query_key_value_bias4), out_sinfo=R.Tensor((batch_size, 1, 7680), dtype="float16"))
            reshape420 = R.call_tir(cls.reshape, (lv18,), out_sinfo=R.Tensor((batch_size, 1, 96, 80), dtype="float16"))
            reshape421 = R.call_tir(cls.reshape1, (reshape420,), out_sinfo=R.Tensor((batch_size, 96, 80), dtype="float16"))
            lv531 = R.call_dps_packed("vm.builtin.attention_kv_cache_attention_with_fused_qkv", (paged_kv_cache, R.prim_value(9), R.prim_value(T.float32(1.0)), reshape421), out_sinfo=R.Tensor((batch_size, 32, 80), dtype="float16"))
            reshape422 = R.call_tir(cls.reshape2, (lv531,), out_sinfo=R.Tensor((batch_size, 1, 32, 80), dtype="float16"))
            reshape423 = R.call_tir(cls.reshape3, (reshape422,), out_sinfo=R.Tensor((batch_size, 1, 2560), dtype="float16"))
            lv18_1 = R.call_tir(cls.fused_dequantize2_fused_NT_matmul1_add1_add2, (gpt_neox_layers_9_attention_dense_q_weight4, gpt_neox_layers_9_attention_dense_q_scale4, reshape423, gpt_neox_layers_9_attention_dense_bias4, lv17_1), out_sinfo=R.Tensor((batch_size, 1, 2560), dtype="float16"))
            layer_norm214 = R.call_tir(cls.layer_norm, (lv18_1, gpt_neox_layers_9_post_attention_layernorm_weight4, gpt_neox_layers_9_post_attention_layernorm_bias4), out_sinfo=R.Tensor((batch_size, 1, 2560), dtype="float16"))
            lv19 = R.call_tir(cls.fused_dequantize3_fused_NT_matmul2_add3_gelu_cast, (gpt_neox_layers_9_mlp_dense_h_to_4h_q_weight4, gpt_neox_layers_9_mlp_dense_h_to_4h_q_scale4, layer_norm214, gpt_neox_layers_9_mlp_dense_h_to_4h_bias4), out_sinfo=R.Tensor((batch_size, 1, 10240), dtype="float16"))
            lv19_1 = R.call_tir(cls.fused_dequantize4_fused_NT_matmul3_add4_cast1_add2, (gpt_neox_layers_9_mlp_dense_4h_to_h_q_weight4, gpt_neox_layers_9_mlp_dense_4h_to_h_q_scale4, lv19, gpt_neox_layers_9_mlp_dense_4h_to_h_bias4, lv18_1), out_sinfo=R.Tensor((batch_size, 1, 2560), dtype="float16"))
            layer_norm215 = R.call_tir(cls.layer_norm, (lv19_1, gpt_neox_layers_10_input_layernorm_weight4, gpt_neox_layers_10_input_layernorm_bias4), out_sinfo=R.Tensor((batch_size, 1, 2560), dtype="float16"))
            lv20 = R.call_tir(cls.fused_dequantize1_fused_NT_matmul_add, (gpt_neox_layers_10_attention_query_key_value_q_weight4, gpt_neox_layers_10_attention_query_key_value_q_scale4, layer_norm215, gpt_neox_layers_10_attention_query_key_value_bias4), out_sinfo=R.Tensor((batch_size, 1, 7680), dtype="float16"))
            reshape424 = R.call_tir(cls.reshape, (lv20,), out_sinfo=R.Tensor((batch_size, 1, 96, 80), dtype="float16"))
            reshape425 = R.call_tir(cls.reshape1, (reshape424,), out_sinfo=R.Tensor((batch_size, 96, 80), dtype="float16"))
            lv536 = R.call_dps_packed("vm.builtin.attention_kv_cache_attention_with_fused_qkv", (paged_kv_cache, R.prim_value(10), R.prim_value(T.float32(1.0)), reshape425), out_sinfo=R.Tensor((batch_size, 32, 80), dtype="float16"))
            reshape426 = R.call_tir(cls.reshape2, (lv536,), out_sinfo=R.Tensor((batch_size, 1, 32, 80), dtype="float16"))
            reshape427 = R.call_tir(cls.reshape3, (reshape426,), out_sinfo=R.Tensor((batch_size, 1, 2560), dtype="float16"))
            lv20_1 = R.call_tir(cls.fused_dequantize2_fused_NT_matmul1_add1_add2, (gpt_neox_layers_10_attention_dense_q_weight4, gpt_neox_layers_10_attention_dense_q_scale4, reshape427, gpt_neox_layers_10_attention_dense_bias4, lv19_1), out_sinfo=R.Tensor((batch_size, 1, 2560), dtype="float16"))
            layer_norm216 = R.call_tir(cls.layer_norm, (lv20_1, gpt_neox_layers_10_post_attention_layernorm_weight4, gpt_neox_layers_10_post_attention_layernorm_bias4), out_sinfo=R.Tensor((batch_size, 1, 2560), dtype="float16"))
            lv21 = R.call_tir(cls.fused_dequantize3_fused_NT_matmul2_add3_gelu_cast, (gpt_neox_layers_10_mlp_dense_h_to_4h_q_weight4, gpt_neox_layers_10_mlp_dense_h_to_4h_q_scale4, layer_norm216, gpt_neox_layers_10_mlp_dense_h_to_4h_bias4), out_sinfo=R.Tensor((batch_size, 1, 10240), dtype="float16"))
            lv21_1 = R.call_tir(cls.fused_dequantize4_fused_NT_matmul3_add4_cast1_add2, (gpt_neox_layers_10_mlp_dense_4h_to_h_q_weight4, gpt_neox_layers_10_mlp_dense_4h_to_h_q_scale4, lv21, gpt_neox_layers_10_mlp_dense_4h_to_h_bias4, lv20_1), out_sinfo=R.Tensor((batch_size, 1, 2560), dtype="float16"))
            layer_norm217 = R.call_tir(cls.layer_norm, (lv21_1, gpt_neox_layers_11_input_layernorm_weight4, gpt_neox_layers_11_input_layernorm_bias4), out_sinfo=R.Tensor((batch_size, 1, 2560), dtype="float16"))
            lv22 = R.call_tir(cls.fused_dequantize1_fused_NT_matmul_add, (gpt_neox_layers_11_attention_query_key_value_q_weight4, gpt_neox_layers_11_attention_query_key_value_q_scale4, layer_norm217, gpt_neox_layers_11_attention_query_key_value_bias4), out_sinfo=R.Tensor((batch_size, 1, 7680), dtype="float16"))
            reshape428 = R.call_tir(cls.reshape, (lv22,), out_sinfo=R.Tensor((batch_size, 1, 96, 80), dtype="float16"))
            reshape429 = R.call_tir(cls.reshape1, (reshape428,), out_sinfo=R.Tensor((batch_size, 96, 80), dtype="float16"))
            lv541 = R.call_dps_packed("vm.builtin.attention_kv_cache_attention_with_fused_qkv", (paged_kv_cache, R.prim_value(11), R.prim_value(T.float32(1.0)), reshape429), out_sinfo=R.Tensor((batch_size, 32, 80), dtype="float16"))
            reshape430 = R.call_tir(cls.reshape2, (lv541,), out_sinfo=R.Tensor((batch_size, 1, 32, 80), dtype="float16"))
            reshape431 = R.call_tir(cls.reshape3, (reshape430,), out_sinfo=R.Tensor((batch_size, 1, 2560), dtype="float16"))
            lv22_1 = R.call_tir(cls.fused_dequantize2_fused_NT_matmul1_add1_add2, (gpt_neox_layers_11_attention_dense_q_weight4, gpt_neox_layers_11_attention_dense_q_scale4, reshape431, gpt_neox_layers_11_attention_dense_bias4, lv21_1), out_sinfo=R.Tensor((batch_size, 1, 2560), dtype="float16"))
            layer_norm218 = R.call_tir(cls.layer_norm, (lv22_1, gpt_neox_layers_11_post_attention_layernorm_weight4, gpt_neox_layers_11_post_attention_layernorm_bias4), out_sinfo=R.Tensor((batch_size, 1, 2560), dtype="float16"))
            lv23 = R.call_tir(cls.fused_dequantize3_fused_NT_matmul2_add3_gelu_cast, (gpt_neox_layers_11_mlp_dense_h_to_4h_q_weight4, gpt_neox_layers_11_mlp_dense_h_to_4h_q_scale4, layer_norm218, gpt_neox_layers_11_mlp_dense_h_to_4h_bias4), out_sinfo=R.Tensor((batch_size, 1, 10240), dtype="float16"))
            lv23_1 = R.call_tir(cls.fused_dequantize4_fused_NT_matmul3_add4_cast1_add2, (gpt_neox_layers_11_mlp_dense_4h_to_h_q_weight4, gpt_neox_layers_11_mlp_dense_4h_to_h_q_scale4, lv23, gpt_neox_layers_11_mlp_dense_4h_to_h_bias4, lv22_1), out_sinfo=R.Tensor((batch_size, 1, 2560), dtype="float16"))
            layer_norm219 = R.call_tir(cls.layer_norm, (lv23_1, gpt_neox_layers_12_input_layernorm_weight4, gpt_neox_layers_12_input_layernorm_bias4), out_sinfo=R.Tensor((batch_size, 1, 2560), dtype="float16"))
            lv24 = R.call_tir(cls.fused_dequantize1_fused_NT_matmul_add, (gpt_neox_layers_12_attention_query_key_value_q_weight4, gpt_neox_layers_12_attention_query_key_value_q_scale4, layer_norm219, gpt_neox_layers_12_attention_query_key_value_bias4), out_sinfo=R.Tensor((batch_size, 1, 7680), dtype="float16"))
            reshape432 = R.call_tir(cls.reshape, (lv24,), out_sinfo=R.Tensor((batch_size, 1, 96, 80), dtype="float16"))
            reshape433 = R.call_tir(cls.reshape1, (reshape432,), out_sinfo=R.Tensor((batch_size, 96, 80), dtype="float16"))
            lv546 = R.call_dps_packed("vm.builtin.attention_kv_cache_attention_with_fused_qkv", (paged_kv_cache, R.prim_value(12), R.prim_value(T.float32(1.0)), reshape433), out_sinfo=R.Tensor((batch_size, 32, 80), dtype="float16"))
            reshape434 = R.call_tir(cls.reshape2, (lv546,), out_sinfo=R.Tensor((batch_size, 1, 32, 80), dtype="float16"))
            reshape435 = R.call_tir(cls.reshape3, (reshape434,), out_sinfo=R.Tensor((batch_size, 1, 2560), dtype="float16"))
            lv24_1 = R.call_tir(cls.fused_dequantize2_fused_NT_matmul1_add1_add2, (gpt_neox_layers_12_attention_dense_q_weight4, gpt_neox_layers_12_attention_dense_q_scale4, reshape435, gpt_neox_layers_12_attention_dense_bias4, lv23_1), out_sinfo=R.Tensor((batch_size, 1, 2560), dtype="float16"))
            layer_norm220 = R.call_tir(cls.layer_norm, (lv24_1, gpt_neox_layers_12_post_attention_layernorm_weight4, gpt_neox_layers_12_post_attention_layernorm_bias4), out_sinfo=R.Tensor((batch_size, 1, 2560), dtype="float16"))
            lv25 = R.call_tir(cls.fused_dequantize3_fused_NT_matmul2_add3_gelu_cast, (gpt_neox_layers_12_mlp_dense_h_to_4h_q_weight4, gpt_neox_layers_12_mlp_dense_h_to_4h_q_scale4, layer_norm220, gpt_neox_layers_12_mlp_dense_h_to_4h_bias4), out_sinfo=R.Tensor((batch_size, 1, 10240), dtype="float16"))
            lv25_1 = R.call_tir(cls.fused_dequantize4_fused_NT_matmul3_add4_cast1_add2, (gpt_neox_layers_12_mlp_dense_4h_to_h_q_weight4, gpt_neox_layers_12_mlp_dense_4h_to_h_q_scale4, lv25, gpt_neox_layers_12_mlp_dense_4h_to_h_bias4, lv24_1), out_sinfo=R.Tensor((batch_size, 1, 2560), dtype="float16"))
            layer_norm221 = R.call_tir(cls.layer_norm, (lv25_1, gpt_neox_layers_13_input_layernorm_weight4, gpt_neox_layers_13_input_layernorm_bias4), out_sinfo=R.Tensor((batch_size, 1, 2560), dtype="float16"))
            lv26 = R.call_tir(cls.fused_dequantize1_fused_NT_matmul_add, (gpt_neox_layers_13_attention_query_key_value_q_weight4, gpt_neox_layers_13_attention_query_key_value_q_scale4, layer_norm221, gpt_neox_layers_13_attention_query_key_value_bias4), out_sinfo=R.Tensor((batch_size, 1, 7680), dtype="float16"))
            reshape436 = R.call_tir(cls.reshape, (lv26,), out_sinfo=R.Tensor((batch_size, 1, 96, 80), dtype="float16"))
            reshape437 = R.call_tir(cls.reshape1, (reshape436,), out_sinfo=R.Tensor((batch_size, 96, 80), dtype="float16"))
            lv551 = R.call_dps_packed("vm.builtin.attention_kv_cache_attention_with_fused_qkv", (paged_kv_cache, R.prim_value(13), R.prim_value(T.float32(1.0)), reshape437), out_sinfo=R.Tensor((batch_size, 32, 80), dtype="float16"))
            reshape438 = R.call_tir(cls.reshape2, (lv551,), out_sinfo=R.Tensor((batch_size, 1, 32, 80), dtype="float16"))
            reshape439 = R.call_tir(cls.reshape3, (reshape438,), out_sinfo=R.Tensor((batch_size, 1, 2560), dtype="float16"))
            lv26_1 = R.call_tir(cls.fused_dequantize2_fused_NT_matmul1_add1_add2, (gpt_neox_layers_13_attention_dense_q_weight4, gpt_neox_layers_13_attention_dense_q_scale4, reshape439, gpt_neox_layers_13_attention_dense_bias4, lv25_1), out_sinfo=R.Tensor((batch_size, 1, 2560), dtype="float16"))
            layer_norm222 = R.call_tir(cls.layer_norm, (lv26_1, gpt_neox_layers_13_post_attention_layernorm_weight4, gpt_neox_layers_13_post_attention_layernorm_bias4), out_sinfo=R.Tensor((batch_size, 1, 2560), dtype="float16"))
            lv27 = R.call_tir(cls.fused_dequantize3_fused_NT_matmul2_add3_gelu_cast, (gpt_neox_layers_13_mlp_dense_h_to_4h_q_weight4, gpt_neox_layers_13_mlp_dense_h_to_4h_q_scale4, layer_norm222, gpt_neox_layers_13_mlp_dense_h_to_4h_bias4), out_sinfo=R.Tensor((batch_size, 1, 10240), dtype="float16"))
            lv27_1 = R.call_tir(cls.fused_dequantize4_fused_NT_matmul3_add4_cast1_add2, (gpt_neox_layers_13_mlp_dense_4h_to_h_q_weight4, gpt_neox_layers_13_mlp_dense_4h_to_h_q_scale4, lv27, gpt_neox_layers_13_mlp_dense_4h_to_h_bias4, lv26_1), out_sinfo=R.Tensor((batch_size, 1, 2560), dtype="float16"))
            layer_norm223 = R.call_tir(cls.layer_norm, (lv27_1, gpt_neox_layers_14_input_layernorm_weight4, gpt_neox_layers_14_input_layernorm_bias4), out_sinfo=R.Tensor((batch_size, 1, 2560), dtype="float16"))
            lv28 = R.call_tir(cls.fused_dequantize1_fused_NT_matmul_add, (gpt_neox_layers_14_attention_query_key_value_q_weight4, gpt_neox_layers_14_attention_query_key_value_q_scale4, layer_norm223, gpt_neox_layers_14_attention_query_key_value_bias4), out_sinfo=R.Tensor((batch_size, 1, 7680), dtype="float16"))
            reshape440 = R.call_tir(cls.reshape, (lv28,), out_sinfo=R.Tensor((batch_size, 1, 96, 80), dtype="float16"))
            reshape441 = R.call_tir(cls.reshape1, (reshape440,), out_sinfo=R.Tensor((batch_size, 96, 80), dtype="float16"))
            lv556 = R.call_dps_packed("vm.builtin.attention_kv_cache_attention_with_fused_qkv", (paged_kv_cache, R.prim_value(14), R.prim_value(T.float32(1.0)), reshape441), out_sinfo=R.Tensor((batch_size, 32, 80), dtype="float16"))
            reshape442 = R.call_tir(cls.reshape2, (lv556,), out_sinfo=R.Tensor((batch_size, 1, 32, 80), dtype="float16"))
            reshape443 = R.call_tir(cls.reshape3, (reshape442,), out_sinfo=R.Tensor((batch_size, 1, 2560), dtype="float16"))
            lv28_1 = R.call_tir(cls.fused_dequantize2_fused_NT_matmul1_add1_add2, (gpt_neox_layers_14_attention_dense_q_weight4, gpt_neox_layers_14_attention_dense_q_scale4, reshape443, gpt_neox_layers_14_attention_dense_bias4, lv27_1), out_sinfo=R.Tensor((batch_size, 1, 2560), dtype="float16"))
            layer_norm224 = R.call_tir(cls.layer_norm, (lv28_1, gpt_neox_layers_14_post_attention_layernorm_weight4, gpt_neox_layers_14_post_attention_layernorm_bias4), out_sinfo=R.Tensor((batch_size, 1, 2560), dtype="float16"))
            lv29 = R.call_tir(cls.fused_dequantize3_fused_NT_matmul2_add3_gelu_cast, (gpt_neox_layers_14_mlp_dense_h_to_4h_q_weight4, gpt_neox_layers_14_mlp_dense_h_to_4h_q_scale4, layer_norm224, gpt_neox_layers_14_mlp_dense_h_to_4h_bias4), out_sinfo=R.Tensor((batch_size, 1, 10240), dtype="float16"))
            lv29_1 = R.call_tir(cls.fused_dequantize4_fused_NT_matmul3_add4_cast1_add2, (gpt_neox_layers_14_mlp_dense_4h_to_h_q_weight4, gpt_neox_layers_14_mlp_dense_4h_to_h_q_scale4, lv29, gpt_neox_layers_14_mlp_dense_4h_to_h_bias4, lv28_1), out_sinfo=R.Tensor((batch_size, 1, 2560), dtype="float16"))
            layer_norm225 = R.call_tir(cls.layer_norm, (lv29_1, gpt_neox_layers_15_input_layernorm_weight4, gpt_neox_layers_15_input_layernorm_bias4), out_sinfo=R.Tensor((batch_size, 1, 2560), dtype="float16"))
            lv30 = R.call_tir(cls.fused_dequantize1_fused_NT_matmul_add, (gpt_neox_layers_15_attention_query_key_value_q_weight4, gpt_neox_layers_15_attention_query_key_value_q_scale4, layer_norm225, gpt_neox_layers_15_attention_query_key_value_bias4), out_sinfo=R.Tensor((batch_size, 1, 7680), dtype="float16"))
            reshape444 = R.call_tir(cls.reshape, (lv30,), out_sinfo=R.Tensor((batch_size, 1, 96, 80), dtype="float16"))
            reshape445 = R.call_tir(cls.reshape1, (reshape444,), out_sinfo=R.Tensor((batch_size, 96, 80), dtype="float16"))
            lv561 = R.call_dps_packed("vm.builtin.attention_kv_cache_attention_with_fused_qkv", (paged_kv_cache, R.prim_value(15), R.prim_value(T.float32(1.0)), reshape445), out_sinfo=R.Tensor((batch_size, 32, 80), dtype="float16"))
            reshape446 = R.call_tir(cls.reshape2, (lv561,), out_sinfo=R.Tensor((batch_size, 1, 32, 80), dtype="float16"))
            reshape447 = R.call_tir(cls.reshape3, (reshape446,), out_sinfo=R.Tensor((batch_size, 1, 2560), dtype="float16"))
            lv30_1 = R.call_tir(cls.fused_dequantize2_fused_NT_matmul1_add1_add2, (gpt_neox_layers_15_attention_dense_q_weight4, gpt_neox_layers_15_attention_dense_q_scale4, reshape447, gpt_neox_layers_15_attention_dense_bias4, lv29_1), out_sinfo=R.Tensor((batch_size, 1, 2560), dtype="float16"))
            layer_norm226 = R.call_tir(cls.layer_norm, (lv30_1, gpt_neox_layers_15_post_attention_layernorm_weight4, gpt_neox_layers_15_post_attention_layernorm_bias4), out_sinfo=R.Tensor((batch_size, 1, 2560), dtype="float16"))
            lv31 = R.call_tir(cls.fused_dequantize3_fused_NT_matmul2_add3_gelu_cast, (gpt_neox_layers_15_mlp_dense_h_to_4h_q_weight4, gpt_neox_layers_15_mlp_dense_h_to_4h_q_scale4, layer_norm226, gpt_neox_layers_15_mlp_dense_h_to_4h_bias4), out_sinfo=R.Tensor((batch_size, 1, 10240), dtype="float16"))
            lv31_1 = R.call_tir(cls.fused_dequantize4_fused_NT_matmul3_add4_cast1_add2, (gpt_neox_layers_15_mlp_dense_4h_to_h_q_weight4, gpt_neox_layers_15_mlp_dense_4h_to_h_q_scale4, lv31, gpt_neox_layers_15_mlp_dense_4h_to_h_bias4, lv30_1), out_sinfo=R.Tensor((batch_size, 1, 2560), dtype="float16"))
            layer_norm227 = R.call_tir(cls.layer_norm, (lv31_1, gpt_neox_layers_16_input_layernorm_weight4, gpt_neox_layers_16_input_layernorm_bias4), out_sinfo=R.Tensor((batch_size, 1, 2560), dtype="float16"))
            lv32 = R.call_tir(cls.fused_dequantize1_fused_NT_matmul_add, (gpt_neox_layers_16_attention_query_key_value_q_weight4, gpt_neox_layers_16_attention_query_key_value_q_scale4, layer_norm227, gpt_neox_layers_16_attention_query_key_value_bias4), out_sinfo=R.Tensor((batch_size, 1, 7680), dtype="float16"))
            reshape448 = R.call_tir(cls.reshape, (lv32,), out_sinfo=R.Tensor((batch_size, 1, 96, 80), dtype="float16"))
            reshape449 = R.call_tir(cls.reshape1, (reshape448,), out_sinfo=R.Tensor((batch_size, 96, 80), dtype="float16"))
            lv566 = R.call_dps_packed("vm.builtin.attention_kv_cache_attention_with_fused_qkv", (paged_kv_cache, R.prim_value(16), R.prim_value(T.float32(1.0)), reshape449), out_sinfo=R.Tensor((batch_size, 32, 80), dtype="float16"))
            reshape450 = R.call_tir(cls.reshape2, (lv566,), out_sinfo=R.Tensor((batch_size, 1, 32, 80), dtype="float16"))
            reshape451 = R.call_tir(cls.reshape3, (reshape450,), out_sinfo=R.Tensor((batch_size, 1, 2560), dtype="float16"))
            lv32_1 = R.call_tir(cls.fused_dequantize2_fused_NT_matmul1_add1_add2, (gpt_neox_layers_16_attention_dense_q_weight4, gpt_neox_layers_16_attention_dense_q_scale4, reshape451, gpt_neox_layers_16_attention_dense_bias4, lv31_1), out_sinfo=R.Tensor((batch_size, 1, 2560), dtype="float16"))
            layer_norm228 = R.call_tir(cls.layer_norm, (lv32_1, gpt_neox_layers_16_post_attention_layernorm_weight4, gpt_neox_layers_16_post_attention_layernorm_bias4), out_sinfo=R.Tensor((batch_size, 1, 2560), dtype="float16"))
            lv33 = R.call_tir(cls.fused_dequantize3_fused_NT_matmul2_add3_gelu_cast, (gpt_neox_layers_16_mlp_dense_h_to_4h_q_weight4, gpt_neox_layers_16_mlp_dense_h_to_4h_q_scale4, layer_norm228, gpt_neox_layers_16_mlp_dense_h_to_4h_bias4), out_sinfo=R.Tensor((batch_size, 1, 10240), dtype="float16"))
            lv33_1 = R.call_tir(cls.fused_dequantize4_fused_NT_matmul3_add4_cast1_add2, (gpt_neox_layers_16_mlp_dense_4h_to_h_q_weight4, gpt_neox_layers_16_mlp_dense_4h_to_h_q_scale4, lv33, gpt_neox_layers_16_mlp_dense_4h_to_h_bias4, lv32_1), out_sinfo=R.Tensor((batch_size, 1, 2560), dtype="float16"))
            layer_norm229 = R.call_tir(cls.layer_norm, (lv33_1, gpt_neox_layers_17_input_layernorm_weight4, gpt_neox_layers_17_input_layernorm_bias4), out_sinfo=R.Tensor((batch_size, 1, 2560), dtype="float16"))
            lv34 = R.call_tir(cls.fused_dequantize1_fused_NT_matmul_add, (gpt_neox_layers_17_attention_query_key_value_q_weight4, gpt_neox_layers_17_attention_query_key_value_q_scale4, layer_norm229, gpt_neox_layers_17_attention_query_key_value_bias4), out_sinfo=R.Tensor((batch_size, 1, 7680), dtype="float16"))
            reshape452 = R.call_tir(cls.reshape, (lv34,), out_sinfo=R.Tensor((batch_size, 1, 96, 80), dtype="float16"))
            reshape453 = R.call_tir(cls.reshape1, (reshape452,), out_sinfo=R.Tensor((batch_size, 96, 80), dtype="float16"))
            lv571 = R.call_dps_packed("vm.builtin.attention_kv_cache_attention_with_fused_qkv", (paged_kv_cache, R.prim_value(17), R.prim_value(T.float32(1.0)), reshape453), out_sinfo=R.Tensor((batch_size, 32, 80), dtype="float16"))
            reshape454 = R.call_tir(cls.reshape2, (lv571,), out_sinfo=R.Tensor((batch_size, 1, 32, 80), dtype="float16"))
            reshape455 = R.call_tir(cls.reshape3, (reshape454,), out_sinfo=R.Tensor((batch_size, 1, 2560), dtype="float16"))
            lv34_1 = R.call_tir(cls.fused_dequantize2_fused_NT_matmul1_add1_add2, (gpt_neox_layers_17_attention_dense_q_weight4, gpt_neox_layers_17_attention_dense_q_scale4, reshape455, gpt_neox_layers_17_attention_dense_bias4, lv33_1), out_sinfo=R.Tensor((batch_size, 1, 2560), dtype="float16"))
            layer_norm230 = R.call_tir(cls.layer_norm, (lv34_1, gpt_neox_layers_17_post_attention_layernorm_weight4, gpt_neox_layers_17_post_attention_layernorm_bias4), out_sinfo=R.Tensor((batch_size, 1, 2560), dtype="float16"))
            lv35 = R.call_tir(cls.fused_dequantize3_fused_NT_matmul2_add3_gelu_cast, (gpt_neox_layers_17_mlp_dense_h_to_4h_q_weight4, gpt_neox_layers_17_mlp_dense_h_to_4h_q_scale4, layer_norm230, gpt_neox_layers_17_mlp_dense_h_to_4h_bias4), out_sinfo=R.Tensor((batch_size, 1, 10240), dtype="float16"))
            lv35_1 = R.call_tir(cls.fused_dequantize4_fused_NT_matmul3_add4_cast1_add2, (gpt_neox_layers_17_mlp_dense_4h_to_h_q_weight4, gpt_neox_layers_17_mlp_dense_4h_to_h_q_scale4, lv35, gpt_neox_layers_17_mlp_dense_4h_to_h_bias4, lv34_1), out_sinfo=R.Tensor((batch_size, 1, 2560), dtype="float16"))
            layer_norm231 = R.call_tir(cls.layer_norm, (lv35_1, gpt_neox_layers_18_input_layernorm_weight4, gpt_neox_layers_18_input_layernorm_bias4), out_sinfo=R.Tensor((batch_size, 1, 2560), dtype="float16"))
            lv36 = R.call_tir(cls.fused_dequantize1_fused_NT_matmul_add, (gpt_neox_layers_18_attention_query_key_value_q_weight4, gpt_neox_layers_18_attention_query_key_value_q_scale4, layer_norm231, gpt_neox_layers_18_attention_query_key_value_bias4), out_sinfo=R.Tensor((batch_size, 1, 7680), dtype="float16"))
            reshape456 = R.call_tir(cls.reshape, (lv36,), out_sinfo=R.Tensor((batch_size, 1, 96, 80), dtype="float16"))
            reshape457 = R.call_tir(cls.reshape1, (reshape456,), out_sinfo=R.Tensor((batch_size, 96, 80), dtype="float16"))
            lv576 = R.call_dps_packed("vm.builtin.attention_kv_cache_attention_with_fused_qkv", (paged_kv_cache, R.prim_value(18), R.prim_value(T.float32(1.0)), reshape457), out_sinfo=R.Tensor((batch_size, 32, 80), dtype="float16"))
            reshape458 = R.call_tir(cls.reshape2, (lv576,), out_sinfo=R.Tensor((batch_size, 1, 32, 80), dtype="float16"))
            reshape459 = R.call_tir(cls.reshape3, (reshape458,), out_sinfo=R.Tensor((batch_size, 1, 2560), dtype="float16"))
            lv36_1 = R.call_tir(cls.fused_dequantize2_fused_NT_matmul1_add1_add2, (gpt_neox_layers_18_attention_dense_q_weight4, gpt_neox_layers_18_attention_dense_q_scale4, reshape459, gpt_neox_layers_18_attention_dense_bias4, lv35_1), out_sinfo=R.Tensor((batch_size, 1, 2560), dtype="float16"))
            layer_norm232 = R.call_tir(cls.layer_norm, (lv36_1, gpt_neox_layers_18_post_attention_layernorm_weight4, gpt_neox_layers_18_post_attention_layernorm_bias4), out_sinfo=R.Tensor((batch_size, 1, 2560), dtype="float16"))
            lv37 = R.call_tir(cls.fused_dequantize3_fused_NT_matmul2_add3_gelu_cast, (gpt_neox_layers_18_mlp_dense_h_to_4h_q_weight4, gpt_neox_layers_18_mlp_dense_h_to_4h_q_scale4, layer_norm232, gpt_neox_layers_18_mlp_dense_h_to_4h_bias4), out_sinfo=R.Tensor((batch_size, 1, 10240), dtype="float16"))
            lv37_1 = R.call_tir(cls.fused_dequantize4_fused_NT_matmul3_add4_cast1_add2, (gpt_neox_layers_18_mlp_dense_4h_to_h_q_weight4, gpt_neox_layers_18_mlp_dense_4h_to_h_q_scale4, lv37, gpt_neox_layers_18_mlp_dense_4h_to_h_bias4, lv36_1), out_sinfo=R.Tensor((batch_size, 1, 2560), dtype="float16"))
            layer_norm233 = R.call_tir(cls.layer_norm, (lv37_1, gpt_neox_layers_19_input_layernorm_weight4, gpt_neox_layers_19_input_layernorm_bias4), out_sinfo=R.Tensor((batch_size, 1, 2560), dtype="float16"))
            lv38 = R.call_tir(cls.fused_dequantize1_fused_NT_matmul_add, (gpt_neox_layers_19_attention_query_key_value_q_weight4, gpt_neox_layers_19_attention_query_key_value_q_scale4, layer_norm233, gpt_neox_layers_19_attention_query_key_value_bias4), out_sinfo=R.Tensor((batch_size, 1, 7680), dtype="float16"))
            reshape460 = R.call_tir(cls.reshape, (lv38,), out_sinfo=R.Tensor((batch_size, 1, 96, 80), dtype="float16"))
            reshape461 = R.call_tir(cls.reshape1, (reshape460,), out_sinfo=R.Tensor((batch_size, 96, 80), dtype="float16"))
            lv581 = R.call_dps_packed("vm.builtin.attention_kv_cache_attention_with_fused_qkv", (paged_kv_cache, R.prim_value(19), R.prim_value(T.float32(1.0)), reshape461), out_sinfo=R.Tensor((batch_size, 32, 80), dtype="float16"))
            reshape462 = R.call_tir(cls.reshape2, (lv581,), out_sinfo=R.Tensor((batch_size, 1, 32, 80), dtype="float16"))
            reshape463 = R.call_tir(cls.reshape3, (reshape462,), out_sinfo=R.Tensor((batch_size, 1, 2560), dtype="float16"))
            lv38_1 = R.call_tir(cls.fused_dequantize2_fused_NT_matmul1_add1_add2, (gpt_neox_layers_19_attention_dense_q_weight4, gpt_neox_layers_19_attention_dense_q_scale4, reshape463, gpt_neox_layers_19_attention_dense_bias4, lv37_1), out_sinfo=R.Tensor((batch_size, 1, 2560), dtype="float16"))
            layer_norm234 = R.call_tir(cls.layer_norm, (lv38_1, gpt_neox_layers_19_post_attention_layernorm_weight4, gpt_neox_layers_19_post_attention_layernorm_bias4), out_sinfo=R.Tensor((batch_size, 1, 2560), dtype="float16"))
            lv39 = R.call_tir(cls.fused_dequantize3_fused_NT_matmul2_add3_gelu_cast, (gpt_neox_layers_19_mlp_dense_h_to_4h_q_weight4, gpt_neox_layers_19_mlp_dense_h_to_4h_q_scale4, layer_norm234, gpt_neox_layers_19_mlp_dense_h_to_4h_bias4), out_sinfo=R.Tensor((batch_size, 1, 10240), dtype="float16"))
            lv39_1 = R.call_tir(cls.fused_dequantize4_fused_NT_matmul3_add4_cast1_add2, (gpt_neox_layers_19_mlp_dense_4h_to_h_q_weight4, gpt_neox_layers_19_mlp_dense_4h_to_h_q_scale4, lv39, gpt_neox_layers_19_mlp_dense_4h_to_h_bias4, lv38_1), out_sinfo=R.Tensor((batch_size, 1, 2560), dtype="float16"))
            layer_norm235 = R.call_tir(cls.layer_norm, (lv39_1, gpt_neox_layers_20_input_layernorm_weight4, gpt_neox_layers_20_input_layernorm_bias4), out_sinfo=R.Tensor((batch_size, 1, 2560), dtype="float16"))
            lv40 = R.call_tir(cls.fused_dequantize1_fused_NT_matmul_add, (gpt_neox_layers_20_attention_query_key_value_q_weight4, gpt_neox_layers_20_attention_query_key_value_q_scale4, layer_norm235, gpt_neox_layers_20_attention_query_key_value_bias4), out_sinfo=R.Tensor((batch_size, 1, 7680), dtype="float16"))
            reshape464 = R.call_tir(cls.reshape, (lv40,), out_sinfo=R.Tensor((batch_size, 1, 96, 80), dtype="float16"))
            reshape465 = R.call_tir(cls.reshape1, (reshape464,), out_sinfo=R.Tensor((batch_size, 96, 80), dtype="float16"))
            lv586 = R.call_dps_packed("vm.builtin.attention_kv_cache_attention_with_fused_qkv", (paged_kv_cache, R.prim_value(20), R.prim_value(T.float32(1.0)), reshape465), out_sinfo=R.Tensor((batch_size, 32, 80), dtype="float16"))
            reshape466 = R.call_tir(cls.reshape2, (lv586,), out_sinfo=R.Tensor((batch_size, 1, 32, 80), dtype="float16"))
            reshape467 = R.call_tir(cls.reshape3, (reshape466,), out_sinfo=R.Tensor((batch_size, 1, 2560), dtype="float16"))
            lv40_1 = R.call_tir(cls.fused_dequantize2_fused_NT_matmul1_add1_add2, (gpt_neox_layers_20_attention_dense_q_weight4, gpt_neox_layers_20_attention_dense_q_scale4, reshape467, gpt_neox_layers_20_attention_dense_bias4, lv39_1), out_sinfo=R.Tensor((batch_size, 1, 2560), dtype="float16"))
            layer_norm236 = R.call_tir(cls.layer_norm, (lv40_1, gpt_neox_layers_20_post_attention_layernorm_weight4, gpt_neox_layers_20_post_attention_layernorm_bias4), out_sinfo=R.Tensor((batch_size, 1, 2560), dtype="float16"))
            lv41 = R.call_tir(cls.fused_dequantize3_fused_NT_matmul2_add3_gelu_cast, (gpt_neox_layers_20_mlp_dense_h_to_4h_q_weight4, gpt_neox_layers_20_mlp_dense_h_to_4h_q_scale4, layer_norm236, gpt_neox_layers_20_mlp_dense_h_to_4h_bias4), out_sinfo=R.Tensor((batch_size, 1, 10240), dtype="float16"))
            lv41_1 = R.call_tir(cls.fused_dequantize4_fused_NT_matmul3_add4_cast1_add2, (gpt_neox_layers_20_mlp_dense_4h_to_h_q_weight4, gpt_neox_layers_20_mlp_dense_4h_to_h_q_scale4, lv41, gpt_neox_layers_20_mlp_dense_4h_to_h_bias4, lv40_1), out_sinfo=R.Tensor((batch_size, 1, 2560), dtype="float16"))
            layer_norm237 = R.call_tir(cls.layer_norm, (lv41_1, gpt_neox_layers_21_input_layernorm_weight4, gpt_neox_layers_21_input_layernorm_bias4), out_sinfo=R.Tensor((batch_size, 1, 2560), dtype="float16"))
            lv42 = R.call_tir(cls.fused_dequantize1_fused_NT_matmul_add, (gpt_neox_layers_21_attention_query_key_value_q_weight4, gpt_neox_layers_21_attention_query_key_value_q_scale4, layer_norm237, gpt_neox_layers_21_attention_query_key_value_bias4), out_sinfo=R.Tensor((batch_size, 1, 7680), dtype="float16"))
            reshape468 = R.call_tir(cls.reshape, (lv42,), out_sinfo=R.Tensor((batch_size, 1, 96, 80), dtype="float16"))
            reshape469 = R.call_tir(cls.reshape1, (reshape468,), out_sinfo=R.Tensor((batch_size, 96, 80), dtype="float16"))
            lv591 = R.call_dps_packed("vm.builtin.attention_kv_cache_attention_with_fused_qkv", (paged_kv_cache, R.prim_value(21), R.prim_value(T.float32(1.0)), reshape469), out_sinfo=R.Tensor((batch_size, 32, 80), dtype="float16"))
            reshape470 = R.call_tir(cls.reshape2, (lv591,), out_sinfo=R.Tensor((batch_size, 1, 32, 80), dtype="float16"))
            reshape471 = R.call_tir(cls.reshape3, (reshape470,), out_sinfo=R.Tensor((batch_size, 1, 2560), dtype="float16"))
            lv42_1 = R.call_tir(cls.fused_dequantize2_fused_NT_matmul1_add1_add2, (gpt_neox_layers_21_attention_dense_q_weight4, gpt_neox_layers_21_attention_dense_q_scale4, reshape471, gpt_neox_layers_21_attention_dense_bias4, lv41_1), out_sinfo=R.Tensor((batch_size, 1, 2560), dtype="float16"))
            layer_norm238 = R.call_tir(cls.layer_norm, (lv42_1, gpt_neox_layers_21_post_attention_layernorm_weight4, gpt_neox_layers_21_post_attention_layernorm_bias4), out_sinfo=R.Tensor((batch_size, 1, 2560), dtype="float16"))
            lv43 = R.call_tir(cls.fused_dequantize3_fused_NT_matmul2_add3_gelu_cast, (gpt_neox_layers_21_mlp_dense_h_to_4h_q_weight4, gpt_neox_layers_21_mlp_dense_h_to_4h_q_scale4, layer_norm238, gpt_neox_layers_21_mlp_dense_h_to_4h_bias4), out_sinfo=R.Tensor((batch_size, 1, 10240), dtype="float16"))
            lv43_1 = R.call_tir(cls.fused_dequantize4_fused_NT_matmul3_add4_cast1_add2, (gpt_neox_layers_21_mlp_dense_4h_to_h_q_weight4, gpt_neox_layers_21_mlp_dense_4h_to_h_q_scale4, lv43, gpt_neox_layers_21_mlp_dense_4h_to_h_bias4, lv42_1), out_sinfo=R.Tensor((batch_size, 1, 2560), dtype="float16"))
            layer_norm239 = R.call_tir(cls.layer_norm, (lv43_1, gpt_neox_layers_22_input_layernorm_weight4, gpt_neox_layers_22_input_layernorm_bias4), out_sinfo=R.Tensor((batch_size, 1, 2560), dtype="float16"))
            lv44 = R.call_tir(cls.fused_dequantize1_fused_NT_matmul_add, (gpt_neox_layers_22_attention_query_key_value_q_weight4, gpt_neox_layers_22_attention_query_key_value_q_scale4, layer_norm239, gpt_neox_layers_22_attention_query_key_value_bias4), out_sinfo=R.Tensor((batch_size, 1, 7680), dtype="float16"))
            reshape472 = R.call_tir(cls.reshape, (lv44,), out_sinfo=R.Tensor((batch_size, 1, 96, 80), dtype="float16"))
            reshape473 = R.call_tir(cls.reshape1, (reshape472,), out_sinfo=R.Tensor((batch_size, 96, 80), dtype="float16"))
            lv596 = R.call_dps_packed("vm.builtin.attention_kv_cache_attention_with_fused_qkv", (paged_kv_cache, R.prim_value(22), R.prim_value(T.float32(1.0)), reshape473), out_sinfo=R.Tensor((batch_size, 32, 80), dtype="float16"))
            reshape474 = R.call_tir(cls.reshape2, (lv596,), out_sinfo=R.Tensor((batch_size, 1, 32, 80), dtype="float16"))
            reshape475 = R.call_tir(cls.reshape3, (reshape474,), out_sinfo=R.Tensor((batch_size, 1, 2560), dtype="float16"))
            lv44_1 = R.call_tir(cls.fused_dequantize2_fused_NT_matmul1_add1_add2, (gpt_neox_layers_22_attention_dense_q_weight4, gpt_neox_layers_22_attention_dense_q_scale4, reshape475, gpt_neox_layers_22_attention_dense_bias4, lv43_1), out_sinfo=R.Tensor((batch_size, 1, 2560), dtype="float16"))
            layer_norm240 = R.call_tir(cls.layer_norm, (lv44_1, gpt_neox_layers_22_post_attention_layernorm_weight4, gpt_neox_layers_22_post_attention_layernorm_bias4), out_sinfo=R.Tensor((batch_size, 1, 2560), dtype="float16"))
            lv45 = R.call_tir(cls.fused_dequantize3_fused_NT_matmul2_add3_gelu_cast, (gpt_neox_layers_22_mlp_dense_h_to_4h_q_weight4, gpt_neox_layers_22_mlp_dense_h_to_4h_q_scale4, layer_norm240, gpt_neox_layers_22_mlp_dense_h_to_4h_bias4), out_sinfo=R.Tensor((batch_size, 1, 10240), dtype="float16"))
            lv45_1 = R.call_tir(cls.fused_dequantize4_fused_NT_matmul3_add4_cast1_add2, (gpt_neox_layers_22_mlp_dense_4h_to_h_q_weight4, gpt_neox_layers_22_mlp_dense_4h_to_h_q_scale4, lv45, gpt_neox_layers_22_mlp_dense_4h_to_h_bias4, lv44_1), out_sinfo=R.Tensor((batch_size, 1, 2560), dtype="float16"))
            layer_norm241 = R.call_tir(cls.layer_norm, (lv45_1, gpt_neox_layers_23_input_layernorm_weight4, gpt_neox_layers_23_input_layernorm_bias4), out_sinfo=R.Tensor((batch_size, 1, 2560), dtype="float16"))
            lv46 = R.call_tir(cls.fused_dequantize1_fused_NT_matmul_add, (gpt_neox_layers_23_attention_query_key_value_q_weight4, gpt_neox_layers_23_attention_query_key_value_q_scale4, layer_norm241, gpt_neox_layers_23_attention_query_key_value_bias4), out_sinfo=R.Tensor((batch_size, 1, 7680), dtype="float16"))
            reshape476 = R.call_tir(cls.reshape, (lv46,), out_sinfo=R.Tensor((batch_size, 1, 96, 80), dtype="float16"))
            reshape477 = R.call_tir(cls.reshape1, (reshape476,), out_sinfo=R.Tensor((batch_size, 96, 80), dtype="float16"))
            lv601 = R.call_dps_packed("vm.builtin.attention_kv_cache_attention_with_fused_qkv", (paged_kv_cache, R.prim_value(23), R.prim_value(T.float32(1.0)), reshape477), out_sinfo=R.Tensor((batch_size, 32, 80), dtype="float16"))
            reshape478 = R.call_tir(cls.reshape2, (lv601,), out_sinfo=R.Tensor((batch_size, 1, 32, 80), dtype="float16"))
            reshape479 = R.call_tir(cls.reshape3, (reshape478,), out_sinfo=R.Tensor((batch_size, 1, 2560), dtype="float16"))
            lv46_1 = R.call_tir(cls.fused_dequantize2_fused_NT_matmul1_add1_add2, (gpt_neox_layers_23_attention_dense_q_weight4, gpt_neox_layers_23_attention_dense_q_scale4, reshape479, gpt_neox_layers_23_attention_dense_bias4, lv45_1), out_sinfo=R.Tensor((batch_size, 1, 2560), dtype="float16"))
            layer_norm242 = R.call_tir(cls.layer_norm, (lv46_1, gpt_neox_layers_23_post_attention_layernorm_weight4, gpt_neox_layers_23_post_attention_layernorm_bias4), out_sinfo=R.Tensor((batch_size, 1, 2560), dtype="float16"))
            lv47 = R.call_tir(cls.fused_dequantize3_fused_NT_matmul2_add3_gelu_cast, (gpt_neox_layers_23_mlp_dense_h_to_4h_q_weight4, gpt_neox_layers_23_mlp_dense_h_to_4h_q_scale4, layer_norm242, gpt_neox_layers_23_mlp_dense_h_to_4h_bias4), out_sinfo=R.Tensor((batch_size, 1, 10240), dtype="float16"))
            lv47_1 = R.call_tir(cls.fused_dequantize4_fused_NT_matmul3_add4_cast1_add2, (gpt_neox_layers_23_mlp_dense_4h_to_h_q_weight4, gpt_neox_layers_23_mlp_dense_4h_to_h_q_scale4, lv47, gpt_neox_layers_23_mlp_dense_4h_to_h_bias4, lv46_1), out_sinfo=R.Tensor((batch_size, 1, 2560), dtype="float16"))
            layer_norm243 = R.call_tir(cls.layer_norm, (lv47_1, gpt_neox_layers_24_input_layernorm_weight4, gpt_neox_layers_24_input_layernorm_bias4), out_sinfo=R.Tensor((batch_size, 1, 2560), dtype="float16"))
            lv48 = R.call_tir(cls.fused_dequantize1_fused_NT_matmul_add, (gpt_neox_layers_24_attention_query_key_value_q_weight4, gpt_neox_layers_24_attention_query_key_value_q_scale4, layer_norm243, gpt_neox_layers_24_attention_query_key_value_bias4), out_sinfo=R.Tensor((batch_size, 1, 7680), dtype="float16"))
            reshape480 = R.call_tir(cls.reshape, (lv48,), out_sinfo=R.Tensor((batch_size, 1, 96, 80), dtype="float16"))
            reshape481 = R.call_tir(cls.reshape1, (reshape480,), out_sinfo=R.Tensor((batch_size, 96, 80), dtype="float16"))
            lv606 = R.call_dps_packed("vm.builtin.attention_kv_cache_attention_with_fused_qkv", (paged_kv_cache, R.prim_value(24), R.prim_value(T.float32(1.0)), reshape481), out_sinfo=R.Tensor((batch_size, 32, 80), dtype="float16"))
            reshape482 = R.call_tir(cls.reshape2, (lv606,), out_sinfo=R.Tensor((batch_size, 1, 32, 80), dtype="float16"))
            reshape483 = R.call_tir(cls.reshape3, (reshape482,), out_sinfo=R.Tensor((batch_size, 1, 2560), dtype="float16"))
            lv48_1 = R.call_tir(cls.fused_dequantize2_fused_NT_matmul1_add1_add2, (gpt_neox_layers_24_attention_dense_q_weight4, gpt_neox_layers_24_attention_dense_q_scale4, reshape483, gpt_neox_layers_24_attention_dense_bias4, lv47_1), out_sinfo=R.Tensor((batch_size, 1, 2560), dtype="float16"))
            layer_norm244 = R.call_tir(cls.layer_norm, (lv48_1, gpt_neox_layers_24_post_attention_layernorm_weight4, gpt_neox_layers_24_post_attention_layernorm_bias4), out_sinfo=R.Tensor((batch_size, 1, 2560), dtype="float16"))
            lv49 = R.call_tir(cls.fused_dequantize3_fused_NT_matmul2_add3_gelu_cast, (gpt_neox_layers_24_mlp_dense_h_to_4h_q_weight4, gpt_neox_layers_24_mlp_dense_h_to_4h_q_scale4, layer_norm244, gpt_neox_layers_24_mlp_dense_h_to_4h_bias4), out_sinfo=R.Tensor((batch_size, 1, 10240), dtype="float16"))
            lv49_1 = R.call_tir(cls.fused_dequantize4_fused_NT_matmul3_add4_cast1_add2, (gpt_neox_layers_24_mlp_dense_4h_to_h_q_weight4, gpt_neox_layers_24_mlp_dense_4h_to_h_q_scale4, lv49, gpt_neox_layers_24_mlp_dense_4h_to_h_bias4, lv48_1), out_sinfo=R.Tensor((batch_size, 1, 2560), dtype="float16"))
            layer_norm245 = R.call_tir(cls.layer_norm, (lv49_1, gpt_neox_layers_25_input_layernorm_weight4, gpt_neox_layers_25_input_layernorm_bias4), out_sinfo=R.Tensor((batch_size, 1, 2560), dtype="float16"))
            lv50 = R.call_tir(cls.fused_dequantize1_fused_NT_matmul_add, (gpt_neox_layers_25_attention_query_key_value_q_weight4, gpt_neox_layers_25_attention_query_key_value_q_scale4, layer_norm245, gpt_neox_layers_25_attention_query_key_value_bias4), out_sinfo=R.Tensor((batch_size, 1, 7680), dtype="float16"))
            reshape484 = R.call_tir(cls.reshape, (lv50,), out_sinfo=R.Tensor((batch_size, 1, 96, 80), dtype="float16"))
            reshape485 = R.call_tir(cls.reshape1, (reshape484,), out_sinfo=R.Tensor((batch_size, 96, 80), dtype="float16"))
            lv611 = R.call_dps_packed("vm.builtin.attention_kv_cache_attention_with_fused_qkv", (paged_kv_cache, R.prim_value(25), R.prim_value(T.float32(1.0)), reshape485), out_sinfo=R.Tensor((batch_size, 32, 80), dtype="float16"))
            reshape486 = R.call_tir(cls.reshape2, (lv611,), out_sinfo=R.Tensor((batch_size, 1, 32, 80), dtype="float16"))
            reshape487 = R.call_tir(cls.reshape3, (reshape486,), out_sinfo=R.Tensor((batch_size, 1, 2560), dtype="float16"))
            lv50_1 = R.call_tir(cls.fused_dequantize2_fused_NT_matmul1_add1_add2, (gpt_neox_layers_25_attention_dense_q_weight4, gpt_neox_layers_25_attention_dense_q_scale4, reshape487, gpt_neox_layers_25_attention_dense_bias4, lv49_1), out_sinfo=R.Tensor((batch_size, 1, 2560), dtype="float16"))
            layer_norm246 = R.call_tir(cls.layer_norm, (lv50_1, gpt_neox_layers_25_post_attention_layernorm_weight4, gpt_neox_layers_25_post_attention_layernorm_bias4), out_sinfo=R.Tensor((batch_size, 1, 2560), dtype="float16"))
            lv51 = R.call_tir(cls.fused_dequantize3_fused_NT_matmul2_add3_gelu_cast, (gpt_neox_layers_25_mlp_dense_h_to_4h_q_weight4, gpt_neox_layers_25_mlp_dense_h_to_4h_q_scale4, layer_norm246, gpt_neox_layers_25_mlp_dense_h_to_4h_bias4), out_sinfo=R.Tensor((batch_size, 1, 10240), dtype="float16"))
            lv51_1 = R.call_tir(cls.fused_dequantize4_fused_NT_matmul3_add4_cast1_add2, (gpt_neox_layers_25_mlp_dense_4h_to_h_q_weight4, gpt_neox_layers_25_mlp_dense_4h_to_h_q_scale4, lv51, gpt_neox_layers_25_mlp_dense_4h_to_h_bias4, lv50_1), out_sinfo=R.Tensor((batch_size, 1, 2560), dtype="float16"))
            layer_norm247 = R.call_tir(cls.layer_norm, (lv51_1, gpt_neox_layers_26_input_layernorm_weight4, gpt_neox_layers_26_input_layernorm_bias4), out_sinfo=R.Tensor((batch_size, 1, 2560), dtype="float16"))
            lv52 = R.call_tir(cls.fused_dequantize1_fused_NT_matmul_add, (gpt_neox_layers_26_attention_query_key_value_q_weight4, gpt_neox_layers_26_attention_query_key_value_q_scale4, layer_norm247, gpt_neox_layers_26_attention_query_key_value_bias4), out_sinfo=R.Tensor((batch_size, 1, 7680), dtype="float16"))
            reshape488 = R.call_tir(cls.reshape, (lv52,), out_sinfo=R.Tensor((batch_size, 1, 96, 80), dtype="float16"))
            reshape489 = R.call_tir(cls.reshape1, (reshape488,), out_sinfo=R.Tensor((batch_size, 96, 80), dtype="float16"))
            lv616 = R.call_dps_packed("vm.builtin.attention_kv_cache_attention_with_fused_qkv", (paged_kv_cache, R.prim_value(26), R.prim_value(T.float32(1.0)), reshape489), out_sinfo=R.Tensor((batch_size, 32, 80), dtype="float16"))
            reshape490 = R.call_tir(cls.reshape2, (lv616,), out_sinfo=R.Tensor((batch_size, 1, 32, 80), dtype="float16"))
            reshape491 = R.call_tir(cls.reshape3, (reshape490,), out_sinfo=R.Tensor((batch_size, 1, 2560), dtype="float16"))
            lv52_1 = R.call_tir(cls.fused_dequantize2_fused_NT_matmul1_add1_add2, (gpt_neox_layers_26_attention_dense_q_weight4, gpt_neox_layers_26_attention_dense_q_scale4, reshape491, gpt_neox_layers_26_attention_dense_bias4, lv51_1), out_sinfo=R.Tensor((batch_size, 1, 2560), dtype="float16"))
            layer_norm248 = R.call_tir(cls.layer_norm, (lv52_1, gpt_neox_layers_26_post_attention_layernorm_weight4, gpt_neox_layers_26_post_attention_layernorm_bias4), out_sinfo=R.Tensor((batch_size, 1, 2560), dtype="float16"))
            lv53 = R.call_tir(cls.fused_dequantize3_fused_NT_matmul2_add3_gelu_cast, (gpt_neox_layers_26_mlp_dense_h_to_4h_q_weight4, gpt_neox_layers_26_mlp_dense_h_to_4h_q_scale4, layer_norm248, gpt_neox_layers_26_mlp_dense_h_to_4h_bias4), out_sinfo=R.Tensor((batch_size, 1, 10240), dtype="float16"))
            lv53_1 = R.call_tir(cls.fused_dequantize4_fused_NT_matmul3_add4_cast1_add2, (gpt_neox_layers_26_mlp_dense_4h_to_h_q_weight4, gpt_neox_layers_26_mlp_dense_4h_to_h_q_scale4, lv53, gpt_neox_layers_26_mlp_dense_4h_to_h_bias4, lv52_1), out_sinfo=R.Tensor((batch_size, 1, 2560), dtype="float16"))
            layer_norm249 = R.call_tir(cls.layer_norm, (lv53_1, gpt_neox_layers_27_input_layernorm_weight4, gpt_neox_layers_27_input_layernorm_bias4), out_sinfo=R.Tensor((batch_size, 1, 2560), dtype="float16"))
            lv54 = R.call_tir(cls.fused_dequantize1_fused_NT_matmul_add, (gpt_neox_layers_27_attention_query_key_value_q_weight4, gpt_neox_layers_27_attention_query_key_value_q_scale4, layer_norm249, gpt_neox_layers_27_attention_query_key_value_bias4), out_sinfo=R.Tensor((batch_size, 1, 7680), dtype="float16"))
            reshape492 = R.call_tir(cls.reshape, (lv54,), out_sinfo=R.Tensor((batch_size, 1, 96, 80), dtype="float16"))
            reshape493 = R.call_tir(cls.reshape1, (reshape492,), out_sinfo=R.Tensor((batch_size, 96, 80), dtype="float16"))
            lv621 = R.call_dps_packed("vm.builtin.attention_kv_cache_attention_with_fused_qkv", (paged_kv_cache, R.prim_value(27), R.prim_value(T.float32(1.0)), reshape493), out_sinfo=R.Tensor((batch_size, 32, 80), dtype="float16"))
            reshape494 = R.call_tir(cls.reshape2, (lv621,), out_sinfo=R.Tensor((batch_size, 1, 32, 80), dtype="float16"))
            reshape495 = R.call_tir(cls.reshape3, (reshape494,), out_sinfo=R.Tensor((batch_size, 1, 2560), dtype="float16"))
            lv54_1 = R.call_tir(cls.fused_dequantize2_fused_NT_matmul1_add1_add2, (gpt_neox_layers_27_attention_dense_q_weight4, gpt_neox_layers_27_attention_dense_q_scale4, reshape495, gpt_neox_layers_27_attention_dense_bias4, lv53_1), out_sinfo=R.Tensor((batch_size, 1, 2560), dtype="float16"))
            layer_norm250 = R.call_tir(cls.layer_norm, (lv54_1, gpt_neox_layers_27_post_attention_layernorm_weight4, gpt_neox_layers_27_post_attention_layernorm_bias4), out_sinfo=R.Tensor((batch_size, 1, 2560), dtype="float16"))
            lv55 = R.call_tir(cls.fused_dequantize3_fused_NT_matmul2_add3_gelu_cast, (gpt_neox_layers_27_mlp_dense_h_to_4h_q_weight4, gpt_neox_layers_27_mlp_dense_h_to_4h_q_scale4, layer_norm250, gpt_neox_layers_27_mlp_dense_h_to_4h_bias4), out_sinfo=R.Tensor((batch_size, 1, 10240), dtype="float16"))
            lv55_1 = R.call_tir(cls.fused_dequantize4_fused_NT_matmul3_add4_cast1_add2, (gpt_neox_layers_27_mlp_dense_4h_to_h_q_weight4, gpt_neox_layers_27_mlp_dense_4h_to_h_q_scale4, lv55, gpt_neox_layers_27_mlp_dense_4h_to_h_bias4, lv54_1), out_sinfo=R.Tensor((batch_size, 1, 2560), dtype="float16"))
            layer_norm251 = R.call_tir(cls.layer_norm, (lv55_1, gpt_neox_layers_28_input_layernorm_weight4, gpt_neox_layers_28_input_layernorm_bias4), out_sinfo=R.Tensor((batch_size, 1, 2560), dtype="float16"))
            lv56 = R.call_tir(cls.fused_dequantize1_fused_NT_matmul_add, (gpt_neox_layers_28_attention_query_key_value_q_weight4, gpt_neox_layers_28_attention_query_key_value_q_scale4, layer_norm251, gpt_neox_layers_28_attention_query_key_value_bias4), out_sinfo=R.Tensor((batch_size, 1, 7680), dtype="float16"))
            reshape496 = R.call_tir(cls.reshape, (lv56,), out_sinfo=R.Tensor((batch_size, 1, 96, 80), dtype="float16"))
            reshape497 = R.call_tir(cls.reshape1, (reshape496,), out_sinfo=R.Tensor((batch_size, 96, 80), dtype="float16"))
            lv626 = R.call_dps_packed("vm.builtin.attention_kv_cache_attention_with_fused_qkv", (paged_kv_cache, R.prim_value(28), R.prim_value(T.float32(1.0)), reshape497), out_sinfo=R.Tensor((batch_size, 32, 80), dtype="float16"))
            reshape498 = R.call_tir(cls.reshape2, (lv626,), out_sinfo=R.Tensor((batch_size, 1, 32, 80), dtype="float16"))
            reshape499 = R.call_tir(cls.reshape3, (reshape498,), out_sinfo=R.Tensor((batch_size, 1, 2560), dtype="float16"))
            lv56_1 = R.call_tir(cls.fused_dequantize2_fused_NT_matmul1_add1_add2, (gpt_neox_layers_28_attention_dense_q_weight4, gpt_neox_layers_28_attention_dense_q_scale4, reshape499, gpt_neox_layers_28_attention_dense_bias4, lv55_1), out_sinfo=R.Tensor((batch_size, 1, 2560), dtype="float16"))
            layer_norm252 = R.call_tir(cls.layer_norm, (lv56_1, gpt_neox_layers_28_post_attention_layernorm_weight4, gpt_neox_layers_28_post_attention_layernorm_bias4), out_sinfo=R.Tensor((batch_size, 1, 2560), dtype="float16"))
            lv57 = R.call_tir(cls.fused_dequantize3_fused_NT_matmul2_add3_gelu_cast, (gpt_neox_layers_28_mlp_dense_h_to_4h_q_weight4, gpt_neox_layers_28_mlp_dense_h_to_4h_q_scale4, layer_norm252, gpt_neox_layers_28_mlp_dense_h_to_4h_bias4), out_sinfo=R.Tensor((batch_size, 1, 10240), dtype="float16"))
            lv57_1 = R.call_tir(cls.fused_dequantize4_fused_NT_matmul3_add4_cast1_add2, (gpt_neox_layers_28_mlp_dense_4h_to_h_q_weight4, gpt_neox_layers_28_mlp_dense_4h_to_h_q_scale4, lv57, gpt_neox_layers_28_mlp_dense_4h_to_h_bias4, lv56_1), out_sinfo=R.Tensor((batch_size, 1, 2560), dtype="float16"))
            layer_norm253 = R.call_tir(cls.layer_norm, (lv57_1, gpt_neox_layers_29_input_layernorm_weight4, gpt_neox_layers_29_input_layernorm_bias4), out_sinfo=R.Tensor((batch_size, 1, 2560), dtype="float16"))
            lv58 = R.call_tir(cls.fused_dequantize1_fused_NT_matmul_add, (gpt_neox_layers_29_attention_query_key_value_q_weight4, gpt_neox_layers_29_attention_query_key_value_q_scale4, layer_norm253, gpt_neox_layers_29_attention_query_key_value_bias4), out_sinfo=R.Tensor((batch_size, 1, 7680), dtype="float16"))
            reshape500 = R.call_tir(cls.reshape, (lv58,), out_sinfo=R.Tensor((batch_size, 1, 96, 80), dtype="float16"))
            reshape501 = R.call_tir(cls.reshape1, (reshape500,), out_sinfo=R.Tensor((batch_size, 96, 80), dtype="float16"))
            lv631 = R.call_dps_packed("vm.builtin.attention_kv_cache_attention_with_fused_qkv", (paged_kv_cache, R.prim_value(29), R.prim_value(T.float32(1.0)), reshape501), out_sinfo=R.Tensor((batch_size, 32, 80), dtype="float16"))
            reshape502 = R.call_tir(cls.reshape2, (lv631,), out_sinfo=R.Tensor((batch_size, 1, 32, 80), dtype="float16"))
            reshape503 = R.call_tir(cls.reshape3, (reshape502,), out_sinfo=R.Tensor((batch_size, 1, 2560), dtype="float16"))
            lv58_1 = R.call_tir(cls.fused_dequantize2_fused_NT_matmul1_add1_add2, (gpt_neox_layers_29_attention_dense_q_weight4, gpt_neox_layers_29_attention_dense_q_scale4, reshape503, gpt_neox_layers_29_attention_dense_bias4, lv57_1), out_sinfo=R.Tensor((batch_size, 1, 2560), dtype="float16"))
            layer_norm254 = R.call_tir(cls.layer_norm, (lv58_1, gpt_neox_layers_29_post_attention_layernorm_weight4, gpt_neox_layers_29_post_attention_layernorm_bias4), out_sinfo=R.Tensor((batch_size, 1, 2560), dtype="float16"))
            lv59 = R.call_tir(cls.fused_dequantize3_fused_NT_matmul2_add3_gelu_cast, (gpt_neox_layers_29_mlp_dense_h_to_4h_q_weight4, gpt_neox_layers_29_mlp_dense_h_to_4h_q_scale4, layer_norm254, gpt_neox_layers_29_mlp_dense_h_to_4h_bias4), out_sinfo=R.Tensor((batch_size, 1, 10240), dtype="float16"))
            lv59_1 = R.call_tir(cls.fused_dequantize4_fused_NT_matmul3_add4_cast1_add2, (gpt_neox_layers_29_mlp_dense_4h_to_h_q_weight4, gpt_neox_layers_29_mlp_dense_4h_to_h_q_scale4, lv59, gpt_neox_layers_29_mlp_dense_4h_to_h_bias4, lv58_1), out_sinfo=R.Tensor((batch_size, 1, 2560), dtype="float16"))
            layer_norm255 = R.call_tir(cls.layer_norm, (lv59_1, gpt_neox_layers_30_input_layernorm_weight4, gpt_neox_layers_30_input_layernorm_bias4), out_sinfo=R.Tensor((batch_size, 1, 2560), dtype="float16"))
            lv60 = R.call_tir(cls.fused_dequantize1_fused_NT_matmul_add, (gpt_neox_layers_30_attention_query_key_value_q_weight4, gpt_neox_layers_30_attention_query_key_value_q_scale4, layer_norm255, gpt_neox_layers_30_attention_query_key_value_bias4), out_sinfo=R.Tensor((batch_size, 1, 7680), dtype="float16"))
            reshape504 = R.call_tir(cls.reshape, (lv60,), out_sinfo=R.Tensor((batch_size, 1, 96, 80), dtype="float16"))
            reshape505 = R.call_tir(cls.reshape1, (reshape504,), out_sinfo=R.Tensor((batch_size, 96, 80), dtype="float16"))
            lv636 = R.call_dps_packed("vm.builtin.attention_kv_cache_attention_with_fused_qkv", (paged_kv_cache, R.prim_value(30), R.prim_value(T.float32(1.0)), reshape505), out_sinfo=R.Tensor((batch_size, 32, 80), dtype="float16"))
            reshape506 = R.call_tir(cls.reshape2, (lv636,), out_sinfo=R.Tensor((batch_size, 1, 32, 80), dtype="float16"))
            reshape507 = R.call_tir(cls.reshape3, (reshape506,), out_sinfo=R.Tensor((batch_size, 1, 2560), dtype="float16"))
            lv60_1 = R.call_tir(cls.fused_dequantize2_fused_NT_matmul1_add1_add2, (gpt_neox_layers_30_attention_dense_q_weight4, gpt_neox_layers_30_attention_dense_q_scale4, reshape507, gpt_neox_layers_30_attention_dense_bias4, lv59_1), out_sinfo=R.Tensor((batch_size, 1, 2560), dtype="float16"))
            layer_norm256 = R.call_tir(cls.layer_norm, (lv60_1, gpt_neox_layers_30_post_attention_layernorm_weight4, gpt_neox_layers_30_post_attention_layernorm_bias4), out_sinfo=R.Tensor((batch_size, 1, 2560), dtype="float16"))
            lv61 = R.call_tir(cls.fused_dequantize3_fused_NT_matmul2_add3_gelu_cast, (gpt_neox_layers_30_mlp_dense_h_to_4h_q_weight4, gpt_neox_layers_30_mlp_dense_h_to_4h_q_scale4, layer_norm256, gpt_neox_layers_30_mlp_dense_h_to_4h_bias4), out_sinfo=R.Tensor((batch_size, 1, 10240), dtype="float16"))
            lv61_1 = R.call_tir(cls.fused_dequantize4_fused_NT_matmul3_add4_cast1_add2, (gpt_neox_layers_30_mlp_dense_4h_to_h_q_weight4, gpt_neox_layers_30_mlp_dense_4h_to_h_q_scale4, lv61, gpt_neox_layers_30_mlp_dense_4h_to_h_bias4, lv60_1), out_sinfo=R.Tensor((batch_size, 1, 2560), dtype="float16"))
            layer_norm257 = R.call_tir(cls.layer_norm, (lv61_1, gpt_neox_layers_31_input_layernorm_weight4, gpt_neox_layers_31_input_layernorm_bias4), out_sinfo=R.Tensor((batch_size, 1, 2560), dtype="float16"))
            lv62 = R.call_tir(cls.fused_dequantize1_fused_NT_matmul_add, (gpt_neox_layers_31_attention_query_key_value_q_weight4, gpt_neox_layers_31_attention_query_key_value_q_scale4, layer_norm257, gpt_neox_layers_31_attention_query_key_value_bias4), out_sinfo=R.Tensor((batch_size, 1, 7680), dtype="float16"))
            reshape508 = R.call_tir(cls.reshape, (lv62,), out_sinfo=R.Tensor((batch_size, 1, 96, 80), dtype="float16"))
            reshape509 = R.call_tir(cls.reshape1, (reshape508,), out_sinfo=R.Tensor((batch_size, 96, 80), dtype="float16"))
            lv641 = R.call_dps_packed("vm.builtin.attention_kv_cache_attention_with_fused_qkv", (paged_kv_cache, R.prim_value(31), R.prim_value(T.float32(1.0)), reshape509), out_sinfo=R.Tensor((batch_size, 32, 80), dtype="float16"))
            reshape510 = R.call_tir(cls.reshape2, (lv641,), out_sinfo=R.Tensor((batch_size, 1, 32, 80), dtype="float16"))
            reshape511 = R.call_tir(cls.reshape3, (reshape510,), out_sinfo=R.Tensor((batch_size, 1, 2560), dtype="float16"))
            lv62_1 = R.call_tir(cls.fused_dequantize2_fused_NT_matmul1_add1_add2, (gpt_neox_layers_31_attention_dense_q_weight4, gpt_neox_layers_31_attention_dense_q_scale4, reshape511, gpt_neox_layers_31_attention_dense_bias4, lv61_1), out_sinfo=R.Tensor((batch_size, 1, 2560), dtype="float16"))
            layer_norm258 = R.call_tir(cls.layer_norm, (lv62_1, gpt_neox_layers_31_post_attention_layernorm_weight4, gpt_neox_layers_31_post_attention_layernorm_bias4), out_sinfo=R.Tensor((batch_size, 1, 2560), dtype="float16"))
            lv63 = R.call_tir(cls.fused_dequantize3_fused_NT_matmul2_add3_gelu_cast, (gpt_neox_layers_31_mlp_dense_h_to_4h_q_weight4, gpt_neox_layers_31_mlp_dense_h_to_4h_q_scale4, layer_norm258, gpt_neox_layers_31_mlp_dense_h_to_4h_bias4), out_sinfo=R.Tensor((batch_size, 1, 10240), dtype="float16"))
            lv63_1 = R.call_tir(cls.fused_dequantize4_fused_NT_matmul3_add4_cast1_add2, (gpt_neox_layers_31_mlp_dense_4h_to_h_q_weight4, gpt_neox_layers_31_mlp_dense_4h_to_h_q_scale4, lv63, gpt_neox_layers_31_mlp_dense_4h_to_h_bias4, lv62_1), out_sinfo=R.Tensor((batch_size, 1, 2560), dtype="float16"))
            layer_norm259 = R.call_tir(cls.layer_norm, (lv63_1, gpt_neox_final_layer_norm_weight4, gpt_neox_final_layer_norm_bias4), out_sinfo=R.Tensor((batch_size, 1, 2560), dtype="float16"))
            lv_2 = R.call_tir(cls.fused_dequantize_fused_NT_matmul4_cast2, (embed_out_q_weight4, embed_out_q_scale4, layer_norm259), out_sinfo=R.Tensor((batch_size, 1, vocab_size), dtype="float32"))
            gv4: R.Tuple(R.Tensor((batch_size, 1, vocab_size), dtype="float32"), R.Object) = lv_2, paged_kv_cache
            R.output(gv4)
        return gv4

    @R.function
    def batch_prefill(input_embeds: R.Tensor((1, "seq_len", 2560), dtype="float16"), logit_positions: R.Tensor(("batch_size",), dtype="int32"), paged_kv_cache: R.Object, packed_params: R.Tuple(R.Tensor(("vocab_size", 320), dtype="uint32"), R.Tensor(("vocab_size", 80), dtype="float16"), R.Tensor((2560,), dtype="float16"), R.Tensor((2560,), dtype="float16"), R.Tensor((2560,), dtype="float16"), R.Tensor((2560,), dtype="float16"), R.Tensor((7680, 320), dtype="uint32"), R.Tensor((7680, 80), dtype="float16"), R.Tensor((7680,), dtype="float16"), R.Tensor((2560, 320), dtype="uint32"), R.Tensor((2560, 80), dtype="float16"), R.Tensor((2560,), dtype="float16"), R.Tensor((10240, 320), dtype="uint32"), R.Tensor((10240, 80), dtype="float16"), R.Tensor((10240,), dtype="float32"), R.Tensor((2560, 1280), dtype="uint32"), R.Tensor((2560, 320), dtype="float16"), R.Tensor((2560,), dtype="float32"), R.Tensor((2560,), dtype="float16"), R.Tensor((2560,), dtype="float16"), R.Tensor((2560,), dtype="float16"), R.Tensor((2560,), dtype="float16"), R.Tensor((7680, 320), dtype="uint32"), R.Tensor((7680, 80), dtype="float16"), R.Tensor((7680,), dtype="float16"), R.Tensor((2560, 320), dtype="uint32"), R.Tensor((2560, 80), dtype="float16"), R.Tensor((2560,), dtype="float16"), R.Tensor((10240, 320), dtype="uint32"), R.Tensor((10240, 80), dtype="float16"), R.Tensor((10240,), dtype="float32"), R.Tensor((2560, 1280), dtype="uint32"), R.Tensor((2560, 320), dtype="float16"), R.Tensor((2560,), dtype="float32"), R.Tensor((2560,), dtype="float16"), R.Tensor((2560,), dtype="float16"), R.Tensor((2560,), dtype="float16"), R.Tensor((2560,), dtype="float16"), R.Tensor((7680, 320), dtype="uint32"), R.Tensor((7680, 80), dtype="float16"), R.Tensor((7680,), dtype="float16"), R.Tensor((2560, 320), dtype="uint32"), R.Tensor((2560, 80), dtype="float16"), R.Tensor((2560,), dtype="float16"), R.Tensor((10240, 320), dtype="uint32"), R.Tensor((10240, 80), dtype="float16"), R.Tensor((10240,), dtype="float32"), R.Tensor((2560, 1280), dtype="uint32"), R.Tensor((2560, 320), dtype="float16"), R.Tensor((2560,), dtype="float32"), R.Tensor((2560,), dtype="float16"), R.Tensor((2560,), dtype="float16"), R.Tensor((2560,), dtype="float16"), R.Tensor((2560,), dtype="float16"), R.Tensor((7680, 320), dtype="uint32"), R.Tensor((7680, 80), dtype="float16"), R.Tensor((7680,), dtype="float16"), R.Tensor((2560, 320), dtype="uint32"), R.Tensor((2560, 80), dtype="float16"), R.Tensor((2560,), dtype="float16"), R.Tensor((10240, 320), dtype="uint32"), R.Tensor((10240, 80), dtype="float16"), R.Tensor((10240,), dtype="float32"), R.Tensor((2560, 1280), dtype="uint32"), R.Tensor((2560, 320), dtype="float16"), R.Tensor((2560,), dtype="float32"), R.Tensor((2560,), dtype="float16"), R.Tensor((2560,), dtype="float16"), R.Tensor((2560,), dtype="float16"), R.Tensor((2560,), dtype="float16"), R.Tensor((7680, 320), dtype="uint32"), R.Tensor((7680, 80), dtype="float16"), R.Tensor((7680,), dtype="float16"), R.Tensor((2560, 320), dtype="uint32"), R.Tensor((2560, 80), dtype="float16"), R.Tensor((2560,), dtype="float16"), R.Tensor((10240, 320), dtype="uint32"), R.Tensor((10240, 80), dtype="float16"), R.Tensor((10240,), dtype="float32"), R.Tensor((2560, 1280), dtype="uint32"), R.Tensor((2560, 320), dtype="float16"), R.Tensor((2560,), dtype="float32"), R.Tensor((2560,), dtype="float16"), R.Tensor((2560,), dtype="float16"), R.Tensor((2560,), dtype="float16"), R.Tensor((2560,), dtype="float16"), R.Tensor((7680, 320), dtype="uint32"), R.Tensor((7680, 80), dtype="float16"), R.Tensor((7680,), dtype="float16"), R.Tensor((2560, 320), dtype="uint32"), R.Tensor((2560, 80), dtype="float16"), R.Tensor((2560,), dtype="float16"), R.Tensor((10240, 320), dtype="uint32"), R.Tensor((10240, 80), dtype="float16"), R.Tensor((10240,), dtype="float32"), R.Tensor((2560, 1280), dtype="uint32"), R.Tensor((2560, 320), dtype="float16"), R.Tensor((2560,), dtype="float32"), R.Tensor((2560,), dtype="float16"), R.Tensor((2560,), dtype="float16"), R.Tensor((2560,), dtype="float16"), R.Tensor((2560,), dtype="float16"), R.Tensor((7680, 320), dtype="uint32"), R.Tensor((7680, 80), dtype="float16"), R.Tensor((7680,), dtype="float16"), R.Tensor((2560, 320), dtype="uint32"), R.Tensor((2560, 80), dtype="float16"), R.Tensor((2560,), dtype="float16"), R.Tensor((10240, 320), dtype="uint32"), R.Tensor((10240, 80), dtype="float16"), R.Tensor((10240,), dtype="float32"), R.Tensor((2560, 1280), dtype="uint32"), R.Tensor((2560, 320), dtype="float16"), R.Tensor((2560,), dtype="float32"), R.Tensor((2560,), dtype="float16"), R.Tensor((2560,), dtype="float16"), R.Tensor((2560,), dtype="float16"), R.Tensor((2560,), dtype="float16"), R.Tensor((7680, 320), dtype="uint32"), R.Tensor((7680, 80), dtype="float16"), R.Tensor((7680,), dtype="float16"), R.Tensor((2560, 320), dtype="uint32"), R.Tensor((2560, 80), dtype="float16"), R.Tensor((2560,), dtype="float16"), R.Tensor((10240, 320), dtype="uint32"), R.Tensor((10240, 80), dtype="float16"), R.Tensor((10240,), dtype="float32"), R.Tensor((2560, 1280), dtype="uint32"), R.Tensor((2560, 320), dtype="float16"), R.Tensor((2560,), dtype="float32"), R.Tensor((2560,), dtype="float16"), R.Tensor((2560,), dtype="float16"), R.Tensor((2560,), dtype="float16"), R.Tensor((2560,), dtype="float16"), R.Tensor((7680, 320), dtype="uint32"), R.Tensor((7680, 80), dtype="float16"), R.Tensor((7680,), dtype="float16"), R.Tensor((2560, 320), dtype="uint32"), R.Tensor((2560, 80), dtype="float16"), R.Tensor((2560,), dtype="float16"), R.Tensor((10240, 320), dtype="uint32"), R.Tensor((10240, 80), dtype="float16"), R.Tensor((10240,), dtype="float32"), R.Tensor((2560, 1280), dtype="uint32"), R.Tensor((2560, 320), dtype="float16"), R.Tensor((2560,), dtype="float32"), R.Tensor((2560,), dtype="float16"), R.Tensor((2560,), dtype="float16"), R.Tensor((2560,), dtype="float16"), R.Tensor((2560,), dtype="float16"), R.Tensor((7680, 320), dtype="uint32"), R.Tensor((7680, 80), dtype="float16"), R.Tensor((7680,), dtype="float16"), R.Tensor((2560, 320), dtype="uint32"), R.Tensor((2560, 80), dtype="float16"), R.Tensor((2560,), dtype="float16"), R.Tensor((10240, 320), dtype="uint32"), R.Tensor((10240, 80), dtype="float16"), R.Tensor((10240,), dtype="float32"), R.Tensor((2560, 1280), dtype="uint32"), R.Tensor((2560, 320), dtype="float16"), R.Tensor((2560,), dtype="float32"), R.Tensor((2560,), dtype="float16"), R.Tensor((2560,), dtype="float16"), R.Tensor((2560,), dtype="float16"), R.Tensor((2560,), dtype="float16"), R.Tensor((7680, 320), dtype="uint32"), R.Tensor((7680, 80), dtype="float16"), R.Tensor((7680,), dtype="float16"), R.Tensor((2560, 320), dtype="uint32"), R.Tensor((2560, 80), dtype="float16"), R.Tensor((2560,), dtype="float16"), R.Tensor((10240, 320), dtype="uint32"), R.Tensor((10240, 80), dtype="float16"), R.Tensor((10240,), dtype="float32"), R.Tensor((2560, 1280), dtype="uint32"), R.Tensor((2560, 320), dtype="float16"), R.Tensor((2560,), dtype="float32"), R.Tensor((2560,), dtype="float16"), R.Tensor((2560,), dtype="float16"), R.Tensor((2560,), dtype="float16"), R.Tensor((2560,), dtype="float16"), R.Tensor((7680, 320), dtype="uint32"), R.Tensor((7680, 80), dtype="float16"), R.Tensor((7680,), dtype="float16"), R.Tensor((2560, 320), dtype="uint32"), R.Tensor((2560, 80), dtype="float16"), R.Tensor((2560,), dtype="float16"), R.Tensor((10240, 320), dtype="uint32"), R.Tensor((10240, 80), dtype="float16"), R.Tensor((10240,), dtype="float32"), R.Tensor((2560, 1280), dtype="uint32"), R.Tensor((2560, 320), dtype="float16"), R.Tensor((2560,), dtype="float32"), R.Tensor((2560,), dtype="float16"), R.Tensor((2560,), dtype="float16"), R.Tensor((2560,), dtype="float16"), R.Tensor((2560,), dtype="float16"), R.Tensor((7680, 320), dtype="uint32"), R.Tensor((7680, 80), dtype="float16"), R.Tensor((7680,), dtype="float16"), R.Tensor((2560, 320), dtype="uint32"), R.Tensor((2560, 80), dtype="float16"), R.Tensor((2560,), dtype="float16"), R.Tensor((10240, 320), dtype="uint32"), R.Tensor((10240, 80), dtype="float16"), R.Tensor((10240,), dtype="float32"), R.Tensor((2560, 1280), dtype="uint32"), R.Tensor((2560, 320), dtype="float16"), R.Tensor((2560,), dtype="float32"), R.Tensor((2560,), dtype="float16"), R.Tensor((2560,), dtype="float16"), R.Tensor((2560,), dtype="float16"), R.Tensor((2560,), dtype="float16"), R.Tensor((7680, 320), dtype="uint32"), R.Tensor((7680, 80), dtype="float16"), R.Tensor((7680,), dtype="float16"), R.Tensor((2560, 320), dtype="uint32"), R.Tensor((2560, 80), dtype="float16"), R.Tensor((2560,), dtype="float16"), R.Tensor((10240, 320), dtype="uint32"), R.Tensor((10240, 80), dtype="float16"), R.Tensor((10240,), dtype="float32"), R.Tensor((2560, 1280), dtype="uint32"), R.Tensor((2560, 320), dtype="float16"), R.Tensor((2560,), dtype="float32"), R.Tensor((2560,), dtype="float16"), R.Tensor((2560,), dtype="float16"), R.Tensor((2560,), dtype="float16"), R.Tensor((2560,), dtype="float16"), R.Tensor((7680, 320), dtype="uint32"), R.Tensor((7680, 80), dtype="float16"), R.Tensor((7680,), dtype="float16"), R.Tensor((2560, 320), dtype="uint32"), R.Tensor((2560, 80), dtype="float16"), R.Tensor((2560,), dtype="float16"), R.Tensor((10240, 320), dtype="uint32"), R.Tensor((10240, 80), dtype="float16"), R.Tensor((10240,), dtype="float32"), R.Tensor((2560, 1280), dtype="uint32"), R.Tensor((2560, 320), dtype="float16"), R.Tensor((2560,), dtype="float32"), R.Tensor((2560,), dtype="float16"), R.Tensor((2560,), dtype="float16"), R.Tensor((2560,), dtype="float16"), R.Tensor((2560,), dtype="float16"), R.Tensor((7680, 320), dtype="uint32"), R.Tensor((7680, 80), dtype="float16"), R.Tensor((7680,), dtype="float16"), R.Tensor((2560, 320), dtype="uint32"), R.Tensor((2560, 80), dtype="float16"), R.Tensor((2560,), dtype="float16"), R.Tensor((10240, 320), dtype="uint32"), R.Tensor((10240, 80), dtype="float16"), R.Tensor((10240,), dtype="float32"), R.Tensor((2560, 1280), dtype="uint32"), R.Tensor((2560, 320), dtype="float16"), R.Tensor((2560,), dtype="float32"), R.Tensor((2560,), dtype="float16"), R.Tensor((2560,), dtype="float16"), R.Tensor((2560,), dtype="float16"), R.Tensor((2560,), dtype="float16"), R.Tensor((7680, 320), dtype="uint32"), R.Tensor((7680, 80), dtype="float16"), R.Tensor((7680,), dtype="float16"), R.Tensor((2560, 320), dtype="uint32"), R.Tensor((2560, 80), dtype="float16"), R.Tensor((2560,), dtype="float16"), R.Tensor((10240, 320), dtype="uint32"), R.Tensor((10240, 80), dtype="float16"), R.Tensor((10240,), dtype="float32"), R.Tensor((2560, 1280), dtype="uint32"), R.Tensor((2560, 320), dtype="float16"), R.Tensor((2560,), dtype="float32"), R.Tensor((2560,), dtype="float16"), R.Tensor((2560,), dtype="float16"), R.Tensor((2560,), dtype="float16"), R.Tensor((2560,), dtype="float16"), R.Tensor((7680, 320), dtype="uint32"), R.Tensor((7680, 80), dtype="float16"), R.Tensor((7680,), dtype="float16"), R.Tensor((2560, 320), dtype="uint32"), R.Tensor((2560, 80), dtype="float16"), R.Tensor((2560,), dtype="float16"), R.Tensor((10240, 320), dtype="uint32"), R.Tensor((10240, 80), dtype="float16"), R.Tensor((10240,), dtype="float32"), R.Tensor((2560, 1280), dtype="uint32"), R.Tensor((2560, 320), dtype="float16"), R.Tensor((2560,), dtype="float32"), R.Tensor((2560,), dtype="float16"), R.Tensor((2560,), dtype="float16"), R.Tensor((2560,), dtype="float16"), R.Tensor((2560,), dtype="float16"), R.Tensor((7680, 320), dtype="uint32"), R.Tensor((7680, 80), dtype="float16"), R.Tensor((7680,), dtype="float16"), R.Tensor((2560, 320), dtype="uint32"), R.Tensor((2560, 80), dtype="float16"), R.Tensor((2560,), dtype="float16"), R.Tensor((10240, 320), dtype="uint32"), R.Tensor((10240, 80), dtype="float16"), R.Tensor((10240,), dtype="float32"), R.Tensor((2560, 1280), dtype="uint32"), R.Tensor((2560, 320), dtype="float16"), R.Tensor((2560,), dtype="float32"), R.Tensor((2560,), dtype="float16"), R.Tensor((2560,), dtype="float16"), R.Tensor((2560,), dtype="float16"), R.Tensor((2560,), dtype="float16"), R.Tensor((7680, 320), dtype="uint32"), R.Tensor((7680, 80), dtype="float16"), R.Tensor((7680,), dtype="float16"), R.Tensor((2560, 320), dtype="uint32"), R.Tensor((2560, 80), dtype="float16"), R.Tensor((2560,), dtype="float16"), R.Tensor((10240, 320), dtype="uint32"), R.Tensor((10240, 80), dtype="float16"), R.Tensor((10240,), dtype="float32"), R.Tensor((2560, 1280), dtype="uint32"), R.Tensor((2560, 320), dtype="float16"), R.Tensor((2560,), dtype="float32"), R.Tensor((2560,), dtype="float16"), R.Tensor((2560,), dtype="float16"), R.Tensor((2560,), dtype="float16"), R.Tensor((2560,), dtype="float16"), R.Tensor((7680, 320), dtype="uint32"), R.Tensor((7680, 80), dtype="float16"), R.Tensor((7680,), dtype="float16"), R.Tensor((2560, 320), dtype="uint32"), R.Tensor((2560, 80), dtype="float16"), R.Tensor((2560,), dtype="float16"), R.Tensor((10240, 320), dtype="uint32"), R.Tensor((10240, 80), dtype="float16"), R.Tensor((10240,), dtype="float32"), R.Tensor((2560, 1280), dtype="uint32"), R.Tensor((2560, 320), dtype="float16"), R.Tensor((2560,), dtype="float32"), R.Tensor((2560,), dtype="float16"), R.Tensor((2560,), dtype="float16"), R.Tensor((2560,), dtype="float16"), R.Tensor((2560,), dtype="float16"), R.Tensor((7680, 320), dtype="uint32"), R.Tensor((7680, 80), dtype="float16"), R.Tensor((7680,), dtype="float16"), R.Tensor((2560, 320), dtype="uint32"), R.Tensor((2560, 80), dtype="float16"), R.Tensor((2560,), dtype="float16"), R.Tensor((10240, 320), dtype="uint32"), R.Tensor((10240, 80), dtype="float16"), R.Tensor((10240,), dtype="float32"), R.Tensor((2560, 1280), dtype="uint32"), R.Tensor((2560, 320), dtype="float16"), R.Tensor((2560,), dtype="float32"), R.Tensor((2560,), dtype="float16"), R.Tensor((2560,), dtype="float16"), R.Tensor((2560,), dtype="float16"), R.Tensor((2560,), dtype="float16"), R.Tensor((7680, 320), dtype="uint32"), R.Tensor((7680, 80), dtype="float16"), R.Tensor((7680,), dtype="float16"), R.Tensor((2560, 320), dtype="uint32"), R.Tensor((2560, 80), dtype="float16"), R.Tensor((2560,), dtype="float16"), R.Tensor((10240, 320), dtype="uint32"), R.Tensor((10240, 80), dtype="float16"), R.Tensor((10240,), dtype="float32"), R.Tensor((2560, 1280), dtype="uint32"), R.Tensor((2560, 320), dtype="float16"), R.Tensor((2560,), dtype="float32"), R.Tensor((2560,), dtype="float16"), R.Tensor((2560,), dtype="float16"), R.Tensor((2560,), dtype="float16"), R.Tensor((2560,), dtype="float16"), R.Tensor((7680, 320), dtype="uint32"), R.Tensor((7680, 80), dtype="float16"), R.Tensor((7680,), dtype="float16"), R.Tensor((2560, 320), dtype="uint32"), R.Tensor((2560, 80), dtype="float16"), R.Tensor((2560,), dtype="float16"), R.Tensor((10240, 320), dtype="uint32"), R.Tensor((10240, 80), dtype="float16"), R.Tensor((10240,), dtype="float32"), R.Tensor((2560, 1280), dtype="uint32"), R.Tensor((2560, 320), dtype="float16"), R.Tensor((2560,), dtype="float32"), R.Tensor((2560,), dtype="float16"), R.Tensor((2560,), dtype="float16"), R.Tensor((2560,), dtype="float16"), R.Tensor((2560,), dtype="float16"), R.Tensor((7680, 320), dtype="uint32"), R.Tensor((7680, 80), dtype="float16"), R.Tensor((7680,), dtype="float16"), R.Tensor((2560, 320), dtype="uint32"), R.Tensor((2560, 80), dtype="float16"), R.Tensor((2560,), dtype="float16"), R.Tensor((10240, 320), dtype="uint32"), R.Tensor((10240, 80), dtype="float16"), R.Tensor((10240,), dtype="float32"), R.Tensor((2560, 1280), dtype="uint32"), R.Tensor((2560, 320), dtype="float16"), R.Tensor((2560,), dtype="float32"), R.Tensor((2560,), dtype="float16"), R.Tensor((2560,), dtype="float16"), R.Tensor((2560,), dtype="float16"), R.Tensor((2560,), dtype="float16"), R.Tensor((7680, 320), dtype="uint32"), R.Tensor((7680, 80), dtype="float16"), R.Tensor((7680,), dtype="float16"), R.Tensor((2560, 320), dtype="uint32"), R.Tensor((2560, 80), dtype="float16"), R.Tensor((2560,), dtype="float16"), R.Tensor((10240, 320), dtype="uint32"), R.Tensor((10240, 80), dtype="float16"), R.Tensor((10240,), dtype="float32"), R.Tensor((2560, 1280), dtype="uint32"), R.Tensor((2560, 320), dtype="float16"), R.Tensor((2560,), dtype="float32"), R.Tensor((2560,), dtype="float16"), R.Tensor((2560,), dtype="float16"), R.Tensor((2560,), dtype="float16"), R.Tensor((2560,), dtype="float16"), R.Tensor((7680, 320), dtype="uint32"), R.Tensor((7680, 80), dtype="float16"), R.Tensor((7680,), dtype="float16"), R.Tensor((2560, 320), dtype="uint32"), R.Tensor((2560, 80), dtype="float16"), R.Tensor((2560,), dtype="float16"), R.Tensor((10240, 320), dtype="uint32"), R.Tensor((10240, 80), dtype="float16"), R.Tensor((10240,), dtype="float32"), R.Tensor((2560, 1280), dtype="uint32"), R.Tensor((2560, 320), dtype="float16"), R.Tensor((2560,), dtype="float32"), R.Tensor((2560,), dtype="float16"), R.Tensor((2560,), dtype="float16"), R.Tensor((2560,), dtype="float16"), R.Tensor((2560,), dtype="float16"), R.Tensor((7680, 320), dtype="uint32"), R.Tensor((7680, 80), dtype="float16"), R.Tensor((7680,), dtype="float16"), R.Tensor((2560, 320), dtype="uint32"), R.Tensor((2560, 80), dtype="float16"), R.Tensor((2560,), dtype="float16"), R.Tensor((10240, 320), dtype="uint32"), R.Tensor((10240, 80), dtype="float16"), R.Tensor((10240,), dtype="float32"), R.Tensor((2560, 1280), dtype="uint32"), R.Tensor((2560, 320), dtype="float16"), R.Tensor((2560,), dtype="float32"), R.Tensor((2560,), dtype="float16"), R.Tensor((2560,), dtype="float16"), R.Tensor((2560,), dtype="float16"), R.Tensor((2560,), dtype="float16"), R.Tensor((7680, 320), dtype="uint32"), R.Tensor((7680, 80), dtype="float16"), R.Tensor((7680,), dtype="float16"), R.Tensor((2560, 320), dtype="uint32"), R.Tensor((2560, 80), dtype="float16"), R.Tensor((2560,), dtype="float16"), R.Tensor((10240, 320), dtype="uint32"), R.Tensor((10240, 80), dtype="float16"), R.Tensor((10240,), dtype="float32"), R.Tensor((2560, 1280), dtype="uint32"), R.Tensor((2560, 320), dtype="float16"), R.Tensor((2560,), dtype="float32"), R.Tensor((2560,), dtype="float16"), R.Tensor((2560,), dtype="float16"), R.Tensor((2560,), dtype="float16"), R.Tensor((2560,), dtype="float16"), R.Tensor((7680, 320), dtype="uint32"), R.Tensor((7680, 80), dtype="float16"), R.Tensor((7680,), dtype="float16"), R.Tensor((2560, 320), dtype="uint32"), R.Tensor((2560, 80), dtype="float16"), R.Tensor((2560,), dtype="float16"), R.Tensor((10240, 320), dtype="uint32"), R.Tensor((10240, 80), dtype="float16"), R.Tensor((10240,), dtype="float32"), R.Tensor((2560, 1280), dtype="uint32"), R.Tensor((2560, 320), dtype="float16"), R.Tensor((2560,), dtype="float32"), R.Tensor((2560,), dtype="float16"), R.Tensor((2560,), dtype="float16"), R.Tensor((2560,), dtype="float16"), R.Tensor((2560,), dtype="float16"), R.Tensor((7680, 320), dtype="uint32"), R.Tensor((7680, 80), dtype="float16"), R.Tensor((7680,), dtype="float16"), R.Tensor((2560, 320), dtype="uint32"), R.Tensor((2560, 80), dtype="float16"), R.Tensor((2560,), dtype="float16"), R.Tensor((10240, 320), dtype="uint32"), R.Tensor((10240, 80), dtype="float16"), R.Tensor((10240,), dtype="float32"), R.Tensor((2560, 1280), dtype="uint32"), R.Tensor((2560, 320), dtype="float16"), R.Tensor((2560,), dtype="float32"), R.Tensor((2560,), dtype="float16"), R.Tensor((2560,), dtype="float16"), R.Tensor((2560,), dtype="float16"), R.Tensor((2560,), dtype="float16"), R.Tensor((7680, 320), dtype="uint32"), R.Tensor((7680, 80), dtype="float16"), R.Tensor((7680,), dtype="float16"), R.Tensor((2560, 320), dtype="uint32"), R.Tensor((2560, 80), dtype="float16"), R.Tensor((2560,), dtype="float16"), R.Tensor((10240, 320), dtype="uint32"), R.Tensor((10240, 80), dtype="float16"), R.Tensor((10240,), dtype="float32"), R.Tensor((2560, 1280), dtype="uint32"), R.Tensor((2560, 320), dtype="float16"), R.Tensor((2560,), dtype="float32"), R.Tensor((2560,), dtype="float16"), R.Tensor((2560,), dtype="float16"), R.Tensor(("vocab_size", 320), dtype="uint32"), R.Tensor(("vocab_size", 80), dtype="float16"))) -> R.Tuple(R.Tensor((1, "batch_size", "vocab_size"), dtype="float32"), R.Object):
        batch_size = T.int64()
        vocab_size = T.int64()
        seq_len = T.int64()
        R.func_attr({"num_input": 3, "pipeline_parallel_stages": 1, "relax.memory_plan_dynamic_func_output": True, "tir_non_negative_var": ["vocab_size"], "tir_var_upper_bound": {"batch_size": 128, "seq_len": 2048, "total_seq_len": 2048}})
        cls = Module
        with R.dataflow():
            gpt_neox_layers_0_input_layernorm_weight3: R.Tensor((2560,), dtype="float16") = packed_params[2]
            gpt_neox_layers_0_input_layernorm_bias3: R.Tensor((2560,), dtype="float16") = packed_params[3]
            gpt_neox_layers_0_post_attention_layernorm_weight3: R.Tensor((2560,), dtype="float16") = packed_params[4]
            gpt_neox_layers_0_post_attention_layernorm_bias3: R.Tensor((2560,), dtype="float16") = packed_params[5]
            gpt_neox_layers_0_attention_query_key_value_q_weight3: R.Tensor((7680, 320), dtype="uint32") = packed_params[6]
            gpt_neox_layers_0_attention_query_key_value_q_scale3: R.Tensor((7680, 80), dtype="float16") = packed_params[7]
            gpt_neox_layers_0_attention_query_key_value_bias3: R.Tensor((7680,), dtype="float16") = packed_params[8]
            gpt_neox_layers_0_attention_dense_q_weight3: R.Tensor((2560, 320), dtype="uint32") = packed_params[9]
            gpt_neox_layers_0_attention_dense_q_scale3: R.Tensor((2560, 80), dtype="float16") = packed_params[10]
            gpt_neox_layers_0_attention_dense_bias3: R.Tensor((2560,), dtype="float16") = packed_params[11]
            gpt_neox_layers_0_mlp_dense_h_to_4h_q_weight3: R.Tensor((10240, 320), dtype="uint32") = packed_params[12]
            gpt_neox_layers_0_mlp_dense_h_to_4h_q_scale3: R.Tensor((10240, 80), dtype="float16") = packed_params[13]
            gpt_neox_layers_0_mlp_dense_h_to_4h_bias3: R.Tensor((10240,), dtype="float32") = packed_params[14]
            gpt_neox_layers_0_mlp_dense_4h_to_h_q_weight3: R.Tensor((2560, 1280), dtype="uint32") = packed_params[15]
            gpt_neox_layers_0_mlp_dense_4h_to_h_q_scale3: R.Tensor((2560, 320), dtype="float16") = packed_params[16]
            gpt_neox_layers_0_mlp_dense_4h_to_h_bias3: R.Tensor((2560,), dtype="float32") = packed_params[17]
            gpt_neox_layers_1_input_layernorm_weight3: R.Tensor((2560,), dtype="float16") = packed_params[18]
            gpt_neox_layers_1_input_layernorm_bias3: R.Tensor((2560,), dtype="float16") = packed_params[19]
            gpt_neox_layers_1_post_attention_layernorm_weight3: R.Tensor((2560,), dtype="float16") = packed_params[20]
            gpt_neox_layers_1_post_attention_layernorm_bias3: R.Tensor((2560,), dtype="float16") = packed_params[21]
            gpt_neox_layers_1_attention_query_key_value_q_weight3: R.Tensor((7680, 320), dtype="uint32") = packed_params[22]
            gpt_neox_layers_1_attention_query_key_value_q_scale3: R.Tensor((7680, 80), dtype="float16") = packed_params[23]
            gpt_neox_layers_1_attention_query_key_value_bias3: R.Tensor((7680,), dtype="float16") = packed_params[24]
            gpt_neox_layers_1_attention_dense_q_weight3: R.Tensor((2560, 320), dtype="uint32") = packed_params[25]
            gpt_neox_layers_1_attention_dense_q_scale3: R.Tensor((2560, 80), dtype="float16") = packed_params[26]
            gpt_neox_layers_1_attention_dense_bias3: R.Tensor((2560,), dtype="float16") = packed_params[27]
            gpt_neox_layers_1_mlp_dense_h_to_4h_q_weight3: R.Tensor((10240, 320), dtype="uint32") = packed_params[28]
            gpt_neox_layers_1_mlp_dense_h_to_4h_q_scale3: R.Tensor((10240, 80), dtype="float16") = packed_params[29]
            gpt_neox_layers_1_mlp_dense_h_to_4h_bias3: R.Tensor((10240,), dtype="float32") = packed_params[30]
            gpt_neox_layers_1_mlp_dense_4h_to_h_q_weight3: R.Tensor((2560, 1280), dtype="uint32") = packed_params[31]
            gpt_neox_layers_1_mlp_dense_4h_to_h_q_scale3: R.Tensor((2560, 320), dtype="float16") = packed_params[32]
            gpt_neox_layers_1_mlp_dense_4h_to_h_bias3: R.Tensor((2560,), dtype="float32") = packed_params[33]
            gpt_neox_layers_2_input_layernorm_weight3: R.Tensor((2560,), dtype="float16") = packed_params[34]
            gpt_neox_layers_2_input_layernorm_bias3: R.Tensor((2560,), dtype="float16") = packed_params[35]
            gpt_neox_layers_2_post_attention_layernorm_weight3: R.Tensor((2560,), dtype="float16") = packed_params[36]
            gpt_neox_layers_2_post_attention_layernorm_bias3: R.Tensor((2560,), dtype="float16") = packed_params[37]
            gpt_neox_layers_2_attention_query_key_value_q_weight3: R.Tensor((7680, 320), dtype="uint32") = packed_params[38]
            gpt_neox_layers_2_attention_query_key_value_q_scale3: R.Tensor((7680, 80), dtype="float16") = packed_params[39]
            gpt_neox_layers_2_attention_query_key_value_bias3: R.Tensor((7680,), dtype="float16") = packed_params[40]
            gpt_neox_layers_2_attention_dense_q_weight3: R.Tensor((2560, 320), dtype="uint32") = packed_params[41]
            gpt_neox_layers_2_attention_dense_q_scale3: R.Tensor((2560, 80), dtype="float16") = packed_params[42]
            gpt_neox_layers_2_attention_dense_bias3: R.Tensor((2560,), dtype="float16") = packed_params[43]
            gpt_neox_layers_2_mlp_dense_h_to_4h_q_weight3: R.Tensor((10240, 320), dtype="uint32") = packed_params[44]
            gpt_neox_layers_2_mlp_dense_h_to_4h_q_scale3: R.Tensor((10240, 80), dtype="float16") = packed_params[45]
            gpt_neox_layers_2_mlp_dense_h_to_4h_bias3: R.Tensor((10240,), dtype="float32") = packed_params[46]
            gpt_neox_layers_2_mlp_dense_4h_to_h_q_weight3: R.Tensor((2560, 1280), dtype="uint32") = packed_params[47]
            gpt_neox_layers_2_mlp_dense_4h_to_h_q_scale3: R.Tensor((2560, 320), dtype="float16") = packed_params[48]
            gpt_neox_layers_2_mlp_dense_4h_to_h_bias3: R.Tensor((2560,), dtype="float32") = packed_params[49]
            gpt_neox_layers_3_input_layernorm_weight3: R.Tensor((2560,), dtype="float16") = packed_params[50]
            gpt_neox_layers_3_input_layernorm_bias3: R.Tensor((2560,), dtype="float16") = packed_params[51]
            gpt_neox_layers_3_post_attention_layernorm_weight3: R.Tensor((2560,), dtype="float16") = packed_params[52]
            gpt_neox_layers_3_post_attention_layernorm_bias3: R.Tensor((2560,), dtype="float16") = packed_params[53]
            gpt_neox_layers_3_attention_query_key_value_q_weight3: R.Tensor((7680, 320), dtype="uint32") = packed_params[54]
            gpt_neox_layers_3_attention_query_key_value_q_scale3: R.Tensor((7680, 80), dtype="float16") = packed_params[55]
            gpt_neox_layers_3_attention_query_key_value_bias3: R.Tensor((7680,), dtype="float16") = packed_params[56]
            gpt_neox_layers_3_attention_dense_q_weight3: R.Tensor((2560, 320), dtype="uint32") = packed_params[57]
            gpt_neox_layers_3_attention_dense_q_scale3: R.Tensor((2560, 80), dtype="float16") = packed_params[58]
            gpt_neox_layers_3_attention_dense_bias3: R.Tensor((2560,), dtype="float16") = packed_params[59]
            gpt_neox_layers_3_mlp_dense_h_to_4h_q_weight3: R.Tensor((10240, 320), dtype="uint32") = packed_params[60]
            gpt_neox_layers_3_mlp_dense_h_to_4h_q_scale3: R.Tensor((10240, 80), dtype="float16") = packed_params[61]
            gpt_neox_layers_3_mlp_dense_h_to_4h_bias3: R.Tensor((10240,), dtype="float32") = packed_params[62]
            gpt_neox_layers_3_mlp_dense_4h_to_h_q_weight3: R.Tensor((2560, 1280), dtype="uint32") = packed_params[63]
            gpt_neox_layers_3_mlp_dense_4h_to_h_q_scale3: R.Tensor((2560, 320), dtype="float16") = packed_params[64]
            gpt_neox_layers_3_mlp_dense_4h_to_h_bias3: R.Tensor((2560,), dtype="float32") = packed_params[65]
            gpt_neox_layers_4_input_layernorm_weight3: R.Tensor((2560,), dtype="float16") = packed_params[66]
            gpt_neox_layers_4_input_layernorm_bias3: R.Tensor((2560,), dtype="float16") = packed_params[67]
            gpt_neox_layers_4_post_attention_layernorm_weight3: R.Tensor((2560,), dtype="float16") = packed_params[68]
            gpt_neox_layers_4_post_attention_layernorm_bias3: R.Tensor((2560,), dtype="float16") = packed_params[69]
            gpt_neox_layers_4_attention_query_key_value_q_weight3: R.Tensor((7680, 320), dtype="uint32") = packed_params[70]
            gpt_neox_layers_4_attention_query_key_value_q_scale3: R.Tensor((7680, 80), dtype="float16") = packed_params[71]
            gpt_neox_layers_4_attention_query_key_value_bias3: R.Tensor((7680,), dtype="float16") = packed_params[72]
            gpt_neox_layers_4_attention_dense_q_weight3: R.Tensor((2560, 320), dtype="uint32") = packed_params[73]
            gpt_neox_layers_4_attention_dense_q_scale3: R.Tensor((2560, 80), dtype="float16") = packed_params[74]
            gpt_neox_layers_4_attention_dense_bias3: R.Tensor((2560,), dtype="float16") = packed_params[75]
            gpt_neox_layers_4_mlp_dense_h_to_4h_q_weight3: R.Tensor((10240, 320), dtype="uint32") = packed_params[76]
            gpt_neox_layers_4_mlp_dense_h_to_4h_q_scale3: R.Tensor((10240, 80), dtype="float16") = packed_params[77]
            gpt_neox_layers_4_mlp_dense_h_to_4h_bias3: R.Tensor((10240,), dtype="float32") = packed_params[78]
            gpt_neox_layers_4_mlp_dense_4h_to_h_q_weight3: R.Tensor((2560, 1280), dtype="uint32") = packed_params[79]
            gpt_neox_layers_4_mlp_dense_4h_to_h_q_scale3: R.Tensor((2560, 320), dtype="float16") = packed_params[80]
            gpt_neox_layers_4_mlp_dense_4h_to_h_bias3: R.Tensor((2560,), dtype="float32") = packed_params[81]
            gpt_neox_layers_5_input_layernorm_weight3: R.Tensor((2560,), dtype="float16") = packed_params[82]
            gpt_neox_layers_5_input_layernorm_bias3: R.Tensor((2560,), dtype="float16") = packed_params[83]
            gpt_neox_layers_5_post_attention_layernorm_weight3: R.Tensor((2560,), dtype="float16") = packed_params[84]
            gpt_neox_layers_5_post_attention_layernorm_bias3: R.Tensor((2560,), dtype="float16") = packed_params[85]
            gpt_neox_layers_5_attention_query_key_value_q_weight3: R.Tensor((7680, 320), dtype="uint32") = packed_params[86]
            gpt_neox_layers_5_attention_query_key_value_q_scale3: R.Tensor((7680, 80), dtype="float16") = packed_params[87]
            gpt_neox_layers_5_attention_query_key_value_bias3: R.Tensor((7680,), dtype="float16") = packed_params[88]
            gpt_neox_layers_5_attention_dense_q_weight3: R.Tensor((2560, 320), dtype="uint32") = packed_params[89]
            gpt_neox_layers_5_attention_dense_q_scale3: R.Tensor((2560, 80), dtype="float16") = packed_params[90]
            gpt_neox_layers_5_attention_dense_bias3: R.Tensor((2560,), dtype="float16") = packed_params[91]
            gpt_neox_layers_5_mlp_dense_h_to_4h_q_weight3: R.Tensor((10240, 320), dtype="uint32") = packed_params[92]
            gpt_neox_layers_5_mlp_dense_h_to_4h_q_scale3: R.Tensor((10240, 80), dtype="float16") = packed_params[93]
            gpt_neox_layers_5_mlp_dense_h_to_4h_bias3: R.Tensor((10240,), dtype="float32") = packed_params[94]
            gpt_neox_layers_5_mlp_dense_4h_to_h_q_weight3: R.Tensor((2560, 1280), dtype="uint32") = packed_params[95]
            gpt_neox_layers_5_mlp_dense_4h_to_h_q_scale3: R.Tensor((2560, 320), dtype="float16") = packed_params[96]
            gpt_neox_layers_5_mlp_dense_4h_to_h_bias3: R.Tensor((2560,), dtype="float32") = packed_params[97]
            gpt_neox_layers_6_input_layernorm_weight3: R.Tensor((2560,), dtype="float16") = packed_params[98]
            gpt_neox_layers_6_input_layernorm_bias3: R.Tensor((2560,), dtype="float16") = packed_params[99]
            gpt_neox_layers_6_post_attention_layernorm_weight3: R.Tensor((2560,), dtype="float16") = packed_params[100]
            gpt_neox_layers_6_post_attention_layernorm_bias3: R.Tensor((2560,), dtype="float16") = packed_params[101]
            gpt_neox_layers_6_attention_query_key_value_q_weight3: R.Tensor((7680, 320), dtype="uint32") = packed_params[102]
            gpt_neox_layers_6_attention_query_key_value_q_scale3: R.Tensor((7680, 80), dtype="float16") = packed_params[103]
            gpt_neox_layers_6_attention_query_key_value_bias3: R.Tensor((7680,), dtype="float16") = packed_params[104]
            gpt_neox_layers_6_attention_dense_q_weight3: R.Tensor((2560, 320), dtype="uint32") = packed_params[105]
            gpt_neox_layers_6_attention_dense_q_scale3: R.Tensor((2560, 80), dtype="float16") = packed_params[106]
            gpt_neox_layers_6_attention_dense_bias3: R.Tensor((2560,), dtype="float16") = packed_params[107]
            gpt_neox_layers_6_mlp_dense_h_to_4h_q_weight3: R.Tensor((10240, 320), dtype="uint32") = packed_params[108]
            gpt_neox_layers_6_mlp_dense_h_to_4h_q_scale3: R.Tensor((10240, 80), dtype="float16") = packed_params[109]
            gpt_neox_layers_6_mlp_dense_h_to_4h_bias3: R.Tensor((10240,), dtype="float32") = packed_params[110]
            gpt_neox_layers_6_mlp_dense_4h_to_h_q_weight3: R.Tensor((2560, 1280), dtype="uint32") = packed_params[111]
            gpt_neox_layers_6_mlp_dense_4h_to_h_q_scale3: R.Tensor((2560, 320), dtype="float16") = packed_params[112]
            gpt_neox_layers_6_mlp_dense_4h_to_h_bias3: R.Tensor((2560,), dtype="float32") = packed_params[113]
            gpt_neox_layers_7_input_layernorm_weight3: R.Tensor((2560,), dtype="float16") = packed_params[114]
            gpt_neox_layers_7_input_layernorm_bias3: R.Tensor((2560,), dtype="float16") = packed_params[115]
            gpt_neox_layers_7_post_attention_layernorm_weight3: R.Tensor((2560,), dtype="float16") = packed_params[116]
            gpt_neox_layers_7_post_attention_layernorm_bias3: R.Tensor((2560,), dtype="float16") = packed_params[117]
            gpt_neox_layers_7_attention_query_key_value_q_weight3: R.Tensor((7680, 320), dtype="uint32") = packed_params[118]
            gpt_neox_layers_7_attention_query_key_value_q_scale3: R.Tensor((7680, 80), dtype="float16") = packed_params[119]
            gpt_neox_layers_7_attention_query_key_value_bias3: R.Tensor((7680,), dtype="float16") = packed_params[120]
            gpt_neox_layers_7_attention_dense_q_weight3: R.Tensor((2560, 320), dtype="uint32") = packed_params[121]
            gpt_neox_layers_7_attention_dense_q_scale3: R.Tensor((2560, 80), dtype="float16") = packed_params[122]
            gpt_neox_layers_7_attention_dense_bias3: R.Tensor((2560,), dtype="float16") = packed_params[123]
            gpt_neox_layers_7_mlp_dense_h_to_4h_q_weight3: R.Tensor((10240, 320), dtype="uint32") = packed_params[124]
            gpt_neox_layers_7_mlp_dense_h_to_4h_q_scale3: R.Tensor((10240, 80), dtype="float16") = packed_params[125]
            gpt_neox_layers_7_mlp_dense_h_to_4h_bias3: R.Tensor((10240,), dtype="float32") = packed_params[126]
            gpt_neox_layers_7_mlp_dense_4h_to_h_q_weight3: R.Tensor((2560, 1280), dtype="uint32") = packed_params[127]
            gpt_neox_layers_7_mlp_dense_4h_to_h_q_scale3: R.Tensor((2560, 320), dtype="float16") = packed_params[128]
            gpt_neox_layers_7_mlp_dense_4h_to_h_bias3: R.Tensor((2560,), dtype="float32") = packed_params[129]
            gpt_neox_layers_8_input_layernorm_weight3: R.Tensor((2560,), dtype="float16") = packed_params[130]
            gpt_neox_layers_8_input_layernorm_bias3: R.Tensor((2560,), dtype="float16") = packed_params[131]
            gpt_neox_layers_8_post_attention_layernorm_weight3: R.Tensor((2560,), dtype="float16") = packed_params[132]
            gpt_neox_layers_8_post_attention_layernorm_bias3: R.Tensor((2560,), dtype="float16") = packed_params[133]
            gpt_neox_layers_8_attention_query_key_value_q_weight3: R.Tensor((7680, 320), dtype="uint32") = packed_params[134]
            gpt_neox_layers_8_attention_query_key_value_q_scale3: R.Tensor((7680, 80), dtype="float16") = packed_params[135]
            gpt_neox_layers_8_attention_query_key_value_bias3: R.Tensor((7680,), dtype="float16") = packed_params[136]
            gpt_neox_layers_8_attention_dense_q_weight3: R.Tensor((2560, 320), dtype="uint32") = packed_params[137]
            gpt_neox_layers_8_attention_dense_q_scale3: R.Tensor((2560, 80), dtype="float16") = packed_params[138]
            gpt_neox_layers_8_attention_dense_bias3: R.Tensor((2560,), dtype="float16") = packed_params[139]
            gpt_neox_layers_8_mlp_dense_h_to_4h_q_weight3: R.Tensor((10240, 320), dtype="uint32") = packed_params[140]
            gpt_neox_layers_8_mlp_dense_h_to_4h_q_scale3: R.Tensor((10240, 80), dtype="float16") = packed_params[141]
            gpt_neox_layers_8_mlp_dense_h_to_4h_bias3: R.Tensor((10240,), dtype="float32") = packed_params[142]
            gpt_neox_layers_8_mlp_dense_4h_to_h_q_weight3: R.Tensor((2560, 1280), dtype="uint32") = packed_params[143]
            gpt_neox_layers_8_mlp_dense_4h_to_h_q_scale3: R.Tensor((2560, 320), dtype="float16") = packed_params[144]
            gpt_neox_layers_8_mlp_dense_4h_to_h_bias3: R.Tensor((2560,), dtype="float32") = packed_params[145]
            gpt_neox_layers_9_input_layernorm_weight3: R.Tensor((2560,), dtype="float16") = packed_params[146]
            gpt_neox_layers_9_input_layernorm_bias3: R.Tensor((2560,), dtype="float16") = packed_params[147]
            gpt_neox_layers_9_post_attention_layernorm_weight3: R.Tensor((2560,), dtype="float16") = packed_params[148]
            gpt_neox_layers_9_post_attention_layernorm_bias3: R.Tensor((2560,), dtype="float16") = packed_params[149]
            gpt_neox_layers_9_attention_query_key_value_q_weight3: R.Tensor((7680, 320), dtype="uint32") = packed_params[150]
            gpt_neox_layers_9_attention_query_key_value_q_scale3: R.Tensor((7680, 80), dtype="float16") = packed_params[151]
            gpt_neox_layers_9_attention_query_key_value_bias3: R.Tensor((7680,), dtype="float16") = packed_params[152]
            gpt_neox_layers_9_attention_dense_q_weight3: R.Tensor((2560, 320), dtype="uint32") = packed_params[153]
            gpt_neox_layers_9_attention_dense_q_scale3: R.Tensor((2560, 80), dtype="float16") = packed_params[154]
            gpt_neox_layers_9_attention_dense_bias3: R.Tensor((2560,), dtype="float16") = packed_params[155]
            gpt_neox_layers_9_mlp_dense_h_to_4h_q_weight3: R.Tensor((10240, 320), dtype="uint32") = packed_params[156]
            gpt_neox_layers_9_mlp_dense_h_to_4h_q_scale3: R.Tensor((10240, 80), dtype="float16") = packed_params[157]
            gpt_neox_layers_9_mlp_dense_h_to_4h_bias3: R.Tensor((10240,), dtype="float32") = packed_params[158]
            gpt_neox_layers_9_mlp_dense_4h_to_h_q_weight3: R.Tensor((2560, 1280), dtype="uint32") = packed_params[159]
            gpt_neox_layers_9_mlp_dense_4h_to_h_q_scale3: R.Tensor((2560, 320), dtype="float16") = packed_params[160]
            gpt_neox_layers_9_mlp_dense_4h_to_h_bias3: R.Tensor((2560,), dtype="float32") = packed_params[161]
            gpt_neox_layers_10_input_layernorm_weight3: R.Tensor((2560,), dtype="float16") = packed_params[162]
            gpt_neox_layers_10_input_layernorm_bias3: R.Tensor((2560,), dtype="float16") = packed_params[163]
            gpt_neox_layers_10_post_attention_layernorm_weight3: R.Tensor((2560,), dtype="float16") = packed_params[164]
            gpt_neox_layers_10_post_attention_layernorm_bias3: R.Tensor((2560,), dtype="float16") = packed_params[165]
            gpt_neox_layers_10_attention_query_key_value_q_weight3: R.Tensor((7680, 320), dtype="uint32") = packed_params[166]
            gpt_neox_layers_10_attention_query_key_value_q_scale3: R.Tensor((7680, 80), dtype="float16") = packed_params[167]
            gpt_neox_layers_10_attention_query_key_value_bias3: R.Tensor((7680,), dtype="float16") = packed_params[168]
            gpt_neox_layers_10_attention_dense_q_weight3: R.Tensor((2560, 320), dtype="uint32") = packed_params[169]
            gpt_neox_layers_10_attention_dense_q_scale3: R.Tensor((2560, 80), dtype="float16") = packed_params[170]
            gpt_neox_layers_10_attention_dense_bias3: R.Tensor((2560,), dtype="float16") = packed_params[171]
            gpt_neox_layers_10_mlp_dense_h_to_4h_q_weight3: R.Tensor((10240, 320), dtype="uint32") = packed_params[172]
            gpt_neox_layers_10_mlp_dense_h_to_4h_q_scale3: R.Tensor((10240, 80), dtype="float16") = packed_params[173]
            gpt_neox_layers_10_mlp_dense_h_to_4h_bias3: R.Tensor((10240,), dtype="float32") = packed_params[174]
            gpt_neox_layers_10_mlp_dense_4h_to_h_q_weight3: R.Tensor((2560, 1280), dtype="uint32") = packed_params[175]
            gpt_neox_layers_10_mlp_dense_4h_to_h_q_scale3: R.Tensor((2560, 320), dtype="float16") = packed_params[176]
            gpt_neox_layers_10_mlp_dense_4h_to_h_bias3: R.Tensor((2560,), dtype="float32") = packed_params[177]
            gpt_neox_layers_11_input_layernorm_weight3: R.Tensor((2560,), dtype="float16") = packed_params[178]
            gpt_neox_layers_11_input_layernorm_bias3: R.Tensor((2560,), dtype="float16") = packed_params[179]
            gpt_neox_layers_11_post_attention_layernorm_weight3: R.Tensor((2560,), dtype="float16") = packed_params[180]
            gpt_neox_layers_11_post_attention_layernorm_bias3: R.Tensor((2560,), dtype="float16") = packed_params[181]
            gpt_neox_layers_11_attention_query_key_value_q_weight3: R.Tensor((7680, 320), dtype="uint32") = packed_params[182]
            gpt_neox_layers_11_attention_query_key_value_q_scale3: R.Tensor((7680, 80), dtype="float16") = packed_params[183]
            gpt_neox_layers_11_attention_query_key_value_bias3: R.Tensor((7680,), dtype="float16") = packed_params[184]
            gpt_neox_layers_11_attention_dense_q_weight3: R.Tensor((2560, 320), dtype="uint32") = packed_params[185]
            gpt_neox_layers_11_attention_dense_q_scale3: R.Tensor((2560, 80), dtype="float16") = packed_params[186]
            gpt_neox_layers_11_attention_dense_bias3: R.Tensor((2560,), dtype="float16") = packed_params[187]
            gpt_neox_layers_11_mlp_dense_h_to_4h_q_weight3: R.Tensor((10240, 320), dtype="uint32") = packed_params[188]
            gpt_neox_layers_11_mlp_dense_h_to_4h_q_scale3: R.Tensor((10240, 80), dtype="float16") = packed_params[189]
            gpt_neox_layers_11_mlp_dense_h_to_4h_bias3: R.Tensor((10240,), dtype="float32") = packed_params[190]
            gpt_neox_layers_11_mlp_dense_4h_to_h_q_weight3: R.Tensor((2560, 1280), dtype="uint32") = packed_params[191]
            gpt_neox_layers_11_mlp_dense_4h_to_h_q_scale3: R.Tensor((2560, 320), dtype="float16") = packed_params[192]
            gpt_neox_layers_11_mlp_dense_4h_to_h_bias3: R.Tensor((2560,), dtype="float32") = packed_params[193]
            gpt_neox_layers_12_input_layernorm_weight3: R.Tensor((2560,), dtype="float16") = packed_params[194]
            gpt_neox_layers_12_input_layernorm_bias3: R.Tensor((2560,), dtype="float16") = packed_params[195]
            gpt_neox_layers_12_post_attention_layernorm_weight3: R.Tensor((2560,), dtype="float16") = packed_params[196]
            gpt_neox_layers_12_post_attention_layernorm_bias3: R.Tensor((2560,), dtype="float16") = packed_params[197]
            gpt_neox_layers_12_attention_query_key_value_q_weight3: R.Tensor((7680, 320), dtype="uint32") = packed_params[198]
            gpt_neox_layers_12_attention_query_key_value_q_scale3: R.Tensor((7680, 80), dtype="float16") = packed_params[199]
            gpt_neox_layers_12_attention_query_key_value_bias3: R.Tensor((7680,), dtype="float16") = packed_params[200]
            gpt_neox_layers_12_attention_dense_q_weight3: R.Tensor((2560, 320), dtype="uint32") = packed_params[201]
            gpt_neox_layers_12_attention_dense_q_scale3: R.Tensor((2560, 80), dtype="float16") = packed_params[202]
            gpt_neox_layers_12_attention_dense_bias3: R.Tensor((2560,), dtype="float16") = packed_params[203]
            gpt_neox_layers_12_mlp_dense_h_to_4h_q_weight3: R.Tensor((10240, 320), dtype="uint32") = packed_params[204]
            gpt_neox_layers_12_mlp_dense_h_to_4h_q_scale3: R.Tensor((10240, 80), dtype="float16") = packed_params[205]
            gpt_neox_layers_12_mlp_dense_h_to_4h_bias3: R.Tensor((10240,), dtype="float32") = packed_params[206]
            gpt_neox_layers_12_mlp_dense_4h_to_h_q_weight3: R.Tensor((2560, 1280), dtype="uint32") = packed_params[207]
            gpt_neox_layers_12_mlp_dense_4h_to_h_q_scale3: R.Tensor((2560, 320), dtype="float16") = packed_params[208]
            gpt_neox_layers_12_mlp_dense_4h_to_h_bias3: R.Tensor((2560,), dtype="float32") = packed_params[209]
            gpt_neox_layers_13_input_layernorm_weight3: R.Tensor((2560,), dtype="float16") = packed_params[210]
            gpt_neox_layers_13_input_layernorm_bias3: R.Tensor((2560,), dtype="float16") = packed_params[211]
            gpt_neox_layers_13_post_attention_layernorm_weight3: R.Tensor((2560,), dtype="float16") = packed_params[212]
            gpt_neox_layers_13_post_attention_layernorm_bias3: R.Tensor((2560,), dtype="float16") = packed_params[213]
            gpt_neox_layers_13_attention_query_key_value_q_weight3: R.Tensor((7680, 320), dtype="uint32") = packed_params[214]
            gpt_neox_layers_13_attention_query_key_value_q_scale3: R.Tensor((7680, 80), dtype="float16") = packed_params[215]
            gpt_neox_layers_13_attention_query_key_value_bias3: R.Tensor((7680,), dtype="float16") = packed_params[216]
            gpt_neox_layers_13_attention_dense_q_weight3: R.Tensor((2560, 320), dtype="uint32") = packed_params[217]
            gpt_neox_layers_13_attention_dense_q_scale3: R.Tensor((2560, 80), dtype="float16") = packed_params[218]
            gpt_neox_layers_13_attention_dense_bias3: R.Tensor((2560,), dtype="float16") = packed_params[219]
            gpt_neox_layers_13_mlp_dense_h_to_4h_q_weight3: R.Tensor((10240, 320), dtype="uint32") = packed_params[220]
            gpt_neox_layers_13_mlp_dense_h_to_4h_q_scale3: R.Tensor((10240, 80), dtype="float16") = packed_params[221]
            gpt_neox_layers_13_mlp_dense_h_to_4h_bias3: R.Tensor((10240,), dtype="float32") = packed_params[222]
            gpt_neox_layers_13_mlp_dense_4h_to_h_q_weight3: R.Tensor((2560, 1280), dtype="uint32") = packed_params[223]
            gpt_neox_layers_13_mlp_dense_4h_to_h_q_scale3: R.Tensor((2560, 320), dtype="float16") = packed_params[224]
            gpt_neox_layers_13_mlp_dense_4h_to_h_bias3: R.Tensor((2560,), dtype="float32") = packed_params[225]
            gpt_neox_layers_14_input_layernorm_weight3: R.Tensor((2560,), dtype="float16") = packed_params[226]
            gpt_neox_layers_14_input_layernorm_bias3: R.Tensor((2560,), dtype="float16") = packed_params[227]
            gpt_neox_layers_14_post_attention_layernorm_weight3: R.Tensor((2560,), dtype="float16") = packed_params[228]
            gpt_neox_layers_14_post_attention_layernorm_bias3: R.Tensor((2560,), dtype="float16") = packed_params[229]
            gpt_neox_layers_14_attention_query_key_value_q_weight3: R.Tensor((7680, 320), dtype="uint32") = packed_params[230]
            gpt_neox_layers_14_attention_query_key_value_q_scale3: R.Tensor((7680, 80), dtype="float16") = packed_params[231]
            gpt_neox_layers_14_attention_query_key_value_bias3: R.Tensor((7680,), dtype="float16") = packed_params[232]
            gpt_neox_layers_14_attention_dense_q_weight3: R.Tensor((2560, 320), dtype="uint32") = packed_params[233]
            gpt_neox_layers_14_attention_dense_q_scale3: R.Tensor((2560, 80), dtype="float16") = packed_params[234]
            gpt_neox_layers_14_attention_dense_bias3: R.Tensor((2560,), dtype="float16") = packed_params[235]
            gpt_neox_layers_14_mlp_dense_h_to_4h_q_weight3: R.Tensor((10240, 320), dtype="uint32") = packed_params[236]
            gpt_neox_layers_14_mlp_dense_h_to_4h_q_scale3: R.Tensor((10240, 80), dtype="float16") = packed_params[237]
            gpt_neox_layers_14_mlp_dense_h_to_4h_bias3: R.Tensor((10240,), dtype="float32") = packed_params[238]
            gpt_neox_layers_14_mlp_dense_4h_to_h_q_weight3: R.Tensor((2560, 1280), dtype="uint32") = packed_params[239]
            gpt_neox_layers_14_mlp_dense_4h_to_h_q_scale3: R.Tensor((2560, 320), dtype="float16") = packed_params[240]
            gpt_neox_layers_14_mlp_dense_4h_to_h_bias3: R.Tensor((2560,), dtype="float32") = packed_params[241]
            gpt_neox_layers_15_input_layernorm_weight3: R.Tensor((2560,), dtype="float16") = packed_params[242]
            gpt_neox_layers_15_input_layernorm_bias3: R.Tensor((2560,), dtype="float16") = packed_params[243]
            gpt_neox_layers_15_post_attention_layernorm_weight3: R.Tensor((2560,), dtype="float16") = packed_params[244]
            gpt_neox_layers_15_post_attention_layernorm_bias3: R.Tensor((2560,), dtype="float16") = packed_params[245]
            gpt_neox_layers_15_attention_query_key_value_q_weight3: R.Tensor((7680, 320), dtype="uint32") = packed_params[246]
            gpt_neox_layers_15_attention_query_key_value_q_scale3: R.Tensor((7680, 80), dtype="float16") = packed_params[247]
            gpt_neox_layers_15_attention_query_key_value_bias3: R.Tensor((7680,), dtype="float16") = packed_params[248]
            gpt_neox_layers_15_attention_dense_q_weight3: R.Tensor((2560, 320), dtype="uint32") = packed_params[249]
            gpt_neox_layers_15_attention_dense_q_scale3: R.Tensor((2560, 80), dtype="float16") = packed_params[250]
            gpt_neox_layers_15_attention_dense_bias3: R.Tensor((2560,), dtype="float16") = packed_params[251]
            gpt_neox_layers_15_mlp_dense_h_to_4h_q_weight3: R.Tensor((10240, 320), dtype="uint32") = packed_params[252]
            gpt_neox_layers_15_mlp_dense_h_to_4h_q_scale3: R.Tensor((10240, 80), dtype="float16") = packed_params[253]
            gpt_neox_layers_15_mlp_dense_h_to_4h_bias3: R.Tensor((10240,), dtype="float32") = packed_params[254]
            gpt_neox_layers_15_mlp_dense_4h_to_h_q_weight3: R.Tensor((2560, 1280), dtype="uint32") = packed_params[255]
            gpt_neox_layers_15_mlp_dense_4h_to_h_q_scale3: R.Tensor((2560, 320), dtype="float16") = packed_params[256]
            gpt_neox_layers_15_mlp_dense_4h_to_h_bias3: R.Tensor((2560,), dtype="float32") = packed_params[257]
            gpt_neox_layers_16_input_layernorm_weight3: R.Tensor((2560,), dtype="float16") = packed_params[258]
            gpt_neox_layers_16_input_layernorm_bias3: R.Tensor((2560,), dtype="float16") = packed_params[259]
            gpt_neox_layers_16_post_attention_layernorm_weight3: R.Tensor((2560,), dtype="float16") = packed_params[260]
            gpt_neox_layers_16_post_attention_layernorm_bias3: R.Tensor((2560,), dtype="float16") = packed_params[261]
            gpt_neox_layers_16_attention_query_key_value_q_weight3: R.Tensor((7680, 320), dtype="uint32") = packed_params[262]
            gpt_neox_layers_16_attention_query_key_value_q_scale3: R.Tensor((7680, 80), dtype="float16") = packed_params[263]
            gpt_neox_layers_16_attention_query_key_value_bias3: R.Tensor((7680,), dtype="float16") = packed_params[264]
            gpt_neox_layers_16_attention_dense_q_weight3: R.Tensor((2560, 320), dtype="uint32") = packed_params[265]
            gpt_neox_layers_16_attention_dense_q_scale3: R.Tensor((2560, 80), dtype="float16") = packed_params[266]
            gpt_neox_layers_16_attention_dense_bias3: R.Tensor((2560,), dtype="float16") = packed_params[267]
            gpt_neox_layers_16_mlp_dense_h_to_4h_q_weight3: R.Tensor((10240, 320), dtype="uint32") = packed_params[268]
            gpt_neox_layers_16_mlp_dense_h_to_4h_q_scale3: R.Tensor((10240, 80), dtype="float16") = packed_params[269]
            gpt_neox_layers_16_mlp_dense_h_to_4h_bias3: R.Tensor((10240,), dtype="float32") = packed_params[270]
            gpt_neox_layers_16_mlp_dense_4h_to_h_q_weight3: R.Tensor((2560, 1280), dtype="uint32") = packed_params[271]
            gpt_neox_layers_16_mlp_dense_4h_to_h_q_scale3: R.Tensor((2560, 320), dtype="float16") = packed_params[272]
            gpt_neox_layers_16_mlp_dense_4h_to_h_bias3: R.Tensor((2560,), dtype="float32") = packed_params[273]
            gpt_neox_layers_17_input_layernorm_weight3: R.Tensor((2560,), dtype="float16") = packed_params[274]
            gpt_neox_layers_17_input_layernorm_bias3: R.Tensor((2560,), dtype="float16") = packed_params[275]
            gpt_neox_layers_17_post_attention_layernorm_weight3: R.Tensor((2560,), dtype="float16") = packed_params[276]
            gpt_neox_layers_17_post_attention_layernorm_bias3: R.Tensor((2560,), dtype="float16") = packed_params[277]
            gpt_neox_layers_17_attention_query_key_value_q_weight3: R.Tensor((7680, 320), dtype="uint32") = packed_params[278]
            gpt_neox_layers_17_attention_query_key_value_q_scale3: R.Tensor((7680, 80), dtype="float16") = packed_params[279]
            gpt_neox_layers_17_attention_query_key_value_bias3: R.Tensor((7680,), dtype="float16") = packed_params[280]
            gpt_neox_layers_17_attention_dense_q_weight3: R.Tensor((2560, 320), dtype="uint32") = packed_params[281]
            gpt_neox_layers_17_attention_dense_q_scale3: R.Tensor((2560, 80), dtype="float16") = packed_params[282]
            gpt_neox_layers_17_attention_dense_bias3: R.Tensor((2560,), dtype="float16") = packed_params[283]
            gpt_neox_layers_17_mlp_dense_h_to_4h_q_weight3: R.Tensor((10240, 320), dtype="uint32") = packed_params[284]
            gpt_neox_layers_17_mlp_dense_h_to_4h_q_scale3: R.Tensor((10240, 80), dtype="float16") = packed_params[285]
            gpt_neox_layers_17_mlp_dense_h_to_4h_bias3: R.Tensor((10240,), dtype="float32") = packed_params[286]
            gpt_neox_layers_17_mlp_dense_4h_to_h_q_weight3: R.Tensor((2560, 1280), dtype="uint32") = packed_params[287]
            gpt_neox_layers_17_mlp_dense_4h_to_h_q_scale3: R.Tensor((2560, 320), dtype="float16") = packed_params[288]
            gpt_neox_layers_17_mlp_dense_4h_to_h_bias3: R.Tensor((2560,), dtype="float32") = packed_params[289]
            gpt_neox_layers_18_input_layernorm_weight3: R.Tensor((2560,), dtype="float16") = packed_params[290]
            gpt_neox_layers_18_input_layernorm_bias3: R.Tensor((2560,), dtype="float16") = packed_params[291]
            gpt_neox_layers_18_post_attention_layernorm_weight3: R.Tensor((2560,), dtype="float16") = packed_params[292]
            gpt_neox_layers_18_post_attention_layernorm_bias3: R.Tensor((2560,), dtype="float16") = packed_params[293]
            gpt_neox_layers_18_attention_query_key_value_q_weight3: R.Tensor((7680, 320), dtype="uint32") = packed_params[294]
            gpt_neox_layers_18_attention_query_key_value_q_scale3: R.Tensor((7680, 80), dtype="float16") = packed_params[295]
            gpt_neox_layers_18_attention_query_key_value_bias3: R.Tensor((7680,), dtype="float16") = packed_params[296]
            gpt_neox_layers_18_attention_dense_q_weight3: R.Tensor((2560, 320), dtype="uint32") = packed_params[297]
            gpt_neox_layers_18_attention_dense_q_scale3: R.Tensor((2560, 80), dtype="float16") = packed_params[298]
            gpt_neox_layers_18_attention_dense_bias3: R.Tensor((2560,), dtype="float16") = packed_params[299]
            gpt_neox_layers_18_mlp_dense_h_to_4h_q_weight3: R.Tensor((10240, 320), dtype="uint32") = packed_params[300]
            gpt_neox_layers_18_mlp_dense_h_to_4h_q_scale3: R.Tensor((10240, 80), dtype="float16") = packed_params[301]
            gpt_neox_layers_18_mlp_dense_h_to_4h_bias3: R.Tensor((10240,), dtype="float32") = packed_params[302]
            gpt_neox_layers_18_mlp_dense_4h_to_h_q_weight3: R.Tensor((2560, 1280), dtype="uint32") = packed_params[303]
            gpt_neox_layers_18_mlp_dense_4h_to_h_q_scale3: R.Tensor((2560, 320), dtype="float16") = packed_params[304]
            gpt_neox_layers_18_mlp_dense_4h_to_h_bias3: R.Tensor((2560,), dtype="float32") = packed_params[305]
            gpt_neox_layers_19_input_layernorm_weight3: R.Tensor((2560,), dtype="float16") = packed_params[306]
            gpt_neox_layers_19_input_layernorm_bias3: R.Tensor((2560,), dtype="float16") = packed_params[307]
            gpt_neox_layers_19_post_attention_layernorm_weight3: R.Tensor((2560,), dtype="float16") = packed_params[308]
            gpt_neox_layers_19_post_attention_layernorm_bias3: R.Tensor((2560,), dtype="float16") = packed_params[309]
            gpt_neox_layers_19_attention_query_key_value_q_weight3: R.Tensor((7680, 320), dtype="uint32") = packed_params[310]
            gpt_neox_layers_19_attention_query_key_value_q_scale3: R.Tensor((7680, 80), dtype="float16") = packed_params[311]
            gpt_neox_layers_19_attention_query_key_value_bias3: R.Tensor((7680,), dtype="float16") = packed_params[312]
            gpt_neox_layers_19_attention_dense_q_weight3: R.Tensor((2560, 320), dtype="uint32") = packed_params[313]
            gpt_neox_layers_19_attention_dense_q_scale3: R.Tensor((2560, 80), dtype="float16") = packed_params[314]
            gpt_neox_layers_19_attention_dense_bias3: R.Tensor((2560,), dtype="float16") = packed_params[315]
            gpt_neox_layers_19_mlp_dense_h_to_4h_q_weight3: R.Tensor((10240, 320), dtype="uint32") = packed_params[316]
            gpt_neox_layers_19_mlp_dense_h_to_4h_q_scale3: R.Tensor((10240, 80), dtype="float16") = packed_params[317]
            gpt_neox_layers_19_mlp_dense_h_to_4h_bias3: R.Tensor((10240,), dtype="float32") = packed_params[318]
            gpt_neox_layers_19_mlp_dense_4h_to_h_q_weight3: R.Tensor((2560, 1280), dtype="uint32") = packed_params[319]
            gpt_neox_layers_19_mlp_dense_4h_to_h_q_scale3: R.Tensor((2560, 320), dtype="float16") = packed_params[320]
            gpt_neox_layers_19_mlp_dense_4h_to_h_bias3: R.Tensor((2560,), dtype="float32") = packed_params[321]
            gpt_neox_layers_20_input_layernorm_weight3: R.Tensor((2560,), dtype="float16") = packed_params[322]
            gpt_neox_layers_20_input_layernorm_bias3: R.Tensor((2560,), dtype="float16") = packed_params[323]
            gpt_neox_layers_20_post_attention_layernorm_weight3: R.Tensor((2560,), dtype="float16") = packed_params[324]
            gpt_neox_layers_20_post_attention_layernorm_bias3: R.Tensor((2560,), dtype="float16") = packed_params[325]
            gpt_neox_layers_20_attention_query_key_value_q_weight3: R.Tensor((7680, 320), dtype="uint32") = packed_params[326]
            gpt_neox_layers_20_attention_query_key_value_q_scale3: R.Tensor((7680, 80), dtype="float16") = packed_params[327]
            gpt_neox_layers_20_attention_query_key_value_bias3: R.Tensor((7680,), dtype="float16") = packed_params[328]
            gpt_neox_layers_20_attention_dense_q_weight3: R.Tensor((2560, 320), dtype="uint32") = packed_params[329]
            gpt_neox_layers_20_attention_dense_q_scale3: R.Tensor((2560, 80), dtype="float16") = packed_params[330]
            gpt_neox_layers_20_attention_dense_bias3: R.Tensor((2560,), dtype="float16") = packed_params[331]
            gpt_neox_layers_20_mlp_dense_h_to_4h_q_weight3: R.Tensor((10240, 320), dtype="uint32") = packed_params[332]
            gpt_neox_layers_20_mlp_dense_h_to_4h_q_scale3: R.Tensor((10240, 80), dtype="float16") = packed_params[333]
            gpt_neox_layers_20_mlp_dense_h_to_4h_bias3: R.Tensor((10240,), dtype="float32") = packed_params[334]
            gpt_neox_layers_20_mlp_dense_4h_to_h_q_weight3: R.Tensor((2560, 1280), dtype="uint32") = packed_params[335]
            gpt_neox_layers_20_mlp_dense_4h_to_h_q_scale3: R.Tensor((2560, 320), dtype="float16") = packed_params[336]
            gpt_neox_layers_20_mlp_dense_4h_to_h_bias3: R.Tensor((2560,), dtype="float32") = packed_params[337]
            gpt_neox_layers_21_input_layernorm_weight3: R.Tensor((2560,), dtype="float16") = packed_params[338]
            gpt_neox_layers_21_input_layernorm_bias3: R.Tensor((2560,), dtype="float16") = packed_params[339]
            gpt_neox_layers_21_post_attention_layernorm_weight3: R.Tensor((2560,), dtype="float16") = packed_params[340]
            gpt_neox_layers_21_post_attention_layernorm_bias3: R.Tensor((2560,), dtype="float16") = packed_params[341]
            gpt_neox_layers_21_attention_query_key_value_q_weight3: R.Tensor((7680, 320), dtype="uint32") = packed_params[342]
            gpt_neox_layers_21_attention_query_key_value_q_scale3: R.Tensor((7680, 80), dtype="float16") = packed_params[343]
            gpt_neox_layers_21_attention_query_key_value_bias3: R.Tensor((7680,), dtype="float16") = packed_params[344]
            gpt_neox_layers_21_attention_dense_q_weight3: R.Tensor((2560, 320), dtype="uint32") = packed_params[345]
            gpt_neox_layers_21_attention_dense_q_scale3: R.Tensor((2560, 80), dtype="float16") = packed_params[346]
            gpt_neox_layers_21_attention_dense_bias3: R.Tensor((2560,), dtype="float16") = packed_params[347]
            gpt_neox_layers_21_mlp_dense_h_to_4h_q_weight3: R.Tensor((10240, 320), dtype="uint32") = packed_params[348]
            gpt_neox_layers_21_mlp_dense_h_to_4h_q_scale3: R.Tensor((10240, 80), dtype="float16") = packed_params[349]
            gpt_neox_layers_21_mlp_dense_h_to_4h_bias3: R.Tensor((10240,), dtype="float32") = packed_params[350]
            gpt_neox_layers_21_mlp_dense_4h_to_h_q_weight3: R.Tensor((2560, 1280), dtype="uint32") = packed_params[351]
            gpt_neox_layers_21_mlp_dense_4h_to_h_q_scale3: R.Tensor((2560, 320), dtype="float16") = packed_params[352]
            gpt_neox_layers_21_mlp_dense_4h_to_h_bias3: R.Tensor((2560,), dtype="float32") = packed_params[353]
            gpt_neox_layers_22_input_layernorm_weight3: R.Tensor((2560,), dtype="float16") = packed_params[354]
            gpt_neox_layers_22_input_layernorm_bias3: R.Tensor((2560,), dtype="float16") = packed_params[355]
            gpt_neox_layers_22_post_attention_layernorm_weight3: R.Tensor((2560,), dtype="float16") = packed_params[356]
            gpt_neox_layers_22_post_attention_layernorm_bias3: R.Tensor((2560,), dtype="float16") = packed_params[357]
            gpt_neox_layers_22_attention_query_key_value_q_weight3: R.Tensor((7680, 320), dtype="uint32") = packed_params[358]
            gpt_neox_layers_22_attention_query_key_value_q_scale3: R.Tensor((7680, 80), dtype="float16") = packed_params[359]
            gpt_neox_layers_22_attention_query_key_value_bias3: R.Tensor((7680,), dtype="float16") = packed_params[360]
            gpt_neox_layers_22_attention_dense_q_weight3: R.Tensor((2560, 320), dtype="uint32") = packed_params[361]
            gpt_neox_layers_22_attention_dense_q_scale3: R.Tensor((2560, 80), dtype="float16") = packed_params[362]
            gpt_neox_layers_22_attention_dense_bias3: R.Tensor((2560,), dtype="float16") = packed_params[363]
            gpt_neox_layers_22_mlp_dense_h_to_4h_q_weight3: R.Tensor((10240, 320), dtype="uint32") = packed_params[364]
            gpt_neox_layers_22_mlp_dense_h_to_4h_q_scale3: R.Tensor((10240, 80), dtype="float16") = packed_params[365]
            gpt_neox_layers_22_mlp_dense_h_to_4h_bias3: R.Tensor((10240,), dtype="float32") = packed_params[366]
            gpt_neox_layers_22_mlp_dense_4h_to_h_q_weight3: R.Tensor((2560, 1280), dtype="uint32") = packed_params[367]
            gpt_neox_layers_22_mlp_dense_4h_to_h_q_scale3: R.Tensor((2560, 320), dtype="float16") = packed_params[368]
            gpt_neox_layers_22_mlp_dense_4h_to_h_bias3: R.Tensor((2560,), dtype="float32") = packed_params[369]
            gpt_neox_layers_23_input_layernorm_weight3: R.Tensor((2560,), dtype="float16") = packed_params[370]
            gpt_neox_layers_23_input_layernorm_bias3: R.Tensor((2560,), dtype="float16") = packed_params[371]
            gpt_neox_layers_23_post_attention_layernorm_weight3: R.Tensor((2560,), dtype="float16") = packed_params[372]
            gpt_neox_layers_23_post_attention_layernorm_bias3: R.Tensor((2560,), dtype="float16") = packed_params[373]
            gpt_neox_layers_23_attention_query_key_value_q_weight3: R.Tensor((7680, 320), dtype="uint32") = packed_params[374]
            gpt_neox_layers_23_attention_query_key_value_q_scale3: R.Tensor((7680, 80), dtype="float16") = packed_params[375]
            gpt_neox_layers_23_attention_query_key_value_bias3: R.Tensor((7680,), dtype="float16") = packed_params[376]
            gpt_neox_layers_23_attention_dense_q_weight3: R.Tensor((2560, 320), dtype="uint32") = packed_params[377]
            gpt_neox_layers_23_attention_dense_q_scale3: R.Tensor((2560, 80), dtype="float16") = packed_params[378]
            gpt_neox_layers_23_attention_dense_bias3: R.Tensor((2560,), dtype="float16") = packed_params[379]
            gpt_neox_layers_23_mlp_dense_h_to_4h_q_weight3: R.Tensor((10240, 320), dtype="uint32") = packed_params[380]
            gpt_neox_layers_23_mlp_dense_h_to_4h_q_scale3: R.Tensor((10240, 80), dtype="float16") = packed_params[381]
            gpt_neox_layers_23_mlp_dense_h_to_4h_bias3: R.Tensor((10240,), dtype="float32") = packed_params[382]
            gpt_neox_layers_23_mlp_dense_4h_to_h_q_weight3: R.Tensor((2560, 1280), dtype="uint32") = packed_params[383]
            gpt_neox_layers_23_mlp_dense_4h_to_h_q_scale3: R.Tensor((2560, 320), dtype="float16") = packed_params[384]
            gpt_neox_layers_23_mlp_dense_4h_to_h_bias3: R.Tensor((2560,), dtype="float32") = packed_params[385]
            gpt_neox_layers_24_input_layernorm_weight3: R.Tensor((2560,), dtype="float16") = packed_params[386]
            gpt_neox_layers_24_input_layernorm_bias3: R.Tensor((2560,), dtype="float16") = packed_params[387]
            gpt_neox_layers_24_post_attention_layernorm_weight3: R.Tensor((2560,), dtype="float16") = packed_params[388]
            gpt_neox_layers_24_post_attention_layernorm_bias3: R.Tensor((2560,), dtype="float16") = packed_params[389]
            gpt_neox_layers_24_attention_query_key_value_q_weight3: R.Tensor((7680, 320), dtype="uint32") = packed_params[390]
            gpt_neox_layers_24_attention_query_key_value_q_scale3: R.Tensor((7680, 80), dtype="float16") = packed_params[391]
            gpt_neox_layers_24_attention_query_key_value_bias3: R.Tensor((7680,), dtype="float16") = packed_params[392]
            gpt_neox_layers_24_attention_dense_q_weight3: R.Tensor((2560, 320), dtype="uint32") = packed_params[393]
            gpt_neox_layers_24_attention_dense_q_scale3: R.Tensor((2560, 80), dtype="float16") = packed_params[394]
            gpt_neox_layers_24_attention_dense_bias3: R.Tensor((2560,), dtype="float16") = packed_params[395]
            gpt_neox_layers_24_mlp_dense_h_to_4h_q_weight3: R.Tensor((10240, 320), dtype="uint32") = packed_params[396]
            gpt_neox_layers_24_mlp_dense_h_to_4h_q_scale3: R.Tensor((10240, 80), dtype="float16") = packed_params[397]
            gpt_neox_layers_24_mlp_dense_h_to_4h_bias3: R.Tensor((10240,), dtype="float32") = packed_params[398]
            gpt_neox_layers_24_mlp_dense_4h_to_h_q_weight3: R.Tensor((2560, 1280), dtype="uint32") = packed_params[399]
            gpt_neox_layers_24_mlp_dense_4h_to_h_q_scale3: R.Tensor((2560, 320), dtype="float16") = packed_params[400]
            gpt_neox_layers_24_mlp_dense_4h_to_h_bias3: R.Tensor((2560,), dtype="float32") = packed_params[401]
            gpt_neox_layers_25_input_layernorm_weight3: R.Tensor((2560,), dtype="float16") = packed_params[402]
            gpt_neox_layers_25_input_layernorm_bias3: R.Tensor((2560,), dtype="float16") = packed_params[403]
            gpt_neox_layers_25_post_attention_layernorm_weight3: R.Tensor((2560,), dtype="float16") = packed_params[404]
            gpt_neox_layers_25_post_attention_layernorm_bias3: R.Tensor((2560,), dtype="float16") = packed_params[405]
            gpt_neox_layers_25_attention_query_key_value_q_weight3: R.Tensor((7680, 320), dtype="uint32") = packed_params[406]
            gpt_neox_layers_25_attention_query_key_value_q_scale3: R.Tensor((7680, 80), dtype="float16") = packed_params[407]
            gpt_neox_layers_25_attention_query_key_value_bias3: R.Tensor((7680,), dtype="float16") = packed_params[408]
            gpt_neox_layers_25_attention_dense_q_weight3: R.Tensor((2560, 320), dtype="uint32") = packed_params[409]
            gpt_neox_layers_25_attention_dense_q_scale3: R.Tensor((2560, 80), dtype="float16") = packed_params[410]
            gpt_neox_layers_25_attention_dense_bias3: R.Tensor((2560,), dtype="float16") = packed_params[411]
            gpt_neox_layers_25_mlp_dense_h_to_4h_q_weight3: R.Tensor((10240, 320), dtype="uint32") = packed_params[412]
            gpt_neox_layers_25_mlp_dense_h_to_4h_q_scale3: R.Tensor((10240, 80), dtype="float16") = packed_params[413]
            gpt_neox_layers_25_mlp_dense_h_to_4h_bias3: R.Tensor((10240,), dtype="float32") = packed_params[414]
            gpt_neox_layers_25_mlp_dense_4h_to_h_q_weight3: R.Tensor((2560, 1280), dtype="uint32") = packed_params[415]
            gpt_neox_layers_25_mlp_dense_4h_to_h_q_scale3: R.Tensor((2560, 320), dtype="float16") = packed_params[416]
            gpt_neox_layers_25_mlp_dense_4h_to_h_bias3: R.Tensor((2560,), dtype="float32") = packed_params[417]
            gpt_neox_layers_26_input_layernorm_weight3: R.Tensor((2560,), dtype="float16") = packed_params[418]
            gpt_neox_layers_26_input_layernorm_bias3: R.Tensor((2560,), dtype="float16") = packed_params[419]
            gpt_neox_layers_26_post_attention_layernorm_weight3: R.Tensor((2560,), dtype="float16") = packed_params[420]
            gpt_neox_layers_26_post_attention_layernorm_bias3: R.Tensor((2560,), dtype="float16") = packed_params[421]
            gpt_neox_layers_26_attention_query_key_value_q_weight3: R.Tensor((7680, 320), dtype="uint32") = packed_params[422]
            gpt_neox_layers_26_attention_query_key_value_q_scale3: R.Tensor((7680, 80), dtype="float16") = packed_params[423]
            gpt_neox_layers_26_attention_query_key_value_bias3: R.Tensor((7680,), dtype="float16") = packed_params[424]
            gpt_neox_layers_26_attention_dense_q_weight3: R.Tensor((2560, 320), dtype="uint32") = packed_params[425]
            gpt_neox_layers_26_attention_dense_q_scale3: R.Tensor((2560, 80), dtype="float16") = packed_params[426]
            gpt_neox_layers_26_attention_dense_bias3: R.Tensor((2560,), dtype="float16") = packed_params[427]
            gpt_neox_layers_26_mlp_dense_h_to_4h_q_weight3: R.Tensor((10240, 320), dtype="uint32") = packed_params[428]
            gpt_neox_layers_26_mlp_dense_h_to_4h_q_scale3: R.Tensor((10240, 80), dtype="float16") = packed_params[429]
            gpt_neox_layers_26_mlp_dense_h_to_4h_bias3: R.Tensor((10240,), dtype="float32") = packed_params[430]
            gpt_neox_layers_26_mlp_dense_4h_to_h_q_weight3: R.Tensor((2560, 1280), dtype="uint32") = packed_params[431]
            gpt_neox_layers_26_mlp_dense_4h_to_h_q_scale3: R.Tensor((2560, 320), dtype="float16") = packed_params[432]
            gpt_neox_layers_26_mlp_dense_4h_to_h_bias3: R.Tensor((2560,), dtype="float32") = packed_params[433]
            gpt_neox_layers_27_input_layernorm_weight3: R.Tensor((2560,), dtype="float16") = packed_params[434]
            gpt_neox_layers_27_input_layernorm_bias3: R.Tensor((2560,), dtype="float16") = packed_params[435]
            gpt_neox_layers_27_post_attention_layernorm_weight3: R.Tensor((2560,), dtype="float16") = packed_params[436]
            gpt_neox_layers_27_post_attention_layernorm_bias3: R.Tensor((2560,), dtype="float16") = packed_params[437]
            gpt_neox_layers_27_attention_query_key_value_q_weight3: R.Tensor((7680, 320), dtype="uint32") = packed_params[438]
            gpt_neox_layers_27_attention_query_key_value_q_scale3: R.Tensor((7680, 80), dtype="float16") = packed_params[439]
            gpt_neox_layers_27_attention_query_key_value_bias3: R.Tensor((7680,), dtype="float16") = packed_params[440]
            gpt_neox_layers_27_attention_dense_q_weight3: R.Tensor((2560, 320), dtype="uint32") = packed_params[441]
            gpt_neox_layers_27_attention_dense_q_scale3: R.Tensor((2560, 80), dtype="float16") = packed_params[442]
            gpt_neox_layers_27_attention_dense_bias3: R.Tensor((2560,), dtype="float16") = packed_params[443]
            gpt_neox_layers_27_mlp_dense_h_to_4h_q_weight3: R.Tensor((10240, 320), dtype="uint32") = packed_params[444]
            gpt_neox_layers_27_mlp_dense_h_to_4h_q_scale3: R.Tensor((10240, 80), dtype="float16") = packed_params[445]
            gpt_neox_layers_27_mlp_dense_h_to_4h_bias3: R.Tensor((10240,), dtype="float32") = packed_params[446]
            gpt_neox_layers_27_mlp_dense_4h_to_h_q_weight3: R.Tensor((2560, 1280), dtype="uint32") = packed_params[447]
            gpt_neox_layers_27_mlp_dense_4h_to_h_q_scale3: R.Tensor((2560, 320), dtype="float16") = packed_params[448]
            gpt_neox_layers_27_mlp_dense_4h_to_h_bias3: R.Tensor((2560,), dtype="float32") = packed_params[449]
            gpt_neox_layers_28_input_layernorm_weight3: R.Tensor((2560,), dtype="float16") = packed_params[450]
            gpt_neox_layers_28_input_layernorm_bias3: R.Tensor((2560,), dtype="float16") = packed_params[451]
            gpt_neox_layers_28_post_attention_layernorm_weight3: R.Tensor((2560,), dtype="float16") = packed_params[452]
            gpt_neox_layers_28_post_attention_layernorm_bias3: R.Tensor((2560,), dtype="float16") = packed_params[453]
            gpt_neox_layers_28_attention_query_key_value_q_weight3: R.Tensor((7680, 320), dtype="uint32") = packed_params[454]
            gpt_neox_layers_28_attention_query_key_value_q_scale3: R.Tensor((7680, 80), dtype="float16") = packed_params[455]
            gpt_neox_layers_28_attention_query_key_value_bias3: R.Tensor((7680,), dtype="float16") = packed_params[456]
            gpt_neox_layers_28_attention_dense_q_weight3: R.Tensor((2560, 320), dtype="uint32") = packed_params[457]
            gpt_neox_layers_28_attention_dense_q_scale3: R.Tensor((2560, 80), dtype="float16") = packed_params[458]
            gpt_neox_layers_28_attention_dense_bias3: R.Tensor((2560,), dtype="float16") = packed_params[459]
            gpt_neox_layers_28_mlp_dense_h_to_4h_q_weight3: R.Tensor((10240, 320), dtype="uint32") = packed_params[460]
            gpt_neox_layers_28_mlp_dense_h_to_4h_q_scale3: R.Tensor((10240, 80), dtype="float16") = packed_params[461]
            gpt_neox_layers_28_mlp_dense_h_to_4h_bias3: R.Tensor((10240,), dtype="float32") = packed_params[462]
            gpt_neox_layers_28_mlp_dense_4h_to_h_q_weight3: R.Tensor((2560, 1280), dtype="uint32") = packed_params[463]
            gpt_neox_layers_28_mlp_dense_4h_to_h_q_scale3: R.Tensor((2560, 320), dtype="float16") = packed_params[464]
            gpt_neox_layers_28_mlp_dense_4h_to_h_bias3: R.Tensor((2560,), dtype="float32") = packed_params[465]
            gpt_neox_layers_29_input_layernorm_weight3: R.Tensor((2560,), dtype="float16") = packed_params[466]
            gpt_neox_layers_29_input_layernorm_bias3: R.Tensor((2560,), dtype="float16") = packed_params[467]
            gpt_neox_layers_29_post_attention_layernorm_weight3: R.Tensor((2560,), dtype="float16") = packed_params[468]
            gpt_neox_layers_29_post_attention_layernorm_bias3: R.Tensor((2560,), dtype="float16") = packed_params[469]
            gpt_neox_layers_29_attention_query_key_value_q_weight3: R.Tensor((7680, 320), dtype="uint32") = packed_params[470]
            gpt_neox_layers_29_attention_query_key_value_q_scale3: R.Tensor((7680, 80), dtype="float16") = packed_params[471]
            gpt_neox_layers_29_attention_query_key_value_bias3: R.Tensor((7680,), dtype="float16") = packed_params[472]
            gpt_neox_layers_29_attention_dense_q_weight3: R.Tensor((2560, 320), dtype="uint32") = packed_params[473]
            gpt_neox_layers_29_attention_dense_q_scale3: R.Tensor((2560, 80), dtype="float16") = packed_params[474]
            gpt_neox_layers_29_attention_dense_bias3: R.Tensor((2560,), dtype="float16") = packed_params[475]
            gpt_neox_layers_29_mlp_dense_h_to_4h_q_weight3: R.Tensor((10240, 320), dtype="uint32") = packed_params[476]
            gpt_neox_layers_29_mlp_dense_h_to_4h_q_scale3: R.Tensor((10240, 80), dtype="float16") = packed_params[477]
            gpt_neox_layers_29_mlp_dense_h_to_4h_bias3: R.Tensor((10240,), dtype="float32") = packed_params[478]
            gpt_neox_layers_29_mlp_dense_4h_to_h_q_weight3: R.Tensor((2560, 1280), dtype="uint32") = packed_params[479]
            gpt_neox_layers_29_mlp_dense_4h_to_h_q_scale3: R.Tensor((2560, 320), dtype="float16") = packed_params[480]
            gpt_neox_layers_29_mlp_dense_4h_to_h_bias3: R.Tensor((2560,), dtype="float32") = packed_params[481]
            gpt_neox_layers_30_input_layernorm_weight3: R.Tensor((2560,), dtype="float16") = packed_params[482]
            gpt_neox_layers_30_input_layernorm_bias3: R.Tensor((2560,), dtype="float16") = packed_params[483]
            gpt_neox_layers_30_post_attention_layernorm_weight3: R.Tensor((2560,), dtype="float16") = packed_params[484]
            gpt_neox_layers_30_post_attention_layernorm_bias3: R.Tensor((2560,), dtype="float16") = packed_params[485]
            gpt_neox_layers_30_attention_query_key_value_q_weight3: R.Tensor((7680, 320), dtype="uint32") = packed_params[486]
            gpt_neox_layers_30_attention_query_key_value_q_scale3: R.Tensor((7680, 80), dtype="float16") = packed_params[487]
            gpt_neox_layers_30_attention_query_key_value_bias3: R.Tensor((7680,), dtype="float16") = packed_params[488]
            gpt_neox_layers_30_attention_dense_q_weight3: R.Tensor((2560, 320), dtype="uint32") = packed_params[489]
            gpt_neox_layers_30_attention_dense_q_scale3: R.Tensor((2560, 80), dtype="float16") = packed_params[490]
            gpt_neox_layers_30_attention_dense_bias3: R.Tensor((2560,), dtype="float16") = packed_params[491]
            gpt_neox_layers_30_mlp_dense_h_to_4h_q_weight3: R.Tensor((10240, 320), dtype="uint32") = packed_params[492]
            gpt_neox_layers_30_mlp_dense_h_to_4h_q_scale3: R.Tensor((10240, 80), dtype="float16") = packed_params[493]
            gpt_neox_layers_30_mlp_dense_h_to_4h_bias3: R.Tensor((10240,), dtype="float32") = packed_params[494]
            gpt_neox_layers_30_mlp_dense_4h_to_h_q_weight3: R.Tensor((2560, 1280), dtype="uint32") = packed_params[495]
            gpt_neox_layers_30_mlp_dense_4h_to_h_q_scale3: R.Tensor((2560, 320), dtype="float16") = packed_params[496]
            gpt_neox_layers_30_mlp_dense_4h_to_h_bias3: R.Tensor((2560,), dtype="float32") = packed_params[497]
            gpt_neox_layers_31_input_layernorm_weight3: R.Tensor((2560,), dtype="float16") = packed_params[498]
            gpt_neox_layers_31_input_layernorm_bias3: R.Tensor((2560,), dtype="float16") = packed_params[499]
            gpt_neox_layers_31_post_attention_layernorm_weight3: R.Tensor((2560,), dtype="float16") = packed_params[500]
            gpt_neox_layers_31_post_attention_layernorm_bias3: R.Tensor((2560,), dtype="float16") = packed_params[501]
            gpt_neox_layers_31_attention_query_key_value_q_weight3: R.Tensor((7680, 320), dtype="uint32") = packed_params[502]
            gpt_neox_layers_31_attention_query_key_value_q_scale3: R.Tensor((7680, 80), dtype="float16") = packed_params[503]
            gpt_neox_layers_31_attention_query_key_value_bias3: R.Tensor((7680,), dtype="float16") = packed_params[504]
            gpt_neox_layers_31_attention_dense_q_weight3: R.Tensor((2560, 320), dtype="uint32") = packed_params[505]
            gpt_neox_layers_31_attention_dense_q_scale3: R.Tensor((2560, 80), dtype="float16") = packed_params[506]
            gpt_neox_layers_31_attention_dense_bias3: R.Tensor((2560,), dtype="float16") = packed_params[507]
            gpt_neox_layers_31_mlp_dense_h_to_4h_q_weight3: R.Tensor((10240, 320), dtype="uint32") = packed_params[508]
            gpt_neox_layers_31_mlp_dense_h_to_4h_q_scale3: R.Tensor((10240, 80), dtype="float16") = packed_params[509]
            gpt_neox_layers_31_mlp_dense_h_to_4h_bias3: R.Tensor((10240,), dtype="float32") = packed_params[510]
            gpt_neox_layers_31_mlp_dense_4h_to_h_q_weight3: R.Tensor((2560, 1280), dtype="uint32") = packed_params[511]
            gpt_neox_layers_31_mlp_dense_4h_to_h_q_scale3: R.Tensor((2560, 320), dtype="float16") = packed_params[512]
            gpt_neox_layers_31_mlp_dense_4h_to_h_bias3: R.Tensor((2560,), dtype="float32") = packed_params[513]
            gpt_neox_final_layer_norm_weight3: R.Tensor((2560,), dtype="float16") = packed_params[514]
            gpt_neox_final_layer_norm_bias3: R.Tensor((2560,), dtype="float16") = packed_params[515]
            embed_out_q_weight3: R.Tensor((vocab_size, 320), dtype="uint32") = packed_params[516]
            embed_out_q_scale3: R.Tensor((vocab_size, 80), dtype="float16") = packed_params[517]
            layer_norm130 = R.call_tir(cls.layer_norm1, (input_embeds, gpt_neox_layers_0_input_layernorm_weight3, gpt_neox_layers_0_input_layernorm_bias3), out_sinfo=R.Tensor((1, seq_len, 2560), dtype="float16"))
            lv64 = R.call_tir(cls.fused_dequantize1_fused_NT_matmul5_add5, (gpt_neox_layers_0_attention_query_key_value_q_weight3, gpt_neox_layers_0_attention_query_key_value_q_scale3, layer_norm130, gpt_neox_layers_0_attention_query_key_value_bias3), out_sinfo=R.Tensor((1, seq_len, 7680), dtype="float16"))
            reshape256 = R.call_tir(cls.reshape4, (lv64,), out_sinfo=R.Tensor((1, seq_len, 96, 80), dtype="float16"))
            reshape257 = R.call_tir(cls.reshape5, (reshape256,), out_sinfo=R.Tensor((seq_len, 96, 80), dtype="float16"))
            lv325 = R.call_dps_packed("vm.builtin.attention_kv_cache_attention_with_fused_qkv", (paged_kv_cache, R.prim_value(0), R.prim_value(T.float32(1.0)), reshape257), out_sinfo=R.Tensor((seq_len, 32, 80), dtype="float16"))
            reshape258 = R.call_tir(cls.reshape6, (lv325,), out_sinfo=R.Tensor((1, seq_len, 32, 80), dtype="float16"))
            reshape259 = R.call_tir(cls.reshape7, (reshape258,), out_sinfo=R.Tensor((1, seq_len, 2560), dtype="float16"))
            lv64_1 = R.call_tir(cls.fused_dequantize2_fused_NT_matmul6_add6_add7, (gpt_neox_layers_0_attention_dense_q_weight3, gpt_neox_layers_0_attention_dense_q_scale3, reshape259, gpt_neox_layers_0_attention_dense_bias3, input_embeds), out_sinfo=R.Tensor((1, seq_len, 2560), dtype="float16"))
            layer_norm131 = R.call_tir(cls.layer_norm1, (lv64_1, gpt_neox_layers_0_post_attention_layernorm_weight3, gpt_neox_layers_0_post_attention_layernorm_bias3), out_sinfo=R.Tensor((1, seq_len, 2560), dtype="float16"))
            lv65 = R.call_tir(cls.fused_dequantize3_fused_NT_matmul7_add8_gelu1_cast3, (gpt_neox_layers_0_mlp_dense_h_to_4h_q_weight3, gpt_neox_layers_0_mlp_dense_h_to_4h_q_scale3, layer_norm131, gpt_neox_layers_0_mlp_dense_h_to_4h_bias3), out_sinfo=R.Tensor((1, seq_len, 10240), dtype="float16"))
            lv65_1 = R.call_tir(cls.fused_dequantize4_fused_NT_matmul8_add9_cast4_add7, (gpt_neox_layers_0_mlp_dense_4h_to_h_q_weight3, gpt_neox_layers_0_mlp_dense_4h_to_h_q_scale3, lv65, gpt_neox_layers_0_mlp_dense_4h_to_h_bias3, lv64_1), out_sinfo=R.Tensor((1, seq_len, 2560), dtype="float16"))
            layer_norm132 = R.call_tir(cls.layer_norm1, (lv65_1, gpt_neox_layers_1_input_layernorm_weight3, gpt_neox_layers_1_input_layernorm_bias3), out_sinfo=R.Tensor((1, seq_len, 2560), dtype="float16"))
            lv66 = R.call_tir(cls.fused_dequantize1_fused_NT_matmul5_add5, (gpt_neox_layers_1_attention_query_key_value_q_weight3, gpt_neox_layers_1_attention_query_key_value_q_scale3, layer_norm132, gpt_neox_layers_1_attention_query_key_value_bias3), out_sinfo=R.Tensor((1, seq_len, 7680), dtype="float16"))
            reshape260 = R.call_tir(cls.reshape4, (lv66,), out_sinfo=R.Tensor((1, seq_len, 96, 80), dtype="float16"))
            reshape261 = R.call_tir(cls.reshape5, (reshape260,), out_sinfo=R.Tensor((seq_len, 96, 80), dtype="float16"))
            lv330 = R.call_dps_packed("vm.builtin.attention_kv_cache_attention_with_fused_qkv", (paged_kv_cache, R.prim_value(1), R.prim_value(T.float32(1.0)), reshape261), out_sinfo=R.Tensor((seq_len, 32, 80), dtype="float16"))
            reshape262 = R.call_tir(cls.reshape6, (lv330,), out_sinfo=R.Tensor((1, seq_len, 32, 80), dtype="float16"))
            reshape263 = R.call_tir(cls.reshape7, (reshape262,), out_sinfo=R.Tensor((1, seq_len, 2560), dtype="float16"))
            lv66_1 = R.call_tir(cls.fused_dequantize2_fused_NT_matmul6_add6_add7, (gpt_neox_layers_1_attention_dense_q_weight3, gpt_neox_layers_1_attention_dense_q_scale3, reshape263, gpt_neox_layers_1_attention_dense_bias3, lv65_1), out_sinfo=R.Tensor((1, seq_len, 2560), dtype="float16"))
            layer_norm133 = R.call_tir(cls.layer_norm1, (lv66_1, gpt_neox_layers_1_post_attention_layernorm_weight3, gpt_neox_layers_1_post_attention_layernorm_bias3), out_sinfo=R.Tensor((1, seq_len, 2560), dtype="float16"))
            lv67 = R.call_tir(cls.fused_dequantize3_fused_NT_matmul7_add8_gelu1_cast3, (gpt_neox_layers_1_mlp_dense_h_to_4h_q_weight3, gpt_neox_layers_1_mlp_dense_h_to_4h_q_scale3, layer_norm133, gpt_neox_layers_1_mlp_dense_h_to_4h_bias3), out_sinfo=R.Tensor((1, seq_len, 10240), dtype="float16"))
            lv67_1 = R.call_tir(cls.fused_dequantize4_fused_NT_matmul8_add9_cast4_add7, (gpt_neox_layers_1_mlp_dense_4h_to_h_q_weight3, gpt_neox_layers_1_mlp_dense_4h_to_h_q_scale3, lv67, gpt_neox_layers_1_mlp_dense_4h_to_h_bias3, lv66_1), out_sinfo=R.Tensor((1, seq_len, 2560), dtype="float16"))
            layer_norm134 = R.call_tir(cls.layer_norm1, (lv67_1, gpt_neox_layers_2_input_layernorm_weight3, gpt_neox_layers_2_input_layernorm_bias3), out_sinfo=R.Tensor((1, seq_len, 2560), dtype="float16"))
            lv68 = R.call_tir(cls.fused_dequantize1_fused_NT_matmul5_add5, (gpt_neox_layers_2_attention_query_key_value_q_weight3, gpt_neox_layers_2_attention_query_key_value_q_scale3, layer_norm134, gpt_neox_layers_2_attention_query_key_value_bias3), out_sinfo=R.Tensor((1, seq_len, 7680), dtype="float16"))
            reshape264 = R.call_tir(cls.reshape4, (lv68,), out_sinfo=R.Tensor((1, seq_len, 96, 80), dtype="float16"))
            reshape265 = R.call_tir(cls.reshape5, (reshape264,), out_sinfo=R.Tensor((seq_len, 96, 80), dtype="float16"))
            lv335 = R.call_dps_packed("vm.builtin.attention_kv_cache_attention_with_fused_qkv", (paged_kv_cache, R.prim_value(2), R.prim_value(T.float32(1.0)), reshape265), out_sinfo=R.Tensor((seq_len, 32, 80), dtype="float16"))
            reshape266 = R.call_tir(cls.reshape6, (lv335,), out_sinfo=R.Tensor((1, seq_len, 32, 80), dtype="float16"))
            reshape267 = R.call_tir(cls.reshape7, (reshape266,), out_sinfo=R.Tensor((1, seq_len, 2560), dtype="float16"))
            lv68_1 = R.call_tir(cls.fused_dequantize2_fused_NT_matmul6_add6_add7, (gpt_neox_layers_2_attention_dense_q_weight3, gpt_neox_layers_2_attention_dense_q_scale3, reshape267, gpt_neox_layers_2_attention_dense_bias3, lv67_1), out_sinfo=R.Tensor((1, seq_len, 2560), dtype="float16"))
            layer_norm135 = R.call_tir(cls.layer_norm1, (lv68_1, gpt_neox_layers_2_post_attention_layernorm_weight3, gpt_neox_layers_2_post_attention_layernorm_bias3), out_sinfo=R.Tensor((1, seq_len, 2560), dtype="float16"))
            lv69 = R.call_tir(cls.fused_dequantize3_fused_NT_matmul7_add8_gelu1_cast3, (gpt_neox_layers_2_mlp_dense_h_to_4h_q_weight3, gpt_neox_layers_2_mlp_dense_h_to_4h_q_scale3, layer_norm135, gpt_neox_layers_2_mlp_dense_h_to_4h_bias3), out_sinfo=R.Tensor((1, seq_len, 10240), dtype="float16"))
            lv69_1 = R.call_tir(cls.fused_dequantize4_fused_NT_matmul8_add9_cast4_add7, (gpt_neox_layers_2_mlp_dense_4h_to_h_q_weight3, gpt_neox_layers_2_mlp_dense_4h_to_h_q_scale3, lv69, gpt_neox_layers_2_mlp_dense_4h_to_h_bias3, lv68_1), out_sinfo=R.Tensor((1, seq_len, 2560), dtype="float16"))
            layer_norm136 = R.call_tir(cls.layer_norm1, (lv69_1, gpt_neox_layers_3_input_layernorm_weight3, gpt_neox_layers_3_input_layernorm_bias3), out_sinfo=R.Tensor((1, seq_len, 2560), dtype="float16"))
            lv70 = R.call_tir(cls.fused_dequantize1_fused_NT_matmul5_add5, (gpt_neox_layers_3_attention_query_key_value_q_weight3, gpt_neox_layers_3_attention_query_key_value_q_scale3, layer_norm136, gpt_neox_layers_3_attention_query_key_value_bias3), out_sinfo=R.Tensor((1, seq_len, 7680), dtype="float16"))
            reshape268 = R.call_tir(cls.reshape4, (lv70,), out_sinfo=R.Tensor((1, seq_len, 96, 80), dtype="float16"))
            reshape269 = R.call_tir(cls.reshape5, (reshape268,), out_sinfo=R.Tensor((seq_len, 96, 80), dtype="float16"))
            lv340 = R.call_dps_packed("vm.builtin.attention_kv_cache_attention_with_fused_qkv", (paged_kv_cache, R.prim_value(3), R.prim_value(T.float32(1.0)), reshape269), out_sinfo=R.Tensor((seq_len, 32, 80), dtype="float16"))
            reshape270 = R.call_tir(cls.reshape6, (lv340,), out_sinfo=R.Tensor((1, seq_len, 32, 80), dtype="float16"))
            reshape271 = R.call_tir(cls.reshape7, (reshape270,), out_sinfo=R.Tensor((1, seq_len, 2560), dtype="float16"))
            lv70_1 = R.call_tir(cls.fused_dequantize2_fused_NT_matmul6_add6_add7, (gpt_neox_layers_3_attention_dense_q_weight3, gpt_neox_layers_3_attention_dense_q_scale3, reshape271, gpt_neox_layers_3_attention_dense_bias3, lv69_1), out_sinfo=R.Tensor((1, seq_len, 2560), dtype="float16"))
            layer_norm137 = R.call_tir(cls.layer_norm1, (lv70_1, gpt_neox_layers_3_post_attention_layernorm_weight3, gpt_neox_layers_3_post_attention_layernorm_bias3), out_sinfo=R.Tensor((1, seq_len, 2560), dtype="float16"))
            lv71 = R.call_tir(cls.fused_dequantize3_fused_NT_matmul7_add8_gelu1_cast3, (gpt_neox_layers_3_mlp_dense_h_to_4h_q_weight3, gpt_neox_layers_3_mlp_dense_h_to_4h_q_scale3, layer_norm137, gpt_neox_layers_3_mlp_dense_h_to_4h_bias3), out_sinfo=R.Tensor((1, seq_len, 10240), dtype="float16"))
            lv71_1 = R.call_tir(cls.fused_dequantize4_fused_NT_matmul8_add9_cast4_add7, (gpt_neox_layers_3_mlp_dense_4h_to_h_q_weight3, gpt_neox_layers_3_mlp_dense_4h_to_h_q_scale3, lv71, gpt_neox_layers_3_mlp_dense_4h_to_h_bias3, lv70_1), out_sinfo=R.Tensor((1, seq_len, 2560), dtype="float16"))
            layer_norm138 = R.call_tir(cls.layer_norm1, (lv71_1, gpt_neox_layers_4_input_layernorm_weight3, gpt_neox_layers_4_input_layernorm_bias3), out_sinfo=R.Tensor((1, seq_len, 2560), dtype="float16"))
            lv72 = R.call_tir(cls.fused_dequantize1_fused_NT_matmul5_add5, (gpt_neox_layers_4_attention_query_key_value_q_weight3, gpt_neox_layers_4_attention_query_key_value_q_scale3, layer_norm138, gpt_neox_layers_4_attention_query_key_value_bias3), out_sinfo=R.Tensor((1, seq_len, 7680), dtype="float16"))
            reshape272 = R.call_tir(cls.reshape4, (lv72,), out_sinfo=R.Tensor((1, seq_len, 96, 80), dtype="float16"))
            reshape273 = R.call_tir(cls.reshape5, (reshape272,), out_sinfo=R.Tensor((seq_len, 96, 80), dtype="float16"))
            lv345 = R.call_dps_packed("vm.builtin.attention_kv_cache_attention_with_fused_qkv", (paged_kv_cache, R.prim_value(4), R.prim_value(T.float32(1.0)), reshape273), out_sinfo=R.Tensor((seq_len, 32, 80), dtype="float16"))
            reshape274 = R.call_tir(cls.reshape6, (lv345,), out_sinfo=R.Tensor((1, seq_len, 32, 80), dtype="float16"))
            reshape275 = R.call_tir(cls.reshape7, (reshape274,), out_sinfo=R.Tensor((1, seq_len, 2560), dtype="float16"))
            lv72_1 = R.call_tir(cls.fused_dequantize2_fused_NT_matmul6_add6_add7, (gpt_neox_layers_4_attention_dense_q_weight3, gpt_neox_layers_4_attention_dense_q_scale3, reshape275, gpt_neox_layers_4_attention_dense_bias3, lv71_1), out_sinfo=R.Tensor((1, seq_len, 2560), dtype="float16"))
            layer_norm139 = R.call_tir(cls.layer_norm1, (lv72_1, gpt_neox_layers_4_post_attention_layernorm_weight3, gpt_neox_layers_4_post_attention_layernorm_bias3), out_sinfo=R.Tensor((1, seq_len, 2560), dtype="float16"))
            lv73 = R.call_tir(cls.fused_dequantize3_fused_NT_matmul7_add8_gelu1_cast3, (gpt_neox_layers_4_mlp_dense_h_to_4h_q_weight3, gpt_neox_layers_4_mlp_dense_h_to_4h_q_scale3, layer_norm139, gpt_neox_layers_4_mlp_dense_h_to_4h_bias3), out_sinfo=R.Tensor((1, seq_len, 10240), dtype="float16"))
            lv73_1 = R.call_tir(cls.fused_dequantize4_fused_NT_matmul8_add9_cast4_add7, (gpt_neox_layers_4_mlp_dense_4h_to_h_q_weight3, gpt_neox_layers_4_mlp_dense_4h_to_h_q_scale3, lv73, gpt_neox_layers_4_mlp_dense_4h_to_h_bias3, lv72_1), out_sinfo=R.Tensor((1, seq_len, 2560), dtype="float16"))
            layer_norm140 = R.call_tir(cls.layer_norm1, (lv73_1, gpt_neox_layers_5_input_layernorm_weight3, gpt_neox_layers_5_input_layernorm_bias3), out_sinfo=R.Tensor((1, seq_len, 2560), dtype="float16"))
            lv74 = R.call_tir(cls.fused_dequantize1_fused_NT_matmul5_add5, (gpt_neox_layers_5_attention_query_key_value_q_weight3, gpt_neox_layers_5_attention_query_key_value_q_scale3, layer_norm140, gpt_neox_layers_5_attention_query_key_value_bias3), out_sinfo=R.Tensor((1, seq_len, 7680), dtype="float16"))
            reshape276 = R.call_tir(cls.reshape4, (lv74,), out_sinfo=R.Tensor((1, seq_len, 96, 80), dtype="float16"))
            reshape277 = R.call_tir(cls.reshape5, (reshape276,), out_sinfo=R.Tensor((seq_len, 96, 80), dtype="float16"))
            lv350 = R.call_dps_packed("vm.builtin.attention_kv_cache_attention_with_fused_qkv", (paged_kv_cache, R.prim_value(5), R.prim_value(T.float32(1.0)), reshape277), out_sinfo=R.Tensor((seq_len, 32, 80), dtype="float16"))
            reshape278 = R.call_tir(cls.reshape6, (lv350,), out_sinfo=R.Tensor((1, seq_len, 32, 80), dtype="float16"))
            reshape279 = R.call_tir(cls.reshape7, (reshape278,), out_sinfo=R.Tensor((1, seq_len, 2560), dtype="float16"))
            lv74_1 = R.call_tir(cls.fused_dequantize2_fused_NT_matmul6_add6_add7, (gpt_neox_layers_5_attention_dense_q_weight3, gpt_neox_layers_5_attention_dense_q_scale3, reshape279, gpt_neox_layers_5_attention_dense_bias3, lv73_1), out_sinfo=R.Tensor((1, seq_len, 2560), dtype="float16"))
            layer_norm141 = R.call_tir(cls.layer_norm1, (lv74_1, gpt_neox_layers_5_post_attention_layernorm_weight3, gpt_neox_layers_5_post_attention_layernorm_bias3), out_sinfo=R.Tensor((1, seq_len, 2560), dtype="float16"))
            lv75 = R.call_tir(cls.fused_dequantize3_fused_NT_matmul7_add8_gelu1_cast3, (gpt_neox_layers_5_mlp_dense_h_to_4h_q_weight3, gpt_neox_layers_5_mlp_dense_h_to_4h_q_scale3, layer_norm141, gpt_neox_layers_5_mlp_dense_h_to_4h_bias3), out_sinfo=R.Tensor((1, seq_len, 10240), dtype="float16"))
            lv75_1 = R.call_tir(cls.fused_dequantize4_fused_NT_matmul8_add9_cast4_add7, (gpt_neox_layers_5_mlp_dense_4h_to_h_q_weight3, gpt_neox_layers_5_mlp_dense_4h_to_h_q_scale3, lv75, gpt_neox_layers_5_mlp_dense_4h_to_h_bias3, lv74_1), out_sinfo=R.Tensor((1, seq_len, 2560), dtype="float16"))
            layer_norm142 = R.call_tir(cls.layer_norm1, (lv75_1, gpt_neox_layers_6_input_layernorm_weight3, gpt_neox_layers_6_input_layernorm_bias3), out_sinfo=R.Tensor((1, seq_len, 2560), dtype="float16"))
            lv76 = R.call_tir(cls.fused_dequantize1_fused_NT_matmul5_add5, (gpt_neox_layers_6_attention_query_key_value_q_weight3, gpt_neox_layers_6_attention_query_key_value_q_scale3, layer_norm142, gpt_neox_layers_6_attention_query_key_value_bias3), out_sinfo=R.Tensor((1, seq_len, 7680), dtype="float16"))
            reshape280 = R.call_tir(cls.reshape4, (lv76,), out_sinfo=R.Tensor((1, seq_len, 96, 80), dtype="float16"))
            reshape281 = R.call_tir(cls.reshape5, (reshape280,), out_sinfo=R.Tensor((seq_len, 96, 80), dtype="float16"))
            lv355 = R.call_dps_packed("vm.builtin.attention_kv_cache_attention_with_fused_qkv", (paged_kv_cache, R.prim_value(6), R.prim_value(T.float32(1.0)), reshape281), out_sinfo=R.Tensor((seq_len, 32, 80), dtype="float16"))
            reshape282 = R.call_tir(cls.reshape6, (lv355,), out_sinfo=R.Tensor((1, seq_len, 32, 80), dtype="float16"))
            reshape283 = R.call_tir(cls.reshape7, (reshape282,), out_sinfo=R.Tensor((1, seq_len, 2560), dtype="float16"))
            lv76_1 = R.call_tir(cls.fused_dequantize2_fused_NT_matmul6_add6_add7, (gpt_neox_layers_6_attention_dense_q_weight3, gpt_neox_layers_6_attention_dense_q_scale3, reshape283, gpt_neox_layers_6_attention_dense_bias3, lv75_1), out_sinfo=R.Tensor((1, seq_len, 2560), dtype="float16"))
            layer_norm143 = R.call_tir(cls.layer_norm1, (lv76_1, gpt_neox_layers_6_post_attention_layernorm_weight3, gpt_neox_layers_6_post_attention_layernorm_bias3), out_sinfo=R.Tensor((1, seq_len, 2560), dtype="float16"))
            lv77 = R.call_tir(cls.fused_dequantize3_fused_NT_matmul7_add8_gelu1_cast3, (gpt_neox_layers_6_mlp_dense_h_to_4h_q_weight3, gpt_neox_layers_6_mlp_dense_h_to_4h_q_scale3, layer_norm143, gpt_neox_layers_6_mlp_dense_h_to_4h_bias3), out_sinfo=R.Tensor((1, seq_len, 10240), dtype="float16"))
            lv77_1 = R.call_tir(cls.fused_dequantize4_fused_NT_matmul8_add9_cast4_add7, (gpt_neox_layers_6_mlp_dense_4h_to_h_q_weight3, gpt_neox_layers_6_mlp_dense_4h_to_h_q_scale3, lv77, gpt_neox_layers_6_mlp_dense_4h_to_h_bias3, lv76_1), out_sinfo=R.Tensor((1, seq_len, 2560), dtype="float16"))
            layer_norm144 = R.call_tir(cls.layer_norm1, (lv77_1, gpt_neox_layers_7_input_layernorm_weight3, gpt_neox_layers_7_input_layernorm_bias3), out_sinfo=R.Tensor((1, seq_len, 2560), dtype="float16"))
            lv78 = R.call_tir(cls.fused_dequantize1_fused_NT_matmul5_add5, (gpt_neox_layers_7_attention_query_key_value_q_weight3, gpt_neox_layers_7_attention_query_key_value_q_scale3, layer_norm144, gpt_neox_layers_7_attention_query_key_value_bias3), out_sinfo=R.Tensor((1, seq_len, 7680), dtype="float16"))
            reshape284 = R.call_tir(cls.reshape4, (lv78,), out_sinfo=R.Tensor((1, seq_len, 96, 80), dtype="float16"))
            reshape285 = R.call_tir(cls.reshape5, (reshape284,), out_sinfo=R.Tensor((seq_len, 96, 80), dtype="float16"))
            lv360 = R.call_dps_packed("vm.builtin.attention_kv_cache_attention_with_fused_qkv", (paged_kv_cache, R.prim_value(7), R.prim_value(T.float32(1.0)), reshape285), out_sinfo=R.Tensor((seq_len, 32, 80), dtype="float16"))
            reshape286 = R.call_tir(cls.reshape6, (lv360,), out_sinfo=R.Tensor((1, seq_len, 32, 80), dtype="float16"))
            reshape287 = R.call_tir(cls.reshape7, (reshape286,), out_sinfo=R.Tensor((1, seq_len, 2560), dtype="float16"))
            lv78_1 = R.call_tir(cls.fused_dequantize2_fused_NT_matmul6_add6_add7, (gpt_neox_layers_7_attention_dense_q_weight3, gpt_neox_layers_7_attention_dense_q_scale3, reshape287, gpt_neox_layers_7_attention_dense_bias3, lv77_1), out_sinfo=R.Tensor((1, seq_len, 2560), dtype="float16"))
            layer_norm145 = R.call_tir(cls.layer_norm1, (lv78_1, gpt_neox_layers_7_post_attention_layernorm_weight3, gpt_neox_layers_7_post_attention_layernorm_bias3), out_sinfo=R.Tensor((1, seq_len, 2560), dtype="float16"))
            lv79 = R.call_tir(cls.fused_dequantize3_fused_NT_matmul7_add8_gelu1_cast3, (gpt_neox_layers_7_mlp_dense_h_to_4h_q_weight3, gpt_neox_layers_7_mlp_dense_h_to_4h_q_scale3, layer_norm145, gpt_neox_layers_7_mlp_dense_h_to_4h_bias3), out_sinfo=R.Tensor((1, seq_len, 10240), dtype="float16"))
            lv79_1 = R.call_tir(cls.fused_dequantize4_fused_NT_matmul8_add9_cast4_add7, (gpt_neox_layers_7_mlp_dense_4h_to_h_q_weight3, gpt_neox_layers_7_mlp_dense_4h_to_h_q_scale3, lv79, gpt_neox_layers_7_mlp_dense_4h_to_h_bias3, lv78_1), out_sinfo=R.Tensor((1, seq_len, 2560), dtype="float16"))
            layer_norm146 = R.call_tir(cls.layer_norm1, (lv79_1, gpt_neox_layers_8_input_layernorm_weight3, gpt_neox_layers_8_input_layernorm_bias3), out_sinfo=R.Tensor((1, seq_len, 2560), dtype="float16"))
            lv80 = R.call_tir(cls.fused_dequantize1_fused_NT_matmul5_add5, (gpt_neox_layers_8_attention_query_key_value_q_weight3, gpt_neox_layers_8_attention_query_key_value_q_scale3, layer_norm146, gpt_neox_layers_8_attention_query_key_value_bias3), out_sinfo=R.Tensor((1, seq_len, 7680), dtype="float16"))
            reshape288 = R.call_tir(cls.reshape4, (lv80,), out_sinfo=R.Tensor((1, seq_len, 96, 80), dtype="float16"))
            reshape289 = R.call_tir(cls.reshape5, (reshape288,), out_sinfo=R.Tensor((seq_len, 96, 80), dtype="float16"))
            lv365 = R.call_dps_packed("vm.builtin.attention_kv_cache_attention_with_fused_qkv", (paged_kv_cache, R.prim_value(8), R.prim_value(T.float32(1.0)), reshape289), out_sinfo=R.Tensor((seq_len, 32, 80), dtype="float16"))
            reshape290 = R.call_tir(cls.reshape6, (lv365,), out_sinfo=R.Tensor((1, seq_len, 32, 80), dtype="float16"))
            reshape291 = R.call_tir(cls.reshape7, (reshape290,), out_sinfo=R.Tensor((1, seq_len, 2560), dtype="float16"))
            lv80_1 = R.call_tir(cls.fused_dequantize2_fused_NT_matmul6_add6_add7, (gpt_neox_layers_8_attention_dense_q_weight3, gpt_neox_layers_8_attention_dense_q_scale3, reshape291, gpt_neox_layers_8_attention_dense_bias3, lv79_1), out_sinfo=R.Tensor((1, seq_len, 2560), dtype="float16"))
            layer_norm147 = R.call_tir(cls.layer_norm1, (lv80_1, gpt_neox_layers_8_post_attention_layernorm_weight3, gpt_neox_layers_8_post_attention_layernorm_bias3), out_sinfo=R.Tensor((1, seq_len, 2560), dtype="float16"))
            lv81 = R.call_tir(cls.fused_dequantize3_fused_NT_matmul7_add8_gelu1_cast3, (gpt_neox_layers_8_mlp_dense_h_to_4h_q_weight3, gpt_neox_layers_8_mlp_dense_h_to_4h_q_scale3, layer_norm147, gpt_neox_layers_8_mlp_dense_h_to_4h_bias3), out_sinfo=R.Tensor((1, seq_len, 10240), dtype="float16"))
            lv81_1 = R.call_tir(cls.fused_dequantize4_fused_NT_matmul8_add9_cast4_add7, (gpt_neox_layers_8_mlp_dense_4h_to_h_q_weight3, gpt_neox_layers_8_mlp_dense_4h_to_h_q_scale3, lv81, gpt_neox_layers_8_mlp_dense_4h_to_h_bias3, lv80_1), out_sinfo=R.Tensor((1, seq_len, 2560), dtype="float16"))
            layer_norm148 = R.call_tir(cls.layer_norm1, (lv81_1, gpt_neox_layers_9_input_layernorm_weight3, gpt_neox_layers_9_input_layernorm_bias3), out_sinfo=R.Tensor((1, seq_len, 2560), dtype="float16"))
            lv82 = R.call_tir(cls.fused_dequantize1_fused_NT_matmul5_add5, (gpt_neox_layers_9_attention_query_key_value_q_weight3, gpt_neox_layers_9_attention_query_key_value_q_scale3, layer_norm148, gpt_neox_layers_9_attention_query_key_value_bias3), out_sinfo=R.Tensor((1, seq_len, 7680), dtype="float16"))
            reshape292 = R.call_tir(cls.reshape4, (lv82,), out_sinfo=R.Tensor((1, seq_len, 96, 80), dtype="float16"))
            reshape293 = R.call_tir(cls.reshape5, (reshape292,), out_sinfo=R.Tensor((seq_len, 96, 80), dtype="float16"))
            lv370 = R.call_dps_packed("vm.builtin.attention_kv_cache_attention_with_fused_qkv", (paged_kv_cache, R.prim_value(9), R.prim_value(T.float32(1.0)), reshape293), out_sinfo=R.Tensor((seq_len, 32, 80), dtype="float16"))
            reshape294 = R.call_tir(cls.reshape6, (lv370,), out_sinfo=R.Tensor((1, seq_len, 32, 80), dtype="float16"))
            reshape295 = R.call_tir(cls.reshape7, (reshape294,), out_sinfo=R.Tensor((1, seq_len, 2560), dtype="float16"))
            lv82_1 = R.call_tir(cls.fused_dequantize2_fused_NT_matmul6_add6_add7, (gpt_neox_layers_9_attention_dense_q_weight3, gpt_neox_layers_9_attention_dense_q_scale3, reshape295, gpt_neox_layers_9_attention_dense_bias3, lv81_1), out_sinfo=R.Tensor((1, seq_len, 2560), dtype="float16"))
            layer_norm149 = R.call_tir(cls.layer_norm1, (lv82_1, gpt_neox_layers_9_post_attention_layernorm_weight3, gpt_neox_layers_9_post_attention_layernorm_bias3), out_sinfo=R.Tensor((1, seq_len, 2560), dtype="float16"))
            lv83 = R.call_tir(cls.fused_dequantize3_fused_NT_matmul7_add8_gelu1_cast3, (gpt_neox_layers_9_mlp_dense_h_to_4h_q_weight3, gpt_neox_layers_9_mlp_dense_h_to_4h_q_scale3, layer_norm149, gpt_neox_layers_9_mlp_dense_h_to_4h_bias3), out_sinfo=R.Tensor((1, seq_len, 10240), dtype="float16"))
            lv83_1 = R.call_tir(cls.fused_dequantize4_fused_NT_matmul8_add9_cast4_add7, (gpt_neox_layers_9_mlp_dense_4h_to_h_q_weight3, gpt_neox_layers_9_mlp_dense_4h_to_h_q_scale3, lv83, gpt_neox_layers_9_mlp_dense_4h_to_h_bias3, lv82_1), out_sinfo=R.Tensor((1, seq_len, 2560), dtype="float16"))
            layer_norm150 = R.call_tir(cls.layer_norm1, (lv83_1, gpt_neox_layers_10_input_layernorm_weight3, gpt_neox_layers_10_input_layernorm_bias3), out_sinfo=R.Tensor((1, seq_len, 2560), dtype="float16"))
            lv84 = R.call_tir(cls.fused_dequantize1_fused_NT_matmul5_add5, (gpt_neox_layers_10_attention_query_key_value_q_weight3, gpt_neox_layers_10_attention_query_key_value_q_scale3, layer_norm150, gpt_neox_layers_10_attention_query_key_value_bias3), out_sinfo=R.Tensor((1, seq_len, 7680), dtype="float16"))
            reshape296 = R.call_tir(cls.reshape4, (lv84,), out_sinfo=R.Tensor((1, seq_len, 96, 80), dtype="float16"))
            reshape297 = R.call_tir(cls.reshape5, (reshape296,), out_sinfo=R.Tensor((seq_len, 96, 80), dtype="float16"))
            lv375 = R.call_dps_packed("vm.builtin.attention_kv_cache_attention_with_fused_qkv", (paged_kv_cache, R.prim_value(10), R.prim_value(T.float32(1.0)), reshape297), out_sinfo=R.Tensor((seq_len, 32, 80), dtype="float16"))
            reshape298 = R.call_tir(cls.reshape6, (lv375,), out_sinfo=R.Tensor((1, seq_len, 32, 80), dtype="float16"))
            reshape299 = R.call_tir(cls.reshape7, (reshape298,), out_sinfo=R.Tensor((1, seq_len, 2560), dtype="float16"))
            lv84_1 = R.call_tir(cls.fused_dequantize2_fused_NT_matmul6_add6_add7, (gpt_neox_layers_10_attention_dense_q_weight3, gpt_neox_layers_10_attention_dense_q_scale3, reshape299, gpt_neox_layers_10_attention_dense_bias3, lv83_1), out_sinfo=R.Tensor((1, seq_len, 2560), dtype="float16"))
            layer_norm151 = R.call_tir(cls.layer_norm1, (lv84_1, gpt_neox_layers_10_post_attention_layernorm_weight3, gpt_neox_layers_10_post_attention_layernorm_bias3), out_sinfo=R.Tensor((1, seq_len, 2560), dtype="float16"))
            lv85 = R.call_tir(cls.fused_dequantize3_fused_NT_matmul7_add8_gelu1_cast3, (gpt_neox_layers_10_mlp_dense_h_to_4h_q_weight3, gpt_neox_layers_10_mlp_dense_h_to_4h_q_scale3, layer_norm151, gpt_neox_layers_10_mlp_dense_h_to_4h_bias3), out_sinfo=R.Tensor((1, seq_len, 10240), dtype="float16"))
            lv85_1 = R.call_tir(cls.fused_dequantize4_fused_NT_matmul8_add9_cast4_add7, (gpt_neox_layers_10_mlp_dense_4h_to_h_q_weight3, gpt_neox_layers_10_mlp_dense_4h_to_h_q_scale3, lv85, gpt_neox_layers_10_mlp_dense_4h_to_h_bias3, lv84_1), out_sinfo=R.Tensor((1, seq_len, 2560), dtype="float16"))
            layer_norm152 = R.call_tir(cls.layer_norm1, (lv85_1, gpt_neox_layers_11_input_layernorm_weight3, gpt_neox_layers_11_input_layernorm_bias3), out_sinfo=R.Tensor((1, seq_len, 2560), dtype="float16"))
            lv86 = R.call_tir(cls.fused_dequantize1_fused_NT_matmul5_add5, (gpt_neox_layers_11_attention_query_key_value_q_weight3, gpt_neox_layers_11_attention_query_key_value_q_scale3, layer_norm152, gpt_neox_layers_11_attention_query_key_value_bias3), out_sinfo=R.Tensor((1, seq_len, 7680), dtype="float16"))
            reshape300 = R.call_tir(cls.reshape4, (lv86,), out_sinfo=R.Tensor((1, seq_len, 96, 80), dtype="float16"))
            reshape301 = R.call_tir(cls.reshape5, (reshape300,), out_sinfo=R.Tensor((seq_len, 96, 80), dtype="float16"))
            lv380 = R.call_dps_packed("vm.builtin.attention_kv_cache_attention_with_fused_qkv", (paged_kv_cache, R.prim_value(11), R.prim_value(T.float32(1.0)), reshape301), out_sinfo=R.Tensor((seq_len, 32, 80), dtype="float16"))
            reshape302 = R.call_tir(cls.reshape6, (lv380,), out_sinfo=R.Tensor((1, seq_len, 32, 80), dtype="float16"))
            reshape303 = R.call_tir(cls.reshape7, (reshape302,), out_sinfo=R.Tensor((1, seq_len, 2560), dtype="float16"))
            lv86_1 = R.call_tir(cls.fused_dequantize2_fused_NT_matmul6_add6_add7, (gpt_neox_layers_11_attention_dense_q_weight3, gpt_neox_layers_11_attention_dense_q_scale3, reshape303, gpt_neox_layers_11_attention_dense_bias3, lv85_1), out_sinfo=R.Tensor((1, seq_len, 2560), dtype="float16"))
            layer_norm153 = R.call_tir(cls.layer_norm1, (lv86_1, gpt_neox_layers_11_post_attention_layernorm_weight3, gpt_neox_layers_11_post_attention_layernorm_bias3), out_sinfo=R.Tensor((1, seq_len, 2560), dtype="float16"))
            lv87 = R.call_tir(cls.fused_dequantize3_fused_NT_matmul7_add8_gelu1_cast3, (gpt_neox_layers_11_mlp_dense_h_to_4h_q_weight3, gpt_neox_layers_11_mlp_dense_h_to_4h_q_scale3, layer_norm153, gpt_neox_layers_11_mlp_dense_h_to_4h_bias3), out_sinfo=R.Tensor((1, seq_len, 10240), dtype="float16"))
            lv87_1 = R.call_tir(cls.fused_dequantize4_fused_NT_matmul8_add9_cast4_add7, (gpt_neox_layers_11_mlp_dense_4h_to_h_q_weight3, gpt_neox_layers_11_mlp_dense_4h_to_h_q_scale3, lv87, gpt_neox_layers_11_mlp_dense_4h_to_h_bias3, lv86_1), out_sinfo=R.Tensor((1, seq_len, 2560), dtype="float16"))
            layer_norm154 = R.call_tir(cls.layer_norm1, (lv87_1, gpt_neox_layers_12_input_layernorm_weight3, gpt_neox_layers_12_input_layernorm_bias3), out_sinfo=R.Tensor((1, seq_len, 2560), dtype="float16"))
            lv88 = R.call_tir(cls.fused_dequantize1_fused_NT_matmul5_add5, (gpt_neox_layers_12_attention_query_key_value_q_weight3, gpt_neox_layers_12_attention_query_key_value_q_scale3, layer_norm154, gpt_neox_layers_12_attention_query_key_value_bias3), out_sinfo=R.Tensor((1, seq_len, 7680), dtype="float16"))
            reshape304 = R.call_tir(cls.reshape4, (lv88,), out_sinfo=R.Tensor((1, seq_len, 96, 80), dtype="float16"))
            reshape305 = R.call_tir(cls.reshape5, (reshape304,), out_sinfo=R.Tensor((seq_len, 96, 80), dtype="float16"))
            lv385 = R.call_dps_packed("vm.builtin.attention_kv_cache_attention_with_fused_qkv", (paged_kv_cache, R.prim_value(12), R.prim_value(T.float32(1.0)), reshape305), out_sinfo=R.Tensor((seq_len, 32, 80), dtype="float16"))
            reshape306 = R.call_tir(cls.reshape6, (lv385,), out_sinfo=R.Tensor((1, seq_len, 32, 80), dtype="float16"))
            reshape307 = R.call_tir(cls.reshape7, (reshape306,), out_sinfo=R.Tensor((1, seq_len, 2560), dtype="float16"))
            lv88_1 = R.call_tir(cls.fused_dequantize2_fused_NT_matmul6_add6_add7, (gpt_neox_layers_12_attention_dense_q_weight3, gpt_neox_layers_12_attention_dense_q_scale3, reshape307, gpt_neox_layers_12_attention_dense_bias3, lv87_1), out_sinfo=R.Tensor((1, seq_len, 2560), dtype="float16"))
            layer_norm155 = R.call_tir(cls.layer_norm1, (lv88_1, gpt_neox_layers_12_post_attention_layernorm_weight3, gpt_neox_layers_12_post_attention_layernorm_bias3), out_sinfo=R.Tensor((1, seq_len, 2560), dtype="float16"))
            lv89 = R.call_tir(cls.fused_dequantize3_fused_NT_matmul7_add8_gelu1_cast3, (gpt_neox_layers_12_mlp_dense_h_to_4h_q_weight3, gpt_neox_layers_12_mlp_dense_h_to_4h_q_scale3, layer_norm155, gpt_neox_layers_12_mlp_dense_h_to_4h_bias3), out_sinfo=R.Tensor((1, seq_len, 10240), dtype="float16"))
            lv89_1 = R.call_tir(cls.fused_dequantize4_fused_NT_matmul8_add9_cast4_add7, (gpt_neox_layers_12_mlp_dense_4h_to_h_q_weight3, gpt_neox_layers_12_mlp_dense_4h_to_h_q_scale3, lv89, gpt_neox_layers_12_mlp_dense_4h_to_h_bias3, lv88_1), out_sinfo=R.Tensor((1, seq_len, 2560), dtype="float16"))
            layer_norm156 = R.call_tir(cls.layer_norm1, (lv89_1, gpt_neox_layers_13_input_layernorm_weight3, gpt_neox_layers_13_input_layernorm_bias3), out_sinfo=R.Tensor((1, seq_len, 2560), dtype="float16"))
            lv90 = R.call_tir(cls.fused_dequantize1_fused_NT_matmul5_add5, (gpt_neox_layers_13_attention_query_key_value_q_weight3, gpt_neox_layers_13_attention_query_key_value_q_scale3, layer_norm156, gpt_neox_layers_13_attention_query_key_value_bias3), out_sinfo=R.Tensor((1, seq_len, 7680), dtype="float16"))
            reshape308 = R.call_tir(cls.reshape4, (lv90,), out_sinfo=R.Tensor((1, seq_len, 96, 80), dtype="float16"))
            reshape309 = R.call_tir(cls.reshape5, (reshape308,), out_sinfo=R.Tensor((seq_len, 96, 80), dtype="float16"))
            lv390 = R.call_dps_packed("vm.builtin.attention_kv_cache_attention_with_fused_qkv", (paged_kv_cache, R.prim_value(13), R.prim_value(T.float32(1.0)), reshape309), out_sinfo=R.Tensor((seq_len, 32, 80), dtype="float16"))
            reshape310 = R.call_tir(cls.reshape6, (lv390,), out_sinfo=R.Tensor((1, seq_len, 32, 80), dtype="float16"))
            reshape311 = R.call_tir(cls.reshape7, (reshape310,), out_sinfo=R.Tensor((1, seq_len, 2560), dtype="float16"))
            lv90_1 = R.call_tir(cls.fused_dequantize2_fused_NT_matmul6_add6_add7, (gpt_neox_layers_13_attention_dense_q_weight3, gpt_neox_layers_13_attention_dense_q_scale3, reshape311, gpt_neox_layers_13_attention_dense_bias3, lv89_1), out_sinfo=R.Tensor((1, seq_len, 2560), dtype="float16"))
            layer_norm157 = R.call_tir(cls.layer_norm1, (lv90_1, gpt_neox_layers_13_post_attention_layernorm_weight3, gpt_neox_layers_13_post_attention_layernorm_bias3), out_sinfo=R.Tensor((1, seq_len, 2560), dtype="float16"))
            lv91 = R.call_tir(cls.fused_dequantize3_fused_NT_matmul7_add8_gelu1_cast3, (gpt_neox_layers_13_mlp_dense_h_to_4h_q_weight3, gpt_neox_layers_13_mlp_dense_h_to_4h_q_scale3, layer_norm157, gpt_neox_layers_13_mlp_dense_h_to_4h_bias3), out_sinfo=R.Tensor((1, seq_len, 10240), dtype="float16"))
            lv91_1 = R.call_tir(cls.fused_dequantize4_fused_NT_matmul8_add9_cast4_add7, (gpt_neox_layers_13_mlp_dense_4h_to_h_q_weight3, gpt_neox_layers_13_mlp_dense_4h_to_h_q_scale3, lv91, gpt_neox_layers_13_mlp_dense_4h_to_h_bias3, lv90_1), out_sinfo=R.Tensor((1, seq_len, 2560), dtype="float16"))
            layer_norm158 = R.call_tir(cls.layer_norm1, (lv91_1, gpt_neox_layers_14_input_layernorm_weight3, gpt_neox_layers_14_input_layernorm_bias3), out_sinfo=R.Tensor((1, seq_len, 2560), dtype="float16"))
            lv92 = R.call_tir(cls.fused_dequantize1_fused_NT_matmul5_add5, (gpt_neox_layers_14_attention_query_key_value_q_weight3, gpt_neox_layers_14_attention_query_key_value_q_scale3, layer_norm158, gpt_neox_layers_14_attention_query_key_value_bias3), out_sinfo=R.Tensor((1, seq_len, 7680), dtype="float16"))
            reshape312 = R.call_tir(cls.reshape4, (lv92,), out_sinfo=R.Tensor((1, seq_len, 96, 80), dtype="float16"))
            reshape313 = R.call_tir(cls.reshape5, (reshape312,), out_sinfo=R.Tensor((seq_len, 96, 80), dtype="float16"))
            lv395 = R.call_dps_packed("vm.builtin.attention_kv_cache_attention_with_fused_qkv", (paged_kv_cache, R.prim_value(14), R.prim_value(T.float32(1.0)), reshape313), out_sinfo=R.Tensor((seq_len, 32, 80), dtype="float16"))
            reshape314 = R.call_tir(cls.reshape6, (lv395,), out_sinfo=R.Tensor((1, seq_len, 32, 80), dtype="float16"))
            reshape315 = R.call_tir(cls.reshape7, (reshape314,), out_sinfo=R.Tensor((1, seq_len, 2560), dtype="float16"))
            lv92_1 = R.call_tir(cls.fused_dequantize2_fused_NT_matmul6_add6_add7, (gpt_neox_layers_14_attention_dense_q_weight3, gpt_neox_layers_14_attention_dense_q_scale3, reshape315, gpt_neox_layers_14_attention_dense_bias3, lv91_1), out_sinfo=R.Tensor((1, seq_len, 2560), dtype="float16"))
            layer_norm159 = R.call_tir(cls.layer_norm1, (lv92_1, gpt_neox_layers_14_post_attention_layernorm_weight3, gpt_neox_layers_14_post_attention_layernorm_bias3), out_sinfo=R.Tensor((1, seq_len, 2560), dtype="float16"))
            lv93 = R.call_tir(cls.fused_dequantize3_fused_NT_matmul7_add8_gelu1_cast3, (gpt_neox_layers_14_mlp_dense_h_to_4h_q_weight3, gpt_neox_layers_14_mlp_dense_h_to_4h_q_scale3, layer_norm159, gpt_neox_layers_14_mlp_dense_h_to_4h_bias3), out_sinfo=R.Tensor((1, seq_len, 10240), dtype="float16"))
            lv93_1 = R.call_tir(cls.fused_dequantize4_fused_NT_matmul8_add9_cast4_add7, (gpt_neox_layers_14_mlp_dense_4h_to_h_q_weight3, gpt_neox_layers_14_mlp_dense_4h_to_h_q_scale3, lv93, gpt_neox_layers_14_mlp_dense_4h_to_h_bias3, lv92_1), out_sinfo=R.Tensor((1, seq_len, 2560), dtype="float16"))
            layer_norm160 = R.call_tir(cls.layer_norm1, (lv93_1, gpt_neox_layers_15_input_layernorm_weight3, gpt_neox_layers_15_input_layernorm_bias3), out_sinfo=R.Tensor((1, seq_len, 2560), dtype="float16"))
            lv94 = R.call_tir(cls.fused_dequantize1_fused_NT_matmul5_add5, (gpt_neox_layers_15_attention_query_key_value_q_weight3, gpt_neox_layers_15_attention_query_key_value_q_scale3, layer_norm160, gpt_neox_layers_15_attention_query_key_value_bias3), out_sinfo=R.Tensor((1, seq_len, 7680), dtype="float16"))
            reshape316 = R.call_tir(cls.reshape4, (lv94,), out_sinfo=R.Tensor((1, seq_len, 96, 80), dtype="float16"))
            reshape317 = R.call_tir(cls.reshape5, (reshape316,), out_sinfo=R.Tensor((seq_len, 96, 80), dtype="float16"))
            lv400 = R.call_dps_packed("vm.builtin.attention_kv_cache_attention_with_fused_qkv", (paged_kv_cache, R.prim_value(15), R.prim_value(T.float32(1.0)), reshape317), out_sinfo=R.Tensor((seq_len, 32, 80), dtype="float16"))
            reshape318 = R.call_tir(cls.reshape6, (lv400,), out_sinfo=R.Tensor((1, seq_len, 32, 80), dtype="float16"))
            reshape319 = R.call_tir(cls.reshape7, (reshape318,), out_sinfo=R.Tensor((1, seq_len, 2560), dtype="float16"))
            lv94_1 = R.call_tir(cls.fused_dequantize2_fused_NT_matmul6_add6_add7, (gpt_neox_layers_15_attention_dense_q_weight3, gpt_neox_layers_15_attention_dense_q_scale3, reshape319, gpt_neox_layers_15_attention_dense_bias3, lv93_1), out_sinfo=R.Tensor((1, seq_len, 2560), dtype="float16"))
            layer_norm161 = R.call_tir(cls.layer_norm1, (lv94_1, gpt_neox_layers_15_post_attention_layernorm_weight3, gpt_neox_layers_15_post_attention_layernorm_bias3), out_sinfo=R.Tensor((1, seq_len, 2560), dtype="float16"))
            lv95 = R.call_tir(cls.fused_dequantize3_fused_NT_matmul7_add8_gelu1_cast3, (gpt_neox_layers_15_mlp_dense_h_to_4h_q_weight3, gpt_neox_layers_15_mlp_dense_h_to_4h_q_scale3, layer_norm161, gpt_neox_layers_15_mlp_dense_h_to_4h_bias3), out_sinfo=R.Tensor((1, seq_len, 10240), dtype="float16"))
            lv95_1 = R.call_tir(cls.fused_dequantize4_fused_NT_matmul8_add9_cast4_add7, (gpt_neox_layers_15_mlp_dense_4h_to_h_q_weight3, gpt_neox_layers_15_mlp_dense_4h_to_h_q_scale3, lv95, gpt_neox_layers_15_mlp_dense_4h_to_h_bias3, lv94_1), out_sinfo=R.Tensor((1, seq_len, 2560), dtype="float16"))
            layer_norm162 = R.call_tir(cls.layer_norm1, (lv95_1, gpt_neox_layers_16_input_layernorm_weight3, gpt_neox_layers_16_input_layernorm_bias3), out_sinfo=R.Tensor((1, seq_len, 2560), dtype="float16"))
            lv96 = R.call_tir(cls.fused_dequantize1_fused_NT_matmul5_add5, (gpt_neox_layers_16_attention_query_key_value_q_weight3, gpt_neox_layers_16_attention_query_key_value_q_scale3, layer_norm162, gpt_neox_layers_16_attention_query_key_value_bias3), out_sinfo=R.Tensor((1, seq_len, 7680), dtype="float16"))
            reshape320 = R.call_tir(cls.reshape4, (lv96,), out_sinfo=R.Tensor((1, seq_len, 96, 80), dtype="float16"))
            reshape321 = R.call_tir(cls.reshape5, (reshape320,), out_sinfo=R.Tensor((seq_len, 96, 80), dtype="float16"))
            lv405 = R.call_dps_packed("vm.builtin.attention_kv_cache_attention_with_fused_qkv", (paged_kv_cache, R.prim_value(16), R.prim_value(T.float32(1.0)), reshape321), out_sinfo=R.Tensor((seq_len, 32, 80), dtype="float16"))
            reshape322 = R.call_tir(cls.reshape6, (lv405,), out_sinfo=R.Tensor((1, seq_len, 32, 80), dtype="float16"))
            reshape323 = R.call_tir(cls.reshape7, (reshape322,), out_sinfo=R.Tensor((1, seq_len, 2560), dtype="float16"))
            lv96_1 = R.call_tir(cls.fused_dequantize2_fused_NT_matmul6_add6_add7, (gpt_neox_layers_16_attention_dense_q_weight3, gpt_neox_layers_16_attention_dense_q_scale3, reshape323, gpt_neox_layers_16_attention_dense_bias3, lv95_1), out_sinfo=R.Tensor((1, seq_len, 2560), dtype="float16"))
            layer_norm163 = R.call_tir(cls.layer_norm1, (lv96_1, gpt_neox_layers_16_post_attention_layernorm_weight3, gpt_neox_layers_16_post_attention_layernorm_bias3), out_sinfo=R.Tensor((1, seq_len, 2560), dtype="float16"))
            lv97 = R.call_tir(cls.fused_dequantize3_fused_NT_matmul7_add8_gelu1_cast3, (gpt_neox_layers_16_mlp_dense_h_to_4h_q_weight3, gpt_neox_layers_16_mlp_dense_h_to_4h_q_scale3, layer_norm163, gpt_neox_layers_16_mlp_dense_h_to_4h_bias3), out_sinfo=R.Tensor((1, seq_len, 10240), dtype="float16"))
            lv97_1 = R.call_tir(cls.fused_dequantize4_fused_NT_matmul8_add9_cast4_add7, (gpt_neox_layers_16_mlp_dense_4h_to_h_q_weight3, gpt_neox_layers_16_mlp_dense_4h_to_h_q_scale3, lv97, gpt_neox_layers_16_mlp_dense_4h_to_h_bias3, lv96_1), out_sinfo=R.Tensor((1, seq_len, 2560), dtype="float16"))
            layer_norm164 = R.call_tir(cls.layer_norm1, (lv97_1, gpt_neox_layers_17_input_layernorm_weight3, gpt_neox_layers_17_input_layernorm_bias3), out_sinfo=R.Tensor((1, seq_len, 2560), dtype="float16"))
            lv98 = R.call_tir(cls.fused_dequantize1_fused_NT_matmul5_add5, (gpt_neox_layers_17_attention_query_key_value_q_weight3, gpt_neox_layers_17_attention_query_key_value_q_scale3, layer_norm164, gpt_neox_layers_17_attention_query_key_value_bias3), out_sinfo=R.Tensor((1, seq_len, 7680), dtype="float16"))
            reshape324 = R.call_tir(cls.reshape4, (lv98,), out_sinfo=R.Tensor((1, seq_len, 96, 80), dtype="float16"))
            reshape325 = R.call_tir(cls.reshape5, (reshape324,), out_sinfo=R.Tensor((seq_len, 96, 80), dtype="float16"))
            lv410 = R.call_dps_packed("vm.builtin.attention_kv_cache_attention_with_fused_qkv", (paged_kv_cache, R.prim_value(17), R.prim_value(T.float32(1.0)), reshape325), out_sinfo=R.Tensor((seq_len, 32, 80), dtype="float16"))
            reshape326 = R.call_tir(cls.reshape6, (lv410,), out_sinfo=R.Tensor((1, seq_len, 32, 80), dtype="float16"))
            reshape327 = R.call_tir(cls.reshape7, (reshape326,), out_sinfo=R.Tensor((1, seq_len, 2560), dtype="float16"))
            lv98_1 = R.call_tir(cls.fused_dequantize2_fused_NT_matmul6_add6_add7, (gpt_neox_layers_17_attention_dense_q_weight3, gpt_neox_layers_17_attention_dense_q_scale3, reshape327, gpt_neox_layers_17_attention_dense_bias3, lv97_1), out_sinfo=R.Tensor((1, seq_len, 2560), dtype="float16"))
            layer_norm165 = R.call_tir(cls.layer_norm1, (lv98_1, gpt_neox_layers_17_post_attention_layernorm_weight3, gpt_neox_layers_17_post_attention_layernorm_bias3), out_sinfo=R.Tensor((1, seq_len, 2560), dtype="float16"))
            lv99 = R.call_tir(cls.fused_dequantize3_fused_NT_matmul7_add8_gelu1_cast3, (gpt_neox_layers_17_mlp_dense_h_to_4h_q_weight3, gpt_neox_layers_17_mlp_dense_h_to_4h_q_scale3, layer_norm165, gpt_neox_layers_17_mlp_dense_h_to_4h_bias3), out_sinfo=R.Tensor((1, seq_len, 10240), dtype="float16"))
            lv99_1 = R.call_tir(cls.fused_dequantize4_fused_NT_matmul8_add9_cast4_add7, (gpt_neox_layers_17_mlp_dense_4h_to_h_q_weight3, gpt_neox_layers_17_mlp_dense_4h_to_h_q_scale3, lv99, gpt_neox_layers_17_mlp_dense_4h_to_h_bias3, lv98_1), out_sinfo=R.Tensor((1, seq_len, 2560), dtype="float16"))
            layer_norm166 = R.call_tir(cls.layer_norm1, (lv99_1, gpt_neox_layers_18_input_layernorm_weight3, gpt_neox_layers_18_input_layernorm_bias3), out_sinfo=R.Tensor((1, seq_len, 2560), dtype="float16"))
            lv100 = R.call_tir(cls.fused_dequantize1_fused_NT_matmul5_add5, (gpt_neox_layers_18_attention_query_key_value_q_weight3, gpt_neox_layers_18_attention_query_key_value_q_scale3, layer_norm166, gpt_neox_layers_18_attention_query_key_value_bias3), out_sinfo=R.Tensor((1, seq_len, 7680), dtype="float16"))
            reshape328 = R.call_tir(cls.reshape4, (lv100,), out_sinfo=R.Tensor((1, seq_len, 96, 80), dtype="float16"))
            reshape329 = R.call_tir(cls.reshape5, (reshape328,), out_sinfo=R.Tensor((seq_len, 96, 80), dtype="float16"))
            lv415 = R.call_dps_packed("vm.builtin.attention_kv_cache_attention_with_fused_qkv", (paged_kv_cache, R.prim_value(18), R.prim_value(T.float32(1.0)), reshape329), out_sinfo=R.Tensor((seq_len, 32, 80), dtype="float16"))
            reshape330 = R.call_tir(cls.reshape6, (lv415,), out_sinfo=R.Tensor((1, seq_len, 32, 80), dtype="float16"))
            reshape331 = R.call_tir(cls.reshape7, (reshape330,), out_sinfo=R.Tensor((1, seq_len, 2560), dtype="float16"))
            lv100_1 = R.call_tir(cls.fused_dequantize2_fused_NT_matmul6_add6_add7, (gpt_neox_layers_18_attention_dense_q_weight3, gpt_neox_layers_18_attention_dense_q_scale3, reshape331, gpt_neox_layers_18_attention_dense_bias3, lv99_1), out_sinfo=R.Tensor((1, seq_len, 2560), dtype="float16"))
            layer_norm167 = R.call_tir(cls.layer_norm1, (lv100_1, gpt_neox_layers_18_post_attention_layernorm_weight3, gpt_neox_layers_18_post_attention_layernorm_bias3), out_sinfo=R.Tensor((1, seq_len, 2560), dtype="float16"))
            lv101 = R.call_tir(cls.fused_dequantize3_fused_NT_matmul7_add8_gelu1_cast3, (gpt_neox_layers_18_mlp_dense_h_to_4h_q_weight3, gpt_neox_layers_18_mlp_dense_h_to_4h_q_scale3, layer_norm167, gpt_neox_layers_18_mlp_dense_h_to_4h_bias3), out_sinfo=R.Tensor((1, seq_len, 10240), dtype="float16"))
            lv101_1 = R.call_tir(cls.fused_dequantize4_fused_NT_matmul8_add9_cast4_add7, (gpt_neox_layers_18_mlp_dense_4h_to_h_q_weight3, gpt_neox_layers_18_mlp_dense_4h_to_h_q_scale3, lv101, gpt_neox_layers_18_mlp_dense_4h_to_h_bias3, lv100_1), out_sinfo=R.Tensor((1, seq_len, 2560), dtype="float16"))
            layer_norm168 = R.call_tir(cls.layer_norm1, (lv101_1, gpt_neox_layers_19_input_layernorm_weight3, gpt_neox_layers_19_input_layernorm_bias3), out_sinfo=R.Tensor((1, seq_len, 2560), dtype="float16"))
            lv102 = R.call_tir(cls.fused_dequantize1_fused_NT_matmul5_add5, (gpt_neox_layers_19_attention_query_key_value_q_weight3, gpt_neox_layers_19_attention_query_key_value_q_scale3, layer_norm168, gpt_neox_layers_19_attention_query_key_value_bias3), out_sinfo=R.Tensor((1, seq_len, 7680), dtype="float16"))
            reshape332 = R.call_tir(cls.reshape4, (lv102,), out_sinfo=R.Tensor((1, seq_len, 96, 80), dtype="float16"))
            reshape333 = R.call_tir(cls.reshape5, (reshape332,), out_sinfo=R.Tensor((seq_len, 96, 80), dtype="float16"))
            lv420 = R.call_dps_packed("vm.builtin.attention_kv_cache_attention_with_fused_qkv", (paged_kv_cache, R.prim_value(19), R.prim_value(T.float32(1.0)), reshape333), out_sinfo=R.Tensor((seq_len, 32, 80), dtype="float16"))
            reshape334 = R.call_tir(cls.reshape6, (lv420,), out_sinfo=R.Tensor((1, seq_len, 32, 80), dtype="float16"))
            reshape335 = R.call_tir(cls.reshape7, (reshape334,), out_sinfo=R.Tensor((1, seq_len, 2560), dtype="float16"))
            lv102_1 = R.call_tir(cls.fused_dequantize2_fused_NT_matmul6_add6_add7, (gpt_neox_layers_19_attention_dense_q_weight3, gpt_neox_layers_19_attention_dense_q_scale3, reshape335, gpt_neox_layers_19_attention_dense_bias3, lv101_1), out_sinfo=R.Tensor((1, seq_len, 2560), dtype="float16"))
            layer_norm169 = R.call_tir(cls.layer_norm1, (lv102_1, gpt_neox_layers_19_post_attention_layernorm_weight3, gpt_neox_layers_19_post_attention_layernorm_bias3), out_sinfo=R.Tensor((1, seq_len, 2560), dtype="float16"))
            lv103 = R.call_tir(cls.fused_dequantize3_fused_NT_matmul7_add8_gelu1_cast3, (gpt_neox_layers_19_mlp_dense_h_to_4h_q_weight3, gpt_neox_layers_19_mlp_dense_h_to_4h_q_scale3, layer_norm169, gpt_neox_layers_19_mlp_dense_h_to_4h_bias3), out_sinfo=R.Tensor((1, seq_len, 10240), dtype="float16"))
            lv103_1 = R.call_tir(cls.fused_dequantize4_fused_NT_matmul8_add9_cast4_add7, (gpt_neox_layers_19_mlp_dense_4h_to_h_q_weight3, gpt_neox_layers_19_mlp_dense_4h_to_h_q_scale3, lv103, gpt_neox_layers_19_mlp_dense_4h_to_h_bias3, lv102_1), out_sinfo=R.Tensor((1, seq_len, 2560), dtype="float16"))
            layer_norm170 = R.call_tir(cls.layer_norm1, (lv103_1, gpt_neox_layers_20_input_layernorm_weight3, gpt_neox_layers_20_input_layernorm_bias3), out_sinfo=R.Tensor((1, seq_len, 2560), dtype="float16"))
            lv104 = R.call_tir(cls.fused_dequantize1_fused_NT_matmul5_add5, (gpt_neox_layers_20_attention_query_key_value_q_weight3, gpt_neox_layers_20_attention_query_key_value_q_scale3, layer_norm170, gpt_neox_layers_20_attention_query_key_value_bias3), out_sinfo=R.Tensor((1, seq_len, 7680), dtype="float16"))
            reshape336 = R.call_tir(cls.reshape4, (lv104,), out_sinfo=R.Tensor((1, seq_len, 96, 80), dtype="float16"))
            reshape337 = R.call_tir(cls.reshape5, (reshape336,), out_sinfo=R.Tensor((seq_len, 96, 80), dtype="float16"))
            lv425 = R.call_dps_packed("vm.builtin.attention_kv_cache_attention_with_fused_qkv", (paged_kv_cache, R.prim_value(20), R.prim_value(T.float32(1.0)), reshape337), out_sinfo=R.Tensor((seq_len, 32, 80), dtype="float16"))
            reshape338 = R.call_tir(cls.reshape6, (lv425,), out_sinfo=R.Tensor((1, seq_len, 32, 80), dtype="float16"))
            reshape339 = R.call_tir(cls.reshape7, (reshape338,), out_sinfo=R.Tensor((1, seq_len, 2560), dtype="float16"))
            lv104_1 = R.call_tir(cls.fused_dequantize2_fused_NT_matmul6_add6_add7, (gpt_neox_layers_20_attention_dense_q_weight3, gpt_neox_layers_20_attention_dense_q_scale3, reshape339, gpt_neox_layers_20_attention_dense_bias3, lv103_1), out_sinfo=R.Tensor((1, seq_len, 2560), dtype="float16"))
            layer_norm171 = R.call_tir(cls.layer_norm1, (lv104_1, gpt_neox_layers_20_post_attention_layernorm_weight3, gpt_neox_layers_20_post_attention_layernorm_bias3), out_sinfo=R.Tensor((1, seq_len, 2560), dtype="float16"))
            lv105 = R.call_tir(cls.fused_dequantize3_fused_NT_matmul7_add8_gelu1_cast3, (gpt_neox_layers_20_mlp_dense_h_to_4h_q_weight3, gpt_neox_layers_20_mlp_dense_h_to_4h_q_scale3, layer_norm171, gpt_neox_layers_20_mlp_dense_h_to_4h_bias3), out_sinfo=R.Tensor((1, seq_len, 10240), dtype="float16"))
            lv105_1 = R.call_tir(cls.fused_dequantize4_fused_NT_matmul8_add9_cast4_add7, (gpt_neox_layers_20_mlp_dense_4h_to_h_q_weight3, gpt_neox_layers_20_mlp_dense_4h_to_h_q_scale3, lv105, gpt_neox_layers_20_mlp_dense_4h_to_h_bias3, lv104_1), out_sinfo=R.Tensor((1, seq_len, 2560), dtype="float16"))
            layer_norm172 = R.call_tir(cls.layer_norm1, (lv105_1, gpt_neox_layers_21_input_layernorm_weight3, gpt_neox_layers_21_input_layernorm_bias3), out_sinfo=R.Tensor((1, seq_len, 2560), dtype="float16"))
            lv106 = R.call_tir(cls.fused_dequantize1_fused_NT_matmul5_add5, (gpt_neox_layers_21_attention_query_key_value_q_weight3, gpt_neox_layers_21_attention_query_key_value_q_scale3, layer_norm172, gpt_neox_layers_21_attention_query_key_value_bias3), out_sinfo=R.Tensor((1, seq_len, 7680), dtype="float16"))
            reshape340 = R.call_tir(cls.reshape4, (lv106,), out_sinfo=R.Tensor((1, seq_len, 96, 80), dtype="float16"))
            reshape341 = R.call_tir(cls.reshape5, (reshape340,), out_sinfo=R.Tensor((seq_len, 96, 80), dtype="float16"))
            lv430 = R.call_dps_packed("vm.builtin.attention_kv_cache_attention_with_fused_qkv", (paged_kv_cache, R.prim_value(21), R.prim_value(T.float32(1.0)), reshape341), out_sinfo=R.Tensor((seq_len, 32, 80), dtype="float16"))
            reshape342 = R.call_tir(cls.reshape6, (lv430,), out_sinfo=R.Tensor((1, seq_len, 32, 80), dtype="float16"))
            reshape343 = R.call_tir(cls.reshape7, (reshape342,), out_sinfo=R.Tensor((1, seq_len, 2560), dtype="float16"))
            lv106_1 = R.call_tir(cls.fused_dequantize2_fused_NT_matmul6_add6_add7, (gpt_neox_layers_21_attention_dense_q_weight3, gpt_neox_layers_21_attention_dense_q_scale3, reshape343, gpt_neox_layers_21_attention_dense_bias3, lv105_1), out_sinfo=R.Tensor((1, seq_len, 2560), dtype="float16"))
            layer_norm173 = R.call_tir(cls.layer_norm1, (lv106_1, gpt_neox_layers_21_post_attention_layernorm_weight3, gpt_neox_layers_21_post_attention_layernorm_bias3), out_sinfo=R.Tensor((1, seq_len, 2560), dtype="float16"))
            lv107 = R.call_tir(cls.fused_dequantize3_fused_NT_matmul7_add8_gelu1_cast3, (gpt_neox_layers_21_mlp_dense_h_to_4h_q_weight3, gpt_neox_layers_21_mlp_dense_h_to_4h_q_scale3, layer_norm173, gpt_neox_layers_21_mlp_dense_h_to_4h_bias3), out_sinfo=R.Tensor((1, seq_len, 10240), dtype="float16"))
            lv107_1 = R.call_tir(cls.fused_dequantize4_fused_NT_matmul8_add9_cast4_add7, (gpt_neox_layers_21_mlp_dense_4h_to_h_q_weight3, gpt_neox_layers_21_mlp_dense_4h_to_h_q_scale3, lv107, gpt_neox_layers_21_mlp_dense_4h_to_h_bias3, lv106_1), out_sinfo=R.Tensor((1, seq_len, 2560), dtype="float16"))
            layer_norm174 = R.call_tir(cls.layer_norm1, (lv107_1, gpt_neox_layers_22_input_layernorm_weight3, gpt_neox_layers_22_input_layernorm_bias3), out_sinfo=R.Tensor((1, seq_len, 2560), dtype="float16"))
            lv108 = R.call_tir(cls.fused_dequantize1_fused_NT_matmul5_add5, (gpt_neox_layers_22_attention_query_key_value_q_weight3, gpt_neox_layers_22_attention_query_key_value_q_scale3, layer_norm174, gpt_neox_layers_22_attention_query_key_value_bias3), out_sinfo=R.Tensor((1, seq_len, 7680), dtype="float16"))
            reshape344 = R.call_tir(cls.reshape4, (lv108,), out_sinfo=R.Tensor((1, seq_len, 96, 80), dtype="float16"))
            reshape345 = R.call_tir(cls.reshape5, (reshape344,), out_sinfo=R.Tensor((seq_len, 96, 80), dtype="float16"))
            lv435 = R.call_dps_packed("vm.builtin.attention_kv_cache_attention_with_fused_qkv", (paged_kv_cache, R.prim_value(22), R.prim_value(T.float32(1.0)), reshape345), out_sinfo=R.Tensor((seq_len, 32, 80), dtype="float16"))
            reshape346 = R.call_tir(cls.reshape6, (lv435,), out_sinfo=R.Tensor((1, seq_len, 32, 80), dtype="float16"))
            reshape347 = R.call_tir(cls.reshape7, (reshape346,), out_sinfo=R.Tensor((1, seq_len, 2560), dtype="float16"))
            lv108_1 = R.call_tir(cls.fused_dequantize2_fused_NT_matmul6_add6_add7, (gpt_neox_layers_22_attention_dense_q_weight3, gpt_neox_layers_22_attention_dense_q_scale3, reshape347, gpt_neox_layers_22_attention_dense_bias3, lv107_1), out_sinfo=R.Tensor((1, seq_len, 2560), dtype="float16"))
            layer_norm175 = R.call_tir(cls.layer_norm1, (lv108_1, gpt_neox_layers_22_post_attention_layernorm_weight3, gpt_neox_layers_22_post_attention_layernorm_bias3), out_sinfo=R.Tensor((1, seq_len, 2560), dtype="float16"))
            lv109 = R.call_tir(cls.fused_dequantize3_fused_NT_matmul7_add8_gelu1_cast3, (gpt_neox_layers_22_mlp_dense_h_to_4h_q_weight3, gpt_neox_layers_22_mlp_dense_h_to_4h_q_scale3, layer_norm175, gpt_neox_layers_22_mlp_dense_h_to_4h_bias3), out_sinfo=R.Tensor((1, seq_len, 10240), dtype="float16"))
            lv109_1 = R.call_tir(cls.fused_dequantize4_fused_NT_matmul8_add9_cast4_add7, (gpt_neox_layers_22_mlp_dense_4h_to_h_q_weight3, gpt_neox_layers_22_mlp_dense_4h_to_h_q_scale3, lv109, gpt_neox_layers_22_mlp_dense_4h_to_h_bias3, lv108_1), out_sinfo=R.Tensor((1, seq_len, 2560), dtype="float16"))
            layer_norm176 = R.call_tir(cls.layer_norm1, (lv109_1, gpt_neox_layers_23_input_layernorm_weight3, gpt_neox_layers_23_input_layernorm_bias3), out_sinfo=R.Tensor((1, seq_len, 2560), dtype="float16"))
            lv110 = R.call_tir(cls.fused_dequantize1_fused_NT_matmul5_add5, (gpt_neox_layers_23_attention_query_key_value_q_weight3, gpt_neox_layers_23_attention_query_key_value_q_scale3, layer_norm176, gpt_neox_layers_23_attention_query_key_value_bias3), out_sinfo=R.Tensor((1, seq_len, 7680), dtype="float16"))
            reshape348 = R.call_tir(cls.reshape4, (lv110,), out_sinfo=R.Tensor((1, seq_len, 96, 80), dtype="float16"))
            reshape349 = R.call_tir(cls.reshape5, (reshape348,), out_sinfo=R.Tensor((seq_len, 96, 80), dtype="float16"))
            lv440 = R.call_dps_packed("vm.builtin.attention_kv_cache_attention_with_fused_qkv", (paged_kv_cache, R.prim_value(23), R.prim_value(T.float32(1.0)), reshape349), out_sinfo=R.Tensor((seq_len, 32, 80), dtype="float16"))
            reshape350 = R.call_tir(cls.reshape6, (lv440,), out_sinfo=R.Tensor((1, seq_len, 32, 80), dtype="float16"))
            reshape351 = R.call_tir(cls.reshape7, (reshape350,), out_sinfo=R.Tensor((1, seq_len, 2560), dtype="float16"))
            lv110_1 = R.call_tir(cls.fused_dequantize2_fused_NT_matmul6_add6_add7, (gpt_neox_layers_23_attention_dense_q_weight3, gpt_neox_layers_23_attention_dense_q_scale3, reshape351, gpt_neox_layers_23_attention_dense_bias3, lv109_1), out_sinfo=R.Tensor((1, seq_len, 2560), dtype="float16"))
            layer_norm177 = R.call_tir(cls.layer_norm1, (lv110_1, gpt_neox_layers_23_post_attention_layernorm_weight3, gpt_neox_layers_23_post_attention_layernorm_bias3), out_sinfo=R.Tensor((1, seq_len, 2560), dtype="float16"))
            lv111 = R.call_tir(cls.fused_dequantize3_fused_NT_matmul7_add8_gelu1_cast3, (gpt_neox_layers_23_mlp_dense_h_to_4h_q_weight3, gpt_neox_layers_23_mlp_dense_h_to_4h_q_scale3, layer_norm177, gpt_neox_layers_23_mlp_dense_h_to_4h_bias3), out_sinfo=R.Tensor((1, seq_len, 10240), dtype="float16"))
            lv111_1 = R.call_tir(cls.fused_dequantize4_fused_NT_matmul8_add9_cast4_add7, (gpt_neox_layers_23_mlp_dense_4h_to_h_q_weight3, gpt_neox_layers_23_mlp_dense_4h_to_h_q_scale3, lv111, gpt_neox_layers_23_mlp_dense_4h_to_h_bias3, lv110_1), out_sinfo=R.Tensor((1, seq_len, 2560), dtype="float16"))
            layer_norm178 = R.call_tir(cls.layer_norm1, (lv111_1, gpt_neox_layers_24_input_layernorm_weight3, gpt_neox_layers_24_input_layernorm_bias3), out_sinfo=R.Tensor((1, seq_len, 2560), dtype="float16"))
            lv112 = R.call_tir(cls.fused_dequantize1_fused_NT_matmul5_add5, (gpt_neox_layers_24_attention_query_key_value_q_weight3, gpt_neox_layers_24_attention_query_key_value_q_scale3, layer_norm178, gpt_neox_layers_24_attention_query_key_value_bias3), out_sinfo=R.Tensor((1, seq_len, 7680), dtype="float16"))
            reshape352 = R.call_tir(cls.reshape4, (lv112,), out_sinfo=R.Tensor((1, seq_len, 96, 80), dtype="float16"))
            reshape353 = R.call_tir(cls.reshape5, (reshape352,), out_sinfo=R.Tensor((seq_len, 96, 80), dtype="float16"))
            lv445 = R.call_dps_packed("vm.builtin.attention_kv_cache_attention_with_fused_qkv", (paged_kv_cache, R.prim_value(24), R.prim_value(T.float32(1.0)), reshape353), out_sinfo=R.Tensor((seq_len, 32, 80), dtype="float16"))
            reshape354 = R.call_tir(cls.reshape6, (lv445,), out_sinfo=R.Tensor((1, seq_len, 32, 80), dtype="float16"))
            reshape355 = R.call_tir(cls.reshape7, (reshape354,), out_sinfo=R.Tensor((1, seq_len, 2560), dtype="float16"))
            lv112_1 = R.call_tir(cls.fused_dequantize2_fused_NT_matmul6_add6_add7, (gpt_neox_layers_24_attention_dense_q_weight3, gpt_neox_layers_24_attention_dense_q_scale3, reshape355, gpt_neox_layers_24_attention_dense_bias3, lv111_1), out_sinfo=R.Tensor((1, seq_len, 2560), dtype="float16"))
            layer_norm179 = R.call_tir(cls.layer_norm1, (lv112_1, gpt_neox_layers_24_post_attention_layernorm_weight3, gpt_neox_layers_24_post_attention_layernorm_bias3), out_sinfo=R.Tensor((1, seq_len, 2560), dtype="float16"))
            lv113 = R.call_tir(cls.fused_dequantize3_fused_NT_matmul7_add8_gelu1_cast3, (gpt_neox_layers_24_mlp_dense_h_to_4h_q_weight3, gpt_neox_layers_24_mlp_dense_h_to_4h_q_scale3, layer_norm179, gpt_neox_layers_24_mlp_dense_h_to_4h_bias3), out_sinfo=R.Tensor((1, seq_len, 10240), dtype="float16"))
            lv113_1 = R.call_tir(cls.fused_dequantize4_fused_NT_matmul8_add9_cast4_add7, (gpt_neox_layers_24_mlp_dense_4h_to_h_q_weight3, gpt_neox_layers_24_mlp_dense_4h_to_h_q_scale3, lv113, gpt_neox_layers_24_mlp_dense_4h_to_h_bias3, lv112_1), out_sinfo=R.Tensor((1, seq_len, 2560), dtype="float16"))
            layer_norm180 = R.call_tir(cls.layer_norm1, (lv113_1, gpt_neox_layers_25_input_layernorm_weight3, gpt_neox_layers_25_input_layernorm_bias3), out_sinfo=R.Tensor((1, seq_len, 2560), dtype="float16"))
            lv114 = R.call_tir(cls.fused_dequantize1_fused_NT_matmul5_add5, (gpt_neox_layers_25_attention_query_key_value_q_weight3, gpt_neox_layers_25_attention_query_key_value_q_scale3, layer_norm180, gpt_neox_layers_25_attention_query_key_value_bias3), out_sinfo=R.Tensor((1, seq_len, 7680), dtype="float16"))
            reshape356 = R.call_tir(cls.reshape4, (lv114,), out_sinfo=R.Tensor((1, seq_len, 96, 80), dtype="float16"))
            reshape357 = R.call_tir(cls.reshape5, (reshape356,), out_sinfo=R.Tensor((seq_len, 96, 80), dtype="float16"))
            lv450 = R.call_dps_packed("vm.builtin.attention_kv_cache_attention_with_fused_qkv", (paged_kv_cache, R.prim_value(25), R.prim_value(T.float32(1.0)), reshape357), out_sinfo=R.Tensor((seq_len, 32, 80), dtype="float16"))
            reshape358 = R.call_tir(cls.reshape6, (lv450,), out_sinfo=R.Tensor((1, seq_len, 32, 80), dtype="float16"))
            reshape359 = R.call_tir(cls.reshape7, (reshape358,), out_sinfo=R.Tensor((1, seq_len, 2560), dtype="float16"))
            lv114_1 = R.call_tir(cls.fused_dequantize2_fused_NT_matmul6_add6_add7, (gpt_neox_layers_25_attention_dense_q_weight3, gpt_neox_layers_25_attention_dense_q_scale3, reshape359, gpt_neox_layers_25_attention_dense_bias3, lv113_1), out_sinfo=R.Tensor((1, seq_len, 2560), dtype="float16"))
            layer_norm181 = R.call_tir(cls.layer_norm1, (lv114_1, gpt_neox_layers_25_post_attention_layernorm_weight3, gpt_neox_layers_25_post_attention_layernorm_bias3), out_sinfo=R.Tensor((1, seq_len, 2560), dtype="float16"))
            lv115 = R.call_tir(cls.fused_dequantize3_fused_NT_matmul7_add8_gelu1_cast3, (gpt_neox_layers_25_mlp_dense_h_to_4h_q_weight3, gpt_neox_layers_25_mlp_dense_h_to_4h_q_scale3, layer_norm181, gpt_neox_layers_25_mlp_dense_h_to_4h_bias3), out_sinfo=R.Tensor((1, seq_len, 10240), dtype="float16"))
            lv115_1 = R.call_tir(cls.fused_dequantize4_fused_NT_matmul8_add9_cast4_add7, (gpt_neox_layers_25_mlp_dense_4h_to_h_q_weight3, gpt_neox_layers_25_mlp_dense_4h_to_h_q_scale3, lv115, gpt_neox_layers_25_mlp_dense_4h_to_h_bias3, lv114_1), out_sinfo=R.Tensor((1, seq_len, 2560), dtype="float16"))
            layer_norm182 = R.call_tir(cls.layer_norm1, (lv115_1, gpt_neox_layers_26_input_layernorm_weight3, gpt_neox_layers_26_input_layernorm_bias3), out_sinfo=R.Tensor((1, seq_len, 2560), dtype="float16"))
            lv116 = R.call_tir(cls.fused_dequantize1_fused_NT_matmul5_add5, (gpt_neox_layers_26_attention_query_key_value_q_weight3, gpt_neox_layers_26_attention_query_key_value_q_scale3, layer_norm182, gpt_neox_layers_26_attention_query_key_value_bias3), out_sinfo=R.Tensor((1, seq_len, 7680), dtype="float16"))
            reshape360 = R.call_tir(cls.reshape4, (lv116,), out_sinfo=R.Tensor((1, seq_len, 96, 80), dtype="float16"))
            reshape361 = R.call_tir(cls.reshape5, (reshape360,), out_sinfo=R.Tensor((seq_len, 96, 80), dtype="float16"))
            lv455 = R.call_dps_packed("vm.builtin.attention_kv_cache_attention_with_fused_qkv", (paged_kv_cache, R.prim_value(26), R.prim_value(T.float32(1.0)), reshape361), out_sinfo=R.Tensor((seq_len, 32, 80), dtype="float16"))
            reshape362 = R.call_tir(cls.reshape6, (lv455,), out_sinfo=R.Tensor((1, seq_len, 32, 80), dtype="float16"))
            reshape363 = R.call_tir(cls.reshape7, (reshape362,), out_sinfo=R.Tensor((1, seq_len, 2560), dtype="float16"))
            lv116_1 = R.call_tir(cls.fused_dequantize2_fused_NT_matmul6_add6_add7, (gpt_neox_layers_26_attention_dense_q_weight3, gpt_neox_layers_26_attention_dense_q_scale3, reshape363, gpt_neox_layers_26_attention_dense_bias3, lv115_1), out_sinfo=R.Tensor((1, seq_len, 2560), dtype="float16"))
            layer_norm183 = R.call_tir(cls.layer_norm1, (lv116_1, gpt_neox_layers_26_post_attention_layernorm_weight3, gpt_neox_layers_26_post_attention_layernorm_bias3), out_sinfo=R.Tensor((1, seq_len, 2560), dtype="float16"))
            lv117 = R.call_tir(cls.fused_dequantize3_fused_NT_matmul7_add8_gelu1_cast3, (gpt_neox_layers_26_mlp_dense_h_to_4h_q_weight3, gpt_neox_layers_26_mlp_dense_h_to_4h_q_scale3, layer_norm183, gpt_neox_layers_26_mlp_dense_h_to_4h_bias3), out_sinfo=R.Tensor((1, seq_len, 10240), dtype="float16"))
            lv117_1 = R.call_tir(cls.fused_dequantize4_fused_NT_matmul8_add9_cast4_add7, (gpt_neox_layers_26_mlp_dense_4h_to_h_q_weight3, gpt_neox_layers_26_mlp_dense_4h_to_h_q_scale3, lv117, gpt_neox_layers_26_mlp_dense_4h_to_h_bias3, lv116_1), out_sinfo=R.Tensor((1, seq_len, 2560), dtype="float16"))
            layer_norm184 = R.call_tir(cls.layer_norm1, (lv117_1, gpt_neox_layers_27_input_layernorm_weight3, gpt_neox_layers_27_input_layernorm_bias3), out_sinfo=R.Tensor((1, seq_len, 2560), dtype="float16"))
            lv118 = R.call_tir(cls.fused_dequantize1_fused_NT_matmul5_add5, (gpt_neox_layers_27_attention_query_key_value_q_weight3, gpt_neox_layers_27_attention_query_key_value_q_scale3, layer_norm184, gpt_neox_layers_27_attention_query_key_value_bias3), out_sinfo=R.Tensor((1, seq_len, 7680), dtype="float16"))
            reshape364 = R.call_tir(cls.reshape4, (lv118,), out_sinfo=R.Tensor((1, seq_len, 96, 80), dtype="float16"))
            reshape365 = R.call_tir(cls.reshape5, (reshape364,), out_sinfo=R.Tensor((seq_len, 96, 80), dtype="float16"))
            lv460 = R.call_dps_packed("vm.builtin.attention_kv_cache_attention_with_fused_qkv", (paged_kv_cache, R.prim_value(27), R.prim_value(T.float32(1.0)), reshape365), out_sinfo=R.Tensor((seq_len, 32, 80), dtype="float16"))
            reshape366 = R.call_tir(cls.reshape6, (lv460,), out_sinfo=R.Tensor((1, seq_len, 32, 80), dtype="float16"))
            reshape367 = R.call_tir(cls.reshape7, (reshape366,), out_sinfo=R.Tensor((1, seq_len, 2560), dtype="float16"))
            lv118_1 = R.call_tir(cls.fused_dequantize2_fused_NT_matmul6_add6_add7, (gpt_neox_layers_27_attention_dense_q_weight3, gpt_neox_layers_27_attention_dense_q_scale3, reshape367, gpt_neox_layers_27_attention_dense_bias3, lv117_1), out_sinfo=R.Tensor((1, seq_len, 2560), dtype="float16"))
            layer_norm185 = R.call_tir(cls.layer_norm1, (lv118_1, gpt_neox_layers_27_post_attention_layernorm_weight3, gpt_neox_layers_27_post_attention_layernorm_bias3), out_sinfo=R.Tensor((1, seq_len, 2560), dtype="float16"))
            lv119 = R.call_tir(cls.fused_dequantize3_fused_NT_matmul7_add8_gelu1_cast3, (gpt_neox_layers_27_mlp_dense_h_to_4h_q_weight3, gpt_neox_layers_27_mlp_dense_h_to_4h_q_scale3, layer_norm185, gpt_neox_layers_27_mlp_dense_h_to_4h_bias3), out_sinfo=R.Tensor((1, seq_len, 10240), dtype="float16"))
            lv119_1 = R.call_tir(cls.fused_dequantize4_fused_NT_matmul8_add9_cast4_add7, (gpt_neox_layers_27_mlp_dense_4h_to_h_q_weight3, gpt_neox_layers_27_mlp_dense_4h_to_h_q_scale3, lv119, gpt_neox_layers_27_mlp_dense_4h_to_h_bias3, lv118_1), out_sinfo=R.Tensor((1, seq_len, 2560), dtype="float16"))
            layer_norm186 = R.call_tir(cls.layer_norm1, (lv119_1, gpt_neox_layers_28_input_layernorm_weight3, gpt_neox_layers_28_input_layernorm_bias3), out_sinfo=R.Tensor((1, seq_len, 2560), dtype="float16"))
            lv120 = R.call_tir(cls.fused_dequantize1_fused_NT_matmul5_add5, (gpt_neox_layers_28_attention_query_key_value_q_weight3, gpt_neox_layers_28_attention_query_key_value_q_scale3, layer_norm186, gpt_neox_layers_28_attention_query_key_value_bias3), out_sinfo=R.Tensor((1, seq_len, 7680), dtype="float16"))
            reshape368 = R.call_tir(cls.reshape4, (lv120,), out_sinfo=R.Tensor((1, seq_len, 96, 80), dtype="float16"))
            reshape369 = R.call_tir(cls.reshape5, (reshape368,), out_sinfo=R.Tensor((seq_len, 96, 80), dtype="float16"))
            lv465 = R.call_dps_packed("vm.builtin.attention_kv_cache_attention_with_fused_qkv", (paged_kv_cache, R.prim_value(28), R.prim_value(T.float32(1.0)), reshape369), out_sinfo=R.Tensor((seq_len, 32, 80), dtype="float16"))
            reshape370 = R.call_tir(cls.reshape6, (lv465,), out_sinfo=R.Tensor((1, seq_len, 32, 80), dtype="float16"))
            reshape371 = R.call_tir(cls.reshape7, (reshape370,), out_sinfo=R.Tensor((1, seq_len, 2560), dtype="float16"))
            lv120_1 = R.call_tir(cls.fused_dequantize2_fused_NT_matmul6_add6_add7, (gpt_neox_layers_28_attention_dense_q_weight3, gpt_neox_layers_28_attention_dense_q_scale3, reshape371, gpt_neox_layers_28_attention_dense_bias3, lv119_1), out_sinfo=R.Tensor((1, seq_len, 2560), dtype="float16"))
            layer_norm187 = R.call_tir(cls.layer_norm1, (lv120_1, gpt_neox_layers_28_post_attention_layernorm_weight3, gpt_neox_layers_28_post_attention_layernorm_bias3), out_sinfo=R.Tensor((1, seq_len, 2560), dtype="float16"))
            lv121 = R.call_tir(cls.fused_dequantize3_fused_NT_matmul7_add8_gelu1_cast3, (gpt_neox_layers_28_mlp_dense_h_to_4h_q_weight3, gpt_neox_layers_28_mlp_dense_h_to_4h_q_scale3, layer_norm187, gpt_neox_layers_28_mlp_dense_h_to_4h_bias3), out_sinfo=R.Tensor((1, seq_len, 10240), dtype="float16"))
            lv121_1 = R.call_tir(cls.fused_dequantize4_fused_NT_matmul8_add9_cast4_add7, (gpt_neox_layers_28_mlp_dense_4h_to_h_q_weight3, gpt_neox_layers_28_mlp_dense_4h_to_h_q_scale3, lv121, gpt_neox_layers_28_mlp_dense_4h_to_h_bias3, lv120_1), out_sinfo=R.Tensor((1, seq_len, 2560), dtype="float16"))
            layer_norm188 = R.call_tir(cls.layer_norm1, (lv121_1, gpt_neox_layers_29_input_layernorm_weight3, gpt_neox_layers_29_input_layernorm_bias3), out_sinfo=R.Tensor((1, seq_len, 2560), dtype="float16"))
            lv122 = R.call_tir(cls.fused_dequantize1_fused_NT_matmul5_add5, (gpt_neox_layers_29_attention_query_key_value_q_weight3, gpt_neox_layers_29_attention_query_key_value_q_scale3, layer_norm188, gpt_neox_layers_29_attention_query_key_value_bias3), out_sinfo=R.Tensor((1, seq_len, 7680), dtype="float16"))
            reshape372 = R.call_tir(cls.reshape4, (lv122,), out_sinfo=R.Tensor((1, seq_len, 96, 80), dtype="float16"))
            reshape373 = R.call_tir(cls.reshape5, (reshape372,), out_sinfo=R.Tensor((seq_len, 96, 80), dtype="float16"))
            lv470 = R.call_dps_packed("vm.builtin.attention_kv_cache_attention_with_fused_qkv", (paged_kv_cache, R.prim_value(29), R.prim_value(T.float32(1.0)), reshape373), out_sinfo=R.Tensor((seq_len, 32, 80), dtype="float16"))
            reshape374 = R.call_tir(cls.reshape6, (lv470,), out_sinfo=R.Tensor((1, seq_len, 32, 80), dtype="float16"))
            reshape375 = R.call_tir(cls.reshape7, (reshape374,), out_sinfo=R.Tensor((1, seq_len, 2560), dtype="float16"))
            lv122_1 = R.call_tir(cls.fused_dequantize2_fused_NT_matmul6_add6_add7, (gpt_neox_layers_29_attention_dense_q_weight3, gpt_neox_layers_29_attention_dense_q_scale3, reshape375, gpt_neox_layers_29_attention_dense_bias3, lv121_1), out_sinfo=R.Tensor((1, seq_len, 2560), dtype="float16"))
            layer_norm189 = R.call_tir(cls.layer_norm1, (lv122_1, gpt_neox_layers_29_post_attention_layernorm_weight3, gpt_neox_layers_29_post_attention_layernorm_bias3), out_sinfo=R.Tensor((1, seq_len, 2560), dtype="float16"))
            lv123 = R.call_tir(cls.fused_dequantize3_fused_NT_matmul7_add8_gelu1_cast3, (gpt_neox_layers_29_mlp_dense_h_to_4h_q_weight3, gpt_neox_layers_29_mlp_dense_h_to_4h_q_scale3, layer_norm189, gpt_neox_layers_29_mlp_dense_h_to_4h_bias3), out_sinfo=R.Tensor((1, seq_len, 10240), dtype="float16"))
            lv123_1 = R.call_tir(cls.fused_dequantize4_fused_NT_matmul8_add9_cast4_add7, (gpt_neox_layers_29_mlp_dense_4h_to_h_q_weight3, gpt_neox_layers_29_mlp_dense_4h_to_h_q_scale3, lv123, gpt_neox_layers_29_mlp_dense_4h_to_h_bias3, lv122_1), out_sinfo=R.Tensor((1, seq_len, 2560), dtype="float16"))
            layer_norm190 = R.call_tir(cls.layer_norm1, (lv123_1, gpt_neox_layers_30_input_layernorm_weight3, gpt_neox_layers_30_input_layernorm_bias3), out_sinfo=R.Tensor((1, seq_len, 2560), dtype="float16"))
            lv124 = R.call_tir(cls.fused_dequantize1_fused_NT_matmul5_add5, (gpt_neox_layers_30_attention_query_key_value_q_weight3, gpt_neox_layers_30_attention_query_key_value_q_scale3, layer_norm190, gpt_neox_layers_30_attention_query_key_value_bias3), out_sinfo=R.Tensor((1, seq_len, 7680), dtype="float16"))
            reshape376 = R.call_tir(cls.reshape4, (lv124,), out_sinfo=R.Tensor((1, seq_len, 96, 80), dtype="float16"))
            reshape377 = R.call_tir(cls.reshape5, (reshape376,), out_sinfo=R.Tensor((seq_len, 96, 80), dtype="float16"))
            lv475 = R.call_dps_packed("vm.builtin.attention_kv_cache_attention_with_fused_qkv", (paged_kv_cache, R.prim_value(30), R.prim_value(T.float32(1.0)), reshape377), out_sinfo=R.Tensor((seq_len, 32, 80), dtype="float16"))
            reshape378 = R.call_tir(cls.reshape6, (lv475,), out_sinfo=R.Tensor((1, seq_len, 32, 80), dtype="float16"))
            reshape379 = R.call_tir(cls.reshape7, (reshape378,), out_sinfo=R.Tensor((1, seq_len, 2560), dtype="float16"))
            lv124_1 = R.call_tir(cls.fused_dequantize2_fused_NT_matmul6_add6_add7, (gpt_neox_layers_30_attention_dense_q_weight3, gpt_neox_layers_30_attention_dense_q_scale3, reshape379, gpt_neox_layers_30_attention_dense_bias3, lv123_1), out_sinfo=R.Tensor((1, seq_len, 2560), dtype="float16"))
            layer_norm191 = R.call_tir(cls.layer_norm1, (lv124_1, gpt_neox_layers_30_post_attention_layernorm_weight3, gpt_neox_layers_30_post_attention_layernorm_bias3), out_sinfo=R.Tensor((1, seq_len, 2560), dtype="float16"))
            lv125 = R.call_tir(cls.fused_dequantize3_fused_NT_matmul7_add8_gelu1_cast3, (gpt_neox_layers_30_mlp_dense_h_to_4h_q_weight3, gpt_neox_layers_30_mlp_dense_h_to_4h_q_scale3, layer_norm191, gpt_neox_layers_30_mlp_dense_h_to_4h_bias3), out_sinfo=R.Tensor((1, seq_len, 10240), dtype="float16"))
            lv125_1 = R.call_tir(cls.fused_dequantize4_fused_NT_matmul8_add9_cast4_add7, (gpt_neox_layers_30_mlp_dense_4h_to_h_q_weight3, gpt_neox_layers_30_mlp_dense_4h_to_h_q_scale3, lv125, gpt_neox_layers_30_mlp_dense_4h_to_h_bias3, lv124_1), out_sinfo=R.Tensor((1, seq_len, 2560), dtype="float16"))
            layer_norm192 = R.call_tir(cls.layer_norm1, (lv125_1, gpt_neox_layers_31_input_layernorm_weight3, gpt_neox_layers_31_input_layernorm_bias3), out_sinfo=R.Tensor((1, seq_len, 2560), dtype="float16"))
            lv126 = R.call_tir(cls.fused_dequantize1_fused_NT_matmul5_add5, (gpt_neox_layers_31_attention_query_key_value_q_weight3, gpt_neox_layers_31_attention_query_key_value_q_scale3, layer_norm192, gpt_neox_layers_31_attention_query_key_value_bias3), out_sinfo=R.Tensor((1, seq_len, 7680), dtype="float16"))
            reshape380 = R.call_tir(cls.reshape4, (lv126,), out_sinfo=R.Tensor((1, seq_len, 96, 80), dtype="float16"))
            reshape381 = R.call_tir(cls.reshape5, (reshape380,), out_sinfo=R.Tensor((seq_len, 96, 80), dtype="float16"))
            lv480 = R.call_dps_packed("vm.builtin.attention_kv_cache_attention_with_fused_qkv", (paged_kv_cache, R.prim_value(31), R.prim_value(T.float32(1.0)), reshape381), out_sinfo=R.Tensor((seq_len, 32, 80), dtype="float16"))
            reshape382 = R.call_tir(cls.reshape6, (lv480,), out_sinfo=R.Tensor((1, seq_len, 32, 80), dtype="float16"))
            reshape383 = R.call_tir(cls.reshape7, (reshape382,), out_sinfo=R.Tensor((1, seq_len, 2560), dtype="float16"))
            lv126_1 = R.call_tir(cls.fused_dequantize2_fused_NT_matmul6_add6_add7, (gpt_neox_layers_31_attention_dense_q_weight3, gpt_neox_layers_31_attention_dense_q_scale3, reshape383, gpt_neox_layers_31_attention_dense_bias3, lv125_1), out_sinfo=R.Tensor((1, seq_len, 2560), dtype="float16"))
            layer_norm193 = R.call_tir(cls.layer_norm1, (lv126_1, gpt_neox_layers_31_post_attention_layernorm_weight3, gpt_neox_layers_31_post_attention_layernorm_bias3), out_sinfo=R.Tensor((1, seq_len, 2560), dtype="float16"))
            lv127 = R.call_tir(cls.fused_dequantize3_fused_NT_matmul7_add8_gelu1_cast3, (gpt_neox_layers_31_mlp_dense_h_to_4h_q_weight3, gpt_neox_layers_31_mlp_dense_h_to_4h_q_scale3, layer_norm193, gpt_neox_layers_31_mlp_dense_h_to_4h_bias3), out_sinfo=R.Tensor((1, seq_len, 10240), dtype="float16"))
            lv127_1 = R.call_tir(cls.fused_dequantize4_fused_NT_matmul8_add9_cast4_add7, (gpt_neox_layers_31_mlp_dense_4h_to_h_q_weight3, gpt_neox_layers_31_mlp_dense_4h_to_h_q_scale3, lv127, gpt_neox_layers_31_mlp_dense_4h_to_h_bias3, lv126_1), out_sinfo=R.Tensor((1, seq_len, 2560), dtype="float16"))
            layer_norm194 = R.call_tir(cls.layer_norm1, (lv127_1, gpt_neox_final_layer_norm_weight3, gpt_neox_final_layer_norm_bias3), out_sinfo=R.Tensor((1, seq_len, 2560), dtype="float16"))
            take1 = R.call_tir(cls.take, (layer_norm194, logit_positions), out_sinfo=R.Tensor((1, batch_size, 2560), dtype="float16"))
            lv1 = R.call_tir(cls.fused_dequantize_fused_NT_matmul9_cast5, (embed_out_q_weight3, embed_out_q_scale3, take1), out_sinfo=R.Tensor((1, batch_size, vocab_size), dtype="float32"))
            gv3: R.Tuple(R.Tensor((1, batch_size, vocab_size), dtype="float32"), R.Object) = lv1, paged_kv_cache
            R.output(gv3)
        return gv3

    @R.function
    def batch_verify(input_embeds: R.Tensor((1, "seq_len", 2560), dtype="float16"), paged_kv_cache: R.Object, packed_params: R.Tuple(R.Tensor(("vocab_size", 320), dtype="uint32"), R.Tensor(("vocab_size", 80), dtype="float16"), R.Tensor((2560,), dtype="float16"), R.Tensor((2560,), dtype="float16"), R.Tensor((2560,), dtype="float16"), R.Tensor((2560,), dtype="float16"), R.Tensor((7680, 320), dtype="uint32"), R.Tensor((7680, 80), dtype="float16"), R.Tensor((7680,), dtype="float16"), R.Tensor((2560, 320), dtype="uint32"), R.Tensor((2560, 80), dtype="float16"), R.Tensor((2560,), dtype="float16"), R.Tensor((10240, 320), dtype="uint32"), R.Tensor((10240, 80), dtype="float16"), R.Tensor((10240,), dtype="float32"), R.Tensor((2560, 1280), dtype="uint32"), R.Tensor((2560, 320), dtype="float16"), R.Tensor((2560,), dtype="float32"), R.Tensor((2560,), dtype="float16"), R.Tensor((2560,), dtype="float16"), R.Tensor((2560,), dtype="float16"), R.Tensor((2560,), dtype="float16"), R.Tensor((7680, 320), dtype="uint32"), R.Tensor((7680, 80), dtype="float16"), R.Tensor((7680,), dtype="float16"), R.Tensor((2560, 320), dtype="uint32"), R.Tensor((2560, 80), dtype="float16"), R.Tensor((2560,), dtype="float16"), R.Tensor((10240, 320), dtype="uint32"), R.Tensor((10240, 80), dtype="float16"), R.Tensor((10240,), dtype="float32"), R.Tensor((2560, 1280), dtype="uint32"), R.Tensor((2560, 320), dtype="float16"), R.Tensor((2560,), dtype="float32"), R.Tensor((2560,), dtype="float16"), R.Tensor((2560,), dtype="float16"), R.Tensor((2560,), dtype="float16"), R.Tensor((2560,), dtype="float16"), R.Tensor((7680, 320), dtype="uint32"), R.Tensor((7680, 80), dtype="float16"), R.Tensor((7680,), dtype="float16"), R.Tensor((2560, 320), dtype="uint32"), R.Tensor((2560, 80), dtype="float16"), R.Tensor((2560,), dtype="float16"), R.Tensor((10240, 320), dtype="uint32"), R.Tensor((10240, 80), dtype="float16"), R.Tensor((10240,), dtype="float32"), R.Tensor((2560, 1280), dtype="uint32"), R.Tensor((2560, 320), dtype="float16"), R.Tensor((2560,), dtype="float32"), R.Tensor((2560,), dtype="float16"), R.Tensor((2560,), dtype="float16"), R.Tensor((2560,), dtype="float16"), R.Tensor((2560,), dtype="float16"), R.Tensor((7680, 320), dtype="uint32"), R.Tensor((7680, 80), dtype="float16"), R.Tensor((7680,), dtype="float16"), R.Tensor((2560, 320), dtype="uint32"), R.Tensor((2560, 80), dtype="float16"), R.Tensor((2560,), dtype="float16"), R.Tensor((10240, 320), dtype="uint32"), R.Tensor((10240, 80), dtype="float16"), R.Tensor((10240,), dtype="float32"), R.Tensor((2560, 1280), dtype="uint32"), R.Tensor((2560, 320), dtype="float16"), R.Tensor((2560,), dtype="float32"), R.Tensor((2560,), dtype="float16"), R.Tensor((2560,), dtype="float16"), R.Tensor((2560,), dtype="float16"), R.Tensor((2560,), dtype="float16"), R.Tensor((7680, 320), dtype="uint32"), R.Tensor((7680, 80), dtype="float16"), R.Tensor((7680,), dtype="float16"), R.Tensor((2560, 320), dtype="uint32"), R.Tensor((2560, 80), dtype="float16"), R.Tensor((2560,), dtype="float16"), R.Tensor((10240, 320), dtype="uint32"), R.Tensor((10240, 80), dtype="float16"), R.Tensor((10240,), dtype="float32"), R.Tensor((2560, 1280), dtype="uint32"), R.Tensor((2560, 320), dtype="float16"), R.Tensor((2560,), dtype="float32"), R.Tensor((2560,), dtype="float16"), R.Tensor((2560,), dtype="float16"), R.Tensor((2560,), dtype="float16"), R.Tensor((2560,), dtype="float16"), R.Tensor((7680, 320), dtype="uint32"), R.Tensor((7680, 80), dtype="float16"), R.Tensor((7680,), dtype="float16"), R.Tensor((2560, 320), dtype="uint32"), R.Tensor((2560, 80), dtype="float16"), R.Tensor((2560,), dtype="float16"), R.Tensor((10240, 320), dtype="uint32"), R.Tensor((10240, 80), dtype="float16"), R.Tensor((10240,), dtype="float32"), R.Tensor((2560, 1280), dtype="uint32"), R.Tensor((2560, 320), dtype="float16"), R.Tensor((2560,), dtype="float32"), R.Tensor((2560,), dtype="float16"), R.Tensor((2560,), dtype="float16"), R.Tensor((2560,), dtype="float16"), R.Tensor((2560,), dtype="float16"), R.Tensor((7680, 320), dtype="uint32"), R.Tensor((7680, 80), dtype="float16"), R.Tensor((7680,), dtype="float16"), R.Tensor((2560, 320), dtype="uint32"), R.Tensor((2560, 80), dtype="float16"), R.Tensor((2560,), dtype="float16"), R.Tensor((10240, 320), dtype="uint32"), R.Tensor((10240, 80), dtype="float16"), R.Tensor((10240,), dtype="float32"), R.Tensor((2560, 1280), dtype="uint32"), R.Tensor((2560, 320), dtype="float16"), R.Tensor((2560,), dtype="float32"), R.Tensor((2560,), dtype="float16"), R.Tensor((2560,), dtype="float16"), R.Tensor((2560,), dtype="float16"), R.Tensor((2560,), dtype="float16"), R.Tensor((7680, 320), dtype="uint32"), R.Tensor((7680, 80), dtype="float16"), R.Tensor((7680,), dtype="float16"), R.Tensor((2560, 320), dtype="uint32"), R.Tensor((2560, 80), dtype="float16"), R.Tensor((2560,), dtype="float16"), R.Tensor((10240, 320), dtype="uint32"), R.Tensor((10240, 80), dtype="float16"), R.Tensor((10240,), dtype="float32"), R.Tensor((2560, 1280), dtype="uint32"), R.Tensor((2560, 320), dtype="float16"), R.Tensor((2560,), dtype="float32"), R.Tensor((2560,), dtype="float16"), R.Tensor((2560,), dtype="float16"), R.Tensor((2560,), dtype="float16"), R.Tensor((2560,), dtype="float16"), R.Tensor((7680, 320), dtype="uint32"), R.Tensor((7680, 80), dtype="float16"), R.Tensor((7680,), dtype="float16"), R.Tensor((2560, 320), dtype="uint32"), R.Tensor((2560, 80), dtype="float16"), R.Tensor((2560,), dtype="float16"), R.Tensor((10240, 320), dtype="uint32"), R.Tensor((10240, 80), dtype="float16"), R.Tensor((10240,), dtype="float32"), R.Tensor((2560, 1280), dtype="uint32"), R.Tensor((2560, 320), dtype="float16"), R.Tensor((2560,), dtype="float32"), R.Tensor((2560,), dtype="float16"), R.Tensor((2560,), dtype="float16"), R.Tensor((2560,), dtype="float16"), R.Tensor((2560,), dtype="float16"), R.Tensor((7680, 320), dtype="uint32"), R.Tensor((7680, 80), dtype="float16"), R.Tensor((7680,), dtype="float16"), R.Tensor((2560, 320), dtype="uint32"), R.Tensor((2560, 80), dtype="float16"), R.Tensor((2560,), dtype="float16"), R.Tensor((10240, 320), dtype="uint32"), R.Tensor((10240, 80), dtype="float16"), R.Tensor((10240,), dtype="float32"), R.Tensor((2560, 1280), dtype="uint32"), R.Tensor((2560, 320), dtype="float16"), R.Tensor((2560,), dtype="float32"), R.Tensor((2560,), dtype="float16"), R.Tensor((2560,), dtype="float16"), R.Tensor((2560,), dtype="float16"), R.Tensor((2560,), dtype="float16"), R.Tensor((7680, 320), dtype="uint32"), R.Tensor((7680, 80), dtype="float16"), R.Tensor((7680,), dtype="float16"), R.Tensor((2560, 320), dtype="uint32"), R.Tensor((2560, 80), dtype="float16"), R.Tensor((2560,), dtype="float16"), R.Tensor((10240, 320), dtype="uint32"), R.Tensor((10240, 80), dtype="float16"), R.Tensor((10240,), dtype="float32"), R.Tensor((2560, 1280), dtype="uint32"), R.Tensor((2560, 320), dtype="float16"), R.Tensor((2560,), dtype="float32"), R.Tensor((2560,), dtype="float16"), R.Tensor((2560,), dtype="float16"), R.Tensor((2560,), dtype="float16"), R.Tensor((2560,), dtype="float16"), R.Tensor((7680, 320), dtype="uint32"), R.Tensor((7680, 80), dtype="float16"), R.Tensor((7680,), dtype="float16"), R.Tensor((2560, 320), dtype="uint32"), R.Tensor((2560, 80), dtype="float16"), R.Tensor((2560,), dtype="float16"), R.Tensor((10240, 320), dtype="uint32"), R.Tensor((10240, 80), dtype="float16"), R.Tensor((10240,), dtype="float32"), R.Tensor((2560, 1280), dtype="uint32"), R.Tensor((2560, 320), dtype="float16"), R.Tensor((2560,), dtype="float32"), R.Tensor((2560,), dtype="float16"), R.Tensor((2560,), dtype="float16"), R.Tensor((2560,), dtype="float16"), R.Tensor((2560,), dtype="float16"), R.Tensor((7680, 320), dtype="uint32"), R.Tensor((7680, 80), dtype="float16"), R.Tensor((7680,), dtype="float16"), R.Tensor((2560, 320), dtype="uint32"), R.Tensor((2560, 80), dtype="float16"), R.Tensor((2560,), dtype="float16"), R.Tensor((10240, 320), dtype="uint32"), R.Tensor((10240, 80), dtype="float16"), R.Tensor((10240,), dtype="float32"), R.Tensor((2560, 1280), dtype="uint32"), R.Tensor((2560, 320), dtype="float16"), R.Tensor((2560,), dtype="float32"), R.Tensor((2560,), dtype="float16"), R.Tensor((2560,), dtype="float16"), R.Tensor((2560,), dtype="float16"), R.Tensor((2560,), dtype="float16"), R.Tensor((7680, 320), dtype="uint32"), R.Tensor((7680, 80), dtype="float16"), R.Tensor((7680,), dtype="float16"), R.Tensor((2560, 320), dtype="uint32"), R.Tensor((2560, 80), dtype="float16"), R.Tensor((2560,), dtype="float16"), R.Tensor((10240, 320), dtype="uint32"), R.Tensor((10240, 80), dtype="float16"), R.Tensor((10240,), dtype="float32"), R.Tensor((2560, 1280), dtype="uint32"), R.Tensor((2560, 320), dtype="float16"), R.Tensor((2560,), dtype="float32"), R.Tensor((2560,), dtype="float16"), R.Tensor((2560,), dtype="float16"), R.Tensor((2560,), dtype="float16"), R.Tensor((2560,), dtype="float16"), R.Tensor((7680, 320), dtype="uint32"), R.Tensor((7680, 80), dtype="float16"), R.Tensor((7680,), dtype="float16"), R.Tensor((2560, 320), dtype="uint32"), R.Tensor((2560, 80), dtype="float16"), R.Tensor((2560,), dtype="float16"), R.Tensor((10240, 320), dtype="uint32"), R.Tensor((10240, 80), dtype="float16"), R.Tensor((10240,), dtype="float32"), R.Tensor((2560, 1280), dtype="uint32"), R.Tensor((2560, 320), dtype="float16"), R.Tensor((2560,), dtype="float32"), R.Tensor((2560,), dtype="float16"), R.Tensor((2560,), dtype="float16"), R.Tensor((2560,), dtype="float16"), R.Tensor((2560,), dtype="float16"), R.Tensor((7680, 320), dtype="uint32"), R.Tensor((7680, 80), dtype="float16"), R.Tensor((7680,), dtype="float16"), R.Tensor((2560, 320), dtype="uint32"), R.Tensor((2560, 80), dtype="float16"), R.Tensor((2560,), dtype="float16"), R.Tensor((10240, 320), dtype="uint32"), R.Tensor((10240, 80), dtype="float16"), R.Tensor((10240,), dtype="float32"), R.Tensor((2560, 1280), dtype="uint32"), R.Tensor((2560, 320), dtype="float16"), R.Tensor((2560,), dtype="float32"), R.Tensor((2560,), dtype="float16"), R.Tensor((2560,), dtype="float16"), R.Tensor((2560,), dtype="float16"), R.Tensor((2560,), dtype="float16"), R.Tensor((7680, 320), dtype="uint32"), R.Tensor((7680, 80), dtype="float16"), R.Tensor((7680,), dtype="float16"), R.Tensor((2560, 320), dtype="uint32"), R.Tensor((2560, 80), dtype="float16"), R.Tensor((2560,), dtype="float16"), R.Tensor((10240, 320), dtype="uint32"), R.Tensor((10240, 80), dtype="float16"), R.Tensor((10240,), dtype="float32"), R.Tensor((2560, 1280), dtype="uint32"), R.Tensor((2560, 320), dtype="float16"), R.Tensor((2560,), dtype="float32"), R.Tensor((2560,), dtype="float16"), R.Tensor((2560,), dtype="float16"), R.Tensor((2560,), dtype="float16"), R.Tensor((2560,), dtype="float16"), R.Tensor((7680, 320), dtype="uint32"), R.Tensor((7680, 80), dtype="float16"), R.Tensor((7680,), dtype="float16"), R.Tensor((2560, 320), dtype="uint32"), R.Tensor((2560, 80), dtype="float16"), R.Tensor((2560,), dtype="float16"), R.Tensor((10240, 320), dtype="uint32"), R.Tensor((10240, 80), dtype="float16"), R.Tensor((10240,), dtype="float32"), R.Tensor((2560, 1280), dtype="uint32"), R.Tensor((2560, 320), dtype="float16"), R.Tensor((2560,), dtype="float32"), R.Tensor((2560,), dtype="float16"), R.Tensor((2560,), dtype="float16"), R.Tensor((2560,), dtype="float16"), R.Tensor((2560,), dtype="float16"), R.Tensor((7680, 320), dtype="uint32"), R.Tensor((7680, 80), dtype="float16"), R.Tensor((7680,), dtype="float16"), R.Tensor((2560, 320), dtype="uint32"), R.Tensor((2560, 80), dtype="float16"), R.Tensor((2560,), dtype="float16"), R.Tensor((10240, 320), dtype="uint32"), R.Tensor((10240, 80), dtype="float16"), R.Tensor((10240,), dtype="float32"), R.Tensor((2560, 1280), dtype="uint32"), R.Tensor((2560, 320), dtype="float16"), R.Tensor((2560,), dtype="float32"), R.Tensor((2560,), dtype="float16"), R.Tensor((2560,), dtype="float16"), R.Tensor((2560,), dtype="float16"), R.Tensor((2560,), dtype="float16"), R.Tensor((7680, 320), dtype="uint32"), R.Tensor((7680, 80), dtype="float16"), R.Tensor((7680,), dtype="float16"), R.Tensor((2560, 320), dtype="uint32"), R.Tensor((2560, 80), dtype="float16"), R.Tensor((2560,), dtype="float16"), R.Tensor((10240, 320), dtype="uint32"), R.Tensor((10240, 80), dtype="float16"), R.Tensor((10240,), dtype="float32"), R.Tensor((2560, 1280), dtype="uint32"), R.Tensor((2560, 320), dtype="float16"), R.Tensor((2560,), dtype="float32"), R.Tensor((2560,), dtype="float16"), R.Tensor((2560,), dtype="float16"), R.Tensor((2560,), dtype="float16"), R.Tensor((2560,), dtype="float16"), R.Tensor((7680, 320), dtype="uint32"), R.Tensor((7680, 80), dtype="float16"), R.Tensor((7680,), dtype="float16"), R.Tensor((2560, 320), dtype="uint32"), R.Tensor((2560, 80), dtype="float16"), R.Tensor((2560,), dtype="float16"), R.Tensor((10240, 320), dtype="uint32"), R.Tensor((10240, 80), dtype="float16"), R.Tensor((10240,), dtype="float32"), R.Tensor((2560, 1280), dtype="uint32"), R.Tensor((2560, 320), dtype="float16"), R.Tensor((2560,), dtype="float32"), R.Tensor((2560,), dtype="float16"), R.Tensor((2560,), dtype="float16"), R.Tensor((2560,), dtype="float16"), R.Tensor((2560,), dtype="float16"), R.Tensor((7680, 320), dtype="uint32"), R.Tensor((7680, 80), dtype="float16"), R.Tensor((7680,), dtype="float16"), R.Tensor((2560, 320), dtype="uint32"), R.Tensor((2560, 80), dtype="float16"), R.Tensor((2560,), dtype="float16"), R.Tensor((10240, 320), dtype="uint32"), R.Tensor((10240, 80), dtype="float16"), R.Tensor((10240,), dtype="float32"), R.Tensor((2560, 1280), dtype="uint32"), R.Tensor((2560, 320), dtype="float16"), R.Tensor((2560,), dtype="float32"), R.Tensor((2560,), dtype="float16"), R.Tensor((2560,), dtype="float16"), R.Tensor((2560,), dtype="float16"), R.Tensor((2560,), dtype="float16"), R.Tensor((7680, 320), dtype="uint32"), R.Tensor((7680, 80), dtype="float16"), R.Tensor((7680,), dtype="float16"), R.Tensor((2560, 320), dtype="uint32"), R.Tensor((2560, 80), dtype="float16"), R.Tensor((2560,), dtype="float16"), R.Tensor((10240, 320), dtype="uint32"), R.Tensor((10240, 80), dtype="float16"), R.Tensor((10240,), dtype="float32"), R.Tensor((2560, 1280), dtype="uint32"), R.Tensor((2560, 320), dtype="float16"), R.Tensor((2560,), dtype="float32"), R.Tensor((2560,), dtype="float16"), R.Tensor((2560,), dtype="float16"), R.Tensor((2560,), dtype="float16"), R.Tensor((2560,), dtype="float16"), R.Tensor((7680, 320), dtype="uint32"), R.Tensor((7680, 80), dtype="float16"), R.Tensor((7680,), dtype="float16"), R.Tensor((2560, 320), dtype="uint32"), R.Tensor((2560, 80), dtype="float16"), R.Tensor((2560,), dtype="float16"), R.Tensor((10240, 320), dtype="uint32"), R.Tensor((10240, 80), dtype="float16"), R.Tensor((10240,), dtype="float32"), R.Tensor((2560, 1280), dtype="uint32"), R.Tensor((2560, 320), dtype="float16"), R.Tensor((2560,), dtype="float32"), R.Tensor((2560,), dtype="float16"), R.Tensor((2560,), dtype="float16"), R.Tensor((2560,), dtype="float16"), R.Tensor((2560,), dtype="float16"), R.Tensor((7680, 320), dtype="uint32"), R.Tensor((7680, 80), dtype="float16"), R.Tensor((7680,), dtype="float16"), R.Tensor((2560, 320), dtype="uint32"), R.Tensor((2560, 80), dtype="float16"), R.Tensor((2560,), dtype="float16"), R.Tensor((10240, 320), dtype="uint32"), R.Tensor((10240, 80), dtype="float16"), R.Tensor((10240,), dtype="float32"), R.Tensor((2560, 1280), dtype="uint32"), R.Tensor((2560, 320), dtype="float16"), R.Tensor((2560,), dtype="float32"), R.Tensor((2560,), dtype="float16"), R.Tensor((2560,), dtype="float16"), R.Tensor((2560,), dtype="float16"), R.Tensor((2560,), dtype="float16"), R.Tensor((7680, 320), dtype="uint32"), R.Tensor((7680, 80), dtype="float16"), R.Tensor((7680,), dtype="float16"), R.Tensor((2560, 320), dtype="uint32"), R.Tensor((2560, 80), dtype="float16"), R.Tensor((2560,), dtype="float16"), R.Tensor((10240, 320), dtype="uint32"), R.Tensor((10240, 80), dtype="float16"), R.Tensor((10240,), dtype="float32"), R.Tensor((2560, 1280), dtype="uint32"), R.Tensor((2560, 320), dtype="float16"), R.Tensor((2560,), dtype="float32"), R.Tensor((2560,), dtype="float16"), R.Tensor((2560,), dtype="float16"), R.Tensor((2560,), dtype="float16"), R.Tensor((2560,), dtype="float16"), R.Tensor((7680, 320), dtype="uint32"), R.Tensor((7680, 80), dtype="float16"), R.Tensor((7680,), dtype="float16"), R.Tensor((2560, 320), dtype="uint32"), R.Tensor((2560, 80), dtype="float16"), R.Tensor((2560,), dtype="float16"), R.Tensor((10240, 320), dtype="uint32"), R.Tensor((10240, 80), dtype="float16"), R.Tensor((10240,), dtype="float32"), R.Tensor((2560, 1280), dtype="uint32"), R.Tensor((2560, 320), dtype="float16"), R.Tensor((2560,), dtype="float32"), R.Tensor((2560,), dtype="float16"), R.Tensor((2560,), dtype="float16"), R.Tensor((2560,), dtype="float16"), R.Tensor((2560,), dtype="float16"), R.Tensor((7680, 320), dtype="uint32"), R.Tensor((7680, 80), dtype="float16"), R.Tensor((7680,), dtype="float16"), R.Tensor((2560, 320), dtype="uint32"), R.Tensor((2560, 80), dtype="float16"), R.Tensor((2560,), dtype="float16"), R.Tensor((10240, 320), dtype="uint32"), R.Tensor((10240, 80), dtype="float16"), R.Tensor((10240,), dtype="float32"), R.Tensor((2560, 1280), dtype="uint32"), R.Tensor((2560, 320), dtype="float16"), R.Tensor((2560,), dtype="float32"), R.Tensor((2560,), dtype="float16"), R.Tensor((2560,), dtype="float16"), R.Tensor((2560,), dtype="float16"), R.Tensor((2560,), dtype="float16"), R.Tensor((7680, 320), dtype="uint32"), R.Tensor((7680, 80), dtype="float16"), R.Tensor((7680,), dtype="float16"), R.Tensor((2560, 320), dtype="uint32"), R.Tensor((2560, 80), dtype="float16"), R.Tensor((2560,), dtype="float16"), R.Tensor((10240, 320), dtype="uint32"), R.Tensor((10240, 80), dtype="float16"), R.Tensor((10240,), dtype="float32"), R.Tensor((2560, 1280), dtype="uint32"), R.Tensor((2560, 320), dtype="float16"), R.Tensor((2560,), dtype="float32"), R.Tensor((2560,), dtype="float16"), R.Tensor((2560,), dtype="float16"), R.Tensor((2560,), dtype="float16"), R.Tensor((2560,), dtype="float16"), R.Tensor((7680, 320), dtype="uint32"), R.Tensor((7680, 80), dtype="float16"), R.Tensor((7680,), dtype="float16"), R.Tensor((2560, 320), dtype="uint32"), R.Tensor((2560, 80), dtype="float16"), R.Tensor((2560,), dtype="float16"), R.Tensor((10240, 320), dtype="uint32"), R.Tensor((10240, 80), dtype="float16"), R.Tensor((10240,), dtype="float32"), R.Tensor((2560, 1280), dtype="uint32"), R.Tensor((2560, 320), dtype="float16"), R.Tensor((2560,), dtype="float32"), R.Tensor((2560,), dtype="float16"), R.Tensor((2560,), dtype="float16"), R.Tensor((2560,), dtype="float16"), R.Tensor((2560,), dtype="float16"), R.Tensor((7680, 320), dtype="uint32"), R.Tensor((7680, 80), dtype="float16"), R.Tensor((7680,), dtype="float16"), R.Tensor((2560, 320), dtype="uint32"), R.Tensor((2560, 80), dtype="float16"), R.Tensor((2560,), dtype="float16"), R.Tensor((10240, 320), dtype="uint32"), R.Tensor((10240, 80), dtype="float16"), R.Tensor((10240,), dtype="float32"), R.Tensor((2560, 1280), dtype="uint32"), R.Tensor((2560, 320), dtype="float16"), R.Tensor((2560,), dtype="float32"), R.Tensor((2560,), dtype="float16"), R.Tensor((2560,), dtype="float16"), R.Tensor((2560,), dtype="float16"), R.Tensor((2560,), dtype="float16"), R.Tensor((7680, 320), dtype="uint32"), R.Tensor((7680, 80), dtype="float16"), R.Tensor((7680,), dtype="float16"), R.Tensor((2560, 320), dtype="uint32"), R.Tensor((2560, 80), dtype="float16"), R.Tensor((2560,), dtype="float16"), R.Tensor((10240, 320), dtype="uint32"), R.Tensor((10240, 80), dtype="float16"), R.Tensor((10240,), dtype="float32"), R.Tensor((2560, 1280), dtype="uint32"), R.Tensor((2560, 320), dtype="float16"), R.Tensor((2560,), dtype="float32"), R.Tensor((2560,), dtype="float16"), R.Tensor((2560,), dtype="float16"), R.Tensor(("vocab_size", 320), dtype="uint32"), R.Tensor(("vocab_size", 80), dtype="float16"))) -> R.Tuple(R.Tensor((1, "seq_len", "vocab_size"), dtype="float32"), R.Object):
        seq_len = T.int64()
        vocab_size = T.int64()
        R.func_attr({"num_input": 2, "pipeline_parallel_stages": 1, "relax.memory_plan_dynamic_func_output": True, "relax.rewrite_cuda_graph.capture_symbolic_vars": ["batch_size", "seq_len"], "tir_non_negative_var": ["vocab_size"], "tir_var_upper_bound": {"batch_size": 128, "seq_len": 2048, "total_seq_len": 2048}})
        cls = Module
        with R.dataflow():
            gpt_neox_layers_0_input_layernorm_weight5: R.Tensor((2560,), dtype="float16") = packed_params[2]
            gpt_neox_layers_0_input_layernorm_bias5: R.Tensor((2560,), dtype="float16") = packed_params[3]
            gpt_neox_layers_0_post_attention_layernorm_weight5: R.Tensor((2560,), dtype="float16") = packed_params[4]
            gpt_neox_layers_0_post_attention_layernorm_bias5: R.Tensor((2560,), dtype="float16") = packed_params[5]
            gpt_neox_layers_0_attention_query_key_value_q_weight5: R.Tensor((7680, 320), dtype="uint32") = packed_params[6]
            gpt_neox_layers_0_attention_query_key_value_q_scale5: R.Tensor((7680, 80), dtype="float16") = packed_params[7]
            gpt_neox_layers_0_attention_query_key_value_bias5: R.Tensor((7680,), dtype="float16") = packed_params[8]
            gpt_neox_layers_0_attention_dense_q_weight5: R.Tensor((2560, 320), dtype="uint32") = packed_params[9]
            gpt_neox_layers_0_attention_dense_q_scale5: R.Tensor((2560, 80), dtype="float16") = packed_params[10]
            gpt_neox_layers_0_attention_dense_bias5: R.Tensor((2560,), dtype="float16") = packed_params[11]
            gpt_neox_layers_0_mlp_dense_h_to_4h_q_weight5: R.Tensor((10240, 320), dtype="uint32") = packed_params[12]
            gpt_neox_layers_0_mlp_dense_h_to_4h_q_scale5: R.Tensor((10240, 80), dtype="float16") = packed_params[13]
            gpt_neox_layers_0_mlp_dense_h_to_4h_bias5: R.Tensor((10240,), dtype="float32") = packed_params[14]
            gpt_neox_layers_0_mlp_dense_4h_to_h_q_weight5: R.Tensor((2560, 1280), dtype="uint32") = packed_params[15]
            gpt_neox_layers_0_mlp_dense_4h_to_h_q_scale5: R.Tensor((2560, 320), dtype="float16") = packed_params[16]
            gpt_neox_layers_0_mlp_dense_4h_to_h_bias5: R.Tensor((2560,), dtype="float32") = packed_params[17]
            gpt_neox_layers_1_input_layernorm_weight5: R.Tensor((2560,), dtype="float16") = packed_params[18]
            gpt_neox_layers_1_input_layernorm_bias5: R.Tensor((2560,), dtype="float16") = packed_params[19]
            gpt_neox_layers_1_post_attention_layernorm_weight5: R.Tensor((2560,), dtype="float16") = packed_params[20]
            gpt_neox_layers_1_post_attention_layernorm_bias5: R.Tensor((2560,), dtype="float16") = packed_params[21]
            gpt_neox_layers_1_attention_query_key_value_q_weight5: R.Tensor((7680, 320), dtype="uint32") = packed_params[22]
            gpt_neox_layers_1_attention_query_key_value_q_scale5: R.Tensor((7680, 80), dtype="float16") = packed_params[23]
            gpt_neox_layers_1_attention_query_key_value_bias5: R.Tensor((7680,), dtype="float16") = packed_params[24]
            gpt_neox_layers_1_attention_dense_q_weight5: R.Tensor((2560, 320), dtype="uint32") = packed_params[25]
            gpt_neox_layers_1_attention_dense_q_scale5: R.Tensor((2560, 80), dtype="float16") = packed_params[26]
            gpt_neox_layers_1_attention_dense_bias5: R.Tensor((2560,), dtype="float16") = packed_params[27]
            gpt_neox_layers_1_mlp_dense_h_to_4h_q_weight5: R.Tensor((10240, 320), dtype="uint32") = packed_params[28]
            gpt_neox_layers_1_mlp_dense_h_to_4h_q_scale5: R.Tensor((10240, 80), dtype="float16") = packed_params[29]
            gpt_neox_layers_1_mlp_dense_h_to_4h_bias5: R.Tensor((10240,), dtype="float32") = packed_params[30]
            gpt_neox_layers_1_mlp_dense_4h_to_h_q_weight5: R.Tensor((2560, 1280), dtype="uint32") = packed_params[31]
            gpt_neox_layers_1_mlp_dense_4h_to_h_q_scale5: R.Tensor((2560, 320), dtype="float16") = packed_params[32]
            gpt_neox_layers_1_mlp_dense_4h_to_h_bias5: R.Tensor((2560,), dtype="float32") = packed_params[33]
            gpt_neox_layers_2_input_layernorm_weight5: R.Tensor((2560,), dtype="float16") = packed_params[34]
            gpt_neox_layers_2_input_layernorm_bias5: R.Tensor((2560,), dtype="float16") = packed_params[35]
            gpt_neox_layers_2_post_attention_layernorm_weight5: R.Tensor((2560,), dtype="float16") = packed_params[36]
            gpt_neox_layers_2_post_attention_layernorm_bias5: R.Tensor((2560,), dtype="float16") = packed_params[37]
            gpt_neox_layers_2_attention_query_key_value_q_weight5: R.Tensor((7680, 320), dtype="uint32") = packed_params[38]
            gpt_neox_layers_2_attention_query_key_value_q_scale5: R.Tensor((7680, 80), dtype="float16") = packed_params[39]
            gpt_neox_layers_2_attention_query_key_value_bias5: R.Tensor((7680,), dtype="float16") = packed_params[40]
            gpt_neox_layers_2_attention_dense_q_weight5: R.Tensor((2560, 320), dtype="uint32") = packed_params[41]
            gpt_neox_layers_2_attention_dense_q_scale5: R.Tensor((2560, 80), dtype="float16") = packed_params[42]
            gpt_neox_layers_2_attention_dense_bias5: R.Tensor((2560,), dtype="float16") = packed_params[43]
            gpt_neox_layers_2_mlp_dense_h_to_4h_q_weight5: R.Tensor((10240, 320), dtype="uint32") = packed_params[44]
            gpt_neox_layers_2_mlp_dense_h_to_4h_q_scale5: R.Tensor((10240, 80), dtype="float16") = packed_params[45]
            gpt_neox_layers_2_mlp_dense_h_to_4h_bias5: R.Tensor((10240,), dtype="float32") = packed_params[46]
            gpt_neox_layers_2_mlp_dense_4h_to_h_q_weight5: R.Tensor((2560, 1280), dtype="uint32") = packed_params[47]
            gpt_neox_layers_2_mlp_dense_4h_to_h_q_scale5: R.Tensor((2560, 320), dtype="float16") = packed_params[48]
            gpt_neox_layers_2_mlp_dense_4h_to_h_bias5: R.Tensor((2560,), dtype="float32") = packed_params[49]
            gpt_neox_layers_3_input_layernorm_weight5: R.Tensor((2560,), dtype="float16") = packed_params[50]
            gpt_neox_layers_3_input_layernorm_bias5: R.Tensor((2560,), dtype="float16") = packed_params[51]
            gpt_neox_layers_3_post_attention_layernorm_weight5: R.Tensor((2560,), dtype="float16") = packed_params[52]
            gpt_neox_layers_3_post_attention_layernorm_bias5: R.Tensor((2560,), dtype="float16") = packed_params[53]
            gpt_neox_layers_3_attention_query_key_value_q_weight5: R.Tensor((7680, 320), dtype="uint32") = packed_params[54]
            gpt_neox_layers_3_attention_query_key_value_q_scale5: R.Tensor((7680, 80), dtype="float16") = packed_params[55]
            gpt_neox_layers_3_attention_query_key_value_bias5: R.Tensor((7680,), dtype="float16") = packed_params[56]
            gpt_neox_layers_3_attention_dense_q_weight5: R.Tensor((2560, 320), dtype="uint32") = packed_params[57]
            gpt_neox_layers_3_attention_dense_q_scale5: R.Tensor((2560, 80), dtype="float16") = packed_params[58]
            gpt_neox_layers_3_attention_dense_bias5: R.Tensor((2560,), dtype="float16") = packed_params[59]
            gpt_neox_layers_3_mlp_dense_h_to_4h_q_weight5: R.Tensor((10240, 320), dtype="uint32") = packed_params[60]
            gpt_neox_layers_3_mlp_dense_h_to_4h_q_scale5: R.Tensor((10240, 80), dtype="float16") = packed_params[61]
            gpt_neox_layers_3_mlp_dense_h_to_4h_bias5: R.Tensor((10240,), dtype="float32") = packed_params[62]
            gpt_neox_layers_3_mlp_dense_4h_to_h_q_weight5: R.Tensor((2560, 1280), dtype="uint32") = packed_params[63]
            gpt_neox_layers_3_mlp_dense_4h_to_h_q_scale5: R.Tensor((2560, 320), dtype="float16") = packed_params[64]
            gpt_neox_layers_3_mlp_dense_4h_to_h_bias5: R.Tensor((2560,), dtype="float32") = packed_params[65]
            gpt_neox_layers_4_input_layernorm_weight5: R.Tensor((2560,), dtype="float16") = packed_params[66]
            gpt_neox_layers_4_input_layernorm_bias5: R.Tensor((2560,), dtype="float16") = packed_params[67]
            gpt_neox_layers_4_post_attention_layernorm_weight5: R.Tensor((2560,), dtype="float16") = packed_params[68]
            gpt_neox_layers_4_post_attention_layernorm_bias5: R.Tensor((2560,), dtype="float16") = packed_params[69]
            gpt_neox_layers_4_attention_query_key_value_q_weight5: R.Tensor((7680, 320), dtype="uint32") = packed_params[70]
            gpt_neox_layers_4_attention_query_key_value_q_scale5: R.Tensor((7680, 80), dtype="float16") = packed_params[71]
            gpt_neox_layers_4_attention_query_key_value_bias5: R.Tensor((7680,), dtype="float16") = packed_params[72]
            gpt_neox_layers_4_attention_dense_q_weight5: R.Tensor((2560, 320), dtype="uint32") = packed_params[73]
            gpt_neox_layers_4_attention_dense_q_scale5: R.Tensor((2560, 80), dtype="float16") = packed_params[74]
            gpt_neox_layers_4_attention_dense_bias5: R.Tensor((2560,), dtype="float16") = packed_params[75]
            gpt_neox_layers_4_mlp_dense_h_to_4h_q_weight5: R.Tensor((10240, 320), dtype="uint32") = packed_params[76]
            gpt_neox_layers_4_mlp_dense_h_to_4h_q_scale5: R.Tensor((10240, 80), dtype="float16") = packed_params[77]
            gpt_neox_layers_4_mlp_dense_h_to_4h_bias5: R.Tensor((10240,), dtype="float32") = packed_params[78]
            gpt_neox_layers_4_mlp_dense_4h_to_h_q_weight5: R.Tensor((2560, 1280), dtype="uint32") = packed_params[79]
            gpt_neox_layers_4_mlp_dense_4h_to_h_q_scale5: R.Tensor((2560, 320), dtype="float16") = packed_params[80]
            gpt_neox_layers_4_mlp_dense_4h_to_h_bias5: R.Tensor((2560,), dtype="float32") = packed_params[81]
            gpt_neox_layers_5_input_layernorm_weight5: R.Tensor((2560,), dtype="float16") = packed_params[82]
            gpt_neox_layers_5_input_layernorm_bias5: R.Tensor((2560,), dtype="float16") = packed_params[83]
            gpt_neox_layers_5_post_attention_layernorm_weight5: R.Tensor((2560,), dtype="float16") = packed_params[84]
            gpt_neox_layers_5_post_attention_layernorm_bias5: R.Tensor((2560,), dtype="float16") = packed_params[85]
            gpt_neox_layers_5_attention_query_key_value_q_weight5: R.Tensor((7680, 320), dtype="uint32") = packed_params[86]
            gpt_neox_layers_5_attention_query_key_value_q_scale5: R.Tensor((7680, 80), dtype="float16") = packed_params[87]
            gpt_neox_layers_5_attention_query_key_value_bias5: R.Tensor((7680,), dtype="float16") = packed_params[88]
            gpt_neox_layers_5_attention_dense_q_weight5: R.Tensor((2560, 320), dtype="uint32") = packed_params[89]
            gpt_neox_layers_5_attention_dense_q_scale5: R.Tensor((2560, 80), dtype="float16") = packed_params[90]
            gpt_neox_layers_5_attention_dense_bias5: R.Tensor((2560,), dtype="float16") = packed_params[91]
            gpt_neox_layers_5_mlp_dense_h_to_4h_q_weight5: R.Tensor((10240, 320), dtype="uint32") = packed_params[92]
            gpt_neox_layers_5_mlp_dense_h_to_4h_q_scale5: R.Tensor((10240, 80), dtype="float16") = packed_params[93]
            gpt_neox_layers_5_mlp_dense_h_to_4h_bias5: R.Tensor((10240,), dtype="float32") = packed_params[94]
            gpt_neox_layers_5_mlp_dense_4h_to_h_q_weight5: R.Tensor((2560, 1280), dtype="uint32") = packed_params[95]
            gpt_neox_layers_5_mlp_dense_4h_to_h_q_scale5: R.Tensor((2560, 320), dtype="float16") = packed_params[96]
            gpt_neox_layers_5_mlp_dense_4h_to_h_bias5: R.Tensor((2560,), dtype="float32") = packed_params[97]
            gpt_neox_layers_6_input_layernorm_weight5: R.Tensor((2560,), dtype="float16") = packed_params[98]
            gpt_neox_layers_6_input_layernorm_bias5: R.Tensor((2560,), dtype="float16") = packed_params[99]
            gpt_neox_layers_6_post_attention_layernorm_weight5: R.Tensor((2560,), dtype="float16") = packed_params[100]
            gpt_neox_layers_6_post_attention_layernorm_bias5: R.Tensor((2560,), dtype="float16") = packed_params[101]
            gpt_neox_layers_6_attention_query_key_value_q_weight5: R.Tensor((7680, 320), dtype="uint32") = packed_params[102]
            gpt_neox_layers_6_attention_query_key_value_q_scale5: R.Tensor((7680, 80), dtype="float16") = packed_params[103]
            gpt_neox_layers_6_attention_query_key_value_bias5: R.Tensor((7680,), dtype="float16") = packed_params[104]
            gpt_neox_layers_6_attention_dense_q_weight5: R.Tensor((2560, 320), dtype="uint32") = packed_params[105]
            gpt_neox_layers_6_attention_dense_q_scale5: R.Tensor((2560, 80), dtype="float16") = packed_params[106]
            gpt_neox_layers_6_attention_dense_bias5: R.Tensor((2560,), dtype="float16") = packed_params[107]
            gpt_neox_layers_6_mlp_dense_h_to_4h_q_weight5: R.Tensor((10240, 320), dtype="uint32") = packed_params[108]
            gpt_neox_layers_6_mlp_dense_h_to_4h_q_scale5: R.Tensor((10240, 80), dtype="float16") = packed_params[109]
            gpt_neox_layers_6_mlp_dense_h_to_4h_bias5: R.Tensor((10240,), dtype="float32") = packed_params[110]
            gpt_neox_layers_6_mlp_dense_4h_to_h_q_weight5: R.Tensor((2560, 1280), dtype="uint32") = packed_params[111]
            gpt_neox_layers_6_mlp_dense_4h_to_h_q_scale5: R.Tensor((2560, 320), dtype="float16") = packed_params[112]
            gpt_neox_layers_6_mlp_dense_4h_to_h_bias5: R.Tensor((2560,), dtype="float32") = packed_params[113]
            gpt_neox_layers_7_input_layernorm_weight5: R.Tensor((2560,), dtype="float16") = packed_params[114]
            gpt_neox_layers_7_input_layernorm_bias5: R.Tensor((2560,), dtype="float16") = packed_params[115]
            gpt_neox_layers_7_post_attention_layernorm_weight5: R.Tensor((2560,), dtype="float16") = packed_params[116]
            gpt_neox_layers_7_post_attention_layernorm_bias5: R.Tensor((2560,), dtype="float16") = packed_params[117]
            gpt_neox_layers_7_attention_query_key_value_q_weight5: R.Tensor((7680, 320), dtype="uint32") = packed_params[118]
            gpt_neox_layers_7_attention_query_key_value_q_scale5: R.Tensor((7680, 80), dtype="float16") = packed_params[119]
            gpt_neox_layers_7_attention_query_key_value_bias5: R.Tensor((7680,), dtype="float16") = packed_params[120]
            gpt_neox_layers_7_attention_dense_q_weight5: R.Tensor((2560, 320), dtype="uint32") = packed_params[121]
            gpt_neox_layers_7_attention_dense_q_scale5: R.Tensor((2560, 80), dtype="float16") = packed_params[122]
            gpt_neox_layers_7_attention_dense_bias5: R.Tensor((2560,), dtype="float16") = packed_params[123]
            gpt_neox_layers_7_mlp_dense_h_to_4h_q_weight5: R.Tensor((10240, 320), dtype="uint32") = packed_params[124]
            gpt_neox_layers_7_mlp_dense_h_to_4h_q_scale5: R.Tensor((10240, 80), dtype="float16") = packed_params[125]
            gpt_neox_layers_7_mlp_dense_h_to_4h_bias5: R.Tensor((10240,), dtype="float32") = packed_params[126]
            gpt_neox_layers_7_mlp_dense_4h_to_h_q_weight5: R.Tensor((2560, 1280), dtype="uint32") = packed_params[127]
            gpt_neox_layers_7_mlp_dense_4h_to_h_q_scale5: R.Tensor((2560, 320), dtype="float16") = packed_params[128]
            gpt_neox_layers_7_mlp_dense_4h_to_h_bias5: R.Tensor((2560,), dtype="float32") = packed_params[129]
            gpt_neox_layers_8_input_layernorm_weight5: R.Tensor((2560,), dtype="float16") = packed_params[130]
            gpt_neox_layers_8_input_layernorm_bias5: R.Tensor((2560,), dtype="float16") = packed_params[131]
            gpt_neox_layers_8_post_attention_layernorm_weight5: R.Tensor((2560,), dtype="float16") = packed_params[132]
            gpt_neox_layers_8_post_attention_layernorm_bias5: R.Tensor((2560,), dtype="float16") = packed_params[133]
            gpt_neox_layers_8_attention_query_key_value_q_weight5: R.Tensor((7680, 320), dtype="uint32") = packed_params[134]
            gpt_neox_layers_8_attention_query_key_value_q_scale5: R.Tensor((7680, 80), dtype="float16") = packed_params[135]
            gpt_neox_layers_8_attention_query_key_value_bias5: R.Tensor((7680,), dtype="float16") = packed_params[136]
            gpt_neox_layers_8_attention_dense_q_weight5: R.Tensor((2560, 320), dtype="uint32") = packed_params[137]
            gpt_neox_layers_8_attention_dense_q_scale5: R.Tensor((2560, 80), dtype="float16") = packed_params[138]
            gpt_neox_layers_8_attention_dense_bias5: R.Tensor((2560,), dtype="float16") = packed_params[139]
            gpt_neox_layers_8_mlp_dense_h_to_4h_q_weight5: R.Tensor((10240, 320), dtype="uint32") = packed_params[140]
            gpt_neox_layers_8_mlp_dense_h_to_4h_q_scale5: R.Tensor((10240, 80), dtype="float16") = packed_params[141]
            gpt_neox_layers_8_mlp_dense_h_to_4h_bias5: R.Tensor((10240,), dtype="float32") = packed_params[142]
            gpt_neox_layers_8_mlp_dense_4h_to_h_q_weight5: R.Tensor((2560, 1280), dtype="uint32") = packed_params[143]
            gpt_neox_layers_8_mlp_dense_4h_to_h_q_scale5: R.Tensor((2560, 320), dtype="float16") = packed_params[144]
            gpt_neox_layers_8_mlp_dense_4h_to_h_bias5: R.Tensor((2560,), dtype="float32") = packed_params[145]
            gpt_neox_layers_9_input_layernorm_weight5: R.Tensor((2560,), dtype="float16") = packed_params[146]
            gpt_neox_layers_9_input_layernorm_bias5: R.Tensor((2560,), dtype="float16") = packed_params[147]
            gpt_neox_layers_9_post_attention_layernorm_weight5: R.Tensor((2560,), dtype="float16") = packed_params[148]
            gpt_neox_layers_9_post_attention_layernorm_bias5: R.Tensor((2560,), dtype="float16") = packed_params[149]
            gpt_neox_layers_9_attention_query_key_value_q_weight5: R.Tensor((7680, 320), dtype="uint32") = packed_params[150]
            gpt_neox_layers_9_attention_query_key_value_q_scale5: R.Tensor((7680, 80), dtype="float16") = packed_params[151]
            gpt_neox_layers_9_attention_query_key_value_bias5: R.Tensor((7680,), dtype="float16") = packed_params[152]
            gpt_neox_layers_9_attention_dense_q_weight5: R.Tensor((2560, 320), dtype="uint32") = packed_params[153]
            gpt_neox_layers_9_attention_dense_q_scale5: R.Tensor((2560, 80), dtype="float16") = packed_params[154]
            gpt_neox_layers_9_attention_dense_bias5: R.Tensor((2560,), dtype="float16") = packed_params[155]
            gpt_neox_layers_9_mlp_dense_h_to_4h_q_weight5: R.Tensor((10240, 320), dtype="uint32") = packed_params[156]
            gpt_neox_layers_9_mlp_dense_h_to_4h_q_scale5: R.Tensor((10240, 80), dtype="float16") = packed_params[157]
            gpt_neox_layers_9_mlp_dense_h_to_4h_bias5: R.Tensor((10240,), dtype="float32") = packed_params[158]
            gpt_neox_layers_9_mlp_dense_4h_to_h_q_weight5: R.Tensor((2560, 1280), dtype="uint32") = packed_params[159]
            gpt_neox_layers_9_mlp_dense_4h_to_h_q_scale5: R.Tensor((2560, 320), dtype="float16") = packed_params[160]
            gpt_neox_layers_9_mlp_dense_4h_to_h_bias5: R.Tensor((2560,), dtype="float32") = packed_params[161]
            gpt_neox_layers_10_input_layernorm_weight5: R.Tensor((2560,), dtype="float16") = packed_params[162]
            gpt_neox_layers_10_input_layernorm_bias5: R.Tensor((2560,), dtype="float16") = packed_params[163]
            gpt_neox_layers_10_post_attention_layernorm_weight5: R.Tensor((2560,), dtype="float16") = packed_params[164]
            gpt_neox_layers_10_post_attention_layernorm_bias5: R.Tensor((2560,), dtype="float16") = packed_params[165]
            gpt_neox_layers_10_attention_query_key_value_q_weight5: R.Tensor((7680, 320), dtype="uint32") = packed_params[166]
            gpt_neox_layers_10_attention_query_key_value_q_scale5: R.Tensor((7680, 80), dtype="float16") = packed_params[167]
            gpt_neox_layers_10_attention_query_key_value_bias5: R.Tensor((7680,), dtype="float16") = packed_params[168]
            gpt_neox_layers_10_attention_dense_q_weight5: R.Tensor((2560, 320), dtype="uint32") = packed_params[169]
            gpt_neox_layers_10_attention_dense_q_scale5: R.Tensor((2560, 80), dtype="float16") = packed_params[170]
            gpt_neox_layers_10_attention_dense_bias5: R.Tensor((2560,), dtype="float16") = packed_params[171]
            gpt_neox_layers_10_mlp_dense_h_to_4h_q_weight5: R.Tensor((10240, 320), dtype="uint32") = packed_params[172]
            gpt_neox_layers_10_mlp_dense_h_to_4h_q_scale5: R.Tensor((10240, 80), dtype="float16") = packed_params[173]
            gpt_neox_layers_10_mlp_dense_h_to_4h_bias5: R.Tensor((10240,), dtype="float32") = packed_params[174]
            gpt_neox_layers_10_mlp_dense_4h_to_h_q_weight5: R.Tensor((2560, 1280), dtype="uint32") = packed_params[175]
            gpt_neox_layers_10_mlp_dense_4h_to_h_q_scale5: R.Tensor((2560, 320), dtype="float16") = packed_params[176]
            gpt_neox_layers_10_mlp_dense_4h_to_h_bias5: R.Tensor((2560,), dtype="float32") = packed_params[177]
            gpt_neox_layers_11_input_layernorm_weight5: R.Tensor((2560,), dtype="float16") = packed_params[178]
            gpt_neox_layers_11_input_layernorm_bias5: R.Tensor((2560,), dtype="float16") = packed_params[179]
            gpt_neox_layers_11_post_attention_layernorm_weight5: R.Tensor((2560,), dtype="float16") = packed_params[180]
            gpt_neox_layers_11_post_attention_layernorm_bias5: R.Tensor((2560,), dtype="float16") = packed_params[181]
            gpt_neox_layers_11_attention_query_key_value_q_weight5: R.Tensor((7680, 320), dtype="uint32") = packed_params[182]
            gpt_neox_layers_11_attention_query_key_value_q_scale5: R.Tensor((7680, 80), dtype="float16") = packed_params[183]
            gpt_neox_layers_11_attention_query_key_value_bias5: R.Tensor((7680,), dtype="float16") = packed_params[184]
            gpt_neox_layers_11_attention_dense_q_weight5: R.Tensor((2560, 320), dtype="uint32") = packed_params[185]
            gpt_neox_layers_11_attention_dense_q_scale5: R.Tensor((2560, 80), dtype="float16") = packed_params[186]
            gpt_neox_layers_11_attention_dense_bias5: R.Tensor((2560,), dtype="float16") = packed_params[187]
            gpt_neox_layers_11_mlp_dense_h_to_4h_q_weight5: R.Tensor((10240, 320), dtype="uint32") = packed_params[188]
            gpt_neox_layers_11_mlp_dense_h_to_4h_q_scale5: R.Tensor((10240, 80), dtype="float16") = packed_params[189]
            gpt_neox_layers_11_mlp_dense_h_to_4h_bias5: R.Tensor((10240,), dtype="float32") = packed_params[190]
            gpt_neox_layers_11_mlp_dense_4h_to_h_q_weight5: R.Tensor((2560, 1280), dtype="uint32") = packed_params[191]
            gpt_neox_layers_11_mlp_dense_4h_to_h_q_scale5: R.Tensor((2560, 320), dtype="float16") = packed_params[192]
            gpt_neox_layers_11_mlp_dense_4h_to_h_bias5: R.Tensor((2560,), dtype="float32") = packed_params[193]
            gpt_neox_layers_12_input_layernorm_weight5: R.Tensor((2560,), dtype="float16") = packed_params[194]
            gpt_neox_layers_12_input_layernorm_bias5: R.Tensor((2560,), dtype="float16") = packed_params[195]
            gpt_neox_layers_12_post_attention_layernorm_weight5: R.Tensor((2560,), dtype="float16") = packed_params[196]
            gpt_neox_layers_12_post_attention_layernorm_bias5: R.Tensor((2560,), dtype="float16") = packed_params[197]
            gpt_neox_layers_12_attention_query_key_value_q_weight5: R.Tensor((7680, 320), dtype="uint32") = packed_params[198]
            gpt_neox_layers_12_attention_query_key_value_q_scale5: R.Tensor((7680, 80), dtype="float16") = packed_params[199]
            gpt_neox_layers_12_attention_query_key_value_bias5: R.Tensor((7680,), dtype="float16") = packed_params[200]
            gpt_neox_layers_12_attention_dense_q_weight5: R.Tensor((2560, 320), dtype="uint32") = packed_params[201]
            gpt_neox_layers_12_attention_dense_q_scale5: R.Tensor((2560, 80), dtype="float16") = packed_params[202]
            gpt_neox_layers_12_attention_dense_bias5: R.Tensor((2560,), dtype="float16") = packed_params[203]
            gpt_neox_layers_12_mlp_dense_h_to_4h_q_weight5: R.Tensor((10240, 320), dtype="uint32") = packed_params[204]
            gpt_neox_layers_12_mlp_dense_h_to_4h_q_scale5: R.Tensor((10240, 80), dtype="float16") = packed_params[205]
            gpt_neox_layers_12_mlp_dense_h_to_4h_bias5: R.Tensor((10240,), dtype="float32") = packed_params[206]
            gpt_neox_layers_12_mlp_dense_4h_to_h_q_weight5: R.Tensor((2560, 1280), dtype="uint32") = packed_params[207]
            gpt_neox_layers_12_mlp_dense_4h_to_h_q_scale5: R.Tensor((2560, 320), dtype="float16") = packed_params[208]
            gpt_neox_layers_12_mlp_dense_4h_to_h_bias5: R.Tensor((2560,), dtype="float32") = packed_params[209]
            gpt_neox_layers_13_input_layernorm_weight5: R.Tensor((2560,), dtype="float16") = packed_params[210]
            gpt_neox_layers_13_input_layernorm_bias5: R.Tensor((2560,), dtype="float16") = packed_params[211]
            gpt_neox_layers_13_post_attention_layernorm_weight5: R.Tensor((2560,), dtype="float16") = packed_params[212]
            gpt_neox_layers_13_post_attention_layernorm_bias5: R.Tensor((2560,), dtype="float16") = packed_params[213]
            gpt_neox_layers_13_attention_query_key_value_q_weight5: R.Tensor((7680, 320), dtype="uint32") = packed_params[214]
            gpt_neox_layers_13_attention_query_key_value_q_scale5: R.Tensor((7680, 80), dtype="float16") = packed_params[215]
            gpt_neox_layers_13_attention_query_key_value_bias5: R.Tensor((7680,), dtype="float16") = packed_params[216]
            gpt_neox_layers_13_attention_dense_q_weight5: R.Tensor((2560, 320), dtype="uint32") = packed_params[217]
            gpt_neox_layers_13_attention_dense_q_scale5: R.Tensor((2560, 80), dtype="float16") = packed_params[218]
            gpt_neox_layers_13_attention_dense_bias5: R.Tensor((2560,), dtype="float16") = packed_params[219]
            gpt_neox_layers_13_mlp_dense_h_to_4h_q_weight5: R.Tensor((10240, 320), dtype="uint32") = packed_params[220]
            gpt_neox_layers_13_mlp_dense_h_to_4h_q_scale5: R.Tensor((10240, 80), dtype="float16") = packed_params[221]
            gpt_neox_layers_13_mlp_dense_h_to_4h_bias5: R.Tensor((10240,), dtype="float32") = packed_params[222]
            gpt_neox_layers_13_mlp_dense_4h_to_h_q_weight5: R.Tensor((2560, 1280), dtype="uint32") = packed_params[223]
            gpt_neox_layers_13_mlp_dense_4h_to_h_q_scale5: R.Tensor((2560, 320), dtype="float16") = packed_params[224]
            gpt_neox_layers_13_mlp_dense_4h_to_h_bias5: R.Tensor((2560,), dtype="float32") = packed_params[225]
            gpt_neox_layers_14_input_layernorm_weight5: R.Tensor((2560,), dtype="float16") = packed_params[226]
            gpt_neox_layers_14_input_layernorm_bias5: R.Tensor((2560,), dtype="float16") = packed_params[227]
            gpt_neox_layers_14_post_attention_layernorm_weight5: R.Tensor((2560,), dtype="float16") = packed_params[228]
            gpt_neox_layers_14_post_attention_layernorm_bias5: R.Tensor((2560,), dtype="float16") = packed_params[229]
            gpt_neox_layers_14_attention_query_key_value_q_weight5: R.Tensor((7680, 320), dtype="uint32") = packed_params[230]
            gpt_neox_layers_14_attention_query_key_value_q_scale5: R.Tensor((7680, 80), dtype="float16") = packed_params[231]
            gpt_neox_layers_14_attention_query_key_value_bias5: R.Tensor((7680,), dtype="float16") = packed_params[232]
            gpt_neox_layers_14_attention_dense_q_weight5: R.Tensor((2560, 320), dtype="uint32") = packed_params[233]
            gpt_neox_layers_14_attention_dense_q_scale5: R.Tensor((2560, 80), dtype="float16") = packed_params[234]
            gpt_neox_layers_14_attention_dense_bias5: R.Tensor((2560,), dtype="float16") = packed_params[235]
            gpt_neox_layers_14_mlp_dense_h_to_4h_q_weight5: R.Tensor((10240, 320), dtype="uint32") = packed_params[236]
            gpt_neox_layers_14_mlp_dense_h_to_4h_q_scale5: R.Tensor((10240, 80), dtype="float16") = packed_params[237]
            gpt_neox_layers_14_mlp_dense_h_to_4h_bias5: R.Tensor((10240,), dtype="float32") = packed_params[238]
            gpt_neox_layers_14_mlp_dense_4h_to_h_q_weight5: R.Tensor((2560, 1280), dtype="uint32") = packed_params[239]
            gpt_neox_layers_14_mlp_dense_4h_to_h_q_scale5: R.Tensor((2560, 320), dtype="float16") = packed_params[240]
            gpt_neox_layers_14_mlp_dense_4h_to_h_bias5: R.Tensor((2560,), dtype="float32") = packed_params[241]
            gpt_neox_layers_15_input_layernorm_weight5: R.Tensor((2560,), dtype="float16") = packed_params[242]
            gpt_neox_layers_15_input_layernorm_bias5: R.Tensor((2560,), dtype="float16") = packed_params[243]
            gpt_neox_layers_15_post_attention_layernorm_weight5: R.Tensor((2560,), dtype="float16") = packed_params[244]
            gpt_neox_layers_15_post_attention_layernorm_bias5: R.Tensor((2560,), dtype="float16") = packed_params[245]
            gpt_neox_layers_15_attention_query_key_value_q_weight5: R.Tensor((7680, 320), dtype="uint32") = packed_params[246]
            gpt_neox_layers_15_attention_query_key_value_q_scale5: R.Tensor((7680, 80), dtype="float16") = packed_params[247]
            gpt_neox_layers_15_attention_query_key_value_bias5: R.Tensor((7680,), dtype="float16") = packed_params[248]
            gpt_neox_layers_15_attention_dense_q_weight5: R.Tensor((2560, 320), dtype="uint32") = packed_params[249]
            gpt_neox_layers_15_attention_dense_q_scale5: R.Tensor((2560, 80), dtype="float16") = packed_params[250]
            gpt_neox_layers_15_attention_dense_bias5: R.Tensor((2560,), dtype="float16") = packed_params[251]
            gpt_neox_layers_15_mlp_dense_h_to_4h_q_weight5: R.Tensor((10240, 320), dtype="uint32") = packed_params[252]
            gpt_neox_layers_15_mlp_dense_h_to_4h_q_scale5: R.Tensor((10240, 80), dtype="float16") = packed_params[253]
            gpt_neox_layers_15_mlp_dense_h_to_4h_bias5: R.Tensor((10240,), dtype="float32") = packed_params[254]
            gpt_neox_layers_15_mlp_dense_4h_to_h_q_weight5: R.Tensor((2560, 1280), dtype="uint32") = packed_params[255]
            gpt_neox_layers_15_mlp_dense_4h_to_h_q_scale5: R.Tensor((2560, 320), dtype="float16") = packed_params[256]
            gpt_neox_layers_15_mlp_dense_4h_to_h_bias5: R.Tensor((2560,), dtype="float32") = packed_params[257]
            gpt_neox_layers_16_input_layernorm_weight5: R.Tensor((2560,), dtype="float16") = packed_params[258]
            gpt_neox_layers_16_input_layernorm_bias5: R.Tensor((2560,), dtype="float16") = packed_params[259]
            gpt_neox_layers_16_post_attention_layernorm_weight5: R.Tensor((2560,), dtype="float16") = packed_params[260]
            gpt_neox_layers_16_post_attention_layernorm_bias5: R.Tensor((2560,), dtype="float16") = packed_params[261]
            gpt_neox_layers_16_attention_query_key_value_q_weight5: R.Tensor((7680, 320), dtype="uint32") = packed_params[262]
            gpt_neox_layers_16_attention_query_key_value_q_scale5: R.Tensor((7680, 80), dtype="float16") = packed_params[263]
            gpt_neox_layers_16_attention_query_key_value_bias5: R.Tensor((7680,), dtype="float16") = packed_params[264]
            gpt_neox_layers_16_attention_dense_q_weight5: R.Tensor((2560, 320), dtype="uint32") = packed_params[265]
            gpt_neox_layers_16_attention_dense_q_scale5: R.Tensor((2560, 80), dtype="float16") = packed_params[266]
            gpt_neox_layers_16_attention_dense_bias5: R.Tensor((2560,), dtype="float16") = packed_params[267]
            gpt_neox_layers_16_mlp_dense_h_to_4h_q_weight5: R.Tensor((10240, 320), dtype="uint32") = packed_params[268]
            gpt_neox_layers_16_mlp_dense_h_to_4h_q_scale5: R.Tensor((10240, 80), dtype="float16") = packed_params[269]
            gpt_neox_layers_16_mlp_dense_h_to_4h_bias5: R.Tensor((10240,), dtype="float32") = packed_params[270]
            gpt_neox_layers_16_mlp_dense_4h_to_h_q_weight5: R.Tensor((2560, 1280), dtype="uint32") = packed_params[271]
            gpt_neox_layers_16_mlp_dense_4h_to_h_q_scale5: R.Tensor((2560, 320), dtype="float16") = packed_params[272]
            gpt_neox_layers_16_mlp_dense_4h_to_h_bias5: R.Tensor((2560,), dtype="float32") = packed_params[273]
            gpt_neox_layers_17_input_layernorm_weight5: R.Tensor((2560,), dtype="float16") = packed_params[274]
            gpt_neox_layers_17_input_layernorm_bias5: R.Tensor((2560,), dtype="float16") = packed_params[275]
            gpt_neox_layers_17_post_attention_layernorm_weight5: R.Tensor((2560,), dtype="float16") = packed_params[276]
            gpt_neox_layers_17_post_attention_layernorm_bias5: R.Tensor((2560,), dtype="float16") = packed_params[277]
            gpt_neox_layers_17_attention_query_key_value_q_weight5: R.Tensor((7680, 320), dtype="uint32") = packed_params[278]
            gpt_neox_layers_17_attention_query_key_value_q_scale5: R.Tensor((7680, 80), dtype="float16") = packed_params[279]
            gpt_neox_layers_17_attention_query_key_value_bias5: R.Tensor((7680,), dtype="float16") = packed_params[280]
            gpt_neox_layers_17_attention_dense_q_weight5: R.Tensor((2560, 320), dtype="uint32") = packed_params[281]
            gpt_neox_layers_17_attention_dense_q_scale5: R.Tensor((2560, 80), dtype="float16") = packed_params[282]
            gpt_neox_layers_17_attention_dense_bias5: R.Tensor((2560,), dtype="float16") = packed_params[283]
            gpt_neox_layers_17_mlp_dense_h_to_4h_q_weight5: R.Tensor((10240, 320), dtype="uint32") = packed_params[284]
            gpt_neox_layers_17_mlp_dense_h_to_4h_q_scale5: R.Tensor((10240, 80), dtype="float16") = packed_params[285]
            gpt_neox_layers_17_mlp_dense_h_to_4h_bias5: R.Tensor((10240,), dtype="float32") = packed_params[286]
            gpt_neox_layers_17_mlp_dense_4h_to_h_q_weight5: R.Tensor((2560, 1280), dtype="uint32") = packed_params[287]
            gpt_neox_layers_17_mlp_dense_4h_to_h_q_scale5: R.Tensor((2560, 320), dtype="float16") = packed_params[288]
            gpt_neox_layers_17_mlp_dense_4h_to_h_bias5: R.Tensor((2560,), dtype="float32") = packed_params[289]
            gpt_neox_layers_18_input_layernorm_weight5: R.Tensor((2560,), dtype="float16") = packed_params[290]
            gpt_neox_layers_18_input_layernorm_bias5: R.Tensor((2560,), dtype="float16") = packed_params[291]
            gpt_neox_layers_18_post_attention_layernorm_weight5: R.Tensor((2560,), dtype="float16") = packed_params[292]
            gpt_neox_layers_18_post_attention_layernorm_bias5: R.Tensor((2560,), dtype="float16") = packed_params[293]
            gpt_neox_layers_18_attention_query_key_value_q_weight5: R.Tensor((7680, 320), dtype="uint32") = packed_params[294]
            gpt_neox_layers_18_attention_query_key_value_q_scale5: R.Tensor((7680, 80), dtype="float16") = packed_params[295]
            gpt_neox_layers_18_attention_query_key_value_bias5: R.Tensor((7680,), dtype="float16") = packed_params[296]
            gpt_neox_layers_18_attention_dense_q_weight5: R.Tensor((2560, 320), dtype="uint32") = packed_params[297]
            gpt_neox_layers_18_attention_dense_q_scale5: R.Tensor((2560, 80), dtype="float16") = packed_params[298]
            gpt_neox_layers_18_attention_dense_bias5: R.Tensor((2560,), dtype="float16") = packed_params[299]
            gpt_neox_layers_18_mlp_dense_h_to_4h_q_weight5: R.Tensor((10240, 320), dtype="uint32") = packed_params[300]
            gpt_neox_layers_18_mlp_dense_h_to_4h_q_scale5: R.Tensor((10240, 80), dtype="float16") = packed_params[301]
            gpt_neox_layers_18_mlp_dense_h_to_4h_bias5: R.Tensor((10240,), dtype="float32") = packed_params[302]
            gpt_neox_layers_18_mlp_dense_4h_to_h_q_weight5: R.Tensor((2560, 1280), dtype="uint32") = packed_params[303]
            gpt_neox_layers_18_mlp_dense_4h_to_h_q_scale5: R.Tensor((2560, 320), dtype="float16") = packed_params[304]
            gpt_neox_layers_18_mlp_dense_4h_to_h_bias5: R.Tensor((2560,), dtype="float32") = packed_params[305]
            gpt_neox_layers_19_input_layernorm_weight5: R.Tensor((2560,), dtype="float16") = packed_params[306]
            gpt_neox_layers_19_input_layernorm_bias5: R.Tensor((2560,), dtype="float16") = packed_params[307]
            gpt_neox_layers_19_post_attention_layernorm_weight5: R.Tensor((2560,), dtype="float16") = packed_params[308]
            gpt_neox_layers_19_post_attention_layernorm_bias5: R.Tensor((2560,), dtype="float16") = packed_params[309]
            gpt_neox_layers_19_attention_query_key_value_q_weight5: R.Tensor((7680, 320), dtype="uint32") = packed_params[310]
            gpt_neox_layers_19_attention_query_key_value_q_scale5: R.Tensor((7680, 80), dtype="float16") = packed_params[311]
            gpt_neox_layers_19_attention_query_key_value_bias5: R.Tensor((7680,), dtype="float16") = packed_params[312]
            gpt_neox_layers_19_attention_dense_q_weight5: R.Tensor((2560, 320), dtype="uint32") = packed_params[313]
            gpt_neox_layers_19_attention_dense_q_scale5: R.Tensor((2560, 80), dtype="float16") = packed_params[314]
            gpt_neox_layers_19_attention_dense_bias5: R.Tensor((2560,), dtype="float16") = packed_params[315]
            gpt_neox_layers_19_mlp_dense_h_to_4h_q_weight5: R.Tensor((10240, 320), dtype="uint32") = packed_params[316]
            gpt_neox_layers_19_mlp_dense_h_to_4h_q_scale5: R.Tensor((10240, 80), dtype="float16") = packed_params[317]
            gpt_neox_layers_19_mlp_dense_h_to_4h_bias5: R.Tensor((10240,), dtype="float32") = packed_params[318]
            gpt_neox_layers_19_mlp_dense_4h_to_h_q_weight5: R.Tensor((2560, 1280), dtype="uint32") = packed_params[319]
            gpt_neox_layers_19_mlp_dense_4h_to_h_q_scale5: R.Tensor((2560, 320), dtype="float16") = packed_params[320]
            gpt_neox_layers_19_mlp_dense_4h_to_h_bias5: R.Tensor((2560,), dtype="float32") = packed_params[321]
            gpt_neox_layers_20_input_layernorm_weight5: R.Tensor((2560,), dtype="float16") = packed_params[322]
            gpt_neox_layers_20_input_layernorm_bias5: R.Tensor((2560,), dtype="float16") = packed_params[323]
            gpt_neox_layers_20_post_attention_layernorm_weight5: R.Tensor((2560,), dtype="float16") = packed_params[324]
            gpt_neox_layers_20_post_attention_layernorm_bias5: R.Tensor((2560,), dtype="float16") = packed_params[325]
            gpt_neox_layers_20_attention_query_key_value_q_weight5: R.Tensor((7680, 320), dtype="uint32") = packed_params[326]
            gpt_neox_layers_20_attention_query_key_value_q_scale5: R.Tensor((7680, 80), dtype="float16") = packed_params[327]
            gpt_neox_layers_20_attention_query_key_value_bias5: R.Tensor((7680,), dtype="float16") = packed_params[328]
            gpt_neox_layers_20_attention_dense_q_weight5: R.Tensor((2560, 320), dtype="uint32") = packed_params[329]
            gpt_neox_layers_20_attention_dense_q_scale5: R.Tensor((2560, 80), dtype="float16") = packed_params[330]
            gpt_neox_layers_20_attention_dense_bias5: R.Tensor((2560,), dtype="float16") = packed_params[331]
            gpt_neox_layers_20_mlp_dense_h_to_4h_q_weight5: R.Tensor((10240, 320), dtype="uint32") = packed_params[332]
            gpt_neox_layers_20_mlp_dense_h_to_4h_q_scale5: R.Tensor((10240, 80), dtype="float16") = packed_params[333]
            gpt_neox_layers_20_mlp_dense_h_to_4h_bias5: R.Tensor((10240,), dtype="float32") = packed_params[334]
            gpt_neox_layers_20_mlp_dense_4h_to_h_q_weight5: R.Tensor((2560, 1280), dtype="uint32") = packed_params[335]
            gpt_neox_layers_20_mlp_dense_4h_to_h_q_scale5: R.Tensor((2560, 320), dtype="float16") = packed_params[336]
            gpt_neox_layers_20_mlp_dense_4h_to_h_bias5: R.Tensor((2560,), dtype="float32") = packed_params[337]
            gpt_neox_layers_21_input_layernorm_weight5: R.Tensor((2560,), dtype="float16") = packed_params[338]
            gpt_neox_layers_21_input_layernorm_bias5: R.Tensor((2560,), dtype="float16") = packed_params[339]
            gpt_neox_layers_21_post_attention_layernorm_weight5: R.Tensor((2560,), dtype="float16") = packed_params[340]
            gpt_neox_layers_21_post_attention_layernorm_bias5: R.Tensor((2560,), dtype="float16") = packed_params[341]
            gpt_neox_layers_21_attention_query_key_value_q_weight5: R.Tensor((7680, 320), dtype="uint32") = packed_params[342]
            gpt_neox_layers_21_attention_query_key_value_q_scale5: R.Tensor((7680, 80), dtype="float16") = packed_params[343]
            gpt_neox_layers_21_attention_query_key_value_bias5: R.Tensor((7680,), dtype="float16") = packed_params[344]
            gpt_neox_layers_21_attention_dense_q_weight5: R.Tensor((2560, 320), dtype="uint32") = packed_params[345]
            gpt_neox_layers_21_attention_dense_q_scale5: R.Tensor((2560, 80), dtype="float16") = packed_params[346]
            gpt_neox_layers_21_attention_dense_bias5: R.Tensor((2560,), dtype="float16") = packed_params[347]
            gpt_neox_layers_21_mlp_dense_h_to_4h_q_weight5: R.Tensor((10240, 320), dtype="uint32") = packed_params[348]
            gpt_neox_layers_21_mlp_dense_h_to_4h_q_scale5: R.Tensor((10240, 80), dtype="float16") = packed_params[349]
            gpt_neox_layers_21_mlp_dense_h_to_4h_bias5: R.Tensor((10240,), dtype="float32") = packed_params[350]
            gpt_neox_layers_21_mlp_dense_4h_to_h_q_weight5: R.Tensor((2560, 1280), dtype="uint32") = packed_params[351]
            gpt_neox_layers_21_mlp_dense_4h_to_h_q_scale5: R.Tensor((2560, 320), dtype="float16") = packed_params[352]
            gpt_neox_layers_21_mlp_dense_4h_to_h_bias5: R.Tensor((2560,), dtype="float32") = packed_params[353]
            gpt_neox_layers_22_input_layernorm_weight5: R.Tensor((2560,), dtype="float16") = packed_params[354]
            gpt_neox_layers_22_input_layernorm_bias5: R.Tensor((2560,), dtype="float16") = packed_params[355]
            gpt_neox_layers_22_post_attention_layernorm_weight5: R.Tensor((2560,), dtype="float16") = packed_params[356]
            gpt_neox_layers_22_post_attention_layernorm_bias5: R.Tensor((2560,), dtype="float16") = packed_params[357]
            gpt_neox_layers_22_attention_query_key_value_q_weight5: R.Tensor((7680, 320), dtype="uint32") = packed_params[358]
            gpt_neox_layers_22_attention_query_key_value_q_scale5: R.Tensor((7680, 80), dtype="float16") = packed_params[359]
            gpt_neox_layers_22_attention_query_key_value_bias5: R.Tensor((7680,), dtype="float16") = packed_params[360]
            gpt_neox_layers_22_attention_dense_q_weight5: R.Tensor((2560, 320), dtype="uint32") = packed_params[361]
            gpt_neox_layers_22_attention_dense_q_scale5: R.Tensor((2560, 80), dtype="float16") = packed_params[362]
            gpt_neox_layers_22_attention_dense_bias5: R.Tensor((2560,), dtype="float16") = packed_params[363]
            gpt_neox_layers_22_mlp_dense_h_to_4h_q_weight5: R.Tensor((10240, 320), dtype="uint32") = packed_params[364]
            gpt_neox_layers_22_mlp_dense_h_to_4h_q_scale5: R.Tensor((10240, 80), dtype="float16") = packed_params[365]
            gpt_neox_layers_22_mlp_dense_h_to_4h_bias5: R.Tensor((10240,), dtype="float32") = packed_params[366]
            gpt_neox_layers_22_mlp_dense_4h_to_h_q_weight5: R.Tensor((2560, 1280), dtype="uint32") = packed_params[367]
            gpt_neox_layers_22_mlp_dense_4h_to_h_q_scale5: R.Tensor((2560, 320), dtype="float16") = packed_params[368]
            gpt_neox_layers_22_mlp_dense_4h_to_h_bias5: R.Tensor((2560,), dtype="float32") = packed_params[369]
            gpt_neox_layers_23_input_layernorm_weight5: R.Tensor((2560,), dtype="float16") = packed_params[370]
            gpt_neox_layers_23_input_layernorm_bias5: R.Tensor((2560,), dtype="float16") = packed_params[371]
            gpt_neox_layers_23_post_attention_layernorm_weight5: R.Tensor((2560,), dtype="float16") = packed_params[372]
            gpt_neox_layers_23_post_attention_layernorm_bias5: R.Tensor((2560,), dtype="float16") = packed_params[373]
            gpt_neox_layers_23_attention_query_key_value_q_weight5: R.Tensor((7680, 320), dtype="uint32") = packed_params[374]
            gpt_neox_layers_23_attention_query_key_value_q_scale5: R.Tensor((7680, 80), dtype="float16") = packed_params[375]
            gpt_neox_layers_23_attention_query_key_value_bias5: R.Tensor((7680,), dtype="float16") = packed_params[376]
            gpt_neox_layers_23_attention_dense_q_weight5: R.Tensor((2560, 320), dtype="uint32") = packed_params[377]
            gpt_neox_layers_23_attention_dense_q_scale5: R.Tensor((2560, 80), dtype="float16") = packed_params[378]
            gpt_neox_layers_23_attention_dense_bias5: R.Tensor((2560,), dtype="float16") = packed_params[379]
            gpt_neox_layers_23_mlp_dense_h_to_4h_q_weight5: R.Tensor((10240, 320), dtype="uint32") = packed_params[380]
            gpt_neox_layers_23_mlp_dense_h_to_4h_q_scale5: R.Tensor((10240, 80), dtype="float16") = packed_params[381]
            gpt_neox_layers_23_mlp_dense_h_to_4h_bias5: R.Tensor((10240,), dtype="float32") = packed_params[382]
            gpt_neox_layers_23_mlp_dense_4h_to_h_q_weight5: R.Tensor((2560, 1280), dtype="uint32") = packed_params[383]
            gpt_neox_layers_23_mlp_dense_4h_to_h_q_scale5: R.Tensor((2560, 320), dtype="float16") = packed_params[384]
            gpt_neox_layers_23_mlp_dense_4h_to_h_bias5: R.Tensor((2560,), dtype="float32") = packed_params[385]
            gpt_neox_layers_24_input_layernorm_weight5: R.Tensor((2560,), dtype="float16") = packed_params[386]
            gpt_neox_layers_24_input_layernorm_bias5: R.Tensor((2560,), dtype="float16") = packed_params[387]
            gpt_neox_layers_24_post_attention_layernorm_weight5: R.Tensor((2560,), dtype="float16") = packed_params[388]
            gpt_neox_layers_24_post_attention_layernorm_bias5: R.Tensor((2560,), dtype="float16") = packed_params[389]
            gpt_neox_layers_24_attention_query_key_value_q_weight5: R.Tensor((7680, 320), dtype="uint32") = packed_params[390]
            gpt_neox_layers_24_attention_query_key_value_q_scale5: R.Tensor((7680, 80), dtype="float16") = packed_params[391]
            gpt_neox_layers_24_attention_query_key_value_bias5: R.Tensor((7680,), dtype="float16") = packed_params[392]
            gpt_neox_layers_24_attention_dense_q_weight5: R.Tensor((2560, 320), dtype="uint32") = packed_params[393]
            gpt_neox_layers_24_attention_dense_q_scale5: R.Tensor((2560, 80), dtype="float16") = packed_params[394]
            gpt_neox_layers_24_attention_dense_bias5: R.Tensor((2560,), dtype="float16") = packed_params[395]
            gpt_neox_layers_24_mlp_dense_h_to_4h_q_weight5: R.Tensor((10240, 320), dtype="uint32") = packed_params[396]
            gpt_neox_layers_24_mlp_dense_h_to_4h_q_scale5: R.Tensor((10240, 80), dtype="float16") = packed_params[397]
            gpt_neox_layers_24_mlp_dense_h_to_4h_bias5: R.Tensor((10240,), dtype="float32") = packed_params[398]
            gpt_neox_layers_24_mlp_dense_4h_to_h_q_weight5: R.Tensor((2560, 1280), dtype="uint32") = packed_params[399]
            gpt_neox_layers_24_mlp_dense_4h_to_h_q_scale5: R.Tensor((2560, 320), dtype="float16") = packed_params[400]
            gpt_neox_layers_24_mlp_dense_4h_to_h_bias5: R.Tensor((2560,), dtype="float32") = packed_params[401]
            gpt_neox_layers_25_input_layernorm_weight5: R.Tensor((2560,), dtype="float16") = packed_params[402]
            gpt_neox_layers_25_input_layernorm_bias5: R.Tensor((2560,), dtype="float16") = packed_params[403]
            gpt_neox_layers_25_post_attention_layernorm_weight5: R.Tensor((2560,), dtype="float16") = packed_params[404]
            gpt_neox_layers_25_post_attention_layernorm_bias5: R.Tensor((2560,), dtype="float16") = packed_params[405]
            gpt_neox_layers_25_attention_query_key_value_q_weight5: R.Tensor((7680, 320), dtype="uint32") = packed_params[406]
            gpt_neox_layers_25_attention_query_key_value_q_scale5: R.Tensor((7680, 80), dtype="float16") = packed_params[407]
            gpt_neox_layers_25_attention_query_key_value_bias5: R.Tensor((7680,), dtype="float16") = packed_params[408]
            gpt_neox_layers_25_attention_dense_q_weight5: R.Tensor((2560, 320), dtype="uint32") = packed_params[409]
            gpt_neox_layers_25_attention_dense_q_scale5: R.Tensor((2560, 80), dtype="float16") = packed_params[410]
            gpt_neox_layers_25_attention_dense_bias5: R.Tensor((2560,), dtype="float16") = packed_params[411]
            gpt_neox_layers_25_mlp_dense_h_to_4h_q_weight5: R.Tensor((10240, 320), dtype="uint32") = packed_params[412]
            gpt_neox_layers_25_mlp_dense_h_to_4h_q_scale5: R.Tensor((10240, 80), dtype="float16") = packed_params[413]
            gpt_neox_layers_25_mlp_dense_h_to_4h_bias5: R.Tensor((10240,), dtype="float32") = packed_params[414]
            gpt_neox_layers_25_mlp_dense_4h_to_h_q_weight5: R.Tensor((2560, 1280), dtype="uint32") = packed_params[415]
            gpt_neox_layers_25_mlp_dense_4h_to_h_q_scale5: R.Tensor((2560, 320), dtype="float16") = packed_params[416]
            gpt_neox_layers_25_mlp_dense_4h_to_h_bias5: R.Tensor((2560,), dtype="float32") = packed_params[417]
            gpt_neox_layers_26_input_layernorm_weight5: R.Tensor((2560,), dtype="float16") = packed_params[418]
            gpt_neox_layers_26_input_layernorm_bias5: R.Tensor((2560,), dtype="float16") = packed_params[419]
            gpt_neox_layers_26_post_attention_layernorm_weight5: R.Tensor((2560,), dtype="float16") = packed_params[420]
            gpt_neox_layers_26_post_attention_layernorm_bias5: R.Tensor((2560,), dtype="float16") = packed_params[421]
            gpt_neox_layers_26_attention_query_key_value_q_weight5: R.Tensor((7680, 320), dtype="uint32") = packed_params[422]
            gpt_neox_layers_26_attention_query_key_value_q_scale5: R.Tensor((7680, 80), dtype="float16") = packed_params[423]
            gpt_neox_layers_26_attention_query_key_value_bias5: R.Tensor((7680,), dtype="float16") = packed_params[424]
            gpt_neox_layers_26_attention_dense_q_weight5: R.Tensor((2560, 320), dtype="uint32") = packed_params[425]
            gpt_neox_layers_26_attention_dense_q_scale5: R.Tensor((2560, 80), dtype="float16") = packed_params[426]
            gpt_neox_layers_26_attention_dense_bias5: R.Tensor((2560,), dtype="float16") = packed_params[427]
            gpt_neox_layers_26_mlp_dense_h_to_4h_q_weight5: R.Tensor((10240, 320), dtype="uint32") = packed_params[428]
            gpt_neox_layers_26_mlp_dense_h_to_4h_q_scale5: R.Tensor((10240, 80), dtype="float16") = packed_params[429]
            gpt_neox_layers_26_mlp_dense_h_to_4h_bias5: R.Tensor((10240,), dtype="float32") = packed_params[430]
            gpt_neox_layers_26_mlp_dense_4h_to_h_q_weight5: R.Tensor((2560, 1280), dtype="uint32") = packed_params[431]
            gpt_neox_layers_26_mlp_dense_4h_to_h_q_scale5: R.Tensor((2560, 320), dtype="float16") = packed_params[432]
            gpt_neox_layers_26_mlp_dense_4h_to_h_bias5: R.Tensor((2560,), dtype="float32") = packed_params[433]
            gpt_neox_layers_27_input_layernorm_weight5: R.Tensor((2560,), dtype="float16") = packed_params[434]
            gpt_neox_layers_27_input_layernorm_bias5: R.Tensor((2560,), dtype="float16") = packed_params[435]
            gpt_neox_layers_27_post_attention_layernorm_weight5: R.Tensor((2560,), dtype="float16") = packed_params[436]
            gpt_neox_layers_27_post_attention_layernorm_bias5: R.Tensor((2560,), dtype="float16") = packed_params[437]
            gpt_neox_layers_27_attention_query_key_value_q_weight5: R.Tensor((7680, 320), dtype="uint32") = packed_params[438]
            gpt_neox_layers_27_attention_query_key_value_q_scale5: R.Tensor((7680, 80), dtype="float16") = packed_params[439]
            gpt_neox_layers_27_attention_query_key_value_bias5: R.Tensor((7680,), dtype="float16") = packed_params[440]
            gpt_neox_layers_27_attention_dense_q_weight5: R.Tensor((2560, 320), dtype="uint32") = packed_params[441]
            gpt_neox_layers_27_attention_dense_q_scale5: R.Tensor((2560, 80), dtype="float16") = packed_params[442]
            gpt_neox_layers_27_attention_dense_bias5: R.Tensor((2560,), dtype="float16") = packed_params[443]
            gpt_neox_layers_27_mlp_dense_h_to_4h_q_weight5: R.Tensor((10240, 320), dtype="uint32") = packed_params[444]
            gpt_neox_layers_27_mlp_dense_h_to_4h_q_scale5: R.Tensor((10240, 80), dtype="float16") = packed_params[445]
            gpt_neox_layers_27_mlp_dense_h_to_4h_bias5: R.Tensor((10240,), dtype="float32") = packed_params[446]
            gpt_neox_layers_27_mlp_dense_4h_to_h_q_weight5: R.Tensor((2560, 1280), dtype="uint32") = packed_params[447]
            gpt_neox_layers_27_mlp_dense_4h_to_h_q_scale5: R.Tensor((2560, 320), dtype="float16") = packed_params[448]
            gpt_neox_layers_27_mlp_dense_4h_to_h_bias5: R.Tensor((2560,), dtype="float32") = packed_params[449]
            gpt_neox_layers_28_input_layernorm_weight5: R.Tensor((2560,), dtype="float16") = packed_params[450]
            gpt_neox_layers_28_input_layernorm_bias5: R.Tensor((2560,), dtype="float16") = packed_params[451]
            gpt_neox_layers_28_post_attention_layernorm_weight5: R.Tensor((2560,), dtype="float16") = packed_params[452]
            gpt_neox_layers_28_post_attention_layernorm_bias5: R.Tensor((2560,), dtype="float16") = packed_params[453]
            gpt_neox_layers_28_attention_query_key_value_q_weight5: R.Tensor((7680, 320), dtype="uint32") = packed_params[454]
            gpt_neox_layers_28_attention_query_key_value_q_scale5: R.Tensor((7680, 80), dtype="float16") = packed_params[455]
            gpt_neox_layers_28_attention_query_key_value_bias5: R.Tensor((7680,), dtype="float16") = packed_params[456]
            gpt_neox_layers_28_attention_dense_q_weight5: R.Tensor((2560, 320), dtype="uint32") = packed_params[457]
            gpt_neox_layers_28_attention_dense_q_scale5: R.Tensor((2560, 80), dtype="float16") = packed_params[458]
            gpt_neox_layers_28_attention_dense_bias5: R.Tensor((2560,), dtype="float16") = packed_params[459]
            gpt_neox_layers_28_mlp_dense_h_to_4h_q_weight5: R.Tensor((10240, 320), dtype="uint32") = packed_params[460]
            gpt_neox_layers_28_mlp_dense_h_to_4h_q_scale5: R.Tensor((10240, 80), dtype="float16") = packed_params[461]
            gpt_neox_layers_28_mlp_dense_h_to_4h_bias5: R.Tensor((10240,), dtype="float32") = packed_params[462]
            gpt_neox_layers_28_mlp_dense_4h_to_h_q_weight5: R.Tensor((2560, 1280), dtype="uint32") = packed_params[463]
            gpt_neox_layers_28_mlp_dense_4h_to_h_q_scale5: R.Tensor((2560, 320), dtype="float16") = packed_params[464]
            gpt_neox_layers_28_mlp_dense_4h_to_h_bias5: R.Tensor((2560,), dtype="float32") = packed_params[465]
            gpt_neox_layers_29_input_layernorm_weight5: R.Tensor((2560,), dtype="float16") = packed_params[466]
            gpt_neox_layers_29_input_layernorm_bias5: R.Tensor((2560,), dtype="float16") = packed_params[467]
            gpt_neox_layers_29_post_attention_layernorm_weight5: R.Tensor((2560,), dtype="float16") = packed_params[468]
            gpt_neox_layers_29_post_attention_layernorm_bias5: R.Tensor((2560,), dtype="float16") = packed_params[469]
            gpt_neox_layers_29_attention_query_key_value_q_weight5: R.Tensor((7680, 320), dtype="uint32") = packed_params[470]
            gpt_neox_layers_29_attention_query_key_value_q_scale5: R.Tensor((7680, 80), dtype="float16") = packed_params[471]
            gpt_neox_layers_29_attention_query_key_value_bias5: R.Tensor((7680,), dtype="float16") = packed_params[472]
            gpt_neox_layers_29_attention_dense_q_weight5: R.Tensor((2560, 320), dtype="uint32") = packed_params[473]
            gpt_neox_layers_29_attention_dense_q_scale5: R.Tensor((2560, 80), dtype="float16") = packed_params[474]
            gpt_neox_layers_29_attention_dense_bias5: R.Tensor((2560,), dtype="float16") = packed_params[475]
            gpt_neox_layers_29_mlp_dense_h_to_4h_q_weight5: R.Tensor((10240, 320), dtype="uint32") = packed_params[476]
            gpt_neox_layers_29_mlp_dense_h_to_4h_q_scale5: R.Tensor((10240, 80), dtype="float16") = packed_params[477]
            gpt_neox_layers_29_mlp_dense_h_to_4h_bias5: R.Tensor((10240,), dtype="float32") = packed_params[478]
            gpt_neox_layers_29_mlp_dense_4h_to_h_q_weight5: R.Tensor((2560, 1280), dtype="uint32") = packed_params[479]
            gpt_neox_layers_29_mlp_dense_4h_to_h_q_scale5: R.Tensor((2560, 320), dtype="float16") = packed_params[480]
            gpt_neox_layers_29_mlp_dense_4h_to_h_bias5: R.Tensor((2560,), dtype="float32") = packed_params[481]
            gpt_neox_layers_30_input_layernorm_weight5: R.Tensor((2560,), dtype="float16") = packed_params[482]
            gpt_neox_layers_30_input_layernorm_bias5: R.Tensor((2560,), dtype="float16") = packed_params[483]
            gpt_neox_layers_30_post_attention_layernorm_weight5: R.Tensor((2560,), dtype="float16") = packed_params[484]
            gpt_neox_layers_30_post_attention_layernorm_bias5: R.Tensor((2560,), dtype="float16") = packed_params[485]
            gpt_neox_layers_30_attention_query_key_value_q_weight5: R.Tensor((7680, 320), dtype="uint32") = packed_params[486]
            gpt_neox_layers_30_attention_query_key_value_q_scale5: R.Tensor((7680, 80), dtype="float16") = packed_params[487]
            gpt_neox_layers_30_attention_query_key_value_bias5: R.Tensor((7680,), dtype="float16") = packed_params[488]
            gpt_neox_layers_30_attention_dense_q_weight5: R.Tensor((2560, 320), dtype="uint32") = packed_params[489]
            gpt_neox_layers_30_attention_dense_q_scale5: R.Tensor((2560, 80), dtype="float16") = packed_params[490]
            gpt_neox_layers_30_attention_dense_bias5: R.Tensor((2560,), dtype="float16") = packed_params[491]
            gpt_neox_layers_30_mlp_dense_h_to_4h_q_weight5: R.Tensor((10240, 320), dtype="uint32") = packed_params[492]
            gpt_neox_layers_30_mlp_dense_h_to_4h_q_scale5: R.Tensor((10240, 80), dtype="float16") = packed_params[493]
            gpt_neox_layers_30_mlp_dense_h_to_4h_bias5: R.Tensor((10240,), dtype="float32") = packed_params[494]
            gpt_neox_layers_30_mlp_dense_4h_to_h_q_weight5: R.Tensor((2560, 1280), dtype="uint32") = packed_params[495]
            gpt_neox_layers_30_mlp_dense_4h_to_h_q_scale5: R.Tensor((2560, 320), dtype="float16") = packed_params[496]
            gpt_neox_layers_30_mlp_dense_4h_to_h_bias5: R.Tensor((2560,), dtype="float32") = packed_params[497]
            gpt_neox_layers_31_input_layernorm_weight5: R.Tensor((2560,), dtype="float16") = packed_params[498]
            gpt_neox_layers_31_input_layernorm_bias5: R.Tensor((2560,), dtype="float16") = packed_params[499]
            gpt_neox_layers_31_post_attention_layernorm_weight5: R.Tensor((2560,), dtype="float16") = packed_params[500]
            gpt_neox_layers_31_post_attention_layernorm_bias5: R.Tensor((2560,), dtype="float16") = packed_params[501]
            gpt_neox_layers_31_attention_query_key_value_q_weight5: R.Tensor((7680, 320), dtype="uint32") = packed_params[502]
            gpt_neox_layers_31_attention_query_key_value_q_scale5: R.Tensor((7680, 80), dtype="float16") = packed_params[503]
            gpt_neox_layers_31_attention_query_key_value_bias5: R.Tensor((7680,), dtype="float16") = packed_params[504]
            gpt_neox_layers_31_attention_dense_q_weight5: R.Tensor((2560, 320), dtype="uint32") = packed_params[505]
            gpt_neox_layers_31_attention_dense_q_scale5: R.Tensor((2560, 80), dtype="float16") = packed_params[506]
            gpt_neox_layers_31_attention_dense_bias5: R.Tensor((2560,), dtype="float16") = packed_params[507]
            gpt_neox_layers_31_mlp_dense_h_to_4h_q_weight5: R.Tensor((10240, 320), dtype="uint32") = packed_params[508]
            gpt_neox_layers_31_mlp_dense_h_to_4h_q_scale5: R.Tensor((10240, 80), dtype="float16") = packed_params[509]
            gpt_neox_layers_31_mlp_dense_h_to_4h_bias5: R.Tensor((10240,), dtype="float32") = packed_params[510]
            gpt_neox_layers_31_mlp_dense_4h_to_h_q_weight5: R.Tensor((2560, 1280), dtype="uint32") = packed_params[511]
            gpt_neox_layers_31_mlp_dense_4h_to_h_q_scale5: R.Tensor((2560, 320), dtype="float16") = packed_params[512]
            gpt_neox_layers_31_mlp_dense_4h_to_h_bias5: R.Tensor((2560,), dtype="float32") = packed_params[513]
            gpt_neox_final_layer_norm_weight5: R.Tensor((2560,), dtype="float16") = packed_params[514]
            gpt_neox_final_layer_norm_bias5: R.Tensor((2560,), dtype="float16") = packed_params[515]
            embed_out_q_weight5: R.Tensor((vocab_size, 320), dtype="uint32") = packed_params[516]
            embed_out_q_scale5: R.Tensor((vocab_size, 80), dtype="float16") = packed_params[517]
            layer_norm260 = R.call_tir(cls.layer_norm1, (input_embeds, gpt_neox_layers_0_input_layernorm_weight5, gpt_neox_layers_0_input_layernorm_bias5), out_sinfo=R.Tensor((1, seq_len, 2560), dtype="float16"))
            lv128 = R.call_tir(cls.fused_dequantize1_fused_NT_matmul5_add5, (gpt_neox_layers_0_attention_query_key_value_q_weight5, gpt_neox_layers_0_attention_query_key_value_q_scale5, layer_norm260, gpt_neox_layers_0_attention_query_key_value_bias5), out_sinfo=R.Tensor((1, seq_len, 7680), dtype="float16"))
            reshape512 = R.call_tir(cls.reshape4, (lv128,), out_sinfo=R.Tensor((1, seq_len, 96, 80), dtype="float16"))
            reshape513 = R.call_tir(cls.reshape5, (reshape512,), out_sinfo=R.Tensor((seq_len, 96, 80), dtype="float16"))
            lv647 = R.call_dps_packed("vm.builtin.attention_kv_cache_attention_with_fused_qkv", (paged_kv_cache, R.prim_value(0), R.prim_value(T.float32(1.0)), reshape513), out_sinfo=R.Tensor((seq_len, 32, 80), dtype="float16"))
            reshape514 = R.call_tir(cls.reshape6, (lv647,), out_sinfo=R.Tensor((1, seq_len, 32, 80), dtype="float16"))
            reshape515 = R.call_tir(cls.reshape7, (reshape514,), out_sinfo=R.Tensor((1, seq_len, 2560), dtype="float16"))
            lv128_1 = R.call_tir(cls.fused_dequantize2_fused_NT_matmul6_add6_add7, (gpt_neox_layers_0_attention_dense_q_weight5, gpt_neox_layers_0_attention_dense_q_scale5, reshape515, gpt_neox_layers_0_attention_dense_bias5, input_embeds), out_sinfo=R.Tensor((1, seq_len, 2560), dtype="float16"))
            layer_norm261 = R.call_tir(cls.layer_norm1, (lv128_1, gpt_neox_layers_0_post_attention_layernorm_weight5, gpt_neox_layers_0_post_attention_layernorm_bias5), out_sinfo=R.Tensor((1, seq_len, 2560), dtype="float16"))
            lv129 = R.call_tir(cls.fused_dequantize3_fused_NT_matmul7_add8_gelu1_cast3, (gpt_neox_layers_0_mlp_dense_h_to_4h_q_weight5, gpt_neox_layers_0_mlp_dense_h_to_4h_q_scale5, layer_norm261, gpt_neox_layers_0_mlp_dense_h_to_4h_bias5), out_sinfo=R.Tensor((1, seq_len, 10240), dtype="float16"))
            lv129_1 = R.call_tir(cls.fused_dequantize4_fused_NT_matmul8_add9_cast4_add7, (gpt_neox_layers_0_mlp_dense_4h_to_h_q_weight5, gpt_neox_layers_0_mlp_dense_4h_to_h_q_scale5, lv129, gpt_neox_layers_0_mlp_dense_4h_to_h_bias5, lv128_1), out_sinfo=R.Tensor((1, seq_len, 2560), dtype="float16"))
            layer_norm262 = R.call_tir(cls.layer_norm1, (lv129_1, gpt_neox_layers_1_input_layernorm_weight5, gpt_neox_layers_1_input_layernorm_bias5), out_sinfo=R.Tensor((1, seq_len, 2560), dtype="float16"))
            lv130 = R.call_tir(cls.fused_dequantize1_fused_NT_matmul5_add5, (gpt_neox_layers_1_attention_query_key_value_q_weight5, gpt_neox_layers_1_attention_query_key_value_q_scale5, layer_norm262, gpt_neox_layers_1_attention_query_key_value_bias5), out_sinfo=R.Tensor((1, seq_len, 7680), dtype="float16"))
            reshape516 = R.call_tir(cls.reshape4, (lv130,), out_sinfo=R.Tensor((1, seq_len, 96, 80), dtype="float16"))
            reshape517 = R.call_tir(cls.reshape5, (reshape516,), out_sinfo=R.Tensor((seq_len, 96, 80), dtype="float16"))
            lv652 = R.call_dps_packed("vm.builtin.attention_kv_cache_attention_with_fused_qkv", (paged_kv_cache, R.prim_value(1), R.prim_value(T.float32(1.0)), reshape517), out_sinfo=R.Tensor((seq_len, 32, 80), dtype="float16"))
            reshape518 = R.call_tir(cls.reshape6, (lv652,), out_sinfo=R.Tensor((1, seq_len, 32, 80), dtype="float16"))
            reshape519 = R.call_tir(cls.reshape7, (reshape518,), out_sinfo=R.Tensor((1, seq_len, 2560), dtype="float16"))
            lv130_1 = R.call_tir(cls.fused_dequantize2_fused_NT_matmul6_add6_add7, (gpt_neox_layers_1_attention_dense_q_weight5, gpt_neox_layers_1_attention_dense_q_scale5, reshape519, gpt_neox_layers_1_attention_dense_bias5, lv129_1), out_sinfo=R.Tensor((1, seq_len, 2560), dtype="float16"))
            layer_norm263 = R.call_tir(cls.layer_norm1, (lv130_1, gpt_neox_layers_1_post_attention_layernorm_weight5, gpt_neox_layers_1_post_attention_layernorm_bias5), out_sinfo=R.Tensor((1, seq_len, 2560), dtype="float16"))
            lv131 = R.call_tir(cls.fused_dequantize3_fused_NT_matmul7_add8_gelu1_cast3, (gpt_neox_layers_1_mlp_dense_h_to_4h_q_weight5, gpt_neox_layers_1_mlp_dense_h_to_4h_q_scale5, layer_norm263, gpt_neox_layers_1_mlp_dense_h_to_4h_bias5), out_sinfo=R.Tensor((1, seq_len, 10240), dtype="float16"))
            lv131_1 = R.call_tir(cls.fused_dequantize4_fused_NT_matmul8_add9_cast4_add7, (gpt_neox_layers_1_mlp_dense_4h_to_h_q_weight5, gpt_neox_layers_1_mlp_dense_4h_to_h_q_scale5, lv131, gpt_neox_layers_1_mlp_dense_4h_to_h_bias5, lv130_1), out_sinfo=R.Tensor((1, seq_len, 2560), dtype="float16"))
            layer_norm264 = R.call_tir(cls.layer_norm1, (lv131_1, gpt_neox_layers_2_input_layernorm_weight5, gpt_neox_layers_2_input_layernorm_bias5), out_sinfo=R.Tensor((1, seq_len, 2560), dtype="float16"))
            lv132 = R.call_tir(cls.fused_dequantize1_fused_NT_matmul5_add5, (gpt_neox_layers_2_attention_query_key_value_q_weight5, gpt_neox_layers_2_attention_query_key_value_q_scale5, layer_norm264, gpt_neox_layers_2_attention_query_key_value_bias5), out_sinfo=R.Tensor((1, seq_len, 7680), dtype="float16"))
            reshape520 = R.call_tir(cls.reshape4, (lv132,), out_sinfo=R.Tensor((1, seq_len, 96, 80), dtype="float16"))
            reshape521 = R.call_tir(cls.reshape5, (reshape520,), out_sinfo=R.Tensor((seq_len, 96, 80), dtype="float16"))
            lv657 = R.call_dps_packed("vm.builtin.attention_kv_cache_attention_with_fused_qkv", (paged_kv_cache, R.prim_value(2), R.prim_value(T.float32(1.0)), reshape521), out_sinfo=R.Tensor((seq_len, 32, 80), dtype="float16"))
            reshape522 = R.call_tir(cls.reshape6, (lv657,), out_sinfo=R.Tensor((1, seq_len, 32, 80), dtype="float16"))
            reshape523 = R.call_tir(cls.reshape7, (reshape522,), out_sinfo=R.Tensor((1, seq_len, 2560), dtype="float16"))
            lv132_1 = R.call_tir(cls.fused_dequantize2_fused_NT_matmul6_add6_add7, (gpt_neox_layers_2_attention_dense_q_weight5, gpt_neox_layers_2_attention_dense_q_scale5, reshape523, gpt_neox_layers_2_attention_dense_bias5, lv131_1), out_sinfo=R.Tensor((1, seq_len, 2560), dtype="float16"))
            layer_norm265 = R.call_tir(cls.layer_norm1, (lv132_1, gpt_neox_layers_2_post_attention_layernorm_weight5, gpt_neox_layers_2_post_attention_layernorm_bias5), out_sinfo=R.Tensor((1, seq_len, 2560), dtype="float16"))
            lv133 = R.call_tir(cls.fused_dequantize3_fused_NT_matmul7_add8_gelu1_cast3, (gpt_neox_layers_2_mlp_dense_h_to_4h_q_weight5, gpt_neox_layers_2_mlp_dense_h_to_4h_q_scale5, layer_norm265, gpt_neox_layers_2_mlp_dense_h_to_4h_bias5), out_sinfo=R.Tensor((1, seq_len, 10240), dtype="float16"))
            lv133_1 = R.call_tir(cls.fused_dequantize4_fused_NT_matmul8_add9_cast4_add7, (gpt_neox_layers_2_mlp_dense_4h_to_h_q_weight5, gpt_neox_layers_2_mlp_dense_4h_to_h_q_scale5, lv133, gpt_neox_layers_2_mlp_dense_4h_to_h_bias5, lv132_1), out_sinfo=R.Tensor((1, seq_len, 2560), dtype="float16"))
            layer_norm266 = R.call_tir(cls.layer_norm1, (lv133_1, gpt_neox_layers_3_input_layernorm_weight5, gpt_neox_layers_3_input_layernorm_bias5), out_sinfo=R.Tensor((1, seq_len, 2560), dtype="float16"))
            lv134 = R.call_tir(cls.fused_dequantize1_fused_NT_matmul5_add5, (gpt_neox_layers_3_attention_query_key_value_q_weight5, gpt_neox_layers_3_attention_query_key_value_q_scale5, layer_norm266, gpt_neox_layers_3_attention_query_key_value_bias5), out_sinfo=R.Tensor((1, seq_len, 7680), dtype="float16"))
            reshape524 = R.call_tir(cls.reshape4, (lv134,), out_sinfo=R.Tensor((1, seq_len, 96, 80), dtype="float16"))
            reshape525 = R.call_tir(cls.reshape5, (reshape524,), out_sinfo=R.Tensor((seq_len, 96, 80), dtype="float16"))
            lv662 = R.call_dps_packed("vm.builtin.attention_kv_cache_attention_with_fused_qkv", (paged_kv_cache, R.prim_value(3), R.prim_value(T.float32(1.0)), reshape525), out_sinfo=R.Tensor((seq_len, 32, 80), dtype="float16"))
            reshape526 = R.call_tir(cls.reshape6, (lv662,), out_sinfo=R.Tensor((1, seq_len, 32, 80), dtype="float16"))
            reshape527 = R.call_tir(cls.reshape7, (reshape526,), out_sinfo=R.Tensor((1, seq_len, 2560), dtype="float16"))
            lv134_1 = R.call_tir(cls.fused_dequantize2_fused_NT_matmul6_add6_add7, (gpt_neox_layers_3_attention_dense_q_weight5, gpt_neox_layers_3_attention_dense_q_scale5, reshape527, gpt_neox_layers_3_attention_dense_bias5, lv133_1), out_sinfo=R.Tensor((1, seq_len, 2560), dtype="float16"))
            layer_norm267 = R.call_tir(cls.layer_norm1, (lv134_1, gpt_neox_layers_3_post_attention_layernorm_weight5, gpt_neox_layers_3_post_attention_layernorm_bias5), out_sinfo=R.Tensor((1, seq_len, 2560), dtype="float16"))
            lv135 = R.call_tir(cls.fused_dequantize3_fused_NT_matmul7_add8_gelu1_cast3, (gpt_neox_layers_3_mlp_dense_h_to_4h_q_weight5, gpt_neox_layers_3_mlp_dense_h_to_4h_q_scale5, layer_norm267, gpt_neox_layers_3_mlp_dense_h_to_4h_bias5), out_sinfo=R.Tensor((1, seq_len, 10240), dtype="float16"))
            lv135_1 = R.call_tir(cls.fused_dequantize4_fused_NT_matmul8_add9_cast4_add7, (gpt_neox_layers_3_mlp_dense_4h_to_h_q_weight5, gpt_neox_layers_3_mlp_dense_4h_to_h_q_scale5, lv135, gpt_neox_layers_3_mlp_dense_4h_to_h_bias5, lv134_1), out_sinfo=R.Tensor((1, seq_len, 2560), dtype="float16"))
            layer_norm268 = R.call_tir(cls.layer_norm1, (lv135_1, gpt_neox_layers_4_input_layernorm_weight5, gpt_neox_layers_4_input_layernorm_bias5), out_sinfo=R.Tensor((1, seq_len, 2560), dtype="float16"))
            lv136 = R.call_tir(cls.fused_dequantize1_fused_NT_matmul5_add5, (gpt_neox_layers_4_attention_query_key_value_q_weight5, gpt_neox_layers_4_attention_query_key_value_q_scale5, layer_norm268, gpt_neox_layers_4_attention_query_key_value_bias5), out_sinfo=R.Tensor((1, seq_len, 7680), dtype="float16"))
            reshape528 = R.call_tir(cls.reshape4, (lv136,), out_sinfo=R.Tensor((1, seq_len, 96, 80), dtype="float16"))
            reshape529 = R.call_tir(cls.reshape5, (reshape528,), out_sinfo=R.Tensor((seq_len, 96, 80), dtype="float16"))
            lv667 = R.call_dps_packed("vm.builtin.attention_kv_cache_attention_with_fused_qkv", (paged_kv_cache, R.prim_value(4), R.prim_value(T.float32(1.0)), reshape529), out_sinfo=R.Tensor((seq_len, 32, 80), dtype="float16"))
            reshape530 = R.call_tir(cls.reshape6, (lv667,), out_sinfo=R.Tensor((1, seq_len, 32, 80), dtype="float16"))
            reshape531 = R.call_tir(cls.reshape7, (reshape530,), out_sinfo=R.Tensor((1, seq_len, 2560), dtype="float16"))
            lv136_1 = R.call_tir(cls.fused_dequantize2_fused_NT_matmul6_add6_add7, (gpt_neox_layers_4_attention_dense_q_weight5, gpt_neox_layers_4_attention_dense_q_scale5, reshape531, gpt_neox_layers_4_attention_dense_bias5, lv135_1), out_sinfo=R.Tensor((1, seq_len, 2560), dtype="float16"))
            layer_norm269 = R.call_tir(cls.layer_norm1, (lv136_1, gpt_neox_layers_4_post_attention_layernorm_weight5, gpt_neox_layers_4_post_attention_layernorm_bias5), out_sinfo=R.Tensor((1, seq_len, 2560), dtype="float16"))
            lv137 = R.call_tir(cls.fused_dequantize3_fused_NT_matmul7_add8_gelu1_cast3, (gpt_neox_layers_4_mlp_dense_h_to_4h_q_weight5, gpt_neox_layers_4_mlp_dense_h_to_4h_q_scale5, layer_norm269, gpt_neox_layers_4_mlp_dense_h_to_4h_bias5), out_sinfo=R.Tensor((1, seq_len, 10240), dtype="float16"))
            lv137_1 = R.call_tir(cls.fused_dequantize4_fused_NT_matmul8_add9_cast4_add7, (gpt_neox_layers_4_mlp_dense_4h_to_h_q_weight5, gpt_neox_layers_4_mlp_dense_4h_to_h_q_scale5, lv137, gpt_neox_layers_4_mlp_dense_4h_to_h_bias5, lv136_1), out_sinfo=R.Tensor((1, seq_len, 2560), dtype="float16"))
            layer_norm270 = R.call_tir(cls.layer_norm1, (lv137_1, gpt_neox_layers_5_input_layernorm_weight5, gpt_neox_layers_5_input_layernorm_bias5), out_sinfo=R.Tensor((1, seq_len, 2560), dtype="float16"))
            lv138 = R.call_tir(cls.fused_dequantize1_fused_NT_matmul5_add5, (gpt_neox_layers_5_attention_query_key_value_q_weight5, gpt_neox_layers_5_attention_query_key_value_q_scale5, layer_norm270, gpt_neox_layers_5_attention_query_key_value_bias5), out_sinfo=R.Tensor((1, seq_len, 7680), dtype="float16"))
            reshape532 = R.call_tir(cls.reshape4, (lv138,), out_sinfo=R.Tensor((1, seq_len, 96, 80), dtype="float16"))
            reshape533 = R.call_tir(cls.reshape5, (reshape532,), out_sinfo=R.Tensor((seq_len, 96, 80), dtype="float16"))
            lv672 = R.call_dps_packed("vm.builtin.attention_kv_cache_attention_with_fused_qkv", (paged_kv_cache, R.prim_value(5), R.prim_value(T.float32(1.0)), reshape533), out_sinfo=R.Tensor((seq_len, 32, 80), dtype="float16"))
            reshape534 = R.call_tir(cls.reshape6, (lv672,), out_sinfo=R.Tensor((1, seq_len, 32, 80), dtype="float16"))
            reshape535 = R.call_tir(cls.reshape7, (reshape534,), out_sinfo=R.Tensor((1, seq_len, 2560), dtype="float16"))
            lv138_1 = R.call_tir(cls.fused_dequantize2_fused_NT_matmul6_add6_add7, (gpt_neox_layers_5_attention_dense_q_weight5, gpt_neox_layers_5_attention_dense_q_scale5, reshape535, gpt_neox_layers_5_attention_dense_bias5, lv137_1), out_sinfo=R.Tensor((1, seq_len, 2560), dtype="float16"))
            layer_norm271 = R.call_tir(cls.layer_norm1, (lv138_1, gpt_neox_layers_5_post_attention_layernorm_weight5, gpt_neox_layers_5_post_attention_layernorm_bias5), out_sinfo=R.Tensor((1, seq_len, 2560), dtype="float16"))
            lv139 = R.call_tir(cls.fused_dequantize3_fused_NT_matmul7_add8_gelu1_cast3, (gpt_neox_layers_5_mlp_dense_h_to_4h_q_weight5, gpt_neox_layers_5_mlp_dense_h_to_4h_q_scale5, layer_norm271, gpt_neox_layers_5_mlp_dense_h_to_4h_bias5), out_sinfo=R.Tensor((1, seq_len, 10240), dtype="float16"))
            lv139_1 = R.call_tir(cls.fused_dequantize4_fused_NT_matmul8_add9_cast4_add7, (gpt_neox_layers_5_mlp_dense_4h_to_h_q_weight5, gpt_neox_layers_5_mlp_dense_4h_to_h_q_scale5, lv139, gpt_neox_layers_5_mlp_dense_4h_to_h_bias5, lv138_1), out_sinfo=R.Tensor((1, seq_len, 2560), dtype="float16"))
            layer_norm272 = R.call_tir(cls.layer_norm1, (lv139_1, gpt_neox_layers_6_input_layernorm_weight5, gpt_neox_layers_6_input_layernorm_bias5), out_sinfo=R.Tensor((1, seq_len, 2560), dtype="float16"))
            lv140 = R.call_tir(cls.fused_dequantize1_fused_NT_matmul5_add5, (gpt_neox_layers_6_attention_query_key_value_q_weight5, gpt_neox_layers_6_attention_query_key_value_q_scale5, layer_norm272, gpt_neox_layers_6_attention_query_key_value_bias5), out_sinfo=R.Tensor((1, seq_len, 7680), dtype="float16"))
            reshape536 = R.call_tir(cls.reshape4, (lv140,), out_sinfo=R.Tensor((1, seq_len, 96, 80), dtype="float16"))
            reshape537 = R.call_tir(cls.reshape5, (reshape536,), out_sinfo=R.Tensor((seq_len, 96, 80), dtype="float16"))
            lv677 = R.call_dps_packed("vm.builtin.attention_kv_cache_attention_with_fused_qkv", (paged_kv_cache, R.prim_value(6), R.prim_value(T.float32(1.0)), reshape537), out_sinfo=R.Tensor((seq_len, 32, 80), dtype="float16"))
            reshape538 = R.call_tir(cls.reshape6, (lv677,), out_sinfo=R.Tensor((1, seq_len, 32, 80), dtype="float16"))
            reshape539 = R.call_tir(cls.reshape7, (reshape538,), out_sinfo=R.Tensor((1, seq_len, 2560), dtype="float16"))
            lv140_1 = R.call_tir(cls.fused_dequantize2_fused_NT_matmul6_add6_add7, (gpt_neox_layers_6_attention_dense_q_weight5, gpt_neox_layers_6_attention_dense_q_scale5, reshape539, gpt_neox_layers_6_attention_dense_bias5, lv139_1), out_sinfo=R.Tensor((1, seq_len, 2560), dtype="float16"))
            layer_norm273 = R.call_tir(cls.layer_norm1, (lv140_1, gpt_neox_layers_6_post_attention_layernorm_weight5, gpt_neox_layers_6_post_attention_layernorm_bias5), out_sinfo=R.Tensor((1, seq_len, 2560), dtype="float16"))
            lv141 = R.call_tir(cls.fused_dequantize3_fused_NT_matmul7_add8_gelu1_cast3, (gpt_neox_layers_6_mlp_dense_h_to_4h_q_weight5, gpt_neox_layers_6_mlp_dense_h_to_4h_q_scale5, layer_norm273, gpt_neox_layers_6_mlp_dense_h_to_4h_bias5), out_sinfo=R.Tensor((1, seq_len, 10240), dtype="float16"))
            lv141_1 = R.call_tir(cls.fused_dequantize4_fused_NT_matmul8_add9_cast4_add7, (gpt_neox_layers_6_mlp_dense_4h_to_h_q_weight5, gpt_neox_layers_6_mlp_dense_4h_to_h_q_scale5, lv141, gpt_neox_layers_6_mlp_dense_4h_to_h_bias5, lv140_1), out_sinfo=R.Tensor((1, seq_len, 2560), dtype="float16"))
            layer_norm274 = R.call_tir(cls.layer_norm1, (lv141_1, gpt_neox_layers_7_input_layernorm_weight5, gpt_neox_layers_7_input_layernorm_bias5), out_sinfo=R.Tensor((1, seq_len, 2560), dtype="float16"))
            lv142 = R.call_tir(cls.fused_dequantize1_fused_NT_matmul5_add5, (gpt_neox_layers_7_attention_query_key_value_q_weight5, gpt_neox_layers_7_attention_query_key_value_q_scale5, layer_norm274, gpt_neox_layers_7_attention_query_key_value_bias5), out_sinfo=R.Tensor((1, seq_len, 7680), dtype="float16"))
            reshape540 = R.call_tir(cls.reshape4, (lv142,), out_sinfo=R.Tensor((1, seq_len, 96, 80), dtype="float16"))
            reshape541 = R.call_tir(cls.reshape5, (reshape540,), out_sinfo=R.Tensor((seq_len, 96, 80), dtype="float16"))
            lv682 = R.call_dps_packed("vm.builtin.attention_kv_cache_attention_with_fused_qkv", (paged_kv_cache, R.prim_value(7), R.prim_value(T.float32(1.0)), reshape541), out_sinfo=R.Tensor((seq_len, 32, 80), dtype="float16"))
            reshape542 = R.call_tir(cls.reshape6, (lv682,), out_sinfo=R.Tensor((1, seq_len, 32, 80), dtype="float16"))
            reshape543 = R.call_tir(cls.reshape7, (reshape542,), out_sinfo=R.Tensor((1, seq_len, 2560), dtype="float16"))
            lv142_1 = R.call_tir(cls.fused_dequantize2_fused_NT_matmul6_add6_add7, (gpt_neox_layers_7_attention_dense_q_weight5, gpt_neox_layers_7_attention_dense_q_scale5, reshape543, gpt_neox_layers_7_attention_dense_bias5, lv141_1), out_sinfo=R.Tensor((1, seq_len, 2560), dtype="float16"))
            layer_norm275 = R.call_tir(cls.layer_norm1, (lv142_1, gpt_neox_layers_7_post_attention_layernorm_weight5, gpt_neox_layers_7_post_attention_layernorm_bias5), out_sinfo=R.Tensor((1, seq_len, 2560), dtype="float16"))
            lv143 = R.call_tir(cls.fused_dequantize3_fused_NT_matmul7_add8_gelu1_cast3, (gpt_neox_layers_7_mlp_dense_h_to_4h_q_weight5, gpt_neox_layers_7_mlp_dense_h_to_4h_q_scale5, layer_norm275, gpt_neox_layers_7_mlp_dense_h_to_4h_bias5), out_sinfo=R.Tensor((1, seq_len, 10240), dtype="float16"))
            lv143_1 = R.call_tir(cls.fused_dequantize4_fused_NT_matmul8_add9_cast4_add7, (gpt_neox_layers_7_mlp_dense_4h_to_h_q_weight5, gpt_neox_layers_7_mlp_dense_4h_to_h_q_scale5, lv143, gpt_neox_layers_7_mlp_dense_4h_to_h_bias5, lv142_1), out_sinfo=R.Tensor((1, seq_len, 2560), dtype="float16"))
            layer_norm276 = R.call_tir(cls.layer_norm1, (lv143_1, gpt_neox_layers_8_input_layernorm_weight5, gpt_neox_layers_8_input_layernorm_bias5), out_sinfo=R.Tensor((1, seq_len, 2560), dtype="float16"))
            lv144 = R.call_tir(cls.fused_dequantize1_fused_NT_matmul5_add5, (gpt_neox_layers_8_attention_query_key_value_q_weight5, gpt_neox_layers_8_attention_query_key_value_q_scale5, layer_norm276, gpt_neox_layers_8_attention_query_key_value_bias5), out_sinfo=R.Tensor((1, seq_len, 7680), dtype="float16"))
            reshape544 = R.call_tir(cls.reshape4, (lv144,), out_sinfo=R.Tensor((1, seq_len, 96, 80), dtype="float16"))
            reshape545 = R.call_tir(cls.reshape5, (reshape544,), out_sinfo=R.Tensor((seq_len, 96, 80), dtype="float16"))
            lv687 = R.call_dps_packed("vm.builtin.attention_kv_cache_attention_with_fused_qkv", (paged_kv_cache, R.prim_value(8), R.prim_value(T.float32(1.0)), reshape545), out_sinfo=R.Tensor((seq_len, 32, 80), dtype="float16"))
            reshape546 = R.call_tir(cls.reshape6, (lv687,), out_sinfo=R.Tensor((1, seq_len, 32, 80), dtype="float16"))
            reshape547 = R.call_tir(cls.reshape7, (reshape546,), out_sinfo=R.Tensor((1, seq_len, 2560), dtype="float16"))
            lv144_1 = R.call_tir(cls.fused_dequantize2_fused_NT_matmul6_add6_add7, (gpt_neox_layers_8_attention_dense_q_weight5, gpt_neox_layers_8_attention_dense_q_scale5, reshape547, gpt_neox_layers_8_attention_dense_bias5, lv143_1), out_sinfo=R.Tensor((1, seq_len, 2560), dtype="float16"))
            layer_norm277 = R.call_tir(cls.layer_norm1, (lv144_1, gpt_neox_layers_8_post_attention_layernorm_weight5, gpt_neox_layers_8_post_attention_layernorm_bias5), out_sinfo=R.Tensor((1, seq_len, 2560), dtype="float16"))
            lv145 = R.call_tir(cls.fused_dequantize3_fused_NT_matmul7_add8_gelu1_cast3, (gpt_neox_layers_8_mlp_dense_h_to_4h_q_weight5, gpt_neox_layers_8_mlp_dense_h_to_4h_q_scale5, layer_norm277, gpt_neox_layers_8_mlp_dense_h_to_4h_bias5), out_sinfo=R.Tensor((1, seq_len, 10240), dtype="float16"))
            lv145_1 = R.call_tir(cls.fused_dequantize4_fused_NT_matmul8_add9_cast4_add7, (gpt_neox_layers_8_mlp_dense_4h_to_h_q_weight5, gpt_neox_layers_8_mlp_dense_4h_to_h_q_scale5, lv145, gpt_neox_layers_8_mlp_dense_4h_to_h_bias5, lv144_1), out_sinfo=R.Tensor((1, seq_len, 2560), dtype="float16"))
            layer_norm278 = R.call_tir(cls.layer_norm1, (lv145_1, gpt_neox_layers_9_input_layernorm_weight5, gpt_neox_layers_9_input_layernorm_bias5), out_sinfo=R.Tensor((1, seq_len, 2560), dtype="float16"))
            lv146 = R.call_tir(cls.fused_dequantize1_fused_NT_matmul5_add5, (gpt_neox_layers_9_attention_query_key_value_q_weight5, gpt_neox_layers_9_attention_query_key_value_q_scale5, layer_norm278, gpt_neox_layers_9_attention_query_key_value_bias5), out_sinfo=R.Tensor((1, seq_len, 7680), dtype="float16"))
            reshape548 = R.call_tir(cls.reshape4, (lv146,), out_sinfo=R.Tensor((1, seq_len, 96, 80), dtype="float16"))
            reshape549 = R.call_tir(cls.reshape5, (reshape548,), out_sinfo=R.Tensor((seq_len, 96, 80), dtype="float16"))
            lv692 = R.call_dps_packed("vm.builtin.attention_kv_cache_attention_with_fused_qkv", (paged_kv_cache, R.prim_value(9), R.prim_value(T.float32(1.0)), reshape549), out_sinfo=R.Tensor((seq_len, 32, 80), dtype="float16"))
            reshape550 = R.call_tir(cls.reshape6, (lv692,), out_sinfo=R.Tensor((1, seq_len, 32, 80), dtype="float16"))
            reshape551 = R.call_tir(cls.reshape7, (reshape550,), out_sinfo=R.Tensor((1, seq_len, 2560), dtype="float16"))
            lv146_1 = R.call_tir(cls.fused_dequantize2_fused_NT_matmul6_add6_add7, (gpt_neox_layers_9_attention_dense_q_weight5, gpt_neox_layers_9_attention_dense_q_scale5, reshape551, gpt_neox_layers_9_attention_dense_bias5, lv145_1), out_sinfo=R.Tensor((1, seq_len, 2560), dtype="float16"))
            layer_norm279 = R.call_tir(cls.layer_norm1, (lv146_1, gpt_neox_layers_9_post_attention_layernorm_weight5, gpt_neox_layers_9_post_attention_layernorm_bias5), out_sinfo=R.Tensor((1, seq_len, 2560), dtype="float16"))
            lv147 = R.call_tir(cls.fused_dequantize3_fused_NT_matmul7_add8_gelu1_cast3, (gpt_neox_layers_9_mlp_dense_h_to_4h_q_weight5, gpt_neox_layers_9_mlp_dense_h_to_4h_q_scale5, layer_norm279, gpt_neox_layers_9_mlp_dense_h_to_4h_bias5), out_sinfo=R.Tensor((1, seq_len, 10240), dtype="float16"))
            lv147_1 = R.call_tir(cls.fused_dequantize4_fused_NT_matmul8_add9_cast4_add7, (gpt_neox_layers_9_mlp_dense_4h_to_h_q_weight5, gpt_neox_layers_9_mlp_dense_4h_to_h_q_scale5, lv147, gpt_neox_layers_9_mlp_dense_4h_to_h_bias5, lv146_1), out_sinfo=R.Tensor((1, seq_len, 2560), dtype="float16"))
            layer_norm280 = R.call_tir(cls.layer_norm1, (lv147_1, gpt_neox_layers_10_input_layernorm_weight5, gpt_neox_layers_10_input_layernorm_bias5), out_sinfo=R.Tensor((1, seq_len, 2560), dtype="float16"))
            lv148 = R.call_tir(cls.fused_dequantize1_fused_NT_matmul5_add5, (gpt_neox_layers_10_attention_query_key_value_q_weight5, gpt_neox_layers_10_attention_query_key_value_q_scale5, layer_norm280, gpt_neox_layers_10_attention_query_key_value_bias5), out_sinfo=R.Tensor((1, seq_len, 7680), dtype="float16"))
            reshape552 = R.call_tir(cls.reshape4, (lv148,), out_sinfo=R.Tensor((1, seq_len, 96, 80), dtype="float16"))
            reshape553 = R.call_tir(cls.reshape5, (reshape552,), out_sinfo=R.Tensor((seq_len, 96, 80), dtype="float16"))
            lv697 = R.call_dps_packed("vm.builtin.attention_kv_cache_attention_with_fused_qkv", (paged_kv_cache, R.prim_value(10), R.prim_value(T.float32(1.0)), reshape553), out_sinfo=R.Tensor((seq_len, 32, 80), dtype="float16"))
            reshape554 = R.call_tir(cls.reshape6, (lv697,), out_sinfo=R.Tensor((1, seq_len, 32, 80), dtype="float16"))
            reshape555 = R.call_tir(cls.reshape7, (reshape554,), out_sinfo=R.Tensor((1, seq_len, 2560), dtype="float16"))
            lv148_1 = R.call_tir(cls.fused_dequantize2_fused_NT_matmul6_add6_add7, (gpt_neox_layers_10_attention_dense_q_weight5, gpt_neox_layers_10_attention_dense_q_scale5, reshape555, gpt_neox_layers_10_attention_dense_bias5, lv147_1), out_sinfo=R.Tensor((1, seq_len, 2560), dtype="float16"))
            layer_norm281 = R.call_tir(cls.layer_norm1, (lv148_1, gpt_neox_layers_10_post_attention_layernorm_weight5, gpt_neox_layers_10_post_attention_layernorm_bias5), out_sinfo=R.Tensor((1, seq_len, 2560), dtype="float16"))
            lv149 = R.call_tir(cls.fused_dequantize3_fused_NT_matmul7_add8_gelu1_cast3, (gpt_neox_layers_10_mlp_dense_h_to_4h_q_weight5, gpt_neox_layers_10_mlp_dense_h_to_4h_q_scale5, layer_norm281, gpt_neox_layers_10_mlp_dense_h_to_4h_bias5), out_sinfo=R.Tensor((1, seq_len, 10240), dtype="float16"))
            lv149_1 = R.call_tir(cls.fused_dequantize4_fused_NT_matmul8_add9_cast4_add7, (gpt_neox_layers_10_mlp_dense_4h_to_h_q_weight5, gpt_neox_layers_10_mlp_dense_4h_to_h_q_scale5, lv149, gpt_neox_layers_10_mlp_dense_4h_to_h_bias5, lv148_1), out_sinfo=R.Tensor((1, seq_len, 2560), dtype="float16"))
            layer_norm282 = R.call_tir(cls.layer_norm1, (lv149_1, gpt_neox_layers_11_input_layernorm_weight5, gpt_neox_layers_11_input_layernorm_bias5), out_sinfo=R.Tensor((1, seq_len, 2560), dtype="float16"))
            lv150 = R.call_tir(cls.fused_dequantize1_fused_NT_matmul5_add5, (gpt_neox_layers_11_attention_query_key_value_q_weight5, gpt_neox_layers_11_attention_query_key_value_q_scale5, layer_norm282, gpt_neox_layers_11_attention_query_key_value_bias5), out_sinfo=R.Tensor((1, seq_len, 7680), dtype="float16"))
            reshape556 = R.call_tir(cls.reshape4, (lv150,), out_sinfo=R.Tensor((1, seq_len, 96, 80), dtype="float16"))
            reshape557 = R.call_tir(cls.reshape5, (reshape556,), out_sinfo=R.Tensor((seq_len, 96, 80), dtype="float16"))
            lv702 = R.call_dps_packed("vm.builtin.attention_kv_cache_attention_with_fused_qkv", (paged_kv_cache, R.prim_value(11), R.prim_value(T.float32(1.0)), reshape557), out_sinfo=R.Tensor((seq_len, 32, 80), dtype="float16"))
            reshape558 = R.call_tir(cls.reshape6, (lv702,), out_sinfo=R.Tensor((1, seq_len, 32, 80), dtype="float16"))
            reshape559 = R.call_tir(cls.reshape7, (reshape558,), out_sinfo=R.Tensor((1, seq_len, 2560), dtype="float16"))
            lv150_1 = R.call_tir(cls.fused_dequantize2_fused_NT_matmul6_add6_add7, (gpt_neox_layers_11_attention_dense_q_weight5, gpt_neox_layers_11_attention_dense_q_scale5, reshape559, gpt_neox_layers_11_attention_dense_bias5, lv149_1), out_sinfo=R.Tensor((1, seq_len, 2560), dtype="float16"))
            layer_norm283 = R.call_tir(cls.layer_norm1, (lv150_1, gpt_neox_layers_11_post_attention_layernorm_weight5, gpt_neox_layers_11_post_attention_layernorm_bias5), out_sinfo=R.Tensor((1, seq_len, 2560), dtype="float16"))
            lv151 = R.call_tir(cls.fused_dequantize3_fused_NT_matmul7_add8_gelu1_cast3, (gpt_neox_layers_11_mlp_dense_h_to_4h_q_weight5, gpt_neox_layers_11_mlp_dense_h_to_4h_q_scale5, layer_norm283, gpt_neox_layers_11_mlp_dense_h_to_4h_bias5), out_sinfo=R.Tensor((1, seq_len, 10240), dtype="float16"))
            lv151_1 = R.call_tir(cls.fused_dequantize4_fused_NT_matmul8_add9_cast4_add7, (gpt_neox_layers_11_mlp_dense_4h_to_h_q_weight5, gpt_neox_layers_11_mlp_dense_4h_to_h_q_scale5, lv151, gpt_neox_layers_11_mlp_dense_4h_to_h_bias5, lv150_1), out_sinfo=R.Tensor((1, seq_len, 2560), dtype="float16"))
            layer_norm284 = R.call_tir(cls.layer_norm1, (lv151_1, gpt_neox_layers_12_input_layernorm_weight5, gpt_neox_layers_12_input_layernorm_bias5), out_sinfo=R.Tensor((1, seq_len, 2560), dtype="float16"))
            lv152 = R.call_tir(cls.fused_dequantize1_fused_NT_matmul5_add5, (gpt_neox_layers_12_attention_query_key_value_q_weight5, gpt_neox_layers_12_attention_query_key_value_q_scale5, layer_norm284, gpt_neox_layers_12_attention_query_key_value_bias5), out_sinfo=R.Tensor((1, seq_len, 7680), dtype="float16"))
            reshape560 = R.call_tir(cls.reshape4, (lv152,), out_sinfo=R.Tensor((1, seq_len, 96, 80), dtype="float16"))
            reshape561 = R.call_tir(cls.reshape5, (reshape560,), out_sinfo=R.Tensor((seq_len, 96, 80), dtype="float16"))
            lv707 = R.call_dps_packed("vm.builtin.attention_kv_cache_attention_with_fused_qkv", (paged_kv_cache, R.prim_value(12), R.prim_value(T.float32(1.0)), reshape561), out_sinfo=R.Tensor((seq_len, 32, 80), dtype="float16"))
            reshape562 = R.call_tir(cls.reshape6, (lv707,), out_sinfo=R.Tensor((1, seq_len, 32, 80), dtype="float16"))
            reshape563 = R.call_tir(cls.reshape7, (reshape562,), out_sinfo=R.Tensor((1, seq_len, 2560), dtype="float16"))
            lv152_1 = R.call_tir(cls.fused_dequantize2_fused_NT_matmul6_add6_add7, (gpt_neox_layers_12_attention_dense_q_weight5, gpt_neox_layers_12_attention_dense_q_scale5, reshape563, gpt_neox_layers_12_attention_dense_bias5, lv151_1), out_sinfo=R.Tensor((1, seq_len, 2560), dtype="float16"))
            layer_norm285 = R.call_tir(cls.layer_norm1, (lv152_1, gpt_neox_layers_12_post_attention_layernorm_weight5, gpt_neox_layers_12_post_attention_layernorm_bias5), out_sinfo=R.Tensor((1, seq_len, 2560), dtype="float16"))
            lv153 = R.call_tir(cls.fused_dequantize3_fused_NT_matmul7_add8_gelu1_cast3, (gpt_neox_layers_12_mlp_dense_h_to_4h_q_weight5, gpt_neox_layers_12_mlp_dense_h_to_4h_q_scale5, layer_norm285, gpt_neox_layers_12_mlp_dense_h_to_4h_bias5), out_sinfo=R.Tensor((1, seq_len, 10240), dtype="float16"))
            lv153_1 = R.call_tir(cls.fused_dequantize4_fused_NT_matmul8_add9_cast4_add7, (gpt_neox_layers_12_mlp_dense_4h_to_h_q_weight5, gpt_neox_layers_12_mlp_dense_4h_to_h_q_scale5, lv153, gpt_neox_layers_12_mlp_dense_4h_to_h_bias5, lv152_1), out_sinfo=R.Tensor((1, seq_len, 2560), dtype="float16"))
            layer_norm286 = R.call_tir(cls.layer_norm1, (lv153_1, gpt_neox_layers_13_input_layernorm_weight5, gpt_neox_layers_13_input_layernorm_bias5), out_sinfo=R.Tensor((1, seq_len, 2560), dtype="float16"))
            lv154 = R.call_tir(cls.fused_dequantize1_fused_NT_matmul5_add5, (gpt_neox_layers_13_attention_query_key_value_q_weight5, gpt_neox_layers_13_attention_query_key_value_q_scale5, layer_norm286, gpt_neox_layers_13_attention_query_key_value_bias5), out_sinfo=R.Tensor((1, seq_len, 7680), dtype="float16"))
            reshape564 = R.call_tir(cls.reshape4, (lv154,), out_sinfo=R.Tensor((1, seq_len, 96, 80), dtype="float16"))
            reshape565 = R.call_tir(cls.reshape5, (reshape564,), out_sinfo=R.Tensor((seq_len, 96, 80), dtype="float16"))
            lv712 = R.call_dps_packed("vm.builtin.attention_kv_cache_attention_with_fused_qkv", (paged_kv_cache, R.prim_value(13), R.prim_value(T.float32(1.0)), reshape565), out_sinfo=R.Tensor((seq_len, 32, 80), dtype="float16"))
            reshape566 = R.call_tir(cls.reshape6, (lv712,), out_sinfo=R.Tensor((1, seq_len, 32, 80), dtype="float16"))
            reshape567 = R.call_tir(cls.reshape7, (reshape566,), out_sinfo=R.Tensor((1, seq_len, 2560), dtype="float16"))
            lv154_1 = R.call_tir(cls.fused_dequantize2_fused_NT_matmul6_add6_add7, (gpt_neox_layers_13_attention_dense_q_weight5, gpt_neox_layers_13_attention_dense_q_scale5, reshape567, gpt_neox_layers_13_attention_dense_bias5, lv153_1), out_sinfo=R.Tensor((1, seq_len, 2560), dtype="float16"))
            layer_norm287 = R.call_tir(cls.layer_norm1, (lv154_1, gpt_neox_layers_13_post_attention_layernorm_weight5, gpt_neox_layers_13_post_attention_layernorm_bias5), out_sinfo=R.Tensor((1, seq_len, 2560), dtype="float16"))
            lv155 = R.call_tir(cls.fused_dequantize3_fused_NT_matmul7_add8_gelu1_cast3, (gpt_neox_layers_13_mlp_dense_h_to_4h_q_weight5, gpt_neox_layers_13_mlp_dense_h_to_4h_q_scale5, layer_norm287, gpt_neox_layers_13_mlp_dense_h_to_4h_bias5), out_sinfo=R.Tensor((1, seq_len, 10240), dtype="float16"))
            lv155_1 = R.call_tir(cls.fused_dequantize4_fused_NT_matmul8_add9_cast4_add7, (gpt_neox_layers_13_mlp_dense_4h_to_h_q_weight5, gpt_neox_layers_13_mlp_dense_4h_to_h_q_scale5, lv155, gpt_neox_layers_13_mlp_dense_4h_to_h_bias5, lv154_1), out_sinfo=R.Tensor((1, seq_len, 2560), dtype="float16"))
            layer_norm288 = R.call_tir(cls.layer_norm1, (lv155_1, gpt_neox_layers_14_input_layernorm_weight5, gpt_neox_layers_14_input_layernorm_bias5), out_sinfo=R.Tensor((1, seq_len, 2560), dtype="float16"))
            lv156 = R.call_tir(cls.fused_dequantize1_fused_NT_matmul5_add5, (gpt_neox_layers_14_attention_query_key_value_q_weight5, gpt_neox_layers_14_attention_query_key_value_q_scale5, layer_norm288, gpt_neox_layers_14_attention_query_key_value_bias5), out_sinfo=R.Tensor((1, seq_len, 7680), dtype="float16"))
            reshape568 = R.call_tir(cls.reshape4, (lv156,), out_sinfo=R.Tensor((1, seq_len, 96, 80), dtype="float16"))
            reshape569 = R.call_tir(cls.reshape5, (reshape568,), out_sinfo=R.Tensor((seq_len, 96, 80), dtype="float16"))
            lv717 = R.call_dps_packed("vm.builtin.attention_kv_cache_attention_with_fused_qkv", (paged_kv_cache, R.prim_value(14), R.prim_value(T.float32(1.0)), reshape569), out_sinfo=R.Tensor((seq_len, 32, 80), dtype="float16"))
            reshape570 = R.call_tir(cls.reshape6, (lv717,), out_sinfo=R.Tensor((1, seq_len, 32, 80), dtype="float16"))
            reshape571 = R.call_tir(cls.reshape7, (reshape570,), out_sinfo=R.Tensor((1, seq_len, 2560), dtype="float16"))
            lv156_1 = R.call_tir(cls.fused_dequantize2_fused_NT_matmul6_add6_add7, (gpt_neox_layers_14_attention_dense_q_weight5, gpt_neox_layers_14_attention_dense_q_scale5, reshape571, gpt_neox_layers_14_attention_dense_bias5, lv155_1), out_sinfo=R.Tensor((1, seq_len, 2560), dtype="float16"))
            layer_norm289 = R.call_tir(cls.layer_norm1, (lv156_1, gpt_neox_layers_14_post_attention_layernorm_weight5, gpt_neox_layers_14_post_attention_layernorm_bias5), out_sinfo=R.Tensor((1, seq_len, 2560), dtype="float16"))
            lv157 = R.call_tir(cls.fused_dequantize3_fused_NT_matmul7_add8_gelu1_cast3, (gpt_neox_layers_14_mlp_dense_h_to_4h_q_weight5, gpt_neox_layers_14_mlp_dense_h_to_4h_q_scale5, layer_norm289, gpt_neox_layers_14_mlp_dense_h_to_4h_bias5), out_sinfo=R.Tensor((1, seq_len, 10240), dtype="float16"))
            lv157_1 = R.call_tir(cls.fused_dequantize4_fused_NT_matmul8_add9_cast4_add7, (gpt_neox_layers_14_mlp_dense_4h_to_h_q_weight5, gpt_neox_layers_14_mlp_dense_4h_to_h_q_scale5, lv157, gpt_neox_layers_14_mlp_dense_4h_to_h_bias5, lv156_1), out_sinfo=R.Tensor((1, seq_len, 2560), dtype="float16"))
            layer_norm290 = R.call_tir(cls.layer_norm1, (lv157_1, gpt_neox_layers_15_input_layernorm_weight5, gpt_neox_layers_15_input_layernorm_bias5), out_sinfo=R.Tensor((1, seq_len, 2560), dtype="float16"))
            lv158 = R.call_tir(cls.fused_dequantize1_fused_NT_matmul5_add5, (gpt_neox_layers_15_attention_query_key_value_q_weight5, gpt_neox_layers_15_attention_query_key_value_q_scale5, layer_norm290, gpt_neox_layers_15_attention_query_key_value_bias5), out_sinfo=R.Tensor((1, seq_len, 7680), dtype="float16"))
            reshape572 = R.call_tir(cls.reshape4, (lv158,), out_sinfo=R.Tensor((1, seq_len, 96, 80), dtype="float16"))
            reshape573 = R.call_tir(cls.reshape5, (reshape572,), out_sinfo=R.Tensor((seq_len, 96, 80), dtype="float16"))
            lv722 = R.call_dps_packed("vm.builtin.attention_kv_cache_attention_with_fused_qkv", (paged_kv_cache, R.prim_value(15), R.prim_value(T.float32(1.0)), reshape573), out_sinfo=R.Tensor((seq_len, 32, 80), dtype="float16"))
            reshape574 = R.call_tir(cls.reshape6, (lv722,), out_sinfo=R.Tensor((1, seq_len, 32, 80), dtype="float16"))
            reshape575 = R.call_tir(cls.reshape7, (reshape574,), out_sinfo=R.Tensor((1, seq_len, 2560), dtype="float16"))
            lv158_1 = R.call_tir(cls.fused_dequantize2_fused_NT_matmul6_add6_add7, (gpt_neox_layers_15_attention_dense_q_weight5, gpt_neox_layers_15_attention_dense_q_scale5, reshape575, gpt_neox_layers_15_attention_dense_bias5, lv157_1), out_sinfo=R.Tensor((1, seq_len, 2560), dtype="float16"))
            layer_norm291 = R.call_tir(cls.layer_norm1, (lv158_1, gpt_neox_layers_15_post_attention_layernorm_weight5, gpt_neox_layers_15_post_attention_layernorm_bias5), out_sinfo=R.Tensor((1, seq_len, 2560), dtype="float16"))
            lv159 = R.call_tir(cls.fused_dequantize3_fused_NT_matmul7_add8_gelu1_cast3, (gpt_neox_layers_15_mlp_dense_h_to_4h_q_weight5, gpt_neox_layers_15_mlp_dense_h_to_4h_q_scale5, layer_norm291, gpt_neox_layers_15_mlp_dense_h_to_4h_bias5), out_sinfo=R.Tensor((1, seq_len, 10240), dtype="float16"))
            lv159_1 = R.call_tir(cls.fused_dequantize4_fused_NT_matmul8_add9_cast4_add7, (gpt_neox_layers_15_mlp_dense_4h_to_h_q_weight5, gpt_neox_layers_15_mlp_dense_4h_to_h_q_scale5, lv159, gpt_neox_layers_15_mlp_dense_4h_to_h_bias5, lv158_1), out_sinfo=R.Tensor((1, seq_len, 2560), dtype="float16"))
            layer_norm292 = R.call_tir(cls.layer_norm1, (lv159_1, gpt_neox_layers_16_input_layernorm_weight5, gpt_neox_layers_16_input_layernorm_bias5), out_sinfo=R.Tensor((1, seq_len, 2560), dtype="float16"))
            lv160 = R.call_tir(cls.fused_dequantize1_fused_NT_matmul5_add5, (gpt_neox_layers_16_attention_query_key_value_q_weight5, gpt_neox_layers_16_attention_query_key_value_q_scale5, layer_norm292, gpt_neox_layers_16_attention_query_key_value_bias5), out_sinfo=R.Tensor((1, seq_len, 7680), dtype="float16"))
            reshape576 = R.call_tir(cls.reshape4, (lv160,), out_sinfo=R.Tensor((1, seq_len, 96, 80), dtype="float16"))
            reshape577 = R.call_tir(cls.reshape5, (reshape576,), out_sinfo=R.Tensor((seq_len, 96, 80), dtype="float16"))
            lv727 = R.call_dps_packed("vm.builtin.attention_kv_cache_attention_with_fused_qkv", (paged_kv_cache, R.prim_value(16), R.prim_value(T.float32(1.0)), reshape577), out_sinfo=R.Tensor((seq_len, 32, 80), dtype="float16"))
            reshape578 = R.call_tir(cls.reshape6, (lv727,), out_sinfo=R.Tensor((1, seq_len, 32, 80), dtype="float16"))
            reshape579 = R.call_tir(cls.reshape7, (reshape578,), out_sinfo=R.Tensor((1, seq_len, 2560), dtype="float16"))
            lv160_1 = R.call_tir(cls.fused_dequantize2_fused_NT_matmul6_add6_add7, (gpt_neox_layers_16_attention_dense_q_weight5, gpt_neox_layers_16_attention_dense_q_scale5, reshape579, gpt_neox_layers_16_attention_dense_bias5, lv159_1), out_sinfo=R.Tensor((1, seq_len, 2560), dtype="float16"))
            layer_norm293 = R.call_tir(cls.layer_norm1, (lv160_1, gpt_neox_layers_16_post_attention_layernorm_weight5, gpt_neox_layers_16_post_attention_layernorm_bias5), out_sinfo=R.Tensor((1, seq_len, 2560), dtype="float16"))
            lv161 = R.call_tir(cls.fused_dequantize3_fused_NT_matmul7_add8_gelu1_cast3, (gpt_neox_layers_16_mlp_dense_h_to_4h_q_weight5, gpt_neox_layers_16_mlp_dense_h_to_4h_q_scale5, layer_norm293, gpt_neox_layers_16_mlp_dense_h_to_4h_bias5), out_sinfo=R.Tensor((1, seq_len, 10240), dtype="float16"))
            lv161_1 = R.call_tir(cls.fused_dequantize4_fused_NT_matmul8_add9_cast4_add7, (gpt_neox_layers_16_mlp_dense_4h_to_h_q_weight5, gpt_neox_layers_16_mlp_dense_4h_to_h_q_scale5, lv161, gpt_neox_layers_16_mlp_dense_4h_to_h_bias5, lv160_1), out_sinfo=R.Tensor((1, seq_len, 2560), dtype="float16"))
            layer_norm294 = R.call_tir(cls.layer_norm1, (lv161_1, gpt_neox_layers_17_input_layernorm_weight5, gpt_neox_layers_17_input_layernorm_bias5), out_sinfo=R.Tensor((1, seq_len, 2560), dtype="float16"))
            lv162 = R.call_tir(cls.fused_dequantize1_fused_NT_matmul5_add5, (gpt_neox_layers_17_attention_query_key_value_q_weight5, gpt_neox_layers_17_attention_query_key_value_q_scale5, layer_norm294, gpt_neox_layers_17_attention_query_key_value_bias5), out_sinfo=R.Tensor((1, seq_len, 7680), dtype="float16"))
            reshape580 = R.call_tir(cls.reshape4, (lv162,), out_sinfo=R.Tensor((1, seq_len, 96, 80), dtype="float16"))
            reshape581 = R.call_tir(cls.reshape5, (reshape580,), out_sinfo=R.Tensor((seq_len, 96, 80), dtype="float16"))
            lv732 = R.call_dps_packed("vm.builtin.attention_kv_cache_attention_with_fused_qkv", (paged_kv_cache, R.prim_value(17), R.prim_value(T.float32(1.0)), reshape581), out_sinfo=R.Tensor((seq_len, 32, 80), dtype="float16"))
            reshape582 = R.call_tir(cls.reshape6, (lv732,), out_sinfo=R.Tensor((1, seq_len, 32, 80), dtype="float16"))
            reshape583 = R.call_tir(cls.reshape7, (reshape582,), out_sinfo=R.Tensor((1, seq_len, 2560), dtype="float16"))
            lv162_1 = R.call_tir(cls.fused_dequantize2_fused_NT_matmul6_add6_add7, (gpt_neox_layers_17_attention_dense_q_weight5, gpt_neox_layers_17_attention_dense_q_scale5, reshape583, gpt_neox_layers_17_attention_dense_bias5, lv161_1), out_sinfo=R.Tensor((1, seq_len, 2560), dtype="float16"))
            layer_norm295 = R.call_tir(cls.layer_norm1, (lv162_1, gpt_neox_layers_17_post_attention_layernorm_weight5, gpt_neox_layers_17_post_attention_layernorm_bias5), out_sinfo=R.Tensor((1, seq_len, 2560), dtype="float16"))
            lv163 = R.call_tir(cls.fused_dequantize3_fused_NT_matmul7_add8_gelu1_cast3, (gpt_neox_layers_17_mlp_dense_h_to_4h_q_weight5, gpt_neox_layers_17_mlp_dense_h_to_4h_q_scale5, layer_norm295, gpt_neox_layers_17_mlp_dense_h_to_4h_bias5), out_sinfo=R.Tensor((1, seq_len, 10240), dtype="float16"))
            lv163_1 = R.call_tir(cls.fused_dequantize4_fused_NT_matmul8_add9_cast4_add7, (gpt_neox_layers_17_mlp_dense_4h_to_h_q_weight5, gpt_neox_layers_17_mlp_dense_4h_to_h_q_scale5, lv163, gpt_neox_layers_17_mlp_dense_4h_to_h_bias5, lv162_1), out_sinfo=R.Tensor((1, seq_len, 2560), dtype="float16"))
            layer_norm296 = R.call_tir(cls.layer_norm1, (lv163_1, gpt_neox_layers_18_input_layernorm_weight5, gpt_neox_layers_18_input_layernorm_bias5), out_sinfo=R.Tensor((1, seq_len, 2560), dtype="float16"))
            lv164 = R.call_tir(cls.fused_dequantize1_fused_NT_matmul5_add5, (gpt_neox_layers_18_attention_query_key_value_q_weight5, gpt_neox_layers_18_attention_query_key_value_q_scale5, layer_norm296, gpt_neox_layers_18_attention_query_key_value_bias5), out_sinfo=R.Tensor((1, seq_len, 7680), dtype="float16"))
            reshape584 = R.call_tir(cls.reshape4, (lv164,), out_sinfo=R.Tensor((1, seq_len, 96, 80), dtype="float16"))
            reshape585 = R.call_tir(cls.reshape5, (reshape584,), out_sinfo=R.Tensor((seq_len, 96, 80), dtype="float16"))
            lv737 = R.call_dps_packed("vm.builtin.attention_kv_cache_attention_with_fused_qkv", (paged_kv_cache, R.prim_value(18), R.prim_value(T.float32(1.0)), reshape585), out_sinfo=R.Tensor((seq_len, 32, 80), dtype="float16"))
            reshape586 = R.call_tir(cls.reshape6, (lv737,), out_sinfo=R.Tensor((1, seq_len, 32, 80), dtype="float16"))
            reshape587 = R.call_tir(cls.reshape7, (reshape586,), out_sinfo=R.Tensor((1, seq_len, 2560), dtype="float16"))
            lv164_1 = R.call_tir(cls.fused_dequantize2_fused_NT_matmul6_add6_add7, (gpt_neox_layers_18_attention_dense_q_weight5, gpt_neox_layers_18_attention_dense_q_scale5, reshape587, gpt_neox_layers_18_attention_dense_bias5, lv163_1), out_sinfo=R.Tensor((1, seq_len, 2560), dtype="float16"))
            layer_norm297 = R.call_tir(cls.layer_norm1, (lv164_1, gpt_neox_layers_18_post_attention_layernorm_weight5, gpt_neox_layers_18_post_attention_layernorm_bias5), out_sinfo=R.Tensor((1, seq_len, 2560), dtype="float16"))
            lv165 = R.call_tir(cls.fused_dequantize3_fused_NT_matmul7_add8_gelu1_cast3, (gpt_neox_layers_18_mlp_dense_h_to_4h_q_weight5, gpt_neox_layers_18_mlp_dense_h_to_4h_q_scale5, layer_norm297, gpt_neox_layers_18_mlp_dense_h_to_4h_bias5), out_sinfo=R.Tensor((1, seq_len, 10240), dtype="float16"))
            lv165_1 = R.call_tir(cls.fused_dequantize4_fused_NT_matmul8_add9_cast4_add7, (gpt_neox_layers_18_mlp_dense_4h_to_h_q_weight5, gpt_neox_layers_18_mlp_dense_4h_to_h_q_scale5, lv165, gpt_neox_layers_18_mlp_dense_4h_to_h_bias5, lv164_1), out_sinfo=R.Tensor((1, seq_len, 2560), dtype="float16"))
            layer_norm298 = R.call_tir(cls.layer_norm1, (lv165_1, gpt_neox_layers_19_input_layernorm_weight5, gpt_neox_layers_19_input_layernorm_bias5), out_sinfo=R.Tensor((1, seq_len, 2560), dtype="float16"))
            lv166 = R.call_tir(cls.fused_dequantize1_fused_NT_matmul5_add5, (gpt_neox_layers_19_attention_query_key_value_q_weight5, gpt_neox_layers_19_attention_query_key_value_q_scale5, layer_norm298, gpt_neox_layers_19_attention_query_key_value_bias5), out_sinfo=R.Tensor((1, seq_len, 7680), dtype="float16"))
            reshape588 = R.call_tir(cls.reshape4, (lv166,), out_sinfo=R.Tensor((1, seq_len, 96, 80), dtype="float16"))
            reshape589 = R.call_tir(cls.reshape5, (reshape588,), out_sinfo=R.Tensor((seq_len, 96, 80), dtype="float16"))
            lv742 = R.call_dps_packed("vm.builtin.attention_kv_cache_attention_with_fused_qkv", (paged_kv_cache, R.prim_value(19), R.prim_value(T.float32(1.0)), reshape589), out_sinfo=R.Tensor((seq_len, 32, 80), dtype="float16"))
            reshape590 = R.call_tir(cls.reshape6, (lv742,), out_sinfo=R.Tensor((1, seq_len, 32, 80), dtype="float16"))
            reshape591 = R.call_tir(cls.reshape7, (reshape590,), out_sinfo=R.Tensor((1, seq_len, 2560), dtype="float16"))
            lv166_1 = R.call_tir(cls.fused_dequantize2_fused_NT_matmul6_add6_add7, (gpt_neox_layers_19_attention_dense_q_weight5, gpt_neox_layers_19_attention_dense_q_scale5, reshape591, gpt_neox_layers_19_attention_dense_bias5, lv165_1), out_sinfo=R.Tensor((1, seq_len, 2560), dtype="float16"))
            layer_norm299 = R.call_tir(cls.layer_norm1, (lv166_1, gpt_neox_layers_19_post_attention_layernorm_weight5, gpt_neox_layers_19_post_attention_layernorm_bias5), out_sinfo=R.Tensor((1, seq_len, 2560), dtype="float16"))
            lv167 = R.call_tir(cls.fused_dequantize3_fused_NT_matmul7_add8_gelu1_cast3, (gpt_neox_layers_19_mlp_dense_h_to_4h_q_weight5, gpt_neox_layers_19_mlp_dense_h_to_4h_q_scale5, layer_norm299, gpt_neox_layers_19_mlp_dense_h_to_4h_bias5), out_sinfo=R.Tensor((1, seq_len, 10240), dtype="float16"))
            lv167_1 = R.call_tir(cls.fused_dequantize4_fused_NT_matmul8_add9_cast4_add7, (gpt_neox_layers_19_mlp_dense_4h_to_h_q_weight5, gpt_neox_layers_19_mlp_dense_4h_to_h_q_scale5, lv167, gpt_neox_layers_19_mlp_dense_4h_to_h_bias5, lv166_1), out_sinfo=R.Tensor((1, seq_len, 2560), dtype="float16"))
            layer_norm300 = R.call_tir(cls.layer_norm1, (lv167_1, gpt_neox_layers_20_input_layernorm_weight5, gpt_neox_layers_20_input_layernorm_bias5), out_sinfo=R.Tensor((1, seq_len, 2560), dtype="float16"))
            lv168 = R.call_tir(cls.fused_dequantize1_fused_NT_matmul5_add5, (gpt_neox_layers_20_attention_query_key_value_q_weight5, gpt_neox_layers_20_attention_query_key_value_q_scale5, layer_norm300, gpt_neox_layers_20_attention_query_key_value_bias5), out_sinfo=R.Tensor((1, seq_len, 7680), dtype="float16"))
            reshape592 = R.call_tir(cls.reshape4, (lv168,), out_sinfo=R.Tensor((1, seq_len, 96, 80), dtype="float16"))
            reshape593 = R.call_tir(cls.reshape5, (reshape592,), out_sinfo=R.Tensor((seq_len, 96, 80), dtype="float16"))
            lv747 = R.call_dps_packed("vm.builtin.attention_kv_cache_attention_with_fused_qkv", (paged_kv_cache, R.prim_value(20), R.prim_value(T.float32(1.0)), reshape593), out_sinfo=R.Tensor((seq_len, 32, 80), dtype="float16"))
            reshape594 = R.call_tir(cls.reshape6, (lv747,), out_sinfo=R.Tensor((1, seq_len, 32, 80), dtype="float16"))
            reshape595 = R.call_tir(cls.reshape7, (reshape594,), out_sinfo=R.Tensor((1, seq_len, 2560), dtype="float16"))
            lv168_1 = R.call_tir(cls.fused_dequantize2_fused_NT_matmul6_add6_add7, (gpt_neox_layers_20_attention_dense_q_weight5, gpt_neox_layers_20_attention_dense_q_scale5, reshape595, gpt_neox_layers_20_attention_dense_bias5, lv167_1), out_sinfo=R.Tensor((1, seq_len, 2560), dtype="float16"))
            layer_norm301 = R.call_tir(cls.layer_norm1, (lv168_1, gpt_neox_layers_20_post_attention_layernorm_weight5, gpt_neox_layers_20_post_attention_layernorm_bias5), out_sinfo=R.Tensor((1, seq_len, 2560), dtype="float16"))
            lv169 = R.call_tir(cls.fused_dequantize3_fused_NT_matmul7_add8_gelu1_cast3, (gpt_neox_layers_20_mlp_dense_h_to_4h_q_weight5, gpt_neox_layers_20_mlp_dense_h_to_4h_q_scale5, layer_norm301, gpt_neox_layers_20_mlp_dense_h_to_4h_bias5), out_sinfo=R.Tensor((1, seq_len, 10240), dtype="float16"))
            lv169_1 = R.call_tir(cls.fused_dequantize4_fused_NT_matmul8_add9_cast4_add7, (gpt_neox_layers_20_mlp_dense_4h_to_h_q_weight5, gpt_neox_layers_20_mlp_dense_4h_to_h_q_scale5, lv169, gpt_neox_layers_20_mlp_dense_4h_to_h_bias5, lv168_1), out_sinfo=R.Tensor((1, seq_len, 2560), dtype="float16"))
            layer_norm302 = R.call_tir(cls.layer_norm1, (lv169_1, gpt_neox_layers_21_input_layernorm_weight5, gpt_neox_layers_21_input_layernorm_bias5), out_sinfo=R.Tensor((1, seq_len, 2560), dtype="float16"))
            lv170 = R.call_tir(cls.fused_dequantize1_fused_NT_matmul5_add5, (gpt_neox_layers_21_attention_query_key_value_q_weight5, gpt_neox_layers_21_attention_query_key_value_q_scale5, layer_norm302, gpt_neox_layers_21_attention_query_key_value_bias5), out_sinfo=R.Tensor((1, seq_len, 7680), dtype="float16"))
            reshape596 = R.call_tir(cls.reshape4, (lv170,), out_sinfo=R.Tensor((1, seq_len, 96, 80), dtype="float16"))
            reshape597 = R.call_tir(cls.reshape5, (reshape596,), out_sinfo=R.Tensor((seq_len, 96, 80), dtype="float16"))
            lv752 = R.call_dps_packed("vm.builtin.attention_kv_cache_attention_with_fused_qkv", (paged_kv_cache, R.prim_value(21), R.prim_value(T.float32(1.0)), reshape597), out_sinfo=R.Tensor((seq_len, 32, 80), dtype="float16"))
            reshape598 = R.call_tir(cls.reshape6, (lv752,), out_sinfo=R.Tensor((1, seq_len, 32, 80), dtype="float16"))
            reshape599 = R.call_tir(cls.reshape7, (reshape598,), out_sinfo=R.Tensor((1, seq_len, 2560), dtype="float16"))
            lv170_1 = R.call_tir(cls.fused_dequantize2_fused_NT_matmul6_add6_add7, (gpt_neox_layers_21_attention_dense_q_weight5, gpt_neox_layers_21_attention_dense_q_scale5, reshape599, gpt_neox_layers_21_attention_dense_bias5, lv169_1), out_sinfo=R.Tensor((1, seq_len, 2560), dtype="float16"))
            layer_norm303 = R.call_tir(cls.layer_norm1, (lv170_1, gpt_neox_layers_21_post_attention_layernorm_weight5, gpt_neox_layers_21_post_attention_layernorm_bias5), out_sinfo=R.Tensor((1, seq_len, 2560), dtype="float16"))
            lv171 = R.call_tir(cls.fused_dequantize3_fused_NT_matmul7_add8_gelu1_cast3, (gpt_neox_layers_21_mlp_dense_h_to_4h_q_weight5, gpt_neox_layers_21_mlp_dense_h_to_4h_q_scale5, layer_norm303, gpt_neox_layers_21_mlp_dense_h_to_4h_bias5), out_sinfo=R.Tensor((1, seq_len, 10240), dtype="float16"))
            lv171_1 = R.call_tir(cls.fused_dequantize4_fused_NT_matmul8_add9_cast4_add7, (gpt_neox_layers_21_mlp_dense_4h_to_h_q_weight5, gpt_neox_layers_21_mlp_dense_4h_to_h_q_scale5, lv171, gpt_neox_layers_21_mlp_dense_4h_to_h_bias5, lv170_1), out_sinfo=R.Tensor((1, seq_len, 2560), dtype="float16"))
            layer_norm304 = R.call_tir(cls.layer_norm1, (lv171_1, gpt_neox_layers_22_input_layernorm_weight5, gpt_neox_layers_22_input_layernorm_bias5), out_sinfo=R.Tensor((1, seq_len, 2560), dtype="float16"))
            lv172 = R.call_tir(cls.fused_dequantize1_fused_NT_matmul5_add5, (gpt_neox_layers_22_attention_query_key_value_q_weight5, gpt_neox_layers_22_attention_query_key_value_q_scale5, layer_norm304, gpt_neox_layers_22_attention_query_key_value_bias5), out_sinfo=R.Tensor((1, seq_len, 7680), dtype="float16"))
            reshape600 = R.call_tir(cls.reshape4, (lv172,), out_sinfo=R.Tensor((1, seq_len, 96, 80), dtype="float16"))
            reshape601 = R.call_tir(cls.reshape5, (reshape600,), out_sinfo=R.Tensor((seq_len, 96, 80), dtype="float16"))
            lv757 = R.call_dps_packed("vm.builtin.attention_kv_cache_attention_with_fused_qkv", (paged_kv_cache, R.prim_value(22), R.prim_value(T.float32(1.0)), reshape601), out_sinfo=R.Tensor((seq_len, 32, 80), dtype="float16"))
            reshape602 = R.call_tir(cls.reshape6, (lv757,), out_sinfo=R.Tensor((1, seq_len, 32, 80), dtype="float16"))
            reshape603 = R.call_tir(cls.reshape7, (reshape602,), out_sinfo=R.Tensor((1, seq_len, 2560), dtype="float16"))
            lv172_1 = R.call_tir(cls.fused_dequantize2_fused_NT_matmul6_add6_add7, (gpt_neox_layers_22_attention_dense_q_weight5, gpt_neox_layers_22_attention_dense_q_scale5, reshape603, gpt_neox_layers_22_attention_dense_bias5, lv171_1), out_sinfo=R.Tensor((1, seq_len, 2560), dtype="float16"))
            layer_norm305 = R.call_tir(cls.layer_norm1, (lv172_1, gpt_neox_layers_22_post_attention_layernorm_weight5, gpt_neox_layers_22_post_attention_layernorm_bias5), out_sinfo=R.Tensor((1, seq_len, 2560), dtype="float16"))
            lv173 = R.call_tir(cls.fused_dequantize3_fused_NT_matmul7_add8_gelu1_cast3, (gpt_neox_layers_22_mlp_dense_h_to_4h_q_weight5, gpt_neox_layers_22_mlp_dense_h_to_4h_q_scale5, layer_norm305, gpt_neox_layers_22_mlp_dense_h_to_4h_bias5), out_sinfo=R.Tensor((1, seq_len, 10240), dtype="float16"))
            lv173_1 = R.call_tir(cls.fused_dequantize4_fused_NT_matmul8_add9_cast4_add7, (gpt_neox_layers_22_mlp_dense_4h_to_h_q_weight5, gpt_neox_layers_22_mlp_dense_4h_to_h_q_scale5, lv173, gpt_neox_layers_22_mlp_dense_4h_to_h_bias5, lv172_1), out_sinfo=R.Tensor((1, seq_len, 2560), dtype="float16"))
            layer_norm306 = R.call_tir(cls.layer_norm1, (lv173_1, gpt_neox_layers_23_input_layernorm_weight5, gpt_neox_layers_23_input_layernorm_bias5), out_sinfo=R.Tensor((1, seq_len, 2560), dtype="float16"))
            lv174 = R.call_tir(cls.fused_dequantize1_fused_NT_matmul5_add5, (gpt_neox_layers_23_attention_query_key_value_q_weight5, gpt_neox_layers_23_attention_query_key_value_q_scale5, layer_norm306, gpt_neox_layers_23_attention_query_key_value_bias5), out_sinfo=R.Tensor((1, seq_len, 7680), dtype="float16"))
            reshape604 = R.call_tir(cls.reshape4, (lv174,), out_sinfo=R.Tensor((1, seq_len, 96, 80), dtype="float16"))
            reshape605 = R.call_tir(cls.reshape5, (reshape604,), out_sinfo=R.Tensor((seq_len, 96, 80), dtype="float16"))
            lv762 = R.call_dps_packed("vm.builtin.attention_kv_cache_attention_with_fused_qkv", (paged_kv_cache, R.prim_value(23), R.prim_value(T.float32(1.0)), reshape605), out_sinfo=R.Tensor((seq_len, 32, 80), dtype="float16"))
            reshape606 = R.call_tir(cls.reshape6, (lv762,), out_sinfo=R.Tensor((1, seq_len, 32, 80), dtype="float16"))
            reshape607 = R.call_tir(cls.reshape7, (reshape606,), out_sinfo=R.Tensor((1, seq_len, 2560), dtype="float16"))
            lv174_1 = R.call_tir(cls.fused_dequantize2_fused_NT_matmul6_add6_add7, (gpt_neox_layers_23_attention_dense_q_weight5, gpt_neox_layers_23_attention_dense_q_scale5, reshape607, gpt_neox_layers_23_attention_dense_bias5, lv173_1), out_sinfo=R.Tensor((1, seq_len, 2560), dtype="float16"))
            layer_norm307 = R.call_tir(cls.layer_norm1, (lv174_1, gpt_neox_layers_23_post_attention_layernorm_weight5, gpt_neox_layers_23_post_attention_layernorm_bias5), out_sinfo=R.Tensor((1, seq_len, 2560), dtype="float16"))
            lv175 = R.call_tir(cls.fused_dequantize3_fused_NT_matmul7_add8_gelu1_cast3, (gpt_neox_layers_23_mlp_dense_h_to_4h_q_weight5, gpt_neox_layers_23_mlp_dense_h_to_4h_q_scale5, layer_norm307, gpt_neox_layers_23_mlp_dense_h_to_4h_bias5), out_sinfo=R.Tensor((1, seq_len, 10240), dtype="float16"))
            lv175_1 = R.call_tir(cls.fused_dequantize4_fused_NT_matmul8_add9_cast4_add7, (gpt_neox_layers_23_mlp_dense_4h_to_h_q_weight5, gpt_neox_layers_23_mlp_dense_4h_to_h_q_scale5, lv175, gpt_neox_layers_23_mlp_dense_4h_to_h_bias5, lv174_1), out_sinfo=R.Tensor((1, seq_len, 2560), dtype="float16"))
            layer_norm308 = R.call_tir(cls.layer_norm1, (lv175_1, gpt_neox_layers_24_input_layernorm_weight5, gpt_neox_layers_24_input_layernorm_bias5), out_sinfo=R.Tensor((1, seq_len, 2560), dtype="float16"))
            lv176 = R.call_tir(cls.fused_dequantize1_fused_NT_matmul5_add5, (gpt_neox_layers_24_attention_query_key_value_q_weight5, gpt_neox_layers_24_attention_query_key_value_q_scale5, layer_norm308, gpt_neox_layers_24_attention_query_key_value_bias5), out_sinfo=R.Tensor((1, seq_len, 7680), dtype="float16"))
            reshape608 = R.call_tir(cls.reshape4, (lv176,), out_sinfo=R.Tensor((1, seq_len, 96, 80), dtype="float16"))
            reshape609 = R.call_tir(cls.reshape5, (reshape608,), out_sinfo=R.Tensor((seq_len, 96, 80), dtype="float16"))
            lv767 = R.call_dps_packed("vm.builtin.attention_kv_cache_attention_with_fused_qkv", (paged_kv_cache, R.prim_value(24), R.prim_value(T.float32(1.0)), reshape609), out_sinfo=R.Tensor((seq_len, 32, 80), dtype="float16"))
            reshape610 = R.call_tir(cls.reshape6, (lv767,), out_sinfo=R.Tensor((1, seq_len, 32, 80), dtype="float16"))
            reshape611 = R.call_tir(cls.reshape7, (reshape610,), out_sinfo=R.Tensor((1, seq_len, 2560), dtype="float16"))
            lv176_1 = R.call_tir(cls.fused_dequantize2_fused_NT_matmul6_add6_add7, (gpt_neox_layers_24_attention_dense_q_weight5, gpt_neox_layers_24_attention_dense_q_scale5, reshape611, gpt_neox_layers_24_attention_dense_bias5, lv175_1), out_sinfo=R.Tensor((1, seq_len, 2560), dtype="float16"))
            layer_norm309 = R.call_tir(cls.layer_norm1, (lv176_1, gpt_neox_layers_24_post_attention_layernorm_weight5, gpt_neox_layers_24_post_attention_layernorm_bias5), out_sinfo=R.Tensor((1, seq_len, 2560), dtype="float16"))
            lv177 = R.call_tir(cls.fused_dequantize3_fused_NT_matmul7_add8_gelu1_cast3, (gpt_neox_layers_24_mlp_dense_h_to_4h_q_weight5, gpt_neox_layers_24_mlp_dense_h_to_4h_q_scale5, layer_norm309, gpt_neox_layers_24_mlp_dense_h_to_4h_bias5), out_sinfo=R.Tensor((1, seq_len, 10240), dtype="float16"))
            lv177_1 = R.call_tir(cls.fused_dequantize4_fused_NT_matmul8_add9_cast4_add7, (gpt_neox_layers_24_mlp_dense_4h_to_h_q_weight5, gpt_neox_layers_24_mlp_dense_4h_to_h_q_scale5, lv177, gpt_neox_layers_24_mlp_dense_4h_to_h_bias5, lv176_1), out_sinfo=R.Tensor((1, seq_len, 2560), dtype="float16"))
            layer_norm310 = R.call_tir(cls.layer_norm1, (lv177_1, gpt_neox_layers_25_input_layernorm_weight5, gpt_neox_layers_25_input_layernorm_bias5), out_sinfo=R.Tensor((1, seq_len, 2560), dtype="float16"))
            lv178 = R.call_tir(cls.fused_dequantize1_fused_NT_matmul5_add5, (gpt_neox_layers_25_attention_query_key_value_q_weight5, gpt_neox_layers_25_attention_query_key_value_q_scale5, layer_norm310, gpt_neox_layers_25_attention_query_key_value_bias5), out_sinfo=R.Tensor((1, seq_len, 7680), dtype="float16"))
            reshape612 = R.call_tir(cls.reshape4, (lv178,), out_sinfo=R.Tensor((1, seq_len, 96, 80), dtype="float16"))
            reshape613 = R.call_tir(cls.reshape5, (reshape612,), out_sinfo=R.Tensor((seq_len, 96, 80), dtype="float16"))
            lv772 = R.call_dps_packed("vm.builtin.attention_kv_cache_attention_with_fused_qkv", (paged_kv_cache, R.prim_value(25), R.prim_value(T.float32(1.0)), reshape613), out_sinfo=R.Tensor((seq_len, 32, 80), dtype="float16"))
            reshape614 = R.call_tir(cls.reshape6, (lv772,), out_sinfo=R.Tensor((1, seq_len, 32, 80), dtype="float16"))
            reshape615 = R.call_tir(cls.reshape7, (reshape614,), out_sinfo=R.Tensor((1, seq_len, 2560), dtype="float16"))
            lv178_1 = R.call_tir(cls.fused_dequantize2_fused_NT_matmul6_add6_add7, (gpt_neox_layers_25_attention_dense_q_weight5, gpt_neox_layers_25_attention_dense_q_scale5, reshape615, gpt_neox_layers_25_attention_dense_bias5, lv177_1), out_sinfo=R.Tensor((1, seq_len, 2560), dtype="float16"))
            layer_norm311 = R.call_tir(cls.layer_norm1, (lv178_1, gpt_neox_layers_25_post_attention_layernorm_weight5, gpt_neox_layers_25_post_attention_layernorm_bias5), out_sinfo=R.Tensor((1, seq_len, 2560), dtype="float16"))
            lv179 = R.call_tir(cls.fused_dequantize3_fused_NT_matmul7_add8_gelu1_cast3, (gpt_neox_layers_25_mlp_dense_h_to_4h_q_weight5, gpt_neox_layers_25_mlp_dense_h_to_4h_q_scale5, layer_norm311, gpt_neox_layers_25_mlp_dense_h_to_4h_bias5), out_sinfo=R.Tensor((1, seq_len, 10240), dtype="float16"))
            lv179_1 = R.call_tir(cls.fused_dequantize4_fused_NT_matmul8_add9_cast4_add7, (gpt_neox_layers_25_mlp_dense_4h_to_h_q_weight5, gpt_neox_layers_25_mlp_dense_4h_to_h_q_scale5, lv179, gpt_neox_layers_25_mlp_dense_4h_to_h_bias5, lv178_1), out_sinfo=R.Tensor((1, seq_len, 2560), dtype="float16"))
            layer_norm312 = R.call_tir(cls.layer_norm1, (lv179_1, gpt_neox_layers_26_input_layernorm_weight5, gpt_neox_layers_26_input_layernorm_bias5), out_sinfo=R.Tensor((1, seq_len, 2560), dtype="float16"))
            lv180 = R.call_tir(cls.fused_dequantize1_fused_NT_matmul5_add5, (gpt_neox_layers_26_attention_query_key_value_q_weight5, gpt_neox_layers_26_attention_query_key_value_q_scale5, layer_norm312, gpt_neox_layers_26_attention_query_key_value_bias5), out_sinfo=R.Tensor((1, seq_len, 7680), dtype="float16"))
            reshape616 = R.call_tir(cls.reshape4, (lv180,), out_sinfo=R.Tensor((1, seq_len, 96, 80), dtype="float16"))
            reshape617 = R.call_tir(cls.reshape5, (reshape616,), out_sinfo=R.Tensor((seq_len, 96, 80), dtype="float16"))
            lv777 = R.call_dps_packed("vm.builtin.attention_kv_cache_attention_with_fused_qkv", (paged_kv_cache, R.prim_value(26), R.prim_value(T.float32(1.0)), reshape617), out_sinfo=R.Tensor((seq_len, 32, 80), dtype="float16"))
            reshape618 = R.call_tir(cls.reshape6, (lv777,), out_sinfo=R.Tensor((1, seq_len, 32, 80), dtype="float16"))
            reshape619 = R.call_tir(cls.reshape7, (reshape618,), out_sinfo=R.Tensor((1, seq_len, 2560), dtype="float16"))
            lv180_1 = R.call_tir(cls.fused_dequantize2_fused_NT_matmul6_add6_add7, (gpt_neox_layers_26_attention_dense_q_weight5, gpt_neox_layers_26_attention_dense_q_scale5, reshape619, gpt_neox_layers_26_attention_dense_bias5, lv179_1), out_sinfo=R.Tensor((1, seq_len, 2560), dtype="float16"))
            layer_norm313 = R.call_tir(cls.layer_norm1, (lv180_1, gpt_neox_layers_26_post_attention_layernorm_weight5, gpt_neox_layers_26_post_attention_layernorm_bias5), out_sinfo=R.Tensor((1, seq_len, 2560), dtype="float16"))
            lv181 = R.call_tir(cls.fused_dequantize3_fused_NT_matmul7_add8_gelu1_cast3, (gpt_neox_layers_26_mlp_dense_h_to_4h_q_weight5, gpt_neox_layers_26_mlp_dense_h_to_4h_q_scale5, layer_norm313, gpt_neox_layers_26_mlp_dense_h_to_4h_bias5), out_sinfo=R.Tensor((1, seq_len, 10240), dtype="float16"))
            lv181_1 = R.call_tir(cls.fused_dequantize4_fused_NT_matmul8_add9_cast4_add7, (gpt_neox_layers_26_mlp_dense_4h_to_h_q_weight5, gpt_neox_layers_26_mlp_dense_4h_to_h_q_scale5, lv181, gpt_neox_layers_26_mlp_dense_4h_to_h_bias5, lv180_1), out_sinfo=R.Tensor((1, seq_len, 2560), dtype="float16"))
            layer_norm314 = R.call_tir(cls.layer_norm1, (lv181_1, gpt_neox_layers_27_input_layernorm_weight5, gpt_neox_layers_27_input_layernorm_bias5), out_sinfo=R.Tensor((1, seq_len, 2560), dtype="float16"))
            lv182 = R.call_tir(cls.fused_dequantize1_fused_NT_matmul5_add5, (gpt_neox_layers_27_attention_query_key_value_q_weight5, gpt_neox_layers_27_attention_query_key_value_q_scale5, layer_norm314, gpt_neox_layers_27_attention_query_key_value_bias5), out_sinfo=R.Tensor((1, seq_len, 7680), dtype="float16"))
            reshape620 = R.call_tir(cls.reshape4, (lv182,), out_sinfo=R.Tensor((1, seq_len, 96, 80), dtype="float16"))
            reshape621 = R.call_tir(cls.reshape5, (reshape620,), out_sinfo=R.Tensor((seq_len, 96, 80), dtype="float16"))
            lv782 = R.call_dps_packed("vm.builtin.attention_kv_cache_attention_with_fused_qkv", (paged_kv_cache, R.prim_value(27), R.prim_value(T.float32(1.0)), reshape621), out_sinfo=R.Tensor((seq_len, 32, 80), dtype="float16"))
            reshape622 = R.call_tir(cls.reshape6, (lv782,), out_sinfo=R.Tensor((1, seq_len, 32, 80), dtype="float16"))
            reshape623 = R.call_tir(cls.reshape7, (reshape622,), out_sinfo=R.Tensor((1, seq_len, 2560), dtype="float16"))
            lv182_1 = R.call_tir(cls.fused_dequantize2_fused_NT_matmul6_add6_add7, (gpt_neox_layers_27_attention_dense_q_weight5, gpt_neox_layers_27_attention_dense_q_scale5, reshape623, gpt_neox_layers_27_attention_dense_bias5, lv181_1), out_sinfo=R.Tensor((1, seq_len, 2560), dtype="float16"))
            layer_norm315 = R.call_tir(cls.layer_norm1, (lv182_1, gpt_neox_layers_27_post_attention_layernorm_weight5, gpt_neox_layers_27_post_attention_layernorm_bias5), out_sinfo=R.Tensor((1, seq_len, 2560), dtype="float16"))
            lv183 = R.call_tir(cls.fused_dequantize3_fused_NT_matmul7_add8_gelu1_cast3, (gpt_neox_layers_27_mlp_dense_h_to_4h_q_weight5, gpt_neox_layers_27_mlp_dense_h_to_4h_q_scale5, layer_norm315, gpt_neox_layers_27_mlp_dense_h_to_4h_bias5), out_sinfo=R.Tensor((1, seq_len, 10240), dtype="float16"))
            lv183_1 = R.call_tir(cls.fused_dequantize4_fused_NT_matmul8_add9_cast4_add7, (gpt_neox_layers_27_mlp_dense_4h_to_h_q_weight5, gpt_neox_layers_27_mlp_dense_4h_to_h_q_scale5, lv183, gpt_neox_layers_27_mlp_dense_4h_to_h_bias5, lv182_1), out_sinfo=R.Tensor((1, seq_len, 2560), dtype="float16"))
            layer_norm316 = R.call_tir(cls.layer_norm1, (lv183_1, gpt_neox_layers_28_input_layernorm_weight5, gpt_neox_layers_28_input_layernorm_bias5), out_sinfo=R.Tensor((1, seq_len, 2560), dtype="float16"))
            lv184 = R.call_tir(cls.fused_dequantize1_fused_NT_matmul5_add5, (gpt_neox_layers_28_attention_query_key_value_q_weight5, gpt_neox_layers_28_attention_query_key_value_q_scale5, layer_norm316, gpt_neox_layers_28_attention_query_key_value_bias5), out_sinfo=R.Tensor((1, seq_len, 7680), dtype="float16"))
            reshape624 = R.call_tir(cls.reshape4, (lv184,), out_sinfo=R.Tensor((1, seq_len, 96, 80), dtype="float16"))
            reshape625 = R.call_tir(cls.reshape5, (reshape624,), out_sinfo=R.Tensor((seq_len, 96, 80), dtype="float16"))
            lv787 = R.call_dps_packed("vm.builtin.attention_kv_cache_attention_with_fused_qkv", (paged_kv_cache, R.prim_value(28), R.prim_value(T.float32(1.0)), reshape625), out_sinfo=R.Tensor((seq_len, 32, 80), dtype="float16"))
            reshape626 = R.call_tir(cls.reshape6, (lv787,), out_sinfo=R.Tensor((1, seq_len, 32, 80), dtype="float16"))
            reshape627 = R.call_tir(cls.reshape7, (reshape626,), out_sinfo=R.Tensor((1, seq_len, 2560), dtype="float16"))
            lv184_1 = R.call_tir(cls.fused_dequantize2_fused_NT_matmul6_add6_add7, (gpt_neox_layers_28_attention_dense_q_weight5, gpt_neox_layers_28_attention_dense_q_scale5, reshape627, gpt_neox_layers_28_attention_dense_bias5, lv183_1), out_sinfo=R.Tensor((1, seq_len, 2560), dtype="float16"))
            layer_norm317 = R.call_tir(cls.layer_norm1, (lv184_1, gpt_neox_layers_28_post_attention_layernorm_weight5, gpt_neox_layers_28_post_attention_layernorm_bias5), out_sinfo=R.Tensor((1, seq_len, 2560), dtype="float16"))
            lv185 = R.call_tir(cls.fused_dequantize3_fused_NT_matmul7_add8_gelu1_cast3, (gpt_neox_layers_28_mlp_dense_h_to_4h_q_weight5, gpt_neox_layers_28_mlp_dense_h_to_4h_q_scale5, layer_norm317, gpt_neox_layers_28_mlp_dense_h_to_4h_bias5), out_sinfo=R.Tensor((1, seq_len, 10240), dtype="float16"))
            lv185_1 = R.call_tir(cls.fused_dequantize4_fused_NT_matmul8_add9_cast4_add7, (gpt_neox_layers_28_mlp_dense_4h_to_h_q_weight5, gpt_neox_layers_28_mlp_dense_4h_to_h_q_scale5, lv185, gpt_neox_layers_28_mlp_dense_4h_to_h_bias5, lv184_1), out_sinfo=R.Tensor((1, seq_len, 2560), dtype="float16"))
            layer_norm318 = R.call_tir(cls.layer_norm1, (lv185_1, gpt_neox_layers_29_input_layernorm_weight5, gpt_neox_layers_29_input_layernorm_bias5), out_sinfo=R.Tensor((1, seq_len, 2560), dtype="float16"))
            lv186 = R.call_tir(cls.fused_dequantize1_fused_NT_matmul5_add5, (gpt_neox_layers_29_attention_query_key_value_q_weight5, gpt_neox_layers_29_attention_query_key_value_q_scale5, layer_norm318, gpt_neox_layers_29_attention_query_key_value_bias5), out_sinfo=R.Tensor((1, seq_len, 7680), dtype="float16"))
            reshape628 = R.call_tir(cls.reshape4, (lv186,), out_sinfo=R.Tensor((1, seq_len, 96, 80), dtype="float16"))
            reshape629 = R.call_tir(cls.reshape5, (reshape628,), out_sinfo=R.Tensor((seq_len, 96, 80), dtype="float16"))
            lv792 = R.call_dps_packed("vm.builtin.attention_kv_cache_attention_with_fused_qkv", (paged_kv_cache, R.prim_value(29), R.prim_value(T.float32(1.0)), reshape629), out_sinfo=R.Tensor((seq_len, 32, 80), dtype="float16"))
            reshape630 = R.call_tir(cls.reshape6, (lv792,), out_sinfo=R.Tensor((1, seq_len, 32, 80), dtype="float16"))
            reshape631 = R.call_tir(cls.reshape7, (reshape630,), out_sinfo=R.Tensor((1, seq_len, 2560), dtype="float16"))
            lv186_1 = R.call_tir(cls.fused_dequantize2_fused_NT_matmul6_add6_add7, (gpt_neox_layers_29_attention_dense_q_weight5, gpt_neox_layers_29_attention_dense_q_scale5, reshape631, gpt_neox_layers_29_attention_dense_bias5, lv185_1), out_sinfo=R.Tensor((1, seq_len, 2560), dtype="float16"))
            layer_norm319 = R.call_tir(cls.layer_norm1, (lv186_1, gpt_neox_layers_29_post_attention_layernorm_weight5, gpt_neox_layers_29_post_attention_layernorm_bias5), out_sinfo=R.Tensor((1, seq_len, 2560), dtype="float16"))
            lv187 = R.call_tir(cls.fused_dequantize3_fused_NT_matmul7_add8_gelu1_cast3, (gpt_neox_layers_29_mlp_dense_h_to_4h_q_weight5, gpt_neox_layers_29_mlp_dense_h_to_4h_q_scale5, layer_norm319, gpt_neox_layers_29_mlp_dense_h_to_4h_bias5), out_sinfo=R.Tensor((1, seq_len, 10240), dtype="float16"))
            lv187_1 = R.call_tir(cls.fused_dequantize4_fused_NT_matmul8_add9_cast4_add7, (gpt_neox_layers_29_mlp_dense_4h_to_h_q_weight5, gpt_neox_layers_29_mlp_dense_4h_to_h_q_scale5, lv187, gpt_neox_layers_29_mlp_dense_4h_to_h_bias5, lv186_1), out_sinfo=R.Tensor((1, seq_len, 2560), dtype="float16"))
            layer_norm320 = R.call_tir(cls.layer_norm1, (lv187_1, gpt_neox_layers_30_input_layernorm_weight5, gpt_neox_layers_30_input_layernorm_bias5), out_sinfo=R.Tensor((1, seq_len, 2560), dtype="float16"))
            lv188 = R.call_tir(cls.fused_dequantize1_fused_NT_matmul5_add5, (gpt_neox_layers_30_attention_query_key_value_q_weight5, gpt_neox_layers_30_attention_query_key_value_q_scale5, layer_norm320, gpt_neox_layers_30_attention_query_key_value_bias5), out_sinfo=R.Tensor((1, seq_len, 7680), dtype="float16"))
            reshape632 = R.call_tir(cls.reshape4, (lv188,), out_sinfo=R.Tensor((1, seq_len, 96, 80), dtype="float16"))
            reshape633 = R.call_tir(cls.reshape5, (reshape632,), out_sinfo=R.Tensor((seq_len, 96, 80), dtype="float16"))
            lv797 = R.call_dps_packed("vm.builtin.attention_kv_cache_attention_with_fused_qkv", (paged_kv_cache, R.prim_value(30), R.prim_value(T.float32(1.0)), reshape633), out_sinfo=R.Tensor((seq_len, 32, 80), dtype="float16"))
            reshape634 = R.call_tir(cls.reshape6, (lv797,), out_sinfo=R.Tensor((1, seq_len, 32, 80), dtype="float16"))
            reshape635 = R.call_tir(cls.reshape7, (reshape634,), out_sinfo=R.Tensor((1, seq_len, 2560), dtype="float16"))
            lv188_1 = R.call_tir(cls.fused_dequantize2_fused_NT_matmul6_add6_add7, (gpt_neox_layers_30_attention_dense_q_weight5, gpt_neox_layers_30_attention_dense_q_scale5, reshape635, gpt_neox_layers_30_attention_dense_bias5, lv187_1), out_sinfo=R.Tensor((1, seq_len, 2560), dtype="float16"))
            layer_norm321 = R.call_tir(cls.layer_norm1, (lv188_1, gpt_neox_layers_30_post_attention_layernorm_weight5, gpt_neox_layers_30_post_attention_layernorm_bias5), out_sinfo=R.Tensor((1, seq_len, 2560), dtype="float16"))
            lv189 = R.call_tir(cls.fused_dequantize3_fused_NT_matmul7_add8_gelu1_cast3, (gpt_neox_layers_30_mlp_dense_h_to_4h_q_weight5, gpt_neox_layers_30_mlp_dense_h_to_4h_q_scale5, layer_norm321, gpt_neox_layers_30_mlp_dense_h_to_4h_bias5), out_sinfo=R.Tensor((1, seq_len, 10240), dtype="float16"))
            lv189_1 = R.call_tir(cls.fused_dequantize4_fused_NT_matmul8_add9_cast4_add7, (gpt_neox_layers_30_mlp_dense_4h_to_h_q_weight5, gpt_neox_layers_30_mlp_dense_4h_to_h_q_scale5, lv189, gpt_neox_layers_30_mlp_dense_4h_to_h_bias5, lv188_1), out_sinfo=R.Tensor((1, seq_len, 2560), dtype="float16"))
            layer_norm322 = R.call_tir(cls.layer_norm1, (lv189_1, gpt_neox_layers_31_input_layernorm_weight5, gpt_neox_layers_31_input_layernorm_bias5), out_sinfo=R.Tensor((1, seq_len, 2560), dtype="float16"))
            lv190 = R.call_tir(cls.fused_dequantize1_fused_NT_matmul5_add5, (gpt_neox_layers_31_attention_query_key_value_q_weight5, gpt_neox_layers_31_attention_query_key_value_q_scale5, layer_norm322, gpt_neox_layers_31_attention_query_key_value_bias5), out_sinfo=R.Tensor((1, seq_len, 7680), dtype="float16"))
            reshape636 = R.call_tir(cls.reshape4, (lv190,), out_sinfo=R.Tensor((1, seq_len, 96, 80), dtype="float16"))
            reshape637 = R.call_tir(cls.reshape5, (reshape636,), out_sinfo=R.Tensor((seq_len, 96, 80), dtype="float16"))
            lv802 = R.call_dps_packed("vm.builtin.attention_kv_cache_attention_with_fused_qkv", (paged_kv_cache, R.prim_value(31), R.prim_value(T.float32(1.0)), reshape637), out_sinfo=R.Tensor((seq_len, 32, 80), dtype="float16"))
            reshape638 = R.call_tir(cls.reshape6, (lv802,), out_sinfo=R.Tensor((1, seq_len, 32, 80), dtype="float16"))
            reshape639 = R.call_tir(cls.reshape7, (reshape638,), out_sinfo=R.Tensor((1, seq_len, 2560), dtype="float16"))
            lv190_1 = R.call_tir(cls.fused_dequantize2_fused_NT_matmul6_add6_add7, (gpt_neox_layers_31_attention_dense_q_weight5, gpt_neox_layers_31_attention_dense_q_scale5, reshape639, gpt_neox_layers_31_attention_dense_bias5, lv189_1), out_sinfo=R.Tensor((1, seq_len, 2560), dtype="float16"))
            layer_norm323 = R.call_tir(cls.layer_norm1, (lv190_1, gpt_neox_layers_31_post_attention_layernorm_weight5, gpt_neox_layers_31_post_attention_layernorm_bias5), out_sinfo=R.Tensor((1, seq_len, 2560), dtype="float16"))
            lv191 = R.call_tir(cls.fused_dequantize3_fused_NT_matmul7_add8_gelu1_cast3, (gpt_neox_layers_31_mlp_dense_h_to_4h_q_weight5, gpt_neox_layers_31_mlp_dense_h_to_4h_q_scale5, layer_norm323, gpt_neox_layers_31_mlp_dense_h_to_4h_bias5), out_sinfo=R.Tensor((1, seq_len, 10240), dtype="float16"))
            lv191_1 = R.call_tir(cls.fused_dequantize4_fused_NT_matmul8_add9_cast4_add7, (gpt_neox_layers_31_mlp_dense_4h_to_h_q_weight5, gpt_neox_layers_31_mlp_dense_4h_to_h_q_scale5, lv191, gpt_neox_layers_31_mlp_dense_4h_to_h_bias5, lv190_1), out_sinfo=R.Tensor((1, seq_len, 2560), dtype="float16"))
            layer_norm324 = R.call_tir(cls.layer_norm1, (lv191_1, gpt_neox_final_layer_norm_weight5, gpt_neox_final_layer_norm_bias5), out_sinfo=R.Tensor((1, seq_len, 2560), dtype="float16"))
            lv2 = R.call_tir(cls.fused_dequantize_fused_NT_matmul9_cast5, (embed_out_q_weight5, embed_out_q_scale5, layer_norm324), out_sinfo=R.Tensor((1, seq_len, vocab_size), dtype="float32"))
            gv5: R.Tuple(R.Tensor((1, seq_len, vocab_size), dtype="float32"), R.Object) = lv2, paged_kv_cache
            R.output(gv5)
        return gv5

    @R.function
    def create_tir_paged_kv_cache(max_batch_size_: R.Shape(["max_batch_size"]), max_total_seq_len_: R.Shape(["max_total_seq_len"]), prefill_chunk_size_: R.Shape(["prefill_chunk_size"]), page_size_: R.Shape(["page_size"]), support_sliding_window_: R.Shape(["support_sliding_window"])) -> R.Object:
        max_batch_size = T.int64()
        max_total_seq_len = T.int64()
        prefill_chunk_size = T.int64()
        page_size = T.int64()
        support_sliding_window = T.int64()
        R.func_attr({"relax.memory_plan_dynamic_func_output": True, "tir_non_negative_var": ["vocab_size"], "tir_var_upper_bound": {"batch_size": 128, "seq_len": 2048, "total_seq_len": 2048}})
        cls = Module
        paged_kv_cache: R.Object = R.call_pure_packed("vm.builtin.paged_attention_kv_cache_create_reduced", R.shape([max_batch_size, max_total_seq_len, prefill_chunk_size, page_size, support_sliding_window]), R.shape([0, 32]), R.prim_value(32), R.prim_value(32), R.prim_value(80), R.prim_value(1), R.prim_value(1), R.prim_value(10000), R.const(0.0, "float16"), cls.tir_kv_cache_transpose_append, cls.batch_prefill_paged_kv, cls.batch_decode_paged_kv, cls.batch_prefill_paged_kv_sliding_window, cls.batch_decode_paged_kv_sliding_window, cls.batch_prefill_ragged_kv, cls.merge_state_inplace, cls.fused_rope, cls.copy_single_page, cls.tir_kv_cache_debug_get_kv, cls.compact_kv_copy, cls.batch_tree_attn, cls.tree_attn_paged_kv, R.prim_value(0), sinfo_args=(R.Object,))
        return paged_kv_cache

    @R.function
    def decode(input_embed: R.Tensor((1, 1, 2560), dtype="float16"), paged_kv_cache: R.Object, packed_params: R.Tuple(R.Tensor(("vocab_size", 320), dtype="uint32"), R.Tensor(("vocab_size", 80), dtype="float16"), R.Tensor((2560,), dtype="float16"), R.Tensor((2560,), dtype="float16"), R.Tensor((2560,), dtype="float16"), R.Tensor((2560,), dtype="float16"), R.Tensor((7680, 320), dtype="uint32"), R.Tensor((7680, 80), dtype="float16"), R.Tensor((7680,), dtype="float16"), R.Tensor((2560, 320), dtype="uint32"), R.Tensor((2560, 80), dtype="float16"), R.Tensor((2560,), dtype="float16"), R.Tensor((10240, 320), dtype="uint32"), R.Tensor((10240, 80), dtype="float16"), R.Tensor((10240,), dtype="float32"), R.Tensor((2560, 1280), dtype="uint32"), R.Tensor((2560, 320), dtype="float16"), R.Tensor((2560,), dtype="float32"), R.Tensor((2560,), dtype="float16"), R.Tensor((2560,), dtype="float16"), R.Tensor((2560,), dtype="float16"), R.Tensor((2560,), dtype="float16"), R.Tensor((7680, 320), dtype="uint32"), R.Tensor((7680, 80), dtype="float16"), R.Tensor((7680,), dtype="float16"), R.Tensor((2560, 320), dtype="uint32"), R.Tensor((2560, 80), dtype="float16"), R.Tensor((2560,), dtype="float16"), R.Tensor((10240, 320), dtype="uint32"), R.Tensor((10240, 80), dtype="float16"), R.Tensor((10240,), dtype="float32"), R.Tensor((2560, 1280), dtype="uint32"), R.Tensor((2560, 320), dtype="float16"), R.Tensor((2560,), dtype="float32"), R.Tensor((2560,), dtype="float16"), R.Tensor((2560,), dtype="float16"), R.Tensor((2560,), dtype="float16"), R.Tensor((2560,), dtype="float16"), R.Tensor((7680, 320), dtype="uint32"), R.Tensor((7680, 80), dtype="float16"), R.Tensor((7680,), dtype="float16"), R.Tensor((2560, 320), dtype="uint32"), R.Tensor((2560, 80), dtype="float16"), R.Tensor((2560,), dtype="float16"), R.Tensor((10240, 320), dtype="uint32"), R.Tensor((10240, 80), dtype="float16"), R.Tensor((10240,), dtype="float32"), R.Tensor((2560, 1280), dtype="uint32"), R.Tensor((2560, 320), dtype="float16"), R.Tensor((2560,), dtype="float32"), R.Tensor((2560,), dtype="float16"), R.Tensor((2560,), dtype="float16"), R.Tensor((2560,), dtype="float16"), R.Tensor((2560,), dtype="float16"), R.Tensor((7680, 320), dtype="uint32"), R.Tensor((7680, 80), dtype="float16"), R.Tensor((7680,), dtype="float16"), R.Tensor((2560, 320), dtype="uint32"), R.Tensor((2560, 80), dtype="float16"), R.Tensor((2560,), dtype="float16"), R.Tensor((10240, 320), dtype="uint32"), R.Tensor((10240, 80), dtype="float16"), R.Tensor((10240,), dtype="float32"), R.Tensor((2560, 1280), dtype="uint32"), R.Tensor((2560, 320), dtype="float16"), R.Tensor((2560,), dtype="float32"), R.Tensor((2560,), dtype="float16"), R.Tensor((2560,), dtype="float16"), R.Tensor((2560,), dtype="float16"), R.Tensor((2560,), dtype="float16"), R.Tensor((7680, 320), dtype="uint32"), R.Tensor((7680, 80), dtype="float16"), R.Tensor((7680,), dtype="float16"), R.Tensor((2560, 320), dtype="uint32"), R.Tensor((2560, 80), dtype="float16"), R.Tensor((2560,), dtype="float16"), R.Tensor((10240, 320), dtype="uint32"), R.Tensor((10240, 80), dtype="float16"), R.Tensor((10240,), dtype="float32"), R.Tensor((2560, 1280), dtype="uint32"), R.Tensor((2560, 320), dtype="float16"), R.Tensor((2560,), dtype="float32"), R.Tensor((2560,), dtype="float16"), R.Tensor((2560,), dtype="float16"), R.Tensor((2560,), dtype="float16"), R.Tensor((2560,), dtype="float16"), R.Tensor((7680, 320), dtype="uint32"), R.Tensor((7680, 80), dtype="float16"), R.Tensor((7680,), dtype="float16"), R.Tensor((2560, 320), dtype="uint32"), R.Tensor((2560, 80), dtype="float16"), R.Tensor((2560,), dtype="float16"), R.Tensor((10240, 320), dtype="uint32"), R.Tensor((10240, 80), dtype="float16"), R.Tensor((10240,), dtype="float32"), R.Tensor((2560, 1280), dtype="uint32"), R.Tensor((2560, 320), dtype="float16"), R.Tensor((2560,), dtype="float32"), R.Tensor((2560,), dtype="float16"), R.Tensor((2560,), dtype="float16"), R.Tensor((2560,), dtype="float16"), R.Tensor((2560,), dtype="float16"), R.Tensor((7680, 320), dtype="uint32"), R.Tensor((7680, 80), dtype="float16"), R.Tensor((7680,), dtype="float16"), R.Tensor((2560, 320), dtype="uint32"), R.Tensor((2560, 80), dtype="float16"), R.Tensor((2560,), dtype="float16"), R.Tensor((10240, 320), dtype="uint32"), R.Tensor((10240, 80), dtype="float16"), R.Tensor((10240,), dtype="float32"), R.Tensor((2560, 1280), dtype="uint32"), R.Tensor((2560, 320), dtype="float16"), R.Tensor((2560,), dtype="float32"), R.Tensor((2560,), dtype="float16"), R.Tensor((2560,), dtype="float16"), R.Tensor((2560,), dtype="float16"), R.Tensor((2560,), dtype="float16"), R.Tensor((7680, 320), dtype="uint32"), R.Tensor((7680, 80), dtype="float16"), R.Tensor((7680,), dtype="float16"), R.Tensor((2560, 320), dtype="uint32"), R.Tensor((2560, 80), dtype="float16"), R.Tensor((2560,), dtype="float16"), R.Tensor((10240, 320), dtype="uint32"), R.Tensor((10240, 80), dtype="float16"), R.Tensor((10240,), dtype="float32"), R.Tensor((2560, 1280), dtype="uint32"), R.Tensor((2560, 320), dtype="float16"), R.Tensor((2560,), dtype="float32"), R.Tensor((2560,), dtype="float16"), R.Tensor((2560,), dtype="float16"), R.Tensor((2560,), dtype="float16"), R.Tensor((2560,), dtype="float16"), R.Tensor((7680, 320), dtype="uint32"), R.Tensor((7680, 80), dtype="float16"), R.Tensor((7680,), dtype="float16"), R.Tensor((2560, 320), dtype="uint32"), R.Tensor((2560, 80), dtype="float16"), R.Tensor((2560,), dtype="float16"), R.Tensor((10240, 320), dtype="uint32"), R.Tensor((10240, 80), dtype="float16"), R.Tensor((10240,), dtype="float32"), R.Tensor((2560, 1280), dtype="uint32"), R.Tensor((2560, 320), dtype="float16"), R.Tensor((2560,), dtype="float32"), R.Tensor((2560,), dtype="float16"), R.Tensor((2560,), dtype="float16"), R.Tensor((2560,), dtype="float16"), R.Tensor((2560,), dtype="float16"), R.Tensor((7680, 320), dtype="uint32"), R.Tensor((7680, 80), dtype="float16"), R.Tensor((7680,), dtype="float16"), R.Tensor((2560, 320), dtype="uint32"), R.Tensor((2560, 80), dtype="float16"), R.Tensor((2560,), dtype="float16"), R.Tensor((10240, 320), dtype="uint32"), R.Tensor((10240, 80), dtype="float16"), R.Tensor((10240,), dtype="float32"), R.Tensor((2560, 1280), dtype="uint32"), R.Tensor((2560, 320), dtype="float16"), R.Tensor((2560,), dtype="float32"), R.Tensor((2560,), dtype="float16"), R.Tensor((2560,), dtype="float16"), R.Tensor((2560,), dtype="float16"), R.Tensor((2560,), dtype="float16"), R.Tensor((7680, 320), dtype="uint32"), R.Tensor((7680, 80), dtype="float16"), R.Tensor((7680,), dtype="float16"), R.Tensor((2560, 320), dtype="uint32"), R.Tensor((2560, 80), dtype="float16"), R.Tensor((2560,), dtype="float16"), R.Tensor((10240, 320), dtype="uint32"), R.Tensor((10240, 80), dtype="float16"), R.Tensor((10240,), dtype="float32"), R.Tensor((2560, 1280), dtype="uint32"), R.Tensor((2560, 320), dtype="float16"), R.Tensor((2560,), dtype="float32"), R.Tensor((2560,), dtype="float16"), R.Tensor((2560,), dtype="float16"), R.Tensor((2560,), dtype="float16"), R.Tensor((2560,), dtype="float16"), R.Tensor((7680, 320), dtype="uint32"), R.Tensor((7680, 80), dtype="float16"), R.Tensor((7680,), dtype="float16"), R.Tensor((2560, 320), dtype="uint32"), R.Tensor((2560, 80), dtype="float16"), R.Tensor((2560,), dtype="float16"), R.Tensor((10240, 320), dtype="uint32"), R.Tensor((10240, 80), dtype="float16"), R.Tensor((10240,), dtype="float32"), R.Tensor((2560, 1280), dtype="uint32"), R.Tensor((2560, 320), dtype="float16"), R.Tensor((2560,), dtype="float32"), R.Tensor((2560,), dtype="float16"), R.Tensor((2560,), dtype="float16"), R.Tensor((2560,), dtype="float16"), R.Tensor((2560,), dtype="float16"), R.Tensor((7680, 320), dtype="uint32"), R.Tensor((7680, 80), dtype="float16"), R.Tensor((7680,), dtype="float16"), R.Tensor((2560, 320), dtype="uint32"), R.Tensor((2560, 80), dtype="float16"), R.Tensor((2560,), dtype="float16"), R.Tensor((10240, 320), dtype="uint32"), R.Tensor((10240, 80), dtype="float16"), R.Tensor((10240,), dtype="float32"), R.Tensor((2560, 1280), dtype="uint32"), R.Tensor((2560, 320), dtype="float16"), R.Tensor((2560,), dtype="float32"), R.Tensor((2560,), dtype="float16"), R.Tensor((2560,), dtype="float16"), R.Tensor((2560,), dtype="float16"), R.Tensor((2560,), dtype="float16"), R.Tensor((7680, 320), dtype="uint32"), R.Tensor((7680, 80), dtype="float16"), R.Tensor((7680,), dtype="float16"), R.Tensor((2560, 320), dtype="uint32"), R.Tensor((2560, 80), dtype="float16"), R.Tensor((2560,), dtype="float16"), R.Tensor((10240, 320), dtype="uint32"), R.Tensor((10240, 80), dtype="float16"), R.Tensor((10240,), dtype="float32"), R.Tensor((2560, 1280), dtype="uint32"), R.Tensor((2560, 320), dtype="float16"), R.Tensor((2560,), dtype="float32"), R.Tensor((2560,), dtype="float16"), R.Tensor((2560,), dtype="float16"), R.Tensor((2560,), dtype="float16"), R.Tensor((2560,), dtype="float16"), R.Tensor((7680, 320), dtype="uint32"), R.Tensor((7680, 80), dtype="float16"), R.Tensor((7680,), dtype="float16"), R.Tensor((2560, 320), dtype="uint32"), R.Tensor((2560, 80), dtype="float16"), R.Tensor((2560,), dtype="float16"), R.Tensor((10240, 320), dtype="uint32"), R.Tensor((10240, 80), dtype="float16"), R.Tensor((10240,), dtype="float32"), R.Tensor((2560, 1280), dtype="uint32"), R.Tensor((2560, 320), dtype="float16"), R.Tensor((2560,), dtype="float32"), R.Tensor((2560,), dtype="float16"), R.Tensor((2560,), dtype="float16"), R.Tensor((2560,), dtype="float16"), R.Tensor((2560,), dtype="float16"), R.Tensor((7680, 320), dtype="uint32"), R.Tensor((7680, 80), dtype="float16"), R.Tensor((7680,), dtype="float16"), R.Tensor((2560, 320), dtype="uint32"), R.Tensor((2560, 80), dtype="float16"), R.Tensor((2560,), dtype="float16"), R.Tensor((10240, 320), dtype="uint32"), R.Tensor((10240, 80), dtype="float16"), R.Tensor((10240,), dtype="float32"), R.Tensor((2560, 1280), dtype="uint32"), R.Tensor((2560, 320), dtype="float16"), R.Tensor((2560,), dtype="float32"), R.Tensor((2560,), dtype="float16"), R.Tensor((2560,), dtype="float16"), R.Tensor((2560,), dtype="float16"), R.Tensor((2560,), dtype="float16"), R.Tensor((7680, 320), dtype="uint32"), R.Tensor((7680, 80), dtype="float16"), R.Tensor((7680,), dtype="float16"), R.Tensor((2560, 320), dtype="uint32"), R.Tensor((2560, 80), dtype="float16"), R.Tensor((2560,), dtype="float16"), R.Tensor((10240, 320), dtype="uint32"), R.Tensor((10240, 80), dtype="float16"), R.Tensor((10240,), dtype="float32"), R.Tensor((2560, 1280), dtype="uint32"), R.Tensor((2560, 320), dtype="float16"), R.Tensor((2560,), dtype="float32"), R.Tensor((2560,), dtype="float16"), R.Tensor((2560,), dtype="float16"), R.Tensor((2560,), dtype="float16"), R.Tensor((2560,), dtype="float16"), R.Tensor((7680, 320), dtype="uint32"), R.Tensor((7680, 80), dtype="float16"), R.Tensor((7680,), dtype="float16"), R.Tensor((2560, 320), dtype="uint32"), R.Tensor((2560, 80), dtype="float16"), R.Tensor((2560,), dtype="float16"), R.Tensor((10240, 320), dtype="uint32"), R.Tensor((10240, 80), dtype="float16"), R.Tensor((10240,), dtype="float32"), R.Tensor((2560, 1280), dtype="uint32"), R.Tensor((2560, 320), dtype="float16"), R.Tensor((2560,), dtype="float32"), R.Tensor((2560,), dtype="float16"), R.Tensor((2560,), dtype="float16"), R.Tensor((2560,), dtype="float16"), R.Tensor((2560,), dtype="float16"), R.Tensor((7680, 320), dtype="uint32"), R.Tensor((7680, 80), dtype="float16"), R.Tensor((7680,), dtype="float16"), R.Tensor((2560, 320), dtype="uint32"), R.Tensor((2560, 80), dtype="float16"), R.Tensor((2560,), dtype="float16"), R.Tensor((10240, 320), dtype="uint32"), R.Tensor((10240, 80), dtype="float16"), R.Tensor((10240,), dtype="float32"), R.Tensor((2560, 1280), dtype="uint32"), R.Tensor((2560, 320), dtype="float16"), R.Tensor((2560,), dtype="float32"), R.Tensor((2560,), dtype="float16"), R.Tensor((2560,), dtype="float16"), R.Tensor((2560,), dtype="float16"), R.Tensor((2560,), dtype="float16"), R.Tensor((7680, 320), dtype="uint32"), R.Tensor((7680, 80), dtype="float16"), R.Tensor((7680,), dtype="float16"), R.Tensor((2560, 320), dtype="uint32"), R.Tensor((2560, 80), dtype="float16"), R.Tensor((2560,), dtype="float16"), R.Tensor((10240, 320), dtype="uint32"), R.Tensor((10240, 80), dtype="float16"), R.Tensor((10240,), dtype="float32"), R.Tensor((2560, 1280), dtype="uint32"), R.Tensor((2560, 320), dtype="float16"), R.Tensor((2560,), dtype="float32"), R.Tensor((2560,), dtype="float16"), R.Tensor((2560,), dtype="float16"), R.Tensor((2560,), dtype="float16"), R.Tensor((2560,), dtype="float16"), R.Tensor((7680, 320), dtype="uint32"), R.Tensor((7680, 80), dtype="float16"), R.Tensor((7680,), dtype="float16"), R.Tensor((2560, 320), dtype="uint32"), R.Tensor((2560, 80), dtype="float16"), R.Tensor((2560,), dtype="float16"), R.Tensor((10240, 320), dtype="uint32"), R.Tensor((10240, 80), dtype="float16"), R.Tensor((10240,), dtype="float32"), R.Tensor((2560, 1280), dtype="uint32"), R.Tensor((2560, 320), dtype="float16"), R.Tensor((2560,), dtype="float32"), R.Tensor((2560,), dtype="float16"), R.Tensor((2560,), dtype="float16"), R.Tensor((2560,), dtype="float16"), R.Tensor((2560,), dtype="float16"), R.Tensor((7680, 320), dtype="uint32"), R.Tensor((7680, 80), dtype="float16"), R.Tensor((7680,), dtype="float16"), R.Tensor((2560, 320), dtype="uint32"), R.Tensor((2560, 80), dtype="float16"), R.Tensor((2560,), dtype="float16"), R.Tensor((10240, 320), dtype="uint32"), R.Tensor((10240, 80), dtype="float16"), R.Tensor((10240,), dtype="float32"), R.Tensor((2560, 1280), dtype="uint32"), R.Tensor((2560, 320), dtype="float16"), R.Tensor((2560,), dtype="float32"), R.Tensor((2560,), dtype="float16"), R.Tensor((2560,), dtype="float16"), R.Tensor((2560,), dtype="float16"), R.Tensor((2560,), dtype="float16"), R.Tensor((7680, 320), dtype="uint32"), R.Tensor((7680, 80), dtype="float16"), R.Tensor((7680,), dtype="float16"), R.Tensor((2560, 320), dtype="uint32"), R.Tensor((2560, 80), dtype="float16"), R.Tensor((2560,), dtype="float16"), R.Tensor((10240, 320), dtype="uint32"), R.Tensor((10240, 80), dtype="float16"), R.Tensor((10240,), dtype="float32"), R.Tensor((2560, 1280), dtype="uint32"), R.Tensor((2560, 320), dtype="float16"), R.Tensor((2560,), dtype="float32"), R.Tensor((2560,), dtype="float16"), R.Tensor((2560,), dtype="float16"), R.Tensor((2560,), dtype="float16"), R.Tensor((2560,), dtype="float16"), R.Tensor((7680, 320), dtype="uint32"), R.Tensor((7680, 80), dtype="float16"), R.Tensor((7680,), dtype="float16"), R.Tensor((2560, 320), dtype="uint32"), R.Tensor((2560, 80), dtype="float16"), R.Tensor((2560,), dtype="float16"), R.Tensor((10240, 320), dtype="uint32"), R.Tensor((10240, 80), dtype="float16"), R.Tensor((10240,), dtype="float32"), R.Tensor((2560, 1280), dtype="uint32"), R.Tensor((2560, 320), dtype="float16"), R.Tensor((2560,), dtype="float32"), R.Tensor((2560,), dtype="float16"), R.Tensor((2560,), dtype="float16"), R.Tensor((2560,), dtype="float16"), R.Tensor((2560,), dtype="float16"), R.Tensor((7680, 320), dtype="uint32"), R.Tensor((7680, 80), dtype="float16"), R.Tensor((7680,), dtype="float16"), R.Tensor((2560, 320), dtype="uint32"), R.Tensor((2560, 80), dtype="float16"), R.Tensor((2560,), dtype="float16"), R.Tensor((10240, 320), dtype="uint32"), R.Tensor((10240, 80), dtype="float16"), R.Tensor((10240,), dtype="float32"), R.Tensor((2560, 1280), dtype="uint32"), R.Tensor((2560, 320), dtype="float16"), R.Tensor((2560,), dtype="float32"), R.Tensor((2560,), dtype="float16"), R.Tensor((2560,), dtype="float16"), R.Tensor((2560,), dtype="float16"), R.Tensor((2560,), dtype="float16"), R.Tensor((7680, 320), dtype="uint32"), R.Tensor((7680, 80), dtype="float16"), R.Tensor((7680,), dtype="float16"), R.Tensor((2560, 320), dtype="uint32"), R.Tensor((2560, 80), dtype="float16"), R.Tensor((2560,), dtype="float16"), R.Tensor((10240, 320), dtype="uint32"), R.Tensor((10240, 80), dtype="float16"), R.Tensor((10240,), dtype="float32"), R.Tensor((2560, 1280), dtype="uint32"), R.Tensor((2560, 320), dtype="float16"), R.Tensor((2560,), dtype="float32"), R.Tensor((2560,), dtype="float16"), R.Tensor((2560,), dtype="float16"), R.Tensor((2560,), dtype="float16"), R.Tensor((2560,), dtype="float16"), R.Tensor((7680, 320), dtype="uint32"), R.Tensor((7680, 80), dtype="float16"), R.Tensor((7680,), dtype="float16"), R.Tensor((2560, 320), dtype="uint32"), R.Tensor((2560, 80), dtype="float16"), R.Tensor((2560,), dtype="float16"), R.Tensor((10240, 320), dtype="uint32"), R.Tensor((10240, 80), dtype="float16"), R.Tensor((10240,), dtype="float32"), R.Tensor((2560, 1280), dtype="uint32"), R.Tensor((2560, 320), dtype="float16"), R.Tensor((2560,), dtype="float32"), R.Tensor((2560,), dtype="float16"), R.Tensor((2560,), dtype="float16"), R.Tensor((2560,), dtype="float16"), R.Tensor((2560,), dtype="float16"), R.Tensor((7680, 320), dtype="uint32"), R.Tensor((7680, 80), dtype="float16"), R.Tensor((7680,), dtype="float16"), R.Tensor((2560, 320), dtype="uint32"), R.Tensor((2560, 80), dtype="float16"), R.Tensor((2560,), dtype="float16"), R.Tensor((10240, 320), dtype="uint32"), R.Tensor((10240, 80), dtype="float16"), R.Tensor((10240,), dtype="float32"), R.Tensor((2560, 1280), dtype="uint32"), R.Tensor((2560, 320), dtype="float16"), R.Tensor((2560,), dtype="float32"), R.Tensor((2560,), dtype="float16"), R.Tensor((2560,), dtype="float16"), R.Tensor((2560,), dtype="float16"), R.Tensor((2560,), dtype="float16"), R.Tensor((7680, 320), dtype="uint32"), R.Tensor((7680, 80), dtype="float16"), R.Tensor((7680,), dtype="float16"), R.Tensor((2560, 320), dtype="uint32"), R.Tensor((2560, 80), dtype="float16"), R.Tensor((2560,), dtype="float16"), R.Tensor((10240, 320), dtype="uint32"), R.Tensor((10240, 80), dtype="float16"), R.Tensor((10240,), dtype="float32"), R.Tensor((2560, 1280), dtype="uint32"), R.Tensor((2560, 320), dtype="float16"), R.Tensor((2560,), dtype="float32"), R.Tensor((2560,), dtype="float16"), R.Tensor((2560,), dtype="float16"), R.Tensor((2560,), dtype="float16"), R.Tensor((2560,), dtype="float16"), R.Tensor((7680, 320), dtype="uint32"), R.Tensor((7680, 80), dtype="float16"), R.Tensor((7680,), dtype="float16"), R.Tensor((2560, 320), dtype="uint32"), R.Tensor((2560, 80), dtype="float16"), R.Tensor((2560,), dtype="float16"), R.Tensor((10240, 320), dtype="uint32"), R.Tensor((10240, 80), dtype="float16"), R.Tensor((10240,), dtype="float32"), R.Tensor((2560, 1280), dtype="uint32"), R.Tensor((2560, 320), dtype="float16"), R.Tensor((2560,), dtype="float32"), R.Tensor((2560,), dtype="float16"), R.Tensor((2560,), dtype="float16"), R.Tensor((2560,), dtype="float16"), R.Tensor((2560,), dtype="float16"), R.Tensor((7680, 320), dtype="uint32"), R.Tensor((7680, 80), dtype="float16"), R.Tensor((7680,), dtype="float16"), R.Tensor((2560, 320), dtype="uint32"), R.Tensor((2560, 80), dtype="float16"), R.Tensor((2560,), dtype="float16"), R.Tensor((10240, 320), dtype="uint32"), R.Tensor((10240, 80), dtype="float16"), R.Tensor((10240,), dtype="float32"), R.Tensor((2560, 1280), dtype="uint32"), R.Tensor((2560, 320), dtype="float16"), R.Tensor((2560,), dtype="float32"), R.Tensor((2560,), dtype="float16"), R.Tensor((2560,), dtype="float16"), R.Tensor((2560,), dtype="float16"), R.Tensor((2560,), dtype="float16"), R.Tensor((7680, 320), dtype="uint32"), R.Tensor((7680, 80), dtype="float16"), R.Tensor((7680,), dtype="float16"), R.Tensor((2560, 320), dtype="uint32"), R.Tensor((2560, 80), dtype="float16"), R.Tensor((2560,), dtype="float16"), R.Tensor((10240, 320), dtype="uint32"), R.Tensor((10240, 80), dtype="float16"), R.Tensor((10240,), dtype="float32"), R.Tensor((2560, 1280), dtype="uint32"), R.Tensor((2560, 320), dtype="float16"), R.Tensor((2560,), dtype="float32"), R.Tensor((2560,), dtype="float16"), R.Tensor((2560,), dtype="float16"), R.Tensor(("vocab_size", 320), dtype="uint32"), R.Tensor(("vocab_size", 80), dtype="float16"))) -> R.Tuple(R.Tensor((1, 1, "vocab_size"), dtype="float32"), R.Object):
        vocab_size = T.int64()
        R.func_attr({"num_input": 2, "pipeline_parallel_stages": 1, "relax.memory_plan_dynamic_func_output": True, "tir_non_negative_var": ["vocab_size"], "tir_var_upper_bound": {"batch_size": 128, "seq_len": 2048, "total_seq_len": 2048}})
        cls = Module
        with R.dataflow():
            gpt_neox_layers_0_input_layernorm_weight2: R.Tensor((2560,), dtype="float16") = packed_params[2]
            gpt_neox_layers_0_input_layernorm_bias2: R.Tensor((2560,), dtype="float16") = packed_params[3]
            gpt_neox_layers_0_post_attention_layernorm_weight2: R.Tensor((2560,), dtype="float16") = packed_params[4]
            gpt_neox_layers_0_post_attention_layernorm_bias2: R.Tensor((2560,), dtype="float16") = packed_params[5]
            gpt_neox_layers_0_attention_query_key_value_q_weight2: R.Tensor((7680, 320), dtype="uint32") = packed_params[6]
            gpt_neox_layers_0_attention_query_key_value_q_scale2: R.Tensor((7680, 80), dtype="float16") = packed_params[7]
            gpt_neox_layers_0_attention_query_key_value_bias2: R.Tensor((7680,), dtype="float16") = packed_params[8]
            gpt_neox_layers_0_attention_dense_q_weight2: R.Tensor((2560, 320), dtype="uint32") = packed_params[9]
            gpt_neox_layers_0_attention_dense_q_scale2: R.Tensor((2560, 80), dtype="float16") = packed_params[10]
            gpt_neox_layers_0_attention_dense_bias2: R.Tensor((2560,), dtype="float16") = packed_params[11]
            gpt_neox_layers_0_mlp_dense_h_to_4h_q_weight2: R.Tensor((10240, 320), dtype="uint32") = packed_params[12]
            gpt_neox_layers_0_mlp_dense_h_to_4h_q_scale2: R.Tensor((10240, 80), dtype="float16") = packed_params[13]
            gpt_neox_layers_0_mlp_dense_h_to_4h_bias2: R.Tensor((10240,), dtype="float32") = packed_params[14]
            gpt_neox_layers_0_mlp_dense_4h_to_h_q_weight2: R.Tensor((2560, 1280), dtype="uint32") = packed_params[15]
            gpt_neox_layers_0_mlp_dense_4h_to_h_q_scale2: R.Tensor((2560, 320), dtype="float16") = packed_params[16]
            gpt_neox_layers_0_mlp_dense_4h_to_h_bias2: R.Tensor((2560,), dtype="float32") = packed_params[17]
            gpt_neox_layers_1_input_layernorm_weight2: R.Tensor((2560,), dtype="float16") = packed_params[18]
            gpt_neox_layers_1_input_layernorm_bias2: R.Tensor((2560,), dtype="float16") = packed_params[19]
            gpt_neox_layers_1_post_attention_layernorm_weight2: R.Tensor((2560,), dtype="float16") = packed_params[20]
            gpt_neox_layers_1_post_attention_layernorm_bias2: R.Tensor((2560,), dtype="float16") = packed_params[21]
            gpt_neox_layers_1_attention_query_key_value_q_weight2: R.Tensor((7680, 320), dtype="uint32") = packed_params[22]
            gpt_neox_layers_1_attention_query_key_value_q_scale2: R.Tensor((7680, 80), dtype="float16") = packed_params[23]
            gpt_neox_layers_1_attention_query_key_value_bias2: R.Tensor((7680,), dtype="float16") = packed_params[24]
            gpt_neox_layers_1_attention_dense_q_weight2: R.Tensor((2560, 320), dtype="uint32") = packed_params[25]
            gpt_neox_layers_1_attention_dense_q_scale2: R.Tensor((2560, 80), dtype="float16") = packed_params[26]
            gpt_neox_layers_1_attention_dense_bias2: R.Tensor((2560,), dtype="float16") = packed_params[27]
            gpt_neox_layers_1_mlp_dense_h_to_4h_q_weight2: R.Tensor((10240, 320), dtype="uint32") = packed_params[28]
            gpt_neox_layers_1_mlp_dense_h_to_4h_q_scale2: R.Tensor((10240, 80), dtype="float16") = packed_params[29]
            gpt_neox_layers_1_mlp_dense_h_to_4h_bias2: R.Tensor((10240,), dtype="float32") = packed_params[30]
            gpt_neox_layers_1_mlp_dense_4h_to_h_q_weight2: R.Tensor((2560, 1280), dtype="uint32") = packed_params[31]
            gpt_neox_layers_1_mlp_dense_4h_to_h_q_scale2: R.Tensor((2560, 320), dtype="float16") = packed_params[32]
            gpt_neox_layers_1_mlp_dense_4h_to_h_bias2: R.Tensor((2560,), dtype="float32") = packed_params[33]
            gpt_neox_layers_2_input_layernorm_weight2: R.Tensor((2560,), dtype="float16") = packed_params[34]
            gpt_neox_layers_2_input_layernorm_bias2: R.Tensor((2560,), dtype="float16") = packed_params[35]
            gpt_neox_layers_2_post_attention_layernorm_weight2: R.Tensor((2560,), dtype="float16") = packed_params[36]
            gpt_neox_layers_2_post_attention_layernorm_bias2: R.Tensor((2560,), dtype="float16") = packed_params[37]
            gpt_neox_layers_2_attention_query_key_value_q_weight2: R.Tensor((7680, 320), dtype="uint32") = packed_params[38]
            gpt_neox_layers_2_attention_query_key_value_q_scale2: R.Tensor((7680, 80), dtype="float16") = packed_params[39]
            gpt_neox_layers_2_attention_query_key_value_bias2: R.Tensor((7680,), dtype="float16") = packed_params[40]
            gpt_neox_layers_2_attention_dense_q_weight2: R.Tensor((2560, 320), dtype="uint32") = packed_params[41]
            gpt_neox_layers_2_attention_dense_q_scale2: R.Tensor((2560, 80), dtype="float16") = packed_params[42]
            gpt_neox_layers_2_attention_dense_bias2: R.Tensor((2560,), dtype="float16") = packed_params[43]
            gpt_neox_layers_2_mlp_dense_h_to_4h_q_weight2: R.Tensor((10240, 320), dtype="uint32") = packed_params[44]
            gpt_neox_layers_2_mlp_dense_h_to_4h_q_scale2: R.Tensor((10240, 80), dtype="float16") = packed_params[45]
            gpt_neox_layers_2_mlp_dense_h_to_4h_bias2: R.Tensor((10240,), dtype="float32") = packed_params[46]
            gpt_neox_layers_2_mlp_dense_4h_to_h_q_weight2: R.Tensor((2560, 1280), dtype="uint32") = packed_params[47]
            gpt_neox_layers_2_mlp_dense_4h_to_h_q_scale2: R.Tensor((2560, 320), dtype="float16") = packed_params[48]
            gpt_neox_layers_2_mlp_dense_4h_to_h_bias2: R.Tensor((2560,), dtype="float32") = packed_params[49]
            gpt_neox_layers_3_input_layernorm_weight2: R.Tensor((2560,), dtype="float16") = packed_params[50]
            gpt_neox_layers_3_input_layernorm_bias2: R.Tensor((2560,), dtype="float16") = packed_params[51]
            gpt_neox_layers_3_post_attention_layernorm_weight2: R.Tensor((2560,), dtype="float16") = packed_params[52]
            gpt_neox_layers_3_post_attention_layernorm_bias2: R.Tensor((2560,), dtype="float16") = packed_params[53]
            gpt_neox_layers_3_attention_query_key_value_q_weight2: R.Tensor((7680, 320), dtype="uint32") = packed_params[54]
            gpt_neox_layers_3_attention_query_key_value_q_scale2: R.Tensor((7680, 80), dtype="float16") = packed_params[55]
            gpt_neox_layers_3_attention_query_key_value_bias2: R.Tensor((7680,), dtype="float16") = packed_params[56]
            gpt_neox_layers_3_attention_dense_q_weight2: R.Tensor((2560, 320), dtype="uint32") = packed_params[57]
            gpt_neox_layers_3_attention_dense_q_scale2: R.Tensor((2560, 80), dtype="float16") = packed_params[58]
            gpt_neox_layers_3_attention_dense_bias2: R.Tensor((2560,), dtype="float16") = packed_params[59]
            gpt_neox_layers_3_mlp_dense_h_to_4h_q_weight2: R.Tensor((10240, 320), dtype="uint32") = packed_params[60]
            gpt_neox_layers_3_mlp_dense_h_to_4h_q_scale2: R.Tensor((10240, 80), dtype="float16") = packed_params[61]
            gpt_neox_layers_3_mlp_dense_h_to_4h_bias2: R.Tensor((10240,), dtype="float32") = packed_params[62]
            gpt_neox_layers_3_mlp_dense_4h_to_h_q_weight2: R.Tensor((2560, 1280), dtype="uint32") = packed_params[63]
            gpt_neox_layers_3_mlp_dense_4h_to_h_q_scale2: R.Tensor((2560, 320), dtype="float16") = packed_params[64]
            gpt_neox_layers_3_mlp_dense_4h_to_h_bias2: R.Tensor((2560,), dtype="float32") = packed_params[65]
            gpt_neox_layers_4_input_layernorm_weight2: R.Tensor((2560,), dtype="float16") = packed_params[66]
            gpt_neox_layers_4_input_layernorm_bias2: R.Tensor((2560,), dtype="float16") = packed_params[67]
            gpt_neox_layers_4_post_attention_layernorm_weight2: R.Tensor((2560,), dtype="float16") = packed_params[68]
            gpt_neox_layers_4_post_attention_layernorm_bias2: R.Tensor((2560,), dtype="float16") = packed_params[69]
            gpt_neox_layers_4_attention_query_key_value_q_weight2: R.Tensor((7680, 320), dtype="uint32") = packed_params[70]
            gpt_neox_layers_4_attention_query_key_value_q_scale2: R.Tensor((7680, 80), dtype="float16") = packed_params[71]
            gpt_neox_layers_4_attention_query_key_value_bias2: R.Tensor((7680,), dtype="float16") = packed_params[72]
            gpt_neox_layers_4_attention_dense_q_weight2: R.Tensor((2560, 320), dtype="uint32") = packed_params[73]
            gpt_neox_layers_4_attention_dense_q_scale2: R.Tensor((2560, 80), dtype="float16") = packed_params[74]
            gpt_neox_layers_4_attention_dense_bias2: R.Tensor((2560,), dtype="float16") = packed_params[75]
            gpt_neox_layers_4_mlp_dense_h_to_4h_q_weight2: R.Tensor((10240, 320), dtype="uint32") = packed_params[76]
            gpt_neox_layers_4_mlp_dense_h_to_4h_q_scale2: R.Tensor((10240, 80), dtype="float16") = packed_params[77]
            gpt_neox_layers_4_mlp_dense_h_to_4h_bias2: R.Tensor((10240,), dtype="float32") = packed_params[78]
            gpt_neox_layers_4_mlp_dense_4h_to_h_q_weight2: R.Tensor((2560, 1280), dtype="uint32") = packed_params[79]
            gpt_neox_layers_4_mlp_dense_4h_to_h_q_scale2: R.Tensor((2560, 320), dtype="float16") = packed_params[80]
            gpt_neox_layers_4_mlp_dense_4h_to_h_bias2: R.Tensor((2560,), dtype="float32") = packed_params[81]
            gpt_neox_layers_5_input_layernorm_weight2: R.Tensor((2560,), dtype="float16") = packed_params[82]
            gpt_neox_layers_5_input_layernorm_bias2: R.Tensor((2560,), dtype="float16") = packed_params[83]
            gpt_neox_layers_5_post_attention_layernorm_weight2: R.Tensor((2560,), dtype="float16") = packed_params[84]
            gpt_neox_layers_5_post_attention_layernorm_bias2: R.Tensor((2560,), dtype="float16") = packed_params[85]
            gpt_neox_layers_5_attention_query_key_value_q_weight2: R.Tensor((7680, 320), dtype="uint32") = packed_params[86]
            gpt_neox_layers_5_attention_query_key_value_q_scale2: R.Tensor((7680, 80), dtype="float16") = packed_params[87]
            gpt_neox_layers_5_attention_query_key_value_bias2: R.Tensor((7680,), dtype="float16") = packed_params[88]
            gpt_neox_layers_5_attention_dense_q_weight2: R.Tensor((2560, 320), dtype="uint32") = packed_params[89]
            gpt_neox_layers_5_attention_dense_q_scale2: R.Tensor((2560, 80), dtype="float16") = packed_params[90]
            gpt_neox_layers_5_attention_dense_bias2: R.Tensor((2560,), dtype="float16") = packed_params[91]
            gpt_neox_layers_5_mlp_dense_h_to_4h_q_weight2: R.Tensor((10240, 320), dtype="uint32") = packed_params[92]
            gpt_neox_layers_5_mlp_dense_h_to_4h_q_scale2: R.Tensor((10240, 80), dtype="float16") = packed_params[93]
            gpt_neox_layers_5_mlp_dense_h_to_4h_bias2: R.Tensor((10240,), dtype="float32") = packed_params[94]
            gpt_neox_layers_5_mlp_dense_4h_to_h_q_weight2: R.Tensor((2560, 1280), dtype="uint32") = packed_params[95]
            gpt_neox_layers_5_mlp_dense_4h_to_h_q_scale2: R.Tensor((2560, 320), dtype="float16") = packed_params[96]
            gpt_neox_layers_5_mlp_dense_4h_to_h_bias2: R.Tensor((2560,), dtype="float32") = packed_params[97]
            gpt_neox_layers_6_input_layernorm_weight2: R.Tensor((2560,), dtype="float16") = packed_params[98]
            gpt_neox_layers_6_input_layernorm_bias2: R.Tensor((2560,), dtype="float16") = packed_params[99]
            gpt_neox_layers_6_post_attention_layernorm_weight2: R.Tensor((2560,), dtype="float16") = packed_params[100]
            gpt_neox_layers_6_post_attention_layernorm_bias2: R.Tensor((2560,), dtype="float16") = packed_params[101]
            gpt_neox_layers_6_attention_query_key_value_q_weight2: R.Tensor((7680, 320), dtype="uint32") = packed_params[102]
            gpt_neox_layers_6_attention_query_key_value_q_scale2: R.Tensor((7680, 80), dtype="float16") = packed_params[103]
            gpt_neox_layers_6_attention_query_key_value_bias2: R.Tensor((7680,), dtype="float16") = packed_params[104]
            gpt_neox_layers_6_attention_dense_q_weight2: R.Tensor((2560, 320), dtype="uint32") = packed_params[105]
            gpt_neox_layers_6_attention_dense_q_scale2: R.Tensor((2560, 80), dtype="float16") = packed_params[106]
            gpt_neox_layers_6_attention_dense_bias2: R.Tensor((2560,), dtype="float16") = packed_params[107]
            gpt_neox_layers_6_mlp_dense_h_to_4h_q_weight2: R.Tensor((10240, 320), dtype="uint32") = packed_params[108]
            gpt_neox_layers_6_mlp_dense_h_to_4h_q_scale2: R.Tensor((10240, 80), dtype="float16") = packed_params[109]
            gpt_neox_layers_6_mlp_dense_h_to_4h_bias2: R.Tensor((10240,), dtype="float32") = packed_params[110]
            gpt_neox_layers_6_mlp_dense_4h_to_h_q_weight2: R.Tensor((2560, 1280), dtype="uint32") = packed_params[111]
            gpt_neox_layers_6_mlp_dense_4h_to_h_q_scale2: R.Tensor((2560, 320), dtype="float16") = packed_params[112]
            gpt_neox_layers_6_mlp_dense_4h_to_h_bias2: R.Tensor((2560,), dtype="float32") = packed_params[113]
            gpt_neox_layers_7_input_layernorm_weight2: R.Tensor((2560,), dtype="float16") = packed_params[114]
            gpt_neox_layers_7_input_layernorm_bias2: R.Tensor((2560,), dtype="float16") = packed_params[115]
            gpt_neox_layers_7_post_attention_layernorm_weight2: R.Tensor((2560,), dtype="float16") = packed_params[116]
            gpt_neox_layers_7_post_attention_layernorm_bias2: R.Tensor((2560,), dtype="float16") = packed_params[117]
            gpt_neox_layers_7_attention_query_key_value_q_weight2: R.Tensor((7680, 320), dtype="uint32") = packed_params[118]
            gpt_neox_layers_7_attention_query_key_value_q_scale2: R.Tensor((7680, 80), dtype="float16") = packed_params[119]
            gpt_neox_layers_7_attention_query_key_value_bias2: R.Tensor((7680,), dtype="float16") = packed_params[120]
            gpt_neox_layers_7_attention_dense_q_weight2: R.Tensor((2560, 320), dtype="uint32") = packed_params[121]
            gpt_neox_layers_7_attention_dense_q_scale2: R.Tensor((2560, 80), dtype="float16") = packed_params[122]
            gpt_neox_layers_7_attention_dense_bias2: R.Tensor((2560,), dtype="float16") = packed_params[123]
            gpt_neox_layers_7_mlp_dense_h_to_4h_q_weight2: R.Tensor((10240, 320), dtype="uint32") = packed_params[124]
            gpt_neox_layers_7_mlp_dense_h_to_4h_q_scale2: R.Tensor((10240, 80), dtype="float16") = packed_params[125]
            gpt_neox_layers_7_mlp_dense_h_to_4h_bias2: R.Tensor((10240,), dtype="float32") = packed_params[126]
            gpt_neox_layers_7_mlp_dense_4h_to_h_q_weight2: R.Tensor((2560, 1280), dtype="uint32") = packed_params[127]
            gpt_neox_layers_7_mlp_dense_4h_to_h_q_scale2: R.Tensor((2560, 320), dtype="float16") = packed_params[128]
            gpt_neox_layers_7_mlp_dense_4h_to_h_bias2: R.Tensor((2560,), dtype="float32") = packed_params[129]
            gpt_neox_layers_8_input_layernorm_weight2: R.Tensor((2560,), dtype="float16") = packed_params[130]
            gpt_neox_layers_8_input_layernorm_bias2: R.Tensor((2560,), dtype="float16") = packed_params[131]
            gpt_neox_layers_8_post_attention_layernorm_weight2: R.Tensor((2560,), dtype="float16") = packed_params[132]
            gpt_neox_layers_8_post_attention_layernorm_bias2: R.Tensor((2560,), dtype="float16") = packed_params[133]
            gpt_neox_layers_8_attention_query_key_value_q_weight2: R.Tensor((7680, 320), dtype="uint32") = packed_params[134]
            gpt_neox_layers_8_attention_query_key_value_q_scale2: R.Tensor((7680, 80), dtype="float16") = packed_params[135]
            gpt_neox_layers_8_attention_query_key_value_bias2: R.Tensor((7680,), dtype="float16") = packed_params[136]
            gpt_neox_layers_8_attention_dense_q_weight2: R.Tensor((2560, 320), dtype="uint32") = packed_params[137]
            gpt_neox_layers_8_attention_dense_q_scale2: R.Tensor((2560, 80), dtype="float16") = packed_params[138]
            gpt_neox_layers_8_attention_dense_bias2: R.Tensor((2560,), dtype="float16") = packed_params[139]
            gpt_neox_layers_8_mlp_dense_h_to_4h_q_weight2: R.Tensor((10240, 320), dtype="uint32") = packed_params[140]
            gpt_neox_layers_8_mlp_dense_h_to_4h_q_scale2: R.Tensor((10240, 80), dtype="float16") = packed_params[141]
            gpt_neox_layers_8_mlp_dense_h_to_4h_bias2: R.Tensor((10240,), dtype="float32") = packed_params[142]
            gpt_neox_layers_8_mlp_dense_4h_to_h_q_weight2: R.Tensor((2560, 1280), dtype="uint32") = packed_params[143]
            gpt_neox_layers_8_mlp_dense_4h_to_h_q_scale2: R.Tensor((2560, 320), dtype="float16") = packed_params[144]
            gpt_neox_layers_8_mlp_dense_4h_to_h_bias2: R.Tensor((2560,), dtype="float32") = packed_params[145]
            gpt_neox_layers_9_input_layernorm_weight2: R.Tensor((2560,), dtype="float16") = packed_params[146]
            gpt_neox_layers_9_input_layernorm_bias2: R.Tensor((2560,), dtype="float16") = packed_params[147]
            gpt_neox_layers_9_post_attention_layernorm_weight2: R.Tensor((2560,), dtype="float16") = packed_params[148]
            gpt_neox_layers_9_post_attention_layernorm_bias2: R.Tensor((2560,), dtype="float16") = packed_params[149]
            gpt_neox_layers_9_attention_query_key_value_q_weight2: R.Tensor((7680, 320), dtype="uint32") = packed_params[150]
            gpt_neox_layers_9_attention_query_key_value_q_scale2: R.Tensor((7680, 80), dtype="float16") = packed_params[151]
            gpt_neox_layers_9_attention_query_key_value_bias2: R.Tensor((7680,), dtype="float16") = packed_params[152]
            gpt_neox_layers_9_attention_dense_q_weight2: R.Tensor((2560, 320), dtype="uint32") = packed_params[153]
            gpt_neox_layers_9_attention_dense_q_scale2: R.Tensor((2560, 80), dtype="float16") = packed_params[154]
            gpt_neox_layers_9_attention_dense_bias2: R.Tensor((2560,), dtype="float16") = packed_params[155]
            gpt_neox_layers_9_mlp_dense_h_to_4h_q_weight2: R.Tensor((10240, 320), dtype="uint32") = packed_params[156]
            gpt_neox_layers_9_mlp_dense_h_to_4h_q_scale2: R.Tensor((10240, 80), dtype="float16") = packed_params[157]
            gpt_neox_layers_9_mlp_dense_h_to_4h_bias2: R.Tensor((10240,), dtype="float32") = packed_params[158]
            gpt_neox_layers_9_mlp_dense_4h_to_h_q_weight2: R.Tensor((2560, 1280), dtype="uint32") = packed_params[159]
            gpt_neox_layers_9_mlp_dense_4h_to_h_q_scale2: R.Tensor((2560, 320), dtype="float16") = packed_params[160]
            gpt_neox_layers_9_mlp_dense_4h_to_h_bias2: R.Tensor((2560,), dtype="float32") = packed_params[161]
            gpt_neox_layers_10_input_layernorm_weight2: R.Tensor((2560,), dtype="float16") = packed_params[162]
            gpt_neox_layers_10_input_layernorm_bias2: R.Tensor((2560,), dtype="float16") = packed_params[163]
            gpt_neox_layers_10_post_attention_layernorm_weight2: R.Tensor((2560,), dtype="float16") = packed_params[164]
            gpt_neox_layers_10_post_attention_layernorm_bias2: R.Tensor((2560,), dtype="float16") = packed_params[165]
            gpt_neox_layers_10_attention_query_key_value_q_weight2: R.Tensor((7680, 320), dtype="uint32") = packed_params[166]
            gpt_neox_layers_10_attention_query_key_value_q_scale2: R.Tensor((7680, 80), dtype="float16") = packed_params[167]
            gpt_neox_layers_10_attention_query_key_value_bias2: R.Tensor((7680,), dtype="float16") = packed_params[168]
            gpt_neox_layers_10_attention_dense_q_weight2: R.Tensor((2560, 320), dtype="uint32") = packed_params[169]
            gpt_neox_layers_10_attention_dense_q_scale2: R.Tensor((2560, 80), dtype="float16") = packed_params[170]
            gpt_neox_layers_10_attention_dense_bias2: R.Tensor((2560,), dtype="float16") = packed_params[171]
            gpt_neox_layers_10_mlp_dense_h_to_4h_q_weight2: R.Tensor((10240, 320), dtype="uint32") = packed_params[172]
            gpt_neox_layers_10_mlp_dense_h_to_4h_q_scale2: R.Tensor((10240, 80), dtype="float16") = packed_params[173]
            gpt_neox_layers_10_mlp_dense_h_to_4h_bias2: R.Tensor((10240,), dtype="float32") = packed_params[174]
            gpt_neox_layers_10_mlp_dense_4h_to_h_q_weight2: R.Tensor((2560, 1280), dtype="uint32") = packed_params[175]
            gpt_neox_layers_10_mlp_dense_4h_to_h_q_scale2: R.Tensor((2560, 320), dtype="float16") = packed_params[176]
            gpt_neox_layers_10_mlp_dense_4h_to_h_bias2: R.Tensor((2560,), dtype="float32") = packed_params[177]
            gpt_neox_layers_11_input_layernorm_weight2: R.Tensor((2560,), dtype="float16") = packed_params[178]
            gpt_neox_layers_11_input_layernorm_bias2: R.Tensor((2560,), dtype="float16") = packed_params[179]
            gpt_neox_layers_11_post_attention_layernorm_weight2: R.Tensor((2560,), dtype="float16") = packed_params[180]
            gpt_neox_layers_11_post_attention_layernorm_bias2: R.Tensor((2560,), dtype="float16") = packed_params[181]
            gpt_neox_layers_11_attention_query_key_value_q_weight2: R.Tensor((7680, 320), dtype="uint32") = packed_params[182]
            gpt_neox_layers_11_attention_query_key_value_q_scale2: R.Tensor((7680, 80), dtype="float16") = packed_params[183]
            gpt_neox_layers_11_attention_query_key_value_bias2: R.Tensor((7680,), dtype="float16") = packed_params[184]
            gpt_neox_layers_11_attention_dense_q_weight2: R.Tensor((2560, 320), dtype="uint32") = packed_params[185]
            gpt_neox_layers_11_attention_dense_q_scale2: R.Tensor((2560, 80), dtype="float16") = packed_params[186]
            gpt_neox_layers_11_attention_dense_bias2: R.Tensor((2560,), dtype="float16") = packed_params[187]
            gpt_neox_layers_11_mlp_dense_h_to_4h_q_weight2: R.Tensor((10240, 320), dtype="uint32") = packed_params[188]
            gpt_neox_layers_11_mlp_dense_h_to_4h_q_scale2: R.Tensor((10240, 80), dtype="float16") = packed_params[189]
            gpt_neox_layers_11_mlp_dense_h_to_4h_bias2: R.Tensor((10240,), dtype="float32") = packed_params[190]
            gpt_neox_layers_11_mlp_dense_4h_to_h_q_weight2: R.Tensor((2560, 1280), dtype="uint32") = packed_params[191]
            gpt_neox_layers_11_mlp_dense_4h_to_h_q_scale2: R.Tensor((2560, 320), dtype="float16") = packed_params[192]
            gpt_neox_layers_11_mlp_dense_4h_to_h_bias2: R.Tensor((2560,), dtype="float32") = packed_params[193]
            gpt_neox_layers_12_input_layernorm_weight2: R.Tensor((2560,), dtype="float16") = packed_params[194]
            gpt_neox_layers_12_input_layernorm_bias2: R.Tensor((2560,), dtype="float16") = packed_params[195]
            gpt_neox_layers_12_post_attention_layernorm_weight2: R.Tensor((2560,), dtype="float16") = packed_params[196]
            gpt_neox_layers_12_post_attention_layernorm_bias2: R.Tensor((2560,), dtype="float16") = packed_params[197]
            gpt_neox_layers_12_attention_query_key_value_q_weight2: R.Tensor((7680, 320), dtype="uint32") = packed_params[198]
            gpt_neox_layers_12_attention_query_key_value_q_scale2: R.Tensor((7680, 80), dtype="float16") = packed_params[199]
            gpt_neox_layers_12_attention_query_key_value_bias2: R.Tensor((7680,), dtype="float16") = packed_params[200]
            gpt_neox_layers_12_attention_dense_q_weight2: R.Tensor((2560, 320), dtype="uint32") = packed_params[201]
            gpt_neox_layers_12_attention_dense_q_scale2: R.Tensor((2560, 80), dtype="float16") = packed_params[202]
            gpt_neox_layers_12_attention_dense_bias2: R.Tensor((2560,), dtype="float16") = packed_params[203]
            gpt_neox_layers_12_mlp_dense_h_to_4h_q_weight2: R.Tensor((10240, 320), dtype="uint32") = packed_params[204]
            gpt_neox_layers_12_mlp_dense_h_to_4h_q_scale2: R.Tensor((10240, 80), dtype="float16") = packed_params[205]
            gpt_neox_layers_12_mlp_dense_h_to_4h_bias2: R.Tensor((10240,), dtype="float32") = packed_params[206]
            gpt_neox_layers_12_mlp_dense_4h_to_h_q_weight2: R.Tensor((2560, 1280), dtype="uint32") = packed_params[207]
            gpt_neox_layers_12_mlp_dense_4h_to_h_q_scale2: R.Tensor((2560, 320), dtype="float16") = packed_params[208]
            gpt_neox_layers_12_mlp_dense_4h_to_h_bias2: R.Tensor((2560,), dtype="float32") = packed_params[209]
            gpt_neox_layers_13_input_layernorm_weight2: R.Tensor((2560,), dtype="float16") = packed_params[210]
            gpt_neox_layers_13_input_layernorm_bias2: R.Tensor((2560,), dtype="float16") = packed_params[211]
            gpt_neox_layers_13_post_attention_layernorm_weight2: R.Tensor((2560,), dtype="float16") = packed_params[212]
            gpt_neox_layers_13_post_attention_layernorm_bias2: R.Tensor((2560,), dtype="float16") = packed_params[213]
            gpt_neox_layers_13_attention_query_key_value_q_weight2: R.Tensor((7680, 320), dtype="uint32") = packed_params[214]
            gpt_neox_layers_13_attention_query_key_value_q_scale2: R.Tensor((7680, 80), dtype="float16") = packed_params[215]
            gpt_neox_layers_13_attention_query_key_value_bias2: R.Tensor((7680,), dtype="float16") = packed_params[216]
            gpt_neox_layers_13_attention_dense_q_weight2: R.Tensor((2560, 320), dtype="uint32") = packed_params[217]
            gpt_neox_layers_13_attention_dense_q_scale2: R.Tensor((2560, 80), dtype="float16") = packed_params[218]
            gpt_neox_layers_13_attention_dense_bias2: R.Tensor((2560,), dtype="float16") = packed_params[219]
            gpt_neox_layers_13_mlp_dense_h_to_4h_q_weight2: R.Tensor((10240, 320), dtype="uint32") = packed_params[220]
            gpt_neox_layers_13_mlp_dense_h_to_4h_q_scale2: R.Tensor((10240, 80), dtype="float16") = packed_params[221]
            gpt_neox_layers_13_mlp_dense_h_to_4h_bias2: R.Tensor((10240,), dtype="float32") = packed_params[222]
            gpt_neox_layers_13_mlp_dense_4h_to_h_q_weight2: R.Tensor((2560, 1280), dtype="uint32") = packed_params[223]
            gpt_neox_layers_13_mlp_dense_4h_to_h_q_scale2: R.Tensor((2560, 320), dtype="float16") = packed_params[224]
            gpt_neox_layers_13_mlp_dense_4h_to_h_bias2: R.Tensor((2560,), dtype="float32") = packed_params[225]
            gpt_neox_layers_14_input_layernorm_weight2: R.Tensor((2560,), dtype="float16") = packed_params[226]
            gpt_neox_layers_14_input_layernorm_bias2: R.Tensor((2560,), dtype="float16") = packed_params[227]
            gpt_neox_layers_14_post_attention_layernorm_weight2: R.Tensor((2560,), dtype="float16") = packed_params[228]
            gpt_neox_layers_14_post_attention_layernorm_bias2: R.Tensor((2560,), dtype="float16") = packed_params[229]
            gpt_neox_layers_14_attention_query_key_value_q_weight2: R.Tensor((7680, 320), dtype="uint32") = packed_params[230]
            gpt_neox_layers_14_attention_query_key_value_q_scale2: R.Tensor((7680, 80), dtype="float16") = packed_params[231]
            gpt_neox_layers_14_attention_query_key_value_bias2: R.Tensor((7680,), dtype="float16") = packed_params[232]
            gpt_neox_layers_14_attention_dense_q_weight2: R.Tensor((2560, 320), dtype="uint32") = packed_params[233]
            gpt_neox_layers_14_attention_dense_q_scale2: R.Tensor((2560, 80), dtype="float16") = packed_params[234]
            gpt_neox_layers_14_attention_dense_bias2: R.Tensor((2560,), dtype="float16") = packed_params[235]
            gpt_neox_layers_14_mlp_dense_h_to_4h_q_weight2: R.Tensor((10240, 320), dtype="uint32") = packed_params[236]
            gpt_neox_layers_14_mlp_dense_h_to_4h_q_scale2: R.Tensor((10240, 80), dtype="float16") = packed_params[237]
            gpt_neox_layers_14_mlp_dense_h_to_4h_bias2: R.Tensor((10240,), dtype="float32") = packed_params[238]
            gpt_neox_layers_14_mlp_dense_4h_to_h_q_weight2: R.Tensor((2560, 1280), dtype="uint32") = packed_params[239]
            gpt_neox_layers_14_mlp_dense_4h_to_h_q_scale2: R.Tensor((2560, 320), dtype="float16") = packed_params[240]
            gpt_neox_layers_14_mlp_dense_4h_to_h_bias2: R.Tensor((2560,), dtype="float32") = packed_params[241]
            gpt_neox_layers_15_input_layernorm_weight2: R.Tensor((2560,), dtype="float16") = packed_params[242]
            gpt_neox_layers_15_input_layernorm_bias2: R.Tensor((2560,), dtype="float16") = packed_params[243]
            gpt_neox_layers_15_post_attention_layernorm_weight2: R.Tensor((2560,), dtype="float16") = packed_params[244]
            gpt_neox_layers_15_post_attention_layernorm_bias2: R.Tensor((2560,), dtype="float16") = packed_params[245]
            gpt_neox_layers_15_attention_query_key_value_q_weight2: R.Tensor((7680, 320), dtype="uint32") = packed_params[246]
            gpt_neox_layers_15_attention_query_key_value_q_scale2: R.Tensor((7680, 80), dtype="float16") = packed_params[247]
            gpt_neox_layers_15_attention_query_key_value_bias2: R.Tensor((7680,), dtype="float16") = packed_params[248]
            gpt_neox_layers_15_attention_dense_q_weight2: R.Tensor((2560, 320), dtype="uint32") = packed_params[249]
            gpt_neox_layers_15_attention_dense_q_scale2: R.Tensor((2560, 80), dtype="float16") = packed_params[250]
            gpt_neox_layers_15_attention_dense_bias2: R.Tensor((2560,), dtype="float16") = packed_params[251]
            gpt_neox_layers_15_mlp_dense_h_to_4h_q_weight2: R.Tensor((10240, 320), dtype="uint32") = packed_params[252]
            gpt_neox_layers_15_mlp_dense_h_to_4h_q_scale2: R.Tensor((10240, 80), dtype="float16") = packed_params[253]
            gpt_neox_layers_15_mlp_dense_h_to_4h_bias2: R.Tensor((10240,), dtype="float32") = packed_params[254]
            gpt_neox_layers_15_mlp_dense_4h_to_h_q_weight2: R.Tensor((2560, 1280), dtype="uint32") = packed_params[255]
            gpt_neox_layers_15_mlp_dense_4h_to_h_q_scale2: R.Tensor((2560, 320), dtype="float16") = packed_params[256]
            gpt_neox_layers_15_mlp_dense_4h_to_h_bias2: R.Tensor((2560,), dtype="float32") = packed_params[257]
            gpt_neox_layers_16_input_layernorm_weight2: R.Tensor((2560,), dtype="float16") = packed_params[258]
            gpt_neox_layers_16_input_layernorm_bias2: R.Tensor((2560,), dtype="float16") = packed_params[259]
            gpt_neox_layers_16_post_attention_layernorm_weight2: R.Tensor((2560,), dtype="float16") = packed_params[260]
            gpt_neox_layers_16_post_attention_layernorm_bias2: R.Tensor((2560,), dtype="float16") = packed_params[261]
            gpt_neox_layers_16_attention_query_key_value_q_weight2: R.Tensor((7680, 320), dtype="uint32") = packed_params[262]
            gpt_neox_layers_16_attention_query_key_value_q_scale2: R.Tensor((7680, 80), dtype="float16") = packed_params[263]
            gpt_neox_layers_16_attention_query_key_value_bias2: R.Tensor((7680,), dtype="float16") = packed_params[264]
            gpt_neox_layers_16_attention_dense_q_weight2: R.Tensor((2560, 320), dtype="uint32") = packed_params[265]
            gpt_neox_layers_16_attention_dense_q_scale2: R.Tensor((2560, 80), dtype="float16") = packed_params[266]
            gpt_neox_layers_16_attention_dense_bias2: R.Tensor((2560,), dtype="float16") = packed_params[267]
            gpt_neox_layers_16_mlp_dense_h_to_4h_q_weight2: R.Tensor((10240, 320), dtype="uint32") = packed_params[268]
            gpt_neox_layers_16_mlp_dense_h_to_4h_q_scale2: R.Tensor((10240, 80), dtype="float16") = packed_params[269]
            gpt_neox_layers_16_mlp_dense_h_to_4h_bias2: R.Tensor((10240,), dtype="float32") = packed_params[270]
            gpt_neox_layers_16_mlp_dense_4h_to_h_q_weight2: R.Tensor((2560, 1280), dtype="uint32") = packed_params[271]
            gpt_neox_layers_16_mlp_dense_4h_to_h_q_scale2: R.Tensor((2560, 320), dtype="float16") = packed_params[272]
            gpt_neox_layers_16_mlp_dense_4h_to_h_bias2: R.Tensor((2560,), dtype="float32") = packed_params[273]
            gpt_neox_layers_17_input_layernorm_weight2: R.Tensor((2560,), dtype="float16") = packed_params[274]
            gpt_neox_layers_17_input_layernorm_bias2: R.Tensor((2560,), dtype="float16") = packed_params[275]
            gpt_neox_layers_17_post_attention_layernorm_weight2: R.Tensor((2560,), dtype="float16") = packed_params[276]
            gpt_neox_layers_17_post_attention_layernorm_bias2: R.Tensor((2560,), dtype="float16") = packed_params[277]
            gpt_neox_layers_17_attention_query_key_value_q_weight2: R.Tensor((7680, 320), dtype="uint32") = packed_params[278]
            gpt_neox_layers_17_attention_query_key_value_q_scale2: R.Tensor((7680, 80), dtype="float16") = packed_params[279]
            gpt_neox_layers_17_attention_query_key_value_bias2: R.Tensor((7680,), dtype="float16") = packed_params[280]
            gpt_neox_layers_17_attention_dense_q_weight2: R.Tensor((2560, 320), dtype="uint32") = packed_params[281]
            gpt_neox_layers_17_attention_dense_q_scale2: R.Tensor((2560, 80), dtype="float16") = packed_params[282]
            gpt_neox_layers_17_attention_dense_bias2: R.Tensor((2560,), dtype="float16") = packed_params[283]
            gpt_neox_layers_17_mlp_dense_h_to_4h_q_weight2: R.Tensor((10240, 320), dtype="uint32") = packed_params[284]
            gpt_neox_layers_17_mlp_dense_h_to_4h_q_scale2: R.Tensor((10240, 80), dtype="float16") = packed_params[285]
            gpt_neox_layers_17_mlp_dense_h_to_4h_bias2: R.Tensor((10240,), dtype="float32") = packed_params[286]
            gpt_neox_layers_17_mlp_dense_4h_to_h_q_weight2: R.Tensor((2560, 1280), dtype="uint32") = packed_params[287]
            gpt_neox_layers_17_mlp_dense_4h_to_h_q_scale2: R.Tensor((2560, 320), dtype="float16") = packed_params[288]
            gpt_neox_layers_17_mlp_dense_4h_to_h_bias2: R.Tensor((2560,), dtype="float32") = packed_params[289]
            gpt_neox_layers_18_input_layernorm_weight2: R.Tensor((2560,), dtype="float16") = packed_params[290]
            gpt_neox_layers_18_input_layernorm_bias2: R.Tensor((2560,), dtype="float16") = packed_params[291]
            gpt_neox_layers_18_post_attention_layernorm_weight2: R.Tensor((2560,), dtype="float16") = packed_params[292]
            gpt_neox_layers_18_post_attention_layernorm_bias2: R.Tensor((2560,), dtype="float16") = packed_params[293]
            gpt_neox_layers_18_attention_query_key_value_q_weight2: R.Tensor((7680, 320), dtype="uint32") = packed_params[294]
            gpt_neox_layers_18_attention_query_key_value_q_scale2: R.Tensor((7680, 80), dtype="float16") = packed_params[295]
            gpt_neox_layers_18_attention_query_key_value_bias2: R.Tensor((7680,), dtype="float16") = packed_params[296]
            gpt_neox_layers_18_attention_dense_q_weight2: R.Tensor((2560, 320), dtype="uint32") = packed_params[297]
            gpt_neox_layers_18_attention_dense_q_scale2: R.Tensor((2560, 80), dtype="float16") = packed_params[298]
            gpt_neox_layers_18_attention_dense_bias2: R.Tensor((2560,), dtype="float16") = packed_params[299]
            gpt_neox_layers_18_mlp_dense_h_to_4h_q_weight2: R.Tensor((10240, 320), dtype="uint32") = packed_params[300]
            gpt_neox_layers_18_mlp_dense_h_to_4h_q_scale2: R.Tensor((10240, 80), dtype="float16") = packed_params[301]
            gpt_neox_layers_18_mlp_dense_h_to_4h_bias2: R.Tensor((10240,), dtype="float32") = packed_params[302]
            gpt_neox_layers_18_mlp_dense_4h_to_h_q_weight2: R.Tensor((2560, 1280), dtype="uint32") = packed_params[303]
            gpt_neox_layers_18_mlp_dense_4h_to_h_q_scale2: R.Tensor((2560, 320), dtype="float16") = packed_params[304]
            gpt_neox_layers_18_mlp_dense_4h_to_h_bias2: R.Tensor((2560,), dtype="float32") = packed_params[305]
            gpt_neox_layers_19_input_layernorm_weight2: R.Tensor((2560,), dtype="float16") = packed_params[306]
            gpt_neox_layers_19_input_layernorm_bias2: R.Tensor((2560,), dtype="float16") = packed_params[307]
            gpt_neox_layers_19_post_attention_layernorm_weight2: R.Tensor((2560,), dtype="float16") = packed_params[308]
            gpt_neox_layers_19_post_attention_layernorm_bias2: R.Tensor((2560,), dtype="float16") = packed_params[309]
            gpt_neox_layers_19_attention_query_key_value_q_weight2: R.Tensor((7680, 320), dtype="uint32") = packed_params[310]
            gpt_neox_layers_19_attention_query_key_value_q_scale2: R.Tensor((7680, 80), dtype="float16") = packed_params[311]
            gpt_neox_layers_19_attention_query_key_value_bias2: R.Tensor((7680,), dtype="float16") = packed_params[312]
            gpt_neox_layers_19_attention_dense_q_weight2: R.Tensor((2560, 320), dtype="uint32") = packed_params[313]
            gpt_neox_layers_19_attention_dense_q_scale2: R.Tensor((2560, 80), dtype="float16") = packed_params[314]
            gpt_neox_layers_19_attention_dense_bias2: R.Tensor((2560,), dtype="float16") = packed_params[315]
            gpt_neox_layers_19_mlp_dense_h_to_4h_q_weight2: R.Tensor((10240, 320), dtype="uint32") = packed_params[316]
            gpt_neox_layers_19_mlp_dense_h_to_4h_q_scale2: R.Tensor((10240, 80), dtype="float16") = packed_params[317]
            gpt_neox_layers_19_mlp_dense_h_to_4h_bias2: R.Tensor((10240,), dtype="float32") = packed_params[318]
            gpt_neox_layers_19_mlp_dense_4h_to_h_q_weight2: R.Tensor((2560, 1280), dtype="uint32") = packed_params[319]
            gpt_neox_layers_19_mlp_dense_4h_to_h_q_scale2: R.Tensor((2560, 320), dtype="float16") = packed_params[320]
            gpt_neox_layers_19_mlp_dense_4h_to_h_bias2: R.Tensor((2560,), dtype="float32") = packed_params[321]
            gpt_neox_layers_20_input_layernorm_weight2: R.Tensor((2560,), dtype="float16") = packed_params[322]
            gpt_neox_layers_20_input_layernorm_bias2: R.Tensor((2560,), dtype="float16") = packed_params[323]
            gpt_neox_layers_20_post_attention_layernorm_weight2: R.Tensor((2560,), dtype="float16") = packed_params[324]
            gpt_neox_layers_20_post_attention_layernorm_bias2: R.Tensor((2560,), dtype="float16") = packed_params[325]
            gpt_neox_layers_20_attention_query_key_value_q_weight2: R.Tensor((7680, 320), dtype="uint32") = packed_params[326]
            gpt_neox_layers_20_attention_query_key_value_q_scale2: R.Tensor((7680, 80), dtype="float16") = packed_params[327]
            gpt_neox_layers_20_attention_query_key_value_bias2: R.Tensor((7680,), dtype="float16") = packed_params[328]
            gpt_neox_layers_20_attention_dense_q_weight2: R.Tensor((2560, 320), dtype="uint32") = packed_params[329]
            gpt_neox_layers_20_attention_dense_q_scale2: R.Tensor((2560, 80), dtype="float16") = packed_params[330]
            gpt_neox_layers_20_attention_dense_bias2: R.Tensor((2560,), dtype="float16") = packed_params[331]
            gpt_neox_layers_20_mlp_dense_h_to_4h_q_weight2: R.Tensor((10240, 320), dtype="uint32") = packed_params[332]
            gpt_neox_layers_20_mlp_dense_h_to_4h_q_scale2: R.Tensor((10240, 80), dtype="float16") = packed_params[333]
            gpt_neox_layers_20_mlp_dense_h_to_4h_bias2: R.Tensor((10240,), dtype="float32") = packed_params[334]
            gpt_neox_layers_20_mlp_dense_4h_to_h_q_weight2: R.Tensor((2560, 1280), dtype="uint32") = packed_params[335]
            gpt_neox_layers_20_mlp_dense_4h_to_h_q_scale2: R.Tensor((2560, 320), dtype="float16") = packed_params[336]
            gpt_neox_layers_20_mlp_dense_4h_to_h_bias2: R.Tensor((2560,), dtype="float32") = packed_params[337]
            gpt_neox_layers_21_input_layernorm_weight2: R.Tensor((2560,), dtype="float16") = packed_params[338]
            gpt_neox_layers_21_input_layernorm_bias2: R.Tensor((2560,), dtype="float16") = packed_params[339]
            gpt_neox_layers_21_post_attention_layernorm_weight2: R.Tensor((2560,), dtype="float16") = packed_params[340]
            gpt_neox_layers_21_post_attention_layernorm_bias2: R.Tensor((2560,), dtype="float16") = packed_params[341]
            gpt_neox_layers_21_attention_query_key_value_q_weight2: R.Tensor((7680, 320), dtype="uint32") = packed_params[342]
            gpt_neox_layers_21_attention_query_key_value_q_scale2: R.Tensor((7680, 80), dtype="float16") = packed_params[343]
            gpt_neox_layers_21_attention_query_key_value_bias2: R.Tensor((7680,), dtype="float16") = packed_params[344]
            gpt_neox_layers_21_attention_dense_q_weight2: R.Tensor((2560, 320), dtype="uint32") = packed_params[345]
            gpt_neox_layers_21_attention_dense_q_scale2: R.Tensor((2560, 80), dtype="float16") = packed_params[346]
            gpt_neox_layers_21_attention_dense_bias2: R.Tensor((2560,), dtype="float16") = packed_params[347]
            gpt_neox_layers_21_mlp_dense_h_to_4h_q_weight2: R.Tensor((10240, 320), dtype="uint32") = packed_params[348]
            gpt_neox_layers_21_mlp_dense_h_to_4h_q_scale2: R.Tensor((10240, 80), dtype="float16") = packed_params[349]
            gpt_neox_layers_21_mlp_dense_h_to_4h_bias2: R.Tensor((10240,), dtype="float32") = packed_params[350]
            gpt_neox_layers_21_mlp_dense_4h_to_h_q_weight2: R.Tensor((2560, 1280), dtype="uint32") = packed_params[351]
            gpt_neox_layers_21_mlp_dense_4h_to_h_q_scale2: R.Tensor((2560, 320), dtype="float16") = packed_params[352]
            gpt_neox_layers_21_mlp_dense_4h_to_h_bias2: R.Tensor((2560,), dtype="float32") = packed_params[353]
            gpt_neox_layers_22_input_layernorm_weight2: R.Tensor((2560,), dtype="float16") = packed_params[354]
            gpt_neox_layers_22_input_layernorm_bias2: R.Tensor((2560,), dtype="float16") = packed_params[355]
            gpt_neox_layers_22_post_attention_layernorm_weight2: R.Tensor((2560,), dtype="float16") = packed_params[356]
            gpt_neox_layers_22_post_attention_layernorm_bias2: R.Tensor((2560,), dtype="float16") = packed_params[357]
            gpt_neox_layers_22_attention_query_key_value_q_weight2: R.Tensor((7680, 320), dtype="uint32") = packed_params[358]
            gpt_neox_layers_22_attention_query_key_value_q_scale2: R.Tensor((7680, 80), dtype="float16") = packed_params[359]
            gpt_neox_layers_22_attention_query_key_value_bias2: R.Tensor((7680,), dtype="float16") = packed_params[360]
            gpt_neox_layers_22_attention_dense_q_weight2: R.Tensor((2560, 320), dtype="uint32") = packed_params[361]
            gpt_neox_layers_22_attention_dense_q_scale2: R.Tensor((2560, 80), dtype="float16") = packed_params[362]
            gpt_neox_layers_22_attention_dense_bias2: R.Tensor((2560,), dtype="float16") = packed_params[363]
            gpt_neox_layers_22_mlp_dense_h_to_4h_q_weight2: R.Tensor((10240, 320), dtype="uint32") = packed_params[364]
            gpt_neox_layers_22_mlp_dense_h_to_4h_q_scale2: R.Tensor((10240, 80), dtype="float16") = packed_params[365]
            gpt_neox_layers_22_mlp_dense_h_to_4h_bias2: R.Tensor((10240,), dtype="float32") = packed_params[366]
            gpt_neox_layers_22_mlp_dense_4h_to_h_q_weight2: R.Tensor((2560, 1280), dtype="uint32") = packed_params[367]
            gpt_neox_layers_22_mlp_dense_4h_to_h_q_scale2: R.Tensor((2560, 320), dtype="float16") = packed_params[368]
            gpt_neox_layers_22_mlp_dense_4h_to_h_bias2: R.Tensor((2560,), dtype="float32") = packed_params[369]
            gpt_neox_layers_23_input_layernorm_weight2: R.Tensor((2560,), dtype="float16") = packed_params[370]
            gpt_neox_layers_23_input_layernorm_bias2: R.Tensor((2560,), dtype="float16") = packed_params[371]
            gpt_neox_layers_23_post_attention_layernorm_weight2: R.Tensor((2560,), dtype="float16") = packed_params[372]
            gpt_neox_layers_23_post_attention_layernorm_bias2: R.Tensor((2560,), dtype="float16") = packed_params[373]
            gpt_neox_layers_23_attention_query_key_value_q_weight2: R.Tensor((7680, 320), dtype="uint32") = packed_params[374]
            gpt_neox_layers_23_attention_query_key_value_q_scale2: R.Tensor((7680, 80), dtype="float16") = packed_params[375]
            gpt_neox_layers_23_attention_query_key_value_bias2: R.Tensor((7680,), dtype="float16") = packed_params[376]
            gpt_neox_layers_23_attention_dense_q_weight2: R.Tensor((2560, 320), dtype="uint32") = packed_params[377]
            gpt_neox_layers_23_attention_dense_q_scale2: R.Tensor((2560, 80), dtype="float16") = packed_params[378]
            gpt_neox_layers_23_attention_dense_bias2: R.Tensor((2560,), dtype="float16") = packed_params[379]
            gpt_neox_layers_23_mlp_dense_h_to_4h_q_weight2: R.Tensor((10240, 320), dtype="uint32") = packed_params[380]
            gpt_neox_layers_23_mlp_dense_h_to_4h_q_scale2: R.Tensor((10240, 80), dtype="float16") = packed_params[381]
            gpt_neox_layers_23_mlp_dense_h_to_4h_bias2: R.Tensor((10240,), dtype="float32") = packed_params[382]
            gpt_neox_layers_23_mlp_dense_4h_to_h_q_weight2: R.Tensor((2560, 1280), dtype="uint32") = packed_params[383]
            gpt_neox_layers_23_mlp_dense_4h_to_h_q_scale2: R.Tensor((2560, 320), dtype="float16") = packed_params[384]
            gpt_neox_layers_23_mlp_dense_4h_to_h_bias2: R.Tensor((2560,), dtype="float32") = packed_params[385]
            gpt_neox_layers_24_input_layernorm_weight2: R.Tensor((2560,), dtype="float16") = packed_params[386]
            gpt_neox_layers_24_input_layernorm_bias2: R.Tensor((2560,), dtype="float16") = packed_params[387]
            gpt_neox_layers_24_post_attention_layernorm_weight2: R.Tensor((2560,), dtype="float16") = packed_params[388]
            gpt_neox_layers_24_post_attention_layernorm_bias2: R.Tensor((2560,), dtype="float16") = packed_params[389]
            gpt_neox_layers_24_attention_query_key_value_q_weight2: R.Tensor((7680, 320), dtype="uint32") = packed_params[390]
            gpt_neox_layers_24_attention_query_key_value_q_scale2: R.Tensor((7680, 80), dtype="float16") = packed_params[391]
            gpt_neox_layers_24_attention_query_key_value_bias2: R.Tensor((7680,), dtype="float16") = packed_params[392]
            gpt_neox_layers_24_attention_dense_q_weight2: R.Tensor((2560, 320), dtype="uint32") = packed_params[393]
            gpt_neox_layers_24_attention_dense_q_scale2: R.Tensor((2560, 80), dtype="float16") = packed_params[394]
            gpt_neox_layers_24_attention_dense_bias2: R.Tensor((2560,), dtype="float16") = packed_params[395]
            gpt_neox_layers_24_mlp_dense_h_to_4h_q_weight2: R.Tensor((10240, 320), dtype="uint32") = packed_params[396]
            gpt_neox_layers_24_mlp_dense_h_to_4h_q_scale2: R.Tensor((10240, 80), dtype="float16") = packed_params[397]
            gpt_neox_layers_24_mlp_dense_h_to_4h_bias2: R.Tensor((10240,), dtype="float32") = packed_params[398]
            gpt_neox_layers_24_mlp_dense_4h_to_h_q_weight2: R.Tensor((2560, 1280), dtype="uint32") = packed_params[399]
            gpt_neox_layers_24_mlp_dense_4h_to_h_q_scale2: R.Tensor((2560, 320), dtype="float16") = packed_params[400]
            gpt_neox_layers_24_mlp_dense_4h_to_h_bias2: R.Tensor((2560,), dtype="float32") = packed_params[401]
            gpt_neox_layers_25_input_layernorm_weight2: R.Tensor((2560,), dtype="float16") = packed_params[402]
            gpt_neox_layers_25_input_layernorm_bias2: R.Tensor((2560,), dtype="float16") = packed_params[403]
            gpt_neox_layers_25_post_attention_layernorm_weight2: R.Tensor((2560,), dtype="float16") = packed_params[404]
            gpt_neox_layers_25_post_attention_layernorm_bias2: R.Tensor((2560,), dtype="float16") = packed_params[405]
            gpt_neox_layers_25_attention_query_key_value_q_weight2: R.Tensor((7680, 320), dtype="uint32") = packed_params[406]
            gpt_neox_layers_25_attention_query_key_value_q_scale2: R.Tensor((7680, 80), dtype="float16") = packed_params[407]
            gpt_neox_layers_25_attention_query_key_value_bias2: R.Tensor((7680,), dtype="float16") = packed_params[408]
            gpt_neox_layers_25_attention_dense_q_weight2: R.Tensor((2560, 320), dtype="uint32") = packed_params[409]
            gpt_neox_layers_25_attention_dense_q_scale2: R.Tensor((2560, 80), dtype="float16") = packed_params[410]
            gpt_neox_layers_25_attention_dense_bias2: R.Tensor((2560,), dtype="float16") = packed_params[411]
            gpt_neox_layers_25_mlp_dense_h_to_4h_q_weight2: R.Tensor((10240, 320), dtype="uint32") = packed_params[412]
            gpt_neox_layers_25_mlp_dense_h_to_4h_q_scale2: R.Tensor((10240, 80), dtype="float16") = packed_params[413]
            gpt_neox_layers_25_mlp_dense_h_to_4h_bias2: R.Tensor((10240,), dtype="float32") = packed_params[414]
            gpt_neox_layers_25_mlp_dense_4h_to_h_q_weight2: R.Tensor((2560, 1280), dtype="uint32") = packed_params[415]
            gpt_neox_layers_25_mlp_dense_4h_to_h_q_scale2: R.Tensor((2560, 320), dtype="float16") = packed_params[416]
            gpt_neox_layers_25_mlp_dense_4h_to_h_bias2: R.Tensor((2560,), dtype="float32") = packed_params[417]
            gpt_neox_layers_26_input_layernorm_weight2: R.Tensor((2560,), dtype="float16") = packed_params[418]
            gpt_neox_layers_26_input_layernorm_bias2: R.Tensor((2560,), dtype="float16") = packed_params[419]
            gpt_neox_layers_26_post_attention_layernorm_weight2: R.Tensor((2560,), dtype="float16") = packed_params[420]
            gpt_neox_layers_26_post_attention_layernorm_bias2: R.Tensor((2560,), dtype="float16") = packed_params[421]
            gpt_neox_layers_26_attention_query_key_value_q_weight2: R.Tensor((7680, 320), dtype="uint32") = packed_params[422]
            gpt_neox_layers_26_attention_query_key_value_q_scale2: R.Tensor((7680, 80), dtype="float16") = packed_params[423]
            gpt_neox_layers_26_attention_query_key_value_bias2: R.Tensor((7680,), dtype="float16") = packed_params[424]
            gpt_neox_layers_26_attention_dense_q_weight2: R.Tensor((2560, 320), dtype="uint32") = packed_params[425]
            gpt_neox_layers_26_attention_dense_q_scale2: R.Tensor((2560, 80), dtype="float16") = packed_params[426]
            gpt_neox_layers_26_attention_dense_bias2: R.Tensor((2560,), dtype="float16") = packed_params[427]
            gpt_neox_layers_26_mlp_dense_h_to_4h_q_weight2: R.Tensor((10240, 320), dtype="uint32") = packed_params[428]
            gpt_neox_layers_26_mlp_dense_h_to_4h_q_scale2: R.Tensor((10240, 80), dtype="float16") = packed_params[429]
            gpt_neox_layers_26_mlp_dense_h_to_4h_bias2: R.Tensor((10240,), dtype="float32") = packed_params[430]
            gpt_neox_layers_26_mlp_dense_4h_to_h_q_weight2: R.Tensor((2560, 1280), dtype="uint32") = packed_params[431]
            gpt_neox_layers_26_mlp_dense_4h_to_h_q_scale2: R.Tensor((2560, 320), dtype="float16") = packed_params[432]
            gpt_neox_layers_26_mlp_dense_4h_to_h_bias2: R.Tensor((2560,), dtype="float32") = packed_params[433]
            gpt_neox_layers_27_input_layernorm_weight2: R.Tensor((2560,), dtype="float16") = packed_params[434]
            gpt_neox_layers_27_input_layernorm_bias2: R.Tensor((2560,), dtype="float16") = packed_params[435]
            gpt_neox_layers_27_post_attention_layernorm_weight2: R.Tensor((2560,), dtype="float16") = packed_params[436]
            gpt_neox_layers_27_post_attention_layernorm_bias2: R.Tensor((2560,), dtype="float16") = packed_params[437]
            gpt_neox_layers_27_attention_query_key_value_q_weight2: R.Tensor((7680, 320), dtype="uint32") = packed_params[438]
            gpt_neox_layers_27_attention_query_key_value_q_scale2: R.Tensor((7680, 80), dtype="float16") = packed_params[439]
            gpt_neox_layers_27_attention_query_key_value_bias2: R.Tensor((7680,), dtype="float16") = packed_params[440]
            gpt_neox_layers_27_attention_dense_q_weight2: R.Tensor((2560, 320), dtype="uint32") = packed_params[441]
            gpt_neox_layers_27_attention_dense_q_scale2: R.Tensor((2560, 80), dtype="float16") = packed_params[442]
            gpt_neox_layers_27_attention_dense_bias2: R.Tensor((2560,), dtype="float16") = packed_params[443]
            gpt_neox_layers_27_mlp_dense_h_to_4h_q_weight2: R.Tensor((10240, 320), dtype="uint32") = packed_params[444]
            gpt_neox_layers_27_mlp_dense_h_to_4h_q_scale2: R.Tensor((10240, 80), dtype="float16") = packed_params[445]
            gpt_neox_layers_27_mlp_dense_h_to_4h_bias2: R.Tensor((10240,), dtype="float32") = packed_params[446]
            gpt_neox_layers_27_mlp_dense_4h_to_h_q_weight2: R.Tensor((2560, 1280), dtype="uint32") = packed_params[447]
            gpt_neox_layers_27_mlp_dense_4h_to_h_q_scale2: R.Tensor((2560, 320), dtype="float16") = packed_params[448]
            gpt_neox_layers_27_mlp_dense_4h_to_h_bias2: R.Tensor((2560,), dtype="float32") = packed_params[449]
            gpt_neox_layers_28_input_layernorm_weight2: R.Tensor((2560,), dtype="float16") = packed_params[450]
            gpt_neox_layers_28_input_layernorm_bias2: R.Tensor((2560,), dtype="float16") = packed_params[451]
            gpt_neox_layers_28_post_attention_layernorm_weight2: R.Tensor((2560,), dtype="float16") = packed_params[452]
            gpt_neox_layers_28_post_attention_layernorm_bias2: R.Tensor((2560,), dtype="float16") = packed_params[453]
            gpt_neox_layers_28_attention_query_key_value_q_weight2: R.Tensor((7680, 320), dtype="uint32") = packed_params[454]
            gpt_neox_layers_28_attention_query_key_value_q_scale2: R.Tensor((7680, 80), dtype="float16") = packed_params[455]
            gpt_neox_layers_28_attention_query_key_value_bias2: R.Tensor((7680,), dtype="float16") = packed_params[456]
            gpt_neox_layers_28_attention_dense_q_weight2: R.Tensor((2560, 320), dtype="uint32") = packed_params[457]
            gpt_neox_layers_28_attention_dense_q_scale2: R.Tensor((2560, 80), dtype="float16") = packed_params[458]
            gpt_neox_layers_28_attention_dense_bias2: R.Tensor((2560,), dtype="float16") = packed_params[459]
            gpt_neox_layers_28_mlp_dense_h_to_4h_q_weight2: R.Tensor((10240, 320), dtype="uint32") = packed_params[460]
            gpt_neox_layers_28_mlp_dense_h_to_4h_q_scale2: R.Tensor((10240, 80), dtype="float16") = packed_params[461]
            gpt_neox_layers_28_mlp_dense_h_to_4h_bias2: R.Tensor((10240,), dtype="float32") = packed_params[462]
            gpt_neox_layers_28_mlp_dense_4h_to_h_q_weight2: R.Tensor((2560, 1280), dtype="uint32") = packed_params[463]
            gpt_neox_layers_28_mlp_dense_4h_to_h_q_scale2: R.Tensor((2560, 320), dtype="float16") = packed_params[464]
            gpt_neox_layers_28_mlp_dense_4h_to_h_bias2: R.Tensor((2560,), dtype="float32") = packed_params[465]
            gpt_neox_layers_29_input_layernorm_weight2: R.Tensor((2560,), dtype="float16") = packed_params[466]
            gpt_neox_layers_29_input_layernorm_bias2: R.Tensor((2560,), dtype="float16") = packed_params[467]
            gpt_neox_layers_29_post_attention_layernorm_weight2: R.Tensor((2560,), dtype="float16") = packed_params[468]
            gpt_neox_layers_29_post_attention_layernorm_bias2: R.Tensor((2560,), dtype="float16") = packed_params[469]
            gpt_neox_layers_29_attention_query_key_value_q_weight2: R.Tensor((7680, 320), dtype="uint32") = packed_params[470]
            gpt_neox_layers_29_attention_query_key_value_q_scale2: R.Tensor((7680, 80), dtype="float16") = packed_params[471]
            gpt_neox_layers_29_attention_query_key_value_bias2: R.Tensor((7680,), dtype="float16") = packed_params[472]
            gpt_neox_layers_29_attention_dense_q_weight2: R.Tensor((2560, 320), dtype="uint32") = packed_params[473]
            gpt_neox_layers_29_attention_dense_q_scale2: R.Tensor((2560, 80), dtype="float16") = packed_params[474]
            gpt_neox_layers_29_attention_dense_bias2: R.Tensor((2560,), dtype="float16") = packed_params[475]
            gpt_neox_layers_29_mlp_dense_h_to_4h_q_weight2: R.Tensor((10240, 320), dtype="uint32") = packed_params[476]
            gpt_neox_layers_29_mlp_dense_h_to_4h_q_scale2: R.Tensor((10240, 80), dtype="float16") = packed_params[477]
            gpt_neox_layers_29_mlp_dense_h_to_4h_bias2: R.Tensor((10240,), dtype="float32") = packed_params[478]
            gpt_neox_layers_29_mlp_dense_4h_to_h_q_weight2: R.Tensor((2560, 1280), dtype="uint32") = packed_params[479]
            gpt_neox_layers_29_mlp_dense_4h_to_h_q_scale2: R.Tensor((2560, 320), dtype="float16") = packed_params[480]
            gpt_neox_layers_29_mlp_dense_4h_to_h_bias2: R.Tensor((2560,), dtype="float32") = packed_params[481]
            gpt_neox_layers_30_input_layernorm_weight2: R.Tensor((2560,), dtype="float16") = packed_params[482]
            gpt_neox_layers_30_input_layernorm_bias2: R.Tensor((2560,), dtype="float16") = packed_params[483]
            gpt_neox_layers_30_post_attention_layernorm_weight2: R.Tensor((2560,), dtype="float16") = packed_params[484]
            gpt_neox_layers_30_post_attention_layernorm_bias2: R.Tensor((2560,), dtype="float16") = packed_params[485]
            gpt_neox_layers_30_attention_query_key_value_q_weight2: R.Tensor((7680, 320), dtype="uint32") = packed_params[486]
            gpt_neox_layers_30_attention_query_key_value_q_scale2: R.Tensor((7680, 80), dtype="float16") = packed_params[487]
            gpt_neox_layers_30_attention_query_key_value_bias2: R.Tensor((7680,), dtype="float16") = packed_params[488]
            gpt_neox_layers_30_attention_dense_q_weight2: R.Tensor((2560, 320), dtype="uint32") = packed_params[489]
            gpt_neox_layers_30_attention_dense_q_scale2: R.Tensor((2560, 80), dtype="float16") = packed_params[490]
            gpt_neox_layers_30_attention_dense_bias2: R.Tensor((2560,), dtype="float16") = packed_params[491]
            gpt_neox_layers_30_mlp_dense_h_to_4h_q_weight2: R.Tensor((10240, 320), dtype="uint32") = packed_params[492]
            gpt_neox_layers_30_mlp_dense_h_to_4h_q_scale2: R.Tensor((10240, 80), dtype="float16") = packed_params[493]
            gpt_neox_layers_30_mlp_dense_h_to_4h_bias2: R.Tensor((10240,), dtype="float32") = packed_params[494]
            gpt_neox_layers_30_mlp_dense_4h_to_h_q_weight2: R.Tensor((2560, 1280), dtype="uint32") = packed_params[495]
            gpt_neox_layers_30_mlp_dense_4h_to_h_q_scale2: R.Tensor((2560, 320), dtype="float16") = packed_params[496]
            gpt_neox_layers_30_mlp_dense_4h_to_h_bias2: R.Tensor((2560,), dtype="float32") = packed_params[497]
            gpt_neox_layers_31_input_layernorm_weight2: R.Tensor((2560,), dtype="float16") = packed_params[498]
            gpt_neox_layers_31_input_layernorm_bias2: R.Tensor((2560,), dtype="float16") = packed_params[499]
            gpt_neox_layers_31_post_attention_layernorm_weight2: R.Tensor((2560,), dtype="float16") = packed_params[500]
            gpt_neox_layers_31_post_attention_layernorm_bias2: R.Tensor((2560,), dtype="float16") = packed_params[501]
            gpt_neox_layers_31_attention_query_key_value_q_weight2: R.Tensor((7680, 320), dtype="uint32") = packed_params[502]
            gpt_neox_layers_31_attention_query_key_value_q_scale2: R.Tensor((7680, 80), dtype="float16") = packed_params[503]
            gpt_neox_layers_31_attention_query_key_value_bias2: R.Tensor((7680,), dtype="float16") = packed_params[504]
            gpt_neox_layers_31_attention_dense_q_weight2: R.Tensor((2560, 320), dtype="uint32") = packed_params[505]
            gpt_neox_layers_31_attention_dense_q_scale2: R.Tensor((2560, 80), dtype="float16") = packed_params[506]
            gpt_neox_layers_31_attention_dense_bias2: R.Tensor((2560,), dtype="float16") = packed_params[507]
            gpt_neox_layers_31_mlp_dense_h_to_4h_q_weight2: R.Tensor((10240, 320), dtype="uint32") = packed_params[508]
            gpt_neox_layers_31_mlp_dense_h_to_4h_q_scale2: R.Tensor((10240, 80), dtype="float16") = packed_params[509]
            gpt_neox_layers_31_mlp_dense_h_to_4h_bias2: R.Tensor((10240,), dtype="float32") = packed_params[510]
            gpt_neox_layers_31_mlp_dense_4h_to_h_q_weight2: R.Tensor((2560, 1280), dtype="uint32") = packed_params[511]
            gpt_neox_layers_31_mlp_dense_4h_to_h_q_scale2: R.Tensor((2560, 320), dtype="float16") = packed_params[512]
            gpt_neox_layers_31_mlp_dense_4h_to_h_bias2: R.Tensor((2560,), dtype="float32") = packed_params[513]
            gpt_neox_final_layer_norm_weight2: R.Tensor((2560,), dtype="float16") = packed_params[514]
            gpt_neox_final_layer_norm_bias2: R.Tensor((2560,), dtype="float16") = packed_params[515]
            embed_out_q_weight2: R.Tensor((vocab_size, 320), dtype="uint32") = packed_params[516]
            embed_out_q_scale2: R.Tensor((vocab_size, 80), dtype="float16") = packed_params[517]
            layer_norm65 = R.call_tir(cls.layer_norm2, (input_embed, gpt_neox_layers_0_input_layernorm_weight2, gpt_neox_layers_0_input_layernorm_bias2), out_sinfo=R.Tensor((1, 1, 2560), dtype="float16"))
            lv192 = R.call_tir(cls.fused_dequantize1_fused_NT_matmul10_add10, (gpt_neox_layers_0_attention_query_key_value_q_weight2, gpt_neox_layers_0_attention_query_key_value_q_scale2, layer_norm65, gpt_neox_layers_0_attention_query_key_value_bias2), out_sinfo=R.Tensor((1, 1, 7680), dtype="float16"))
            lv388 = R.call_tir(cls.fused_reshape8_reshape9, (lv192,), out_sinfo=R.Tensor((1, 96, 80), dtype="float16"))
            lv164 = R.call_dps_packed("vm.builtin.attention_kv_cache_attention_with_fused_qkv", (paged_kv_cache, R.prim_value(0), R.prim_value(T.float32(1.0)), lv388), out_sinfo=R.Tensor((1, 32, 80), dtype="float16"))
            lv389 = R.call_tir(cls.fused_reshape10_reshape11, (lv164,), out_sinfo=R.Tensor((1, 1, 2560), dtype="float16"))
            lv192_1 = R.call_tir(cls.fused_dequantize2_fused_NT_matmul11_add11_add12, (gpt_neox_layers_0_attention_dense_q_weight2, gpt_neox_layers_0_attention_dense_q_scale2, lv389, gpt_neox_layers_0_attention_dense_bias2, input_embed), out_sinfo=R.Tensor((1, 1, 2560), dtype="float16"))
            layer_norm66 = R.call_tir(cls.layer_norm2, (lv192_1, gpt_neox_layers_0_post_attention_layernorm_weight2, gpt_neox_layers_0_post_attention_layernorm_bias2), out_sinfo=R.Tensor((1, 1, 2560), dtype="float16"))
            lv193 = R.call_tir(cls.fused_dequantize3_fused_NT_matmul12_add13_gelu2_cast6, (gpt_neox_layers_0_mlp_dense_h_to_4h_q_weight2, gpt_neox_layers_0_mlp_dense_h_to_4h_q_scale2, layer_norm66, gpt_neox_layers_0_mlp_dense_h_to_4h_bias2), out_sinfo=R.Tensor((1, 1, 10240), dtype="float16"))
            lv193_1 = R.call_tir(cls.fused_dequantize4_fused_NT_matmul13_add14_cast7_add12, (gpt_neox_layers_0_mlp_dense_4h_to_h_q_weight2, gpt_neox_layers_0_mlp_dense_4h_to_h_q_scale2, lv193, gpt_neox_layers_0_mlp_dense_4h_to_h_bias2, lv192_1), out_sinfo=R.Tensor((1, 1, 2560), dtype="float16"))
            layer_norm67 = R.call_tir(cls.layer_norm2, (lv193_1, gpt_neox_layers_1_input_layernorm_weight2, gpt_neox_layers_1_input_layernorm_bias2), out_sinfo=R.Tensor((1, 1, 2560), dtype="float16"))
            lv194 = R.call_tir(cls.fused_dequantize1_fused_NT_matmul10_add10, (gpt_neox_layers_1_attention_query_key_value_q_weight2, gpt_neox_layers_1_attention_query_key_value_q_scale2, layer_norm67, gpt_neox_layers_1_attention_query_key_value_bias2), out_sinfo=R.Tensor((1, 1, 7680), dtype="float16"))
            lv394 = R.call_tir(cls.fused_reshape8_reshape9, (lv194,), out_sinfo=R.Tensor((1, 96, 80), dtype="float16"))
            lv169 = R.call_dps_packed("vm.builtin.attention_kv_cache_attention_with_fused_qkv", (paged_kv_cache, R.prim_value(1), R.prim_value(T.float32(1.0)), lv394), out_sinfo=R.Tensor((1, 32, 80), dtype="float16"))
            lv395 = R.call_tir(cls.fused_reshape10_reshape11, (lv169,), out_sinfo=R.Tensor((1, 1, 2560), dtype="float16"))
            lv194_1 = R.call_tir(cls.fused_dequantize2_fused_NT_matmul11_add11_add12, (gpt_neox_layers_1_attention_dense_q_weight2, gpt_neox_layers_1_attention_dense_q_scale2, lv395, gpt_neox_layers_1_attention_dense_bias2, lv193_1), out_sinfo=R.Tensor((1, 1, 2560), dtype="float16"))
            layer_norm68 = R.call_tir(cls.layer_norm2, (lv194_1, gpt_neox_layers_1_post_attention_layernorm_weight2, gpt_neox_layers_1_post_attention_layernorm_bias2), out_sinfo=R.Tensor((1, 1, 2560), dtype="float16"))
            lv195 = R.call_tir(cls.fused_dequantize3_fused_NT_matmul12_add13_gelu2_cast6, (gpt_neox_layers_1_mlp_dense_h_to_4h_q_weight2, gpt_neox_layers_1_mlp_dense_h_to_4h_q_scale2, layer_norm68, gpt_neox_layers_1_mlp_dense_h_to_4h_bias2), out_sinfo=R.Tensor((1, 1, 10240), dtype="float16"))
            lv195_1 = R.call_tir(cls.fused_dequantize4_fused_NT_matmul13_add14_cast7_add12, (gpt_neox_layers_1_mlp_dense_4h_to_h_q_weight2, gpt_neox_layers_1_mlp_dense_4h_to_h_q_scale2, lv195, gpt_neox_layers_1_mlp_dense_4h_to_h_bias2, lv194_1), out_sinfo=R.Tensor((1, 1, 2560), dtype="float16"))
            layer_norm69 = R.call_tir(cls.layer_norm2, (lv195_1, gpt_neox_layers_2_input_layernorm_weight2, gpt_neox_layers_2_input_layernorm_bias2), out_sinfo=R.Tensor((1, 1, 2560), dtype="float16"))
            lv196 = R.call_tir(cls.fused_dequantize1_fused_NT_matmul10_add10, (gpt_neox_layers_2_attention_query_key_value_q_weight2, gpt_neox_layers_2_attention_query_key_value_q_scale2, layer_norm69, gpt_neox_layers_2_attention_query_key_value_bias2), out_sinfo=R.Tensor((1, 1, 7680), dtype="float16"))
            lv400 = R.call_tir(cls.fused_reshape8_reshape9, (lv196,), out_sinfo=R.Tensor((1, 96, 80), dtype="float16"))
            lv174 = R.call_dps_packed("vm.builtin.attention_kv_cache_attention_with_fused_qkv", (paged_kv_cache, R.prim_value(2), R.prim_value(T.float32(1.0)), lv400), out_sinfo=R.Tensor((1, 32, 80), dtype="float16"))
            lv401 = R.call_tir(cls.fused_reshape10_reshape11, (lv174,), out_sinfo=R.Tensor((1, 1, 2560), dtype="float16"))
            lv196_1 = R.call_tir(cls.fused_dequantize2_fused_NT_matmul11_add11_add12, (gpt_neox_layers_2_attention_dense_q_weight2, gpt_neox_layers_2_attention_dense_q_scale2, lv401, gpt_neox_layers_2_attention_dense_bias2, lv195_1), out_sinfo=R.Tensor((1, 1, 2560), dtype="float16"))
            layer_norm70 = R.call_tir(cls.layer_norm2, (lv196_1, gpt_neox_layers_2_post_attention_layernorm_weight2, gpt_neox_layers_2_post_attention_layernorm_bias2), out_sinfo=R.Tensor((1, 1, 2560), dtype="float16"))
            lv197 = R.call_tir(cls.fused_dequantize3_fused_NT_matmul12_add13_gelu2_cast6, (gpt_neox_layers_2_mlp_dense_h_to_4h_q_weight2, gpt_neox_layers_2_mlp_dense_h_to_4h_q_scale2, layer_norm70, gpt_neox_layers_2_mlp_dense_h_to_4h_bias2), out_sinfo=R.Tensor((1, 1, 10240), dtype="float16"))
            lv197_1 = R.call_tir(cls.fused_dequantize4_fused_NT_matmul13_add14_cast7_add12, (gpt_neox_layers_2_mlp_dense_4h_to_h_q_weight2, gpt_neox_layers_2_mlp_dense_4h_to_h_q_scale2, lv197, gpt_neox_layers_2_mlp_dense_4h_to_h_bias2, lv196_1), out_sinfo=R.Tensor((1, 1, 2560), dtype="float16"))
            layer_norm71 = R.call_tir(cls.layer_norm2, (lv197_1, gpt_neox_layers_3_input_layernorm_weight2, gpt_neox_layers_3_input_layernorm_bias2), out_sinfo=R.Tensor((1, 1, 2560), dtype="float16"))
            lv198 = R.call_tir(cls.fused_dequantize1_fused_NT_matmul10_add10, (gpt_neox_layers_3_attention_query_key_value_q_weight2, gpt_neox_layers_3_attention_query_key_value_q_scale2, layer_norm71, gpt_neox_layers_3_attention_query_key_value_bias2), out_sinfo=R.Tensor((1, 1, 7680), dtype="float16"))
            lv406 = R.call_tir(cls.fused_reshape8_reshape9, (lv198,), out_sinfo=R.Tensor((1, 96, 80), dtype="float16"))
            lv179 = R.call_dps_packed("vm.builtin.attention_kv_cache_attention_with_fused_qkv", (paged_kv_cache, R.prim_value(3), R.prim_value(T.float32(1.0)), lv406), out_sinfo=R.Tensor((1, 32, 80), dtype="float16"))
            lv407 = R.call_tir(cls.fused_reshape10_reshape11, (lv179,), out_sinfo=R.Tensor((1, 1, 2560), dtype="float16"))
            lv198_1 = R.call_tir(cls.fused_dequantize2_fused_NT_matmul11_add11_add12, (gpt_neox_layers_3_attention_dense_q_weight2, gpt_neox_layers_3_attention_dense_q_scale2, lv407, gpt_neox_layers_3_attention_dense_bias2, lv197_1), out_sinfo=R.Tensor((1, 1, 2560), dtype="float16"))
            layer_norm72 = R.call_tir(cls.layer_norm2, (lv198_1, gpt_neox_layers_3_post_attention_layernorm_weight2, gpt_neox_layers_3_post_attention_layernorm_bias2), out_sinfo=R.Tensor((1, 1, 2560), dtype="float16"))
            lv199 = R.call_tir(cls.fused_dequantize3_fused_NT_matmul12_add13_gelu2_cast6, (gpt_neox_layers_3_mlp_dense_h_to_4h_q_weight2, gpt_neox_layers_3_mlp_dense_h_to_4h_q_scale2, layer_norm72, gpt_neox_layers_3_mlp_dense_h_to_4h_bias2), out_sinfo=R.Tensor((1, 1, 10240), dtype="float16"))
            lv199_1 = R.call_tir(cls.fused_dequantize4_fused_NT_matmul13_add14_cast7_add12, (gpt_neox_layers_3_mlp_dense_4h_to_h_q_weight2, gpt_neox_layers_3_mlp_dense_4h_to_h_q_scale2, lv199, gpt_neox_layers_3_mlp_dense_4h_to_h_bias2, lv198_1), out_sinfo=R.Tensor((1, 1, 2560), dtype="float16"))
            layer_norm73 = R.call_tir(cls.layer_norm2, (lv199_1, gpt_neox_layers_4_input_layernorm_weight2, gpt_neox_layers_4_input_layernorm_bias2), out_sinfo=R.Tensor((1, 1, 2560), dtype="float16"))
            lv200 = R.call_tir(cls.fused_dequantize1_fused_NT_matmul10_add10, (gpt_neox_layers_4_attention_query_key_value_q_weight2, gpt_neox_layers_4_attention_query_key_value_q_scale2, layer_norm73, gpt_neox_layers_4_attention_query_key_value_bias2), out_sinfo=R.Tensor((1, 1, 7680), dtype="float16"))
            lv412 = R.call_tir(cls.fused_reshape8_reshape9, (lv200,), out_sinfo=R.Tensor((1, 96, 80), dtype="float16"))
            lv184 = R.call_dps_packed("vm.builtin.attention_kv_cache_attention_with_fused_qkv", (paged_kv_cache, R.prim_value(4), R.prim_value(T.float32(1.0)), lv412), out_sinfo=R.Tensor((1, 32, 80), dtype="float16"))
            lv413 = R.call_tir(cls.fused_reshape10_reshape11, (lv184,), out_sinfo=R.Tensor((1, 1, 2560), dtype="float16"))
            lv200_1 = R.call_tir(cls.fused_dequantize2_fused_NT_matmul11_add11_add12, (gpt_neox_layers_4_attention_dense_q_weight2, gpt_neox_layers_4_attention_dense_q_scale2, lv413, gpt_neox_layers_4_attention_dense_bias2, lv199_1), out_sinfo=R.Tensor((1, 1, 2560), dtype="float16"))
            layer_norm74 = R.call_tir(cls.layer_norm2, (lv200_1, gpt_neox_layers_4_post_attention_layernorm_weight2, gpt_neox_layers_4_post_attention_layernorm_bias2), out_sinfo=R.Tensor((1, 1, 2560), dtype="float16"))
            lv201 = R.call_tir(cls.fused_dequantize3_fused_NT_matmul12_add13_gelu2_cast6, (gpt_neox_layers_4_mlp_dense_h_to_4h_q_weight2, gpt_neox_layers_4_mlp_dense_h_to_4h_q_scale2, layer_norm74, gpt_neox_layers_4_mlp_dense_h_to_4h_bias2), out_sinfo=R.Tensor((1, 1, 10240), dtype="float16"))
            lv201_1 = R.call_tir(cls.fused_dequantize4_fused_NT_matmul13_add14_cast7_add12, (gpt_neox_layers_4_mlp_dense_4h_to_h_q_weight2, gpt_neox_layers_4_mlp_dense_4h_to_h_q_scale2, lv201, gpt_neox_layers_4_mlp_dense_4h_to_h_bias2, lv200_1), out_sinfo=R.Tensor((1, 1, 2560), dtype="float16"))
            layer_norm75 = R.call_tir(cls.layer_norm2, (lv201_1, gpt_neox_layers_5_input_layernorm_weight2, gpt_neox_layers_5_input_layernorm_bias2), out_sinfo=R.Tensor((1, 1, 2560), dtype="float16"))
            lv202 = R.call_tir(cls.fused_dequantize1_fused_NT_matmul10_add10, (gpt_neox_layers_5_attention_query_key_value_q_weight2, gpt_neox_layers_5_attention_query_key_value_q_scale2, layer_norm75, gpt_neox_layers_5_attention_query_key_value_bias2), out_sinfo=R.Tensor((1, 1, 7680), dtype="float16"))
            lv418 = R.call_tir(cls.fused_reshape8_reshape9, (lv202,), out_sinfo=R.Tensor((1, 96, 80), dtype="float16"))
            lv189 = R.call_dps_packed("vm.builtin.attention_kv_cache_attention_with_fused_qkv", (paged_kv_cache, R.prim_value(5), R.prim_value(T.float32(1.0)), lv418), out_sinfo=R.Tensor((1, 32, 80), dtype="float16"))
            lv419 = R.call_tir(cls.fused_reshape10_reshape11, (lv189,), out_sinfo=R.Tensor((1, 1, 2560), dtype="float16"))
            lv202_1 = R.call_tir(cls.fused_dequantize2_fused_NT_matmul11_add11_add12, (gpt_neox_layers_5_attention_dense_q_weight2, gpt_neox_layers_5_attention_dense_q_scale2, lv419, gpt_neox_layers_5_attention_dense_bias2, lv201_1), out_sinfo=R.Tensor((1, 1, 2560), dtype="float16"))
            layer_norm76 = R.call_tir(cls.layer_norm2, (lv202_1, gpt_neox_layers_5_post_attention_layernorm_weight2, gpt_neox_layers_5_post_attention_layernorm_bias2), out_sinfo=R.Tensor((1, 1, 2560), dtype="float16"))
            lv203 = R.call_tir(cls.fused_dequantize3_fused_NT_matmul12_add13_gelu2_cast6, (gpt_neox_layers_5_mlp_dense_h_to_4h_q_weight2, gpt_neox_layers_5_mlp_dense_h_to_4h_q_scale2, layer_norm76, gpt_neox_layers_5_mlp_dense_h_to_4h_bias2), out_sinfo=R.Tensor((1, 1, 10240), dtype="float16"))
            lv203_1 = R.call_tir(cls.fused_dequantize4_fused_NT_matmul13_add14_cast7_add12, (gpt_neox_layers_5_mlp_dense_4h_to_h_q_weight2, gpt_neox_layers_5_mlp_dense_4h_to_h_q_scale2, lv203, gpt_neox_layers_5_mlp_dense_4h_to_h_bias2, lv202_1), out_sinfo=R.Tensor((1, 1, 2560), dtype="float16"))
            layer_norm77 = R.call_tir(cls.layer_norm2, (lv203_1, gpt_neox_layers_6_input_layernorm_weight2, gpt_neox_layers_6_input_layernorm_bias2), out_sinfo=R.Tensor((1, 1, 2560), dtype="float16"))
            lv204 = R.call_tir(cls.fused_dequantize1_fused_NT_matmul10_add10, (gpt_neox_layers_6_attention_query_key_value_q_weight2, gpt_neox_layers_6_attention_query_key_value_q_scale2, layer_norm77, gpt_neox_layers_6_attention_query_key_value_bias2), out_sinfo=R.Tensor((1, 1, 7680), dtype="float16"))
            lv424 = R.call_tir(cls.fused_reshape8_reshape9, (lv204,), out_sinfo=R.Tensor((1, 96, 80), dtype="float16"))
            lv194_2 = R.call_dps_packed("vm.builtin.attention_kv_cache_attention_with_fused_qkv", (paged_kv_cache, R.prim_value(6), R.prim_value(T.float32(1.0)), lv424), out_sinfo=R.Tensor((1, 32, 80), dtype="float16"))
            lv425 = R.call_tir(cls.fused_reshape10_reshape11, (lv194_2,), out_sinfo=R.Tensor((1, 1, 2560), dtype="float16"))
            lv204_1 = R.call_tir(cls.fused_dequantize2_fused_NT_matmul11_add11_add12, (gpt_neox_layers_6_attention_dense_q_weight2, gpt_neox_layers_6_attention_dense_q_scale2, lv425, gpt_neox_layers_6_attention_dense_bias2, lv203_1), out_sinfo=R.Tensor((1, 1, 2560), dtype="float16"))
            layer_norm78 = R.call_tir(cls.layer_norm2, (lv204_1, gpt_neox_layers_6_post_attention_layernorm_weight2, gpt_neox_layers_6_post_attention_layernorm_bias2), out_sinfo=R.Tensor((1, 1, 2560), dtype="float16"))
            lv205 = R.call_tir(cls.fused_dequantize3_fused_NT_matmul12_add13_gelu2_cast6, (gpt_neox_layers_6_mlp_dense_h_to_4h_q_weight2, gpt_neox_layers_6_mlp_dense_h_to_4h_q_scale2, layer_norm78, gpt_neox_layers_6_mlp_dense_h_to_4h_bias2), out_sinfo=R.Tensor((1, 1, 10240), dtype="float16"))
            lv205_1 = R.call_tir(cls.fused_dequantize4_fused_NT_matmul13_add14_cast7_add12, (gpt_neox_layers_6_mlp_dense_4h_to_h_q_weight2, gpt_neox_layers_6_mlp_dense_4h_to_h_q_scale2, lv205, gpt_neox_layers_6_mlp_dense_4h_to_h_bias2, lv204_1), out_sinfo=R.Tensor((1, 1, 2560), dtype="float16"))
            layer_norm79 = R.call_tir(cls.layer_norm2, (lv205_1, gpt_neox_layers_7_input_layernorm_weight2, gpt_neox_layers_7_input_layernorm_bias2), out_sinfo=R.Tensor((1, 1, 2560), dtype="float16"))
            lv206 = R.call_tir(cls.fused_dequantize1_fused_NT_matmul10_add10, (gpt_neox_layers_7_attention_query_key_value_q_weight2, gpt_neox_layers_7_attention_query_key_value_q_scale2, layer_norm79, gpt_neox_layers_7_attention_query_key_value_bias2), out_sinfo=R.Tensor((1, 1, 7680), dtype="float16"))
            lv430 = R.call_tir(cls.fused_reshape8_reshape9, (lv206,), out_sinfo=R.Tensor((1, 96, 80), dtype="float16"))
            lv199_2 = R.call_dps_packed("vm.builtin.attention_kv_cache_attention_with_fused_qkv", (paged_kv_cache, R.prim_value(7), R.prim_value(T.float32(1.0)), lv430), out_sinfo=R.Tensor((1, 32, 80), dtype="float16"))
            lv431 = R.call_tir(cls.fused_reshape10_reshape11, (lv199_2,), out_sinfo=R.Tensor((1, 1, 2560), dtype="float16"))
            lv206_1 = R.call_tir(cls.fused_dequantize2_fused_NT_matmul11_add11_add12, (gpt_neox_layers_7_attention_dense_q_weight2, gpt_neox_layers_7_attention_dense_q_scale2, lv431, gpt_neox_layers_7_attention_dense_bias2, lv205_1), out_sinfo=R.Tensor((1, 1, 2560), dtype="float16"))
            layer_norm80 = R.call_tir(cls.layer_norm2, (lv206_1, gpt_neox_layers_7_post_attention_layernorm_weight2, gpt_neox_layers_7_post_attention_layernorm_bias2), out_sinfo=R.Tensor((1, 1, 2560), dtype="float16"))
            lv207 = R.call_tir(cls.fused_dequantize3_fused_NT_matmul12_add13_gelu2_cast6, (gpt_neox_layers_7_mlp_dense_h_to_4h_q_weight2, gpt_neox_layers_7_mlp_dense_h_to_4h_q_scale2, layer_norm80, gpt_neox_layers_7_mlp_dense_h_to_4h_bias2), out_sinfo=R.Tensor((1, 1, 10240), dtype="float16"))
            lv207_1 = R.call_tir(cls.fused_dequantize4_fused_NT_matmul13_add14_cast7_add12, (gpt_neox_layers_7_mlp_dense_4h_to_h_q_weight2, gpt_neox_layers_7_mlp_dense_4h_to_h_q_scale2, lv207, gpt_neox_layers_7_mlp_dense_4h_to_h_bias2, lv206_1), out_sinfo=R.Tensor((1, 1, 2560), dtype="float16"))
            layer_norm81 = R.call_tir(cls.layer_norm2, (lv207_1, gpt_neox_layers_8_input_layernorm_weight2, gpt_neox_layers_8_input_layernorm_bias2), out_sinfo=R.Tensor((1, 1, 2560), dtype="float16"))
            lv208 = R.call_tir(cls.fused_dequantize1_fused_NT_matmul10_add10, (gpt_neox_layers_8_attention_query_key_value_q_weight2, gpt_neox_layers_8_attention_query_key_value_q_scale2, layer_norm81, gpt_neox_layers_8_attention_query_key_value_bias2), out_sinfo=R.Tensor((1, 1, 7680), dtype="float16"))
            lv436 = R.call_tir(cls.fused_reshape8_reshape9, (lv208,), out_sinfo=R.Tensor((1, 96, 80), dtype="float16"))
            lv204_2 = R.call_dps_packed("vm.builtin.attention_kv_cache_attention_with_fused_qkv", (paged_kv_cache, R.prim_value(8), R.prim_value(T.float32(1.0)), lv436), out_sinfo=R.Tensor((1, 32, 80), dtype="float16"))
            lv437 = R.call_tir(cls.fused_reshape10_reshape11, (lv204_2,), out_sinfo=R.Tensor((1, 1, 2560), dtype="float16"))
            lv208_1 = R.call_tir(cls.fused_dequantize2_fused_NT_matmul11_add11_add12, (gpt_neox_layers_8_attention_dense_q_weight2, gpt_neox_layers_8_attention_dense_q_scale2, lv437, gpt_neox_layers_8_attention_dense_bias2, lv207_1), out_sinfo=R.Tensor((1, 1, 2560), dtype="float16"))
            layer_norm82 = R.call_tir(cls.layer_norm2, (lv208_1, gpt_neox_layers_8_post_attention_layernorm_weight2, gpt_neox_layers_8_post_attention_layernorm_bias2), out_sinfo=R.Tensor((1, 1, 2560), dtype="float16"))
            lv209 = R.call_tir(cls.fused_dequantize3_fused_NT_matmul12_add13_gelu2_cast6, (gpt_neox_layers_8_mlp_dense_h_to_4h_q_weight2, gpt_neox_layers_8_mlp_dense_h_to_4h_q_scale2, layer_norm82, gpt_neox_layers_8_mlp_dense_h_to_4h_bias2), out_sinfo=R.Tensor((1, 1, 10240), dtype="float16"))
            lv209_1 = R.call_tir(cls.fused_dequantize4_fused_NT_matmul13_add14_cast7_add12, (gpt_neox_layers_8_mlp_dense_4h_to_h_q_weight2, gpt_neox_layers_8_mlp_dense_4h_to_h_q_scale2, lv209, gpt_neox_layers_8_mlp_dense_4h_to_h_bias2, lv208_1), out_sinfo=R.Tensor((1, 1, 2560), dtype="float16"))
            layer_norm83 = R.call_tir(cls.layer_norm2, (lv209_1, gpt_neox_layers_9_input_layernorm_weight2, gpt_neox_layers_9_input_layernorm_bias2), out_sinfo=R.Tensor((1, 1, 2560), dtype="float16"))
            lv210 = R.call_tir(cls.fused_dequantize1_fused_NT_matmul10_add10, (gpt_neox_layers_9_attention_query_key_value_q_weight2, gpt_neox_layers_9_attention_query_key_value_q_scale2, layer_norm83, gpt_neox_layers_9_attention_query_key_value_bias2), out_sinfo=R.Tensor((1, 1, 7680), dtype="float16"))
            lv442 = R.call_tir(cls.fused_reshape8_reshape9, (lv210,), out_sinfo=R.Tensor((1, 96, 80), dtype="float16"))
            lv209_2 = R.call_dps_packed("vm.builtin.attention_kv_cache_attention_with_fused_qkv", (paged_kv_cache, R.prim_value(9), R.prim_value(T.float32(1.0)), lv442), out_sinfo=R.Tensor((1, 32, 80), dtype="float16"))
            lv443 = R.call_tir(cls.fused_reshape10_reshape11, (lv209_2,), out_sinfo=R.Tensor((1, 1, 2560), dtype="float16"))
            lv210_1 = R.call_tir(cls.fused_dequantize2_fused_NT_matmul11_add11_add12, (gpt_neox_layers_9_attention_dense_q_weight2, gpt_neox_layers_9_attention_dense_q_scale2, lv443, gpt_neox_layers_9_attention_dense_bias2, lv209_1), out_sinfo=R.Tensor((1, 1, 2560), dtype="float16"))
            layer_norm84 = R.call_tir(cls.layer_norm2, (lv210_1, gpt_neox_layers_9_post_attention_layernorm_weight2, gpt_neox_layers_9_post_attention_layernorm_bias2), out_sinfo=R.Tensor((1, 1, 2560), dtype="float16"))
            lv211 = R.call_tir(cls.fused_dequantize3_fused_NT_matmul12_add13_gelu2_cast6, (gpt_neox_layers_9_mlp_dense_h_to_4h_q_weight2, gpt_neox_layers_9_mlp_dense_h_to_4h_q_scale2, layer_norm84, gpt_neox_layers_9_mlp_dense_h_to_4h_bias2), out_sinfo=R.Tensor((1, 1, 10240), dtype="float16"))
            lv211_1 = R.call_tir(cls.fused_dequantize4_fused_NT_matmul13_add14_cast7_add12, (gpt_neox_layers_9_mlp_dense_4h_to_h_q_weight2, gpt_neox_layers_9_mlp_dense_4h_to_h_q_scale2, lv211, gpt_neox_layers_9_mlp_dense_4h_to_h_bias2, lv210_1), out_sinfo=R.Tensor((1, 1, 2560), dtype="float16"))
            layer_norm85 = R.call_tir(cls.layer_norm2, (lv211_1, gpt_neox_layers_10_input_layernorm_weight2, gpt_neox_layers_10_input_layernorm_bias2), out_sinfo=R.Tensor((1, 1, 2560), dtype="float16"))
            lv212 = R.call_tir(cls.fused_dequantize1_fused_NT_matmul10_add10, (gpt_neox_layers_10_attention_query_key_value_q_weight2, gpt_neox_layers_10_attention_query_key_value_q_scale2, layer_norm85, gpt_neox_layers_10_attention_query_key_value_bias2), out_sinfo=R.Tensor((1, 1, 7680), dtype="float16"))
            lv448 = R.call_tir(cls.fused_reshape8_reshape9, (lv212,), out_sinfo=R.Tensor((1, 96, 80), dtype="float16"))
            lv214 = R.call_dps_packed("vm.builtin.attention_kv_cache_attention_with_fused_qkv", (paged_kv_cache, R.prim_value(10), R.prim_value(T.float32(1.0)), lv448), out_sinfo=R.Tensor((1, 32, 80), dtype="float16"))
            lv449 = R.call_tir(cls.fused_reshape10_reshape11, (lv214,), out_sinfo=R.Tensor((1, 1, 2560), dtype="float16"))
            lv212_1 = R.call_tir(cls.fused_dequantize2_fused_NT_matmul11_add11_add12, (gpt_neox_layers_10_attention_dense_q_weight2, gpt_neox_layers_10_attention_dense_q_scale2, lv449, gpt_neox_layers_10_attention_dense_bias2, lv211_1), out_sinfo=R.Tensor((1, 1, 2560), dtype="float16"))
            layer_norm86 = R.call_tir(cls.layer_norm2, (lv212_1, gpt_neox_layers_10_post_attention_layernorm_weight2, gpt_neox_layers_10_post_attention_layernorm_bias2), out_sinfo=R.Tensor((1, 1, 2560), dtype="float16"))
            lv213 = R.call_tir(cls.fused_dequantize3_fused_NT_matmul12_add13_gelu2_cast6, (gpt_neox_layers_10_mlp_dense_h_to_4h_q_weight2, gpt_neox_layers_10_mlp_dense_h_to_4h_q_scale2, layer_norm86, gpt_neox_layers_10_mlp_dense_h_to_4h_bias2), out_sinfo=R.Tensor((1, 1, 10240), dtype="float16"))
            lv213_1 = R.call_tir(cls.fused_dequantize4_fused_NT_matmul13_add14_cast7_add12, (gpt_neox_layers_10_mlp_dense_4h_to_h_q_weight2, gpt_neox_layers_10_mlp_dense_4h_to_h_q_scale2, lv213, gpt_neox_layers_10_mlp_dense_4h_to_h_bias2, lv212_1), out_sinfo=R.Tensor((1, 1, 2560), dtype="float16"))
            layer_norm87 = R.call_tir(cls.layer_norm2, (lv213_1, gpt_neox_layers_11_input_layernorm_weight2, gpt_neox_layers_11_input_layernorm_bias2), out_sinfo=R.Tensor((1, 1, 2560), dtype="float16"))
            lv214_1 = R.call_tir(cls.fused_dequantize1_fused_NT_matmul10_add10, (gpt_neox_layers_11_attention_query_key_value_q_weight2, gpt_neox_layers_11_attention_query_key_value_q_scale2, layer_norm87, gpt_neox_layers_11_attention_query_key_value_bias2), out_sinfo=R.Tensor((1, 1, 7680), dtype="float16"))
            lv454 = R.call_tir(cls.fused_reshape8_reshape9, (lv214_1,), out_sinfo=R.Tensor((1, 96, 80), dtype="float16"))
            lv219 = R.call_dps_packed("vm.builtin.attention_kv_cache_attention_with_fused_qkv", (paged_kv_cache, R.prim_value(11), R.prim_value(T.float32(1.0)), lv454), out_sinfo=R.Tensor((1, 32, 80), dtype="float16"))
            lv455 = R.call_tir(cls.fused_reshape10_reshape11, (lv219,), out_sinfo=R.Tensor((1, 1, 2560), dtype="float16"))
            lv214_2 = R.call_tir(cls.fused_dequantize2_fused_NT_matmul11_add11_add12, (gpt_neox_layers_11_attention_dense_q_weight2, gpt_neox_layers_11_attention_dense_q_scale2, lv455, gpt_neox_layers_11_attention_dense_bias2, lv213_1), out_sinfo=R.Tensor((1, 1, 2560), dtype="float16"))
            layer_norm88 = R.call_tir(cls.layer_norm2, (lv214_2, gpt_neox_layers_11_post_attention_layernorm_weight2, gpt_neox_layers_11_post_attention_layernorm_bias2), out_sinfo=R.Tensor((1, 1, 2560), dtype="float16"))
            lv215 = R.call_tir(cls.fused_dequantize3_fused_NT_matmul12_add13_gelu2_cast6, (gpt_neox_layers_11_mlp_dense_h_to_4h_q_weight2, gpt_neox_layers_11_mlp_dense_h_to_4h_q_scale2, layer_norm88, gpt_neox_layers_11_mlp_dense_h_to_4h_bias2), out_sinfo=R.Tensor((1, 1, 10240), dtype="float16"))
            lv215_1 = R.call_tir(cls.fused_dequantize4_fused_NT_matmul13_add14_cast7_add12, (gpt_neox_layers_11_mlp_dense_4h_to_h_q_weight2, gpt_neox_layers_11_mlp_dense_4h_to_h_q_scale2, lv215, gpt_neox_layers_11_mlp_dense_4h_to_h_bias2, lv214_2), out_sinfo=R.Tensor((1, 1, 2560), dtype="float16"))
            layer_norm89 = R.call_tir(cls.layer_norm2, (lv215_1, gpt_neox_layers_12_input_layernorm_weight2, gpt_neox_layers_12_input_layernorm_bias2), out_sinfo=R.Tensor((1, 1, 2560), dtype="float16"))
            lv216 = R.call_tir(cls.fused_dequantize1_fused_NT_matmul10_add10, (gpt_neox_layers_12_attention_query_key_value_q_weight2, gpt_neox_layers_12_attention_query_key_value_q_scale2, layer_norm89, gpt_neox_layers_12_attention_query_key_value_bias2), out_sinfo=R.Tensor((1, 1, 7680), dtype="float16"))
            lv460 = R.call_tir(cls.fused_reshape8_reshape9, (lv216,), out_sinfo=R.Tensor((1, 96, 80), dtype="float16"))
            lv224 = R.call_dps_packed("vm.builtin.attention_kv_cache_attention_with_fused_qkv", (paged_kv_cache, R.prim_value(12), R.prim_value(T.float32(1.0)), lv460), out_sinfo=R.Tensor((1, 32, 80), dtype="float16"))
            lv461 = R.call_tir(cls.fused_reshape10_reshape11, (lv224,), out_sinfo=R.Tensor((1, 1, 2560), dtype="float16"))
            lv216_1 = R.call_tir(cls.fused_dequantize2_fused_NT_matmul11_add11_add12, (gpt_neox_layers_12_attention_dense_q_weight2, gpt_neox_layers_12_attention_dense_q_scale2, lv461, gpt_neox_layers_12_attention_dense_bias2, lv215_1), out_sinfo=R.Tensor((1, 1, 2560), dtype="float16"))
            layer_norm90 = R.call_tir(cls.layer_norm2, (lv216_1, gpt_neox_layers_12_post_attention_layernorm_weight2, gpt_neox_layers_12_post_attention_layernorm_bias2), out_sinfo=R.Tensor((1, 1, 2560), dtype="float16"))
            lv217 = R.call_tir(cls.fused_dequantize3_fused_NT_matmul12_add13_gelu2_cast6, (gpt_neox_layers_12_mlp_dense_h_to_4h_q_weight2, gpt_neox_layers_12_mlp_dense_h_to_4h_q_scale2, layer_norm90, gpt_neox_layers_12_mlp_dense_h_to_4h_bias2), out_sinfo=R.Tensor((1, 1, 10240), dtype="float16"))
            lv217_1 = R.call_tir(cls.fused_dequantize4_fused_NT_matmul13_add14_cast7_add12, (gpt_neox_layers_12_mlp_dense_4h_to_h_q_weight2, gpt_neox_layers_12_mlp_dense_4h_to_h_q_scale2, lv217, gpt_neox_layers_12_mlp_dense_4h_to_h_bias2, lv216_1), out_sinfo=R.Tensor((1, 1, 2560), dtype="float16"))
            layer_norm91 = R.call_tir(cls.layer_norm2, (lv217_1, gpt_neox_layers_13_input_layernorm_weight2, gpt_neox_layers_13_input_layernorm_bias2), out_sinfo=R.Tensor((1, 1, 2560), dtype="float16"))
            lv218 = R.call_tir(cls.fused_dequantize1_fused_NT_matmul10_add10, (gpt_neox_layers_13_attention_query_key_value_q_weight2, gpt_neox_layers_13_attention_query_key_value_q_scale2, layer_norm91, gpt_neox_layers_13_attention_query_key_value_bias2), out_sinfo=R.Tensor((1, 1, 7680), dtype="float16"))
            lv466 = R.call_tir(cls.fused_reshape8_reshape9, (lv218,), out_sinfo=R.Tensor((1, 96, 80), dtype="float16"))
            lv229 = R.call_dps_packed("vm.builtin.attention_kv_cache_attention_with_fused_qkv", (paged_kv_cache, R.prim_value(13), R.prim_value(T.float32(1.0)), lv466), out_sinfo=R.Tensor((1, 32, 80), dtype="float16"))
            lv467 = R.call_tir(cls.fused_reshape10_reshape11, (lv229,), out_sinfo=R.Tensor((1, 1, 2560), dtype="float16"))
            lv218_1 = R.call_tir(cls.fused_dequantize2_fused_NT_matmul11_add11_add12, (gpt_neox_layers_13_attention_dense_q_weight2, gpt_neox_layers_13_attention_dense_q_scale2, lv467, gpt_neox_layers_13_attention_dense_bias2, lv217_1), out_sinfo=R.Tensor((1, 1, 2560), dtype="float16"))
            layer_norm92 = R.call_tir(cls.layer_norm2, (lv218_1, gpt_neox_layers_13_post_attention_layernorm_weight2, gpt_neox_layers_13_post_attention_layernorm_bias2), out_sinfo=R.Tensor((1, 1, 2560), dtype="float16"))
            lv219_1 = R.call_tir(cls.fused_dequantize3_fused_NT_matmul12_add13_gelu2_cast6, (gpt_neox_layers_13_mlp_dense_h_to_4h_q_weight2, gpt_neox_layers_13_mlp_dense_h_to_4h_q_scale2, layer_norm92, gpt_neox_layers_13_mlp_dense_h_to_4h_bias2), out_sinfo=R.Tensor((1, 1, 10240), dtype="float16"))
            lv219_2 = R.call_tir(cls.fused_dequantize4_fused_NT_matmul13_add14_cast7_add12, (gpt_neox_layers_13_mlp_dense_4h_to_h_q_weight2, gpt_neox_layers_13_mlp_dense_4h_to_h_q_scale2, lv219_1, gpt_neox_layers_13_mlp_dense_4h_to_h_bias2, lv218_1), out_sinfo=R.Tensor((1, 1, 2560), dtype="float16"))
            layer_norm93 = R.call_tir(cls.layer_norm2, (lv219_2, gpt_neox_layers_14_input_layernorm_weight2, gpt_neox_layers_14_input_layernorm_bias2), out_sinfo=R.Tensor((1, 1, 2560), dtype="float16"))
            lv220 = R.call_tir(cls.fused_dequantize1_fused_NT_matmul10_add10, (gpt_neox_layers_14_attention_query_key_value_q_weight2, gpt_neox_layers_14_attention_query_key_value_q_scale2, layer_norm93, gpt_neox_layers_14_attention_query_key_value_bias2), out_sinfo=R.Tensor((1, 1, 7680), dtype="float16"))
            lv472 = R.call_tir(cls.fused_reshape8_reshape9, (lv220,), out_sinfo=R.Tensor((1, 96, 80), dtype="float16"))
            lv234 = R.call_dps_packed("vm.builtin.attention_kv_cache_attention_with_fused_qkv", (paged_kv_cache, R.prim_value(14), R.prim_value(T.float32(1.0)), lv472), out_sinfo=R.Tensor((1, 32, 80), dtype="float16"))
            lv473 = R.call_tir(cls.fused_reshape10_reshape11, (lv234,), out_sinfo=R.Tensor((1, 1, 2560), dtype="float16"))
            lv220_1 = R.call_tir(cls.fused_dequantize2_fused_NT_matmul11_add11_add12, (gpt_neox_layers_14_attention_dense_q_weight2, gpt_neox_layers_14_attention_dense_q_scale2, lv473, gpt_neox_layers_14_attention_dense_bias2, lv219_2), out_sinfo=R.Tensor((1, 1, 2560), dtype="float16"))
            layer_norm94 = R.call_tir(cls.layer_norm2, (lv220_1, gpt_neox_layers_14_post_attention_layernorm_weight2, gpt_neox_layers_14_post_attention_layernorm_bias2), out_sinfo=R.Tensor((1, 1, 2560), dtype="float16"))
            lv221 = R.call_tir(cls.fused_dequantize3_fused_NT_matmul12_add13_gelu2_cast6, (gpt_neox_layers_14_mlp_dense_h_to_4h_q_weight2, gpt_neox_layers_14_mlp_dense_h_to_4h_q_scale2, layer_norm94, gpt_neox_layers_14_mlp_dense_h_to_4h_bias2), out_sinfo=R.Tensor((1, 1, 10240), dtype="float16"))
            lv221_1 = R.call_tir(cls.fused_dequantize4_fused_NT_matmul13_add14_cast7_add12, (gpt_neox_layers_14_mlp_dense_4h_to_h_q_weight2, gpt_neox_layers_14_mlp_dense_4h_to_h_q_scale2, lv221, gpt_neox_layers_14_mlp_dense_4h_to_h_bias2, lv220_1), out_sinfo=R.Tensor((1, 1, 2560), dtype="float16"))
            layer_norm95 = R.call_tir(cls.layer_norm2, (lv221_1, gpt_neox_layers_15_input_layernorm_weight2, gpt_neox_layers_15_input_layernorm_bias2), out_sinfo=R.Tensor((1, 1, 2560), dtype="float16"))
            lv222 = R.call_tir(cls.fused_dequantize1_fused_NT_matmul10_add10, (gpt_neox_layers_15_attention_query_key_value_q_weight2, gpt_neox_layers_15_attention_query_key_value_q_scale2, layer_norm95, gpt_neox_layers_15_attention_query_key_value_bias2), out_sinfo=R.Tensor((1, 1, 7680), dtype="float16"))
            lv478 = R.call_tir(cls.fused_reshape8_reshape9, (lv222,), out_sinfo=R.Tensor((1, 96, 80), dtype="float16"))
            lv239 = R.call_dps_packed("vm.builtin.attention_kv_cache_attention_with_fused_qkv", (paged_kv_cache, R.prim_value(15), R.prim_value(T.float32(1.0)), lv478), out_sinfo=R.Tensor((1, 32, 80), dtype="float16"))
            lv479 = R.call_tir(cls.fused_reshape10_reshape11, (lv239,), out_sinfo=R.Tensor((1, 1, 2560), dtype="float16"))
            lv222_1 = R.call_tir(cls.fused_dequantize2_fused_NT_matmul11_add11_add12, (gpt_neox_layers_15_attention_dense_q_weight2, gpt_neox_layers_15_attention_dense_q_scale2, lv479, gpt_neox_layers_15_attention_dense_bias2, lv221_1), out_sinfo=R.Tensor((1, 1, 2560), dtype="float16"))
            layer_norm96 = R.call_tir(cls.layer_norm2, (lv222_1, gpt_neox_layers_15_post_attention_layernorm_weight2, gpt_neox_layers_15_post_attention_layernorm_bias2), out_sinfo=R.Tensor((1, 1, 2560), dtype="float16"))
            lv223 = R.call_tir(cls.fused_dequantize3_fused_NT_matmul12_add13_gelu2_cast6, (gpt_neox_layers_15_mlp_dense_h_to_4h_q_weight2, gpt_neox_layers_15_mlp_dense_h_to_4h_q_scale2, layer_norm96, gpt_neox_layers_15_mlp_dense_h_to_4h_bias2), out_sinfo=R.Tensor((1, 1, 10240), dtype="float16"))
            lv223_1 = R.call_tir(cls.fused_dequantize4_fused_NT_matmul13_add14_cast7_add12, (gpt_neox_layers_15_mlp_dense_4h_to_h_q_weight2, gpt_neox_layers_15_mlp_dense_4h_to_h_q_scale2, lv223, gpt_neox_layers_15_mlp_dense_4h_to_h_bias2, lv222_1), out_sinfo=R.Tensor((1, 1, 2560), dtype="float16"))
            layer_norm97 = R.call_tir(cls.layer_norm2, (lv223_1, gpt_neox_layers_16_input_layernorm_weight2, gpt_neox_layers_16_input_layernorm_bias2), out_sinfo=R.Tensor((1, 1, 2560), dtype="float16"))
            lv224_1 = R.call_tir(cls.fused_dequantize1_fused_NT_matmul10_add10, (gpt_neox_layers_16_attention_query_key_value_q_weight2, gpt_neox_layers_16_attention_query_key_value_q_scale2, layer_norm97, gpt_neox_layers_16_attention_query_key_value_bias2), out_sinfo=R.Tensor((1, 1, 7680), dtype="float16"))
            lv484 = R.call_tir(cls.fused_reshape8_reshape9, (lv224_1,), out_sinfo=R.Tensor((1, 96, 80), dtype="float16"))
            lv244 = R.call_dps_packed("vm.builtin.attention_kv_cache_attention_with_fused_qkv", (paged_kv_cache, R.prim_value(16), R.prim_value(T.float32(1.0)), lv484), out_sinfo=R.Tensor((1, 32, 80), dtype="float16"))
            lv485 = R.call_tir(cls.fused_reshape10_reshape11, (lv244,), out_sinfo=R.Tensor((1, 1, 2560), dtype="float16"))
            lv224_2 = R.call_tir(cls.fused_dequantize2_fused_NT_matmul11_add11_add12, (gpt_neox_layers_16_attention_dense_q_weight2, gpt_neox_layers_16_attention_dense_q_scale2, lv485, gpt_neox_layers_16_attention_dense_bias2, lv223_1), out_sinfo=R.Tensor((1, 1, 2560), dtype="float16"))
            layer_norm98 = R.call_tir(cls.layer_norm2, (lv224_2, gpt_neox_layers_16_post_attention_layernorm_weight2, gpt_neox_layers_16_post_attention_layernorm_bias2), out_sinfo=R.Tensor((1, 1, 2560), dtype="float16"))
            lv225 = R.call_tir(cls.fused_dequantize3_fused_NT_matmul12_add13_gelu2_cast6, (gpt_neox_layers_16_mlp_dense_h_to_4h_q_weight2, gpt_neox_layers_16_mlp_dense_h_to_4h_q_scale2, layer_norm98, gpt_neox_layers_16_mlp_dense_h_to_4h_bias2), out_sinfo=R.Tensor((1, 1, 10240), dtype="float16"))
            lv225_1 = R.call_tir(cls.fused_dequantize4_fused_NT_matmul13_add14_cast7_add12, (gpt_neox_layers_16_mlp_dense_4h_to_h_q_weight2, gpt_neox_layers_16_mlp_dense_4h_to_h_q_scale2, lv225, gpt_neox_layers_16_mlp_dense_4h_to_h_bias2, lv224_2), out_sinfo=R.Tensor((1, 1, 2560), dtype="float16"))
            layer_norm99 = R.call_tir(cls.layer_norm2, (lv225_1, gpt_neox_layers_17_input_layernorm_weight2, gpt_neox_layers_17_input_layernorm_bias2), out_sinfo=R.Tensor((1, 1, 2560), dtype="float16"))
            lv226 = R.call_tir(cls.fused_dequantize1_fused_NT_matmul10_add10, (gpt_neox_layers_17_attention_query_key_value_q_weight2, gpt_neox_layers_17_attention_query_key_value_q_scale2, layer_norm99, gpt_neox_layers_17_attention_query_key_value_bias2), out_sinfo=R.Tensor((1, 1, 7680), dtype="float16"))
            lv490 = R.call_tir(cls.fused_reshape8_reshape9, (lv226,), out_sinfo=R.Tensor((1, 96, 80), dtype="float16"))
            lv249 = R.call_dps_packed("vm.builtin.attention_kv_cache_attention_with_fused_qkv", (paged_kv_cache, R.prim_value(17), R.prim_value(T.float32(1.0)), lv490), out_sinfo=R.Tensor((1, 32, 80), dtype="float16"))
            lv491 = R.call_tir(cls.fused_reshape10_reshape11, (lv249,), out_sinfo=R.Tensor((1, 1, 2560), dtype="float16"))
            lv226_1 = R.call_tir(cls.fused_dequantize2_fused_NT_matmul11_add11_add12, (gpt_neox_layers_17_attention_dense_q_weight2, gpt_neox_layers_17_attention_dense_q_scale2, lv491, gpt_neox_layers_17_attention_dense_bias2, lv225_1), out_sinfo=R.Tensor((1, 1, 2560), dtype="float16"))
            layer_norm100 = R.call_tir(cls.layer_norm2, (lv226_1, gpt_neox_layers_17_post_attention_layernorm_weight2, gpt_neox_layers_17_post_attention_layernorm_bias2), out_sinfo=R.Tensor((1, 1, 2560), dtype="float16"))
            lv227 = R.call_tir(cls.fused_dequantize3_fused_NT_matmul12_add13_gelu2_cast6, (gpt_neox_layers_17_mlp_dense_h_to_4h_q_weight2, gpt_neox_layers_17_mlp_dense_h_to_4h_q_scale2, layer_norm100, gpt_neox_layers_17_mlp_dense_h_to_4h_bias2), out_sinfo=R.Tensor((1, 1, 10240), dtype="float16"))
            lv227_1 = R.call_tir(cls.fused_dequantize4_fused_NT_matmul13_add14_cast7_add12, (gpt_neox_layers_17_mlp_dense_4h_to_h_q_weight2, gpt_neox_layers_17_mlp_dense_4h_to_h_q_scale2, lv227, gpt_neox_layers_17_mlp_dense_4h_to_h_bias2, lv226_1), out_sinfo=R.Tensor((1, 1, 2560), dtype="float16"))
            layer_norm101 = R.call_tir(cls.layer_norm2, (lv227_1, gpt_neox_layers_18_input_layernorm_weight2, gpt_neox_layers_18_input_layernorm_bias2), out_sinfo=R.Tensor((1, 1, 2560), dtype="float16"))
            lv228 = R.call_tir(cls.fused_dequantize1_fused_NT_matmul10_add10, (gpt_neox_layers_18_attention_query_key_value_q_weight2, gpt_neox_layers_18_attention_query_key_value_q_scale2, layer_norm101, gpt_neox_layers_18_attention_query_key_value_bias2), out_sinfo=R.Tensor((1, 1, 7680), dtype="float16"))
            lv496 = R.call_tir(cls.fused_reshape8_reshape9, (lv228,), out_sinfo=R.Tensor((1, 96, 80), dtype="float16"))
            lv254 = R.call_dps_packed("vm.builtin.attention_kv_cache_attention_with_fused_qkv", (paged_kv_cache, R.prim_value(18), R.prim_value(T.float32(1.0)), lv496), out_sinfo=R.Tensor((1, 32, 80), dtype="float16"))
            lv497 = R.call_tir(cls.fused_reshape10_reshape11, (lv254,), out_sinfo=R.Tensor((1, 1, 2560), dtype="float16"))
            lv228_1 = R.call_tir(cls.fused_dequantize2_fused_NT_matmul11_add11_add12, (gpt_neox_layers_18_attention_dense_q_weight2, gpt_neox_layers_18_attention_dense_q_scale2, lv497, gpt_neox_layers_18_attention_dense_bias2, lv227_1), out_sinfo=R.Tensor((1, 1, 2560), dtype="float16"))
            layer_norm102 = R.call_tir(cls.layer_norm2, (lv228_1, gpt_neox_layers_18_post_attention_layernorm_weight2, gpt_neox_layers_18_post_attention_layernorm_bias2), out_sinfo=R.Tensor((1, 1, 2560), dtype="float16"))
            lv229_1 = R.call_tir(cls.fused_dequantize3_fused_NT_matmul12_add13_gelu2_cast6, (gpt_neox_layers_18_mlp_dense_h_to_4h_q_weight2, gpt_neox_layers_18_mlp_dense_h_to_4h_q_scale2, layer_norm102, gpt_neox_layers_18_mlp_dense_h_to_4h_bias2), out_sinfo=R.Tensor((1, 1, 10240), dtype="float16"))
            lv229_2 = R.call_tir(cls.fused_dequantize4_fused_NT_matmul13_add14_cast7_add12, (gpt_neox_layers_18_mlp_dense_4h_to_h_q_weight2, gpt_neox_layers_18_mlp_dense_4h_to_h_q_scale2, lv229_1, gpt_neox_layers_18_mlp_dense_4h_to_h_bias2, lv228_1), out_sinfo=R.Tensor((1, 1, 2560), dtype="float16"))
            layer_norm103 = R.call_tir(cls.layer_norm2, (lv229_2, gpt_neox_layers_19_input_layernorm_weight2, gpt_neox_layers_19_input_layernorm_bias2), out_sinfo=R.Tensor((1, 1, 2560), dtype="float16"))
            lv230 = R.call_tir(cls.fused_dequantize1_fused_NT_matmul10_add10, (gpt_neox_layers_19_attention_query_key_value_q_weight2, gpt_neox_layers_19_attention_query_key_value_q_scale2, layer_norm103, gpt_neox_layers_19_attention_query_key_value_bias2), out_sinfo=R.Tensor((1, 1, 7680), dtype="float16"))
            lv502 = R.call_tir(cls.fused_reshape8_reshape9, (lv230,), out_sinfo=R.Tensor((1, 96, 80), dtype="float16"))
            lv259 = R.call_dps_packed("vm.builtin.attention_kv_cache_attention_with_fused_qkv", (paged_kv_cache, R.prim_value(19), R.prim_value(T.float32(1.0)), lv502), out_sinfo=R.Tensor((1, 32, 80), dtype="float16"))
            lv503 = R.call_tir(cls.fused_reshape10_reshape11, (lv259,), out_sinfo=R.Tensor((1, 1, 2560), dtype="float16"))
            lv230_1 = R.call_tir(cls.fused_dequantize2_fused_NT_matmul11_add11_add12, (gpt_neox_layers_19_attention_dense_q_weight2, gpt_neox_layers_19_attention_dense_q_scale2, lv503, gpt_neox_layers_19_attention_dense_bias2, lv229_2), out_sinfo=R.Tensor((1, 1, 2560), dtype="float16"))
            layer_norm104 = R.call_tir(cls.layer_norm2, (lv230_1, gpt_neox_layers_19_post_attention_layernorm_weight2, gpt_neox_layers_19_post_attention_layernorm_bias2), out_sinfo=R.Tensor((1, 1, 2560), dtype="float16"))
            lv231 = R.call_tir(cls.fused_dequantize3_fused_NT_matmul12_add13_gelu2_cast6, (gpt_neox_layers_19_mlp_dense_h_to_4h_q_weight2, gpt_neox_layers_19_mlp_dense_h_to_4h_q_scale2, layer_norm104, gpt_neox_layers_19_mlp_dense_h_to_4h_bias2), out_sinfo=R.Tensor((1, 1, 10240), dtype="float16"))
            lv231_1 = R.call_tir(cls.fused_dequantize4_fused_NT_matmul13_add14_cast7_add12, (gpt_neox_layers_19_mlp_dense_4h_to_h_q_weight2, gpt_neox_layers_19_mlp_dense_4h_to_h_q_scale2, lv231, gpt_neox_layers_19_mlp_dense_4h_to_h_bias2, lv230_1), out_sinfo=R.Tensor((1, 1, 2560), dtype="float16"))
            layer_norm105 = R.call_tir(cls.layer_norm2, (lv231_1, gpt_neox_layers_20_input_layernorm_weight2, gpt_neox_layers_20_input_layernorm_bias2), out_sinfo=R.Tensor((1, 1, 2560), dtype="float16"))
            lv232 = R.call_tir(cls.fused_dequantize1_fused_NT_matmul10_add10, (gpt_neox_layers_20_attention_query_key_value_q_weight2, gpt_neox_layers_20_attention_query_key_value_q_scale2, layer_norm105, gpt_neox_layers_20_attention_query_key_value_bias2), out_sinfo=R.Tensor((1, 1, 7680), dtype="float16"))
            lv508 = R.call_tir(cls.fused_reshape8_reshape9, (lv232,), out_sinfo=R.Tensor((1, 96, 80), dtype="float16"))
            lv264 = R.call_dps_packed("vm.builtin.attention_kv_cache_attention_with_fused_qkv", (paged_kv_cache, R.prim_value(20), R.prim_value(T.float32(1.0)), lv508), out_sinfo=R.Tensor((1, 32, 80), dtype="float16"))
            lv509 = R.call_tir(cls.fused_reshape10_reshape11, (lv264,), out_sinfo=R.Tensor((1, 1, 2560), dtype="float16"))
            lv232_1 = R.call_tir(cls.fused_dequantize2_fused_NT_matmul11_add11_add12, (gpt_neox_layers_20_attention_dense_q_weight2, gpt_neox_layers_20_attention_dense_q_scale2, lv509, gpt_neox_layers_20_attention_dense_bias2, lv231_1), out_sinfo=R.Tensor((1, 1, 2560), dtype="float16"))
            layer_norm106 = R.call_tir(cls.layer_norm2, (lv232_1, gpt_neox_layers_20_post_attention_layernorm_weight2, gpt_neox_layers_20_post_attention_layernorm_bias2), out_sinfo=R.Tensor((1, 1, 2560), dtype="float16"))
            lv233 = R.call_tir(cls.fused_dequantize3_fused_NT_matmul12_add13_gelu2_cast6, (gpt_neox_layers_20_mlp_dense_h_to_4h_q_weight2, gpt_neox_layers_20_mlp_dense_h_to_4h_q_scale2, layer_norm106, gpt_neox_layers_20_mlp_dense_h_to_4h_bias2), out_sinfo=R.Tensor((1, 1, 10240), dtype="float16"))
            lv233_1 = R.call_tir(cls.fused_dequantize4_fused_NT_matmul13_add14_cast7_add12, (gpt_neox_layers_20_mlp_dense_4h_to_h_q_weight2, gpt_neox_layers_20_mlp_dense_4h_to_h_q_scale2, lv233, gpt_neox_layers_20_mlp_dense_4h_to_h_bias2, lv232_1), out_sinfo=R.Tensor((1, 1, 2560), dtype="float16"))
            layer_norm107 = R.call_tir(cls.layer_norm2, (lv233_1, gpt_neox_layers_21_input_layernorm_weight2, gpt_neox_layers_21_input_layernorm_bias2), out_sinfo=R.Tensor((1, 1, 2560), dtype="float16"))
            lv234_1 = R.call_tir(cls.fused_dequantize1_fused_NT_matmul10_add10, (gpt_neox_layers_21_attention_query_key_value_q_weight2, gpt_neox_layers_21_attention_query_key_value_q_scale2, layer_norm107, gpt_neox_layers_21_attention_query_key_value_bias2), out_sinfo=R.Tensor((1, 1, 7680), dtype="float16"))
            lv514 = R.call_tir(cls.fused_reshape8_reshape9, (lv234_1,), out_sinfo=R.Tensor((1, 96, 80), dtype="float16"))
            lv269 = R.call_dps_packed("vm.builtin.attention_kv_cache_attention_with_fused_qkv", (paged_kv_cache, R.prim_value(21), R.prim_value(T.float32(1.0)), lv514), out_sinfo=R.Tensor((1, 32, 80), dtype="float16"))
            lv515 = R.call_tir(cls.fused_reshape10_reshape11, (lv269,), out_sinfo=R.Tensor((1, 1, 2560), dtype="float16"))
            lv234_2 = R.call_tir(cls.fused_dequantize2_fused_NT_matmul11_add11_add12, (gpt_neox_layers_21_attention_dense_q_weight2, gpt_neox_layers_21_attention_dense_q_scale2, lv515, gpt_neox_layers_21_attention_dense_bias2, lv233_1), out_sinfo=R.Tensor((1, 1, 2560), dtype="float16"))
            layer_norm108 = R.call_tir(cls.layer_norm2, (lv234_2, gpt_neox_layers_21_post_attention_layernorm_weight2, gpt_neox_layers_21_post_attention_layernorm_bias2), out_sinfo=R.Tensor((1, 1, 2560), dtype="float16"))
            lv235 = R.call_tir(cls.fused_dequantize3_fused_NT_matmul12_add13_gelu2_cast6, (gpt_neox_layers_21_mlp_dense_h_to_4h_q_weight2, gpt_neox_layers_21_mlp_dense_h_to_4h_q_scale2, layer_norm108, gpt_neox_layers_21_mlp_dense_h_to_4h_bias2), out_sinfo=R.Tensor((1, 1, 10240), dtype="float16"))
            lv235_1 = R.call_tir(cls.fused_dequantize4_fused_NT_matmul13_add14_cast7_add12, (gpt_neox_layers_21_mlp_dense_4h_to_h_q_weight2, gpt_neox_layers_21_mlp_dense_4h_to_h_q_scale2, lv235, gpt_neox_layers_21_mlp_dense_4h_to_h_bias2, lv234_2), out_sinfo=R.Tensor((1, 1, 2560), dtype="float16"))
            layer_norm109 = R.call_tir(cls.layer_norm2, (lv235_1, gpt_neox_layers_22_input_layernorm_weight2, gpt_neox_layers_22_input_layernorm_bias2), out_sinfo=R.Tensor((1, 1, 2560), dtype="float16"))
            lv236 = R.call_tir(cls.fused_dequantize1_fused_NT_matmul10_add10, (gpt_neox_layers_22_attention_query_key_value_q_weight2, gpt_neox_layers_22_attention_query_key_value_q_scale2, layer_norm109, gpt_neox_layers_22_attention_query_key_value_bias2), out_sinfo=R.Tensor((1, 1, 7680), dtype="float16"))
            lv520 = R.call_tir(cls.fused_reshape8_reshape9, (lv236,), out_sinfo=R.Tensor((1, 96, 80), dtype="float16"))
            lv274 = R.call_dps_packed("vm.builtin.attention_kv_cache_attention_with_fused_qkv", (paged_kv_cache, R.prim_value(22), R.prim_value(T.float32(1.0)), lv520), out_sinfo=R.Tensor((1, 32, 80), dtype="float16"))
            lv521 = R.call_tir(cls.fused_reshape10_reshape11, (lv274,), out_sinfo=R.Tensor((1, 1, 2560), dtype="float16"))
            lv236_1 = R.call_tir(cls.fused_dequantize2_fused_NT_matmul11_add11_add12, (gpt_neox_layers_22_attention_dense_q_weight2, gpt_neox_layers_22_attention_dense_q_scale2, lv521, gpt_neox_layers_22_attention_dense_bias2, lv235_1), out_sinfo=R.Tensor((1, 1, 2560), dtype="float16"))
            layer_norm110 = R.call_tir(cls.layer_norm2, (lv236_1, gpt_neox_layers_22_post_attention_layernorm_weight2, gpt_neox_layers_22_post_attention_layernorm_bias2), out_sinfo=R.Tensor((1, 1, 2560), dtype="float16"))
            lv237 = R.call_tir(cls.fused_dequantize3_fused_NT_matmul12_add13_gelu2_cast6, (gpt_neox_layers_22_mlp_dense_h_to_4h_q_weight2, gpt_neox_layers_22_mlp_dense_h_to_4h_q_scale2, layer_norm110, gpt_neox_layers_22_mlp_dense_h_to_4h_bias2), out_sinfo=R.Tensor((1, 1, 10240), dtype="float16"))
            lv237_1 = R.call_tir(cls.fused_dequantize4_fused_NT_matmul13_add14_cast7_add12, (gpt_neox_layers_22_mlp_dense_4h_to_h_q_weight2, gpt_neox_layers_22_mlp_dense_4h_to_h_q_scale2, lv237, gpt_neox_layers_22_mlp_dense_4h_to_h_bias2, lv236_1), out_sinfo=R.Tensor((1, 1, 2560), dtype="float16"))
            layer_norm111 = R.call_tir(cls.layer_norm2, (lv237_1, gpt_neox_layers_23_input_layernorm_weight2, gpt_neox_layers_23_input_layernorm_bias2), out_sinfo=R.Tensor((1, 1, 2560), dtype="float16"))
            lv238 = R.call_tir(cls.fused_dequantize1_fused_NT_matmul10_add10, (gpt_neox_layers_23_attention_query_key_value_q_weight2, gpt_neox_layers_23_attention_query_key_value_q_scale2, layer_norm111, gpt_neox_layers_23_attention_query_key_value_bias2), out_sinfo=R.Tensor((1, 1, 7680), dtype="float16"))
            lv526 = R.call_tir(cls.fused_reshape8_reshape9, (lv238,), out_sinfo=R.Tensor((1, 96, 80), dtype="float16"))
            lv279 = R.call_dps_packed("vm.builtin.attention_kv_cache_attention_with_fused_qkv", (paged_kv_cache, R.prim_value(23), R.prim_value(T.float32(1.0)), lv526), out_sinfo=R.Tensor((1, 32, 80), dtype="float16"))
            lv527 = R.call_tir(cls.fused_reshape10_reshape11, (lv279,), out_sinfo=R.Tensor((1, 1, 2560), dtype="float16"))
            lv238_1 = R.call_tir(cls.fused_dequantize2_fused_NT_matmul11_add11_add12, (gpt_neox_layers_23_attention_dense_q_weight2, gpt_neox_layers_23_attention_dense_q_scale2, lv527, gpt_neox_layers_23_attention_dense_bias2, lv237_1), out_sinfo=R.Tensor((1, 1, 2560), dtype="float16"))
            layer_norm112 = R.call_tir(cls.layer_norm2, (lv238_1, gpt_neox_layers_23_post_attention_layernorm_weight2, gpt_neox_layers_23_post_attention_layernorm_bias2), out_sinfo=R.Tensor((1, 1, 2560), dtype="float16"))
            lv239_1 = R.call_tir(cls.fused_dequantize3_fused_NT_matmul12_add13_gelu2_cast6, (gpt_neox_layers_23_mlp_dense_h_to_4h_q_weight2, gpt_neox_layers_23_mlp_dense_h_to_4h_q_scale2, layer_norm112, gpt_neox_layers_23_mlp_dense_h_to_4h_bias2), out_sinfo=R.Tensor((1, 1, 10240), dtype="float16"))
            lv239_2 = R.call_tir(cls.fused_dequantize4_fused_NT_matmul13_add14_cast7_add12, (gpt_neox_layers_23_mlp_dense_4h_to_h_q_weight2, gpt_neox_layers_23_mlp_dense_4h_to_h_q_scale2, lv239_1, gpt_neox_layers_23_mlp_dense_4h_to_h_bias2, lv238_1), out_sinfo=R.Tensor((1, 1, 2560), dtype="float16"))
            layer_norm113 = R.call_tir(cls.layer_norm2, (lv239_2, gpt_neox_layers_24_input_layernorm_weight2, gpt_neox_layers_24_input_layernorm_bias2), out_sinfo=R.Tensor((1, 1, 2560), dtype="float16"))
            lv240 = R.call_tir(cls.fused_dequantize1_fused_NT_matmul10_add10, (gpt_neox_layers_24_attention_query_key_value_q_weight2, gpt_neox_layers_24_attention_query_key_value_q_scale2, layer_norm113, gpt_neox_layers_24_attention_query_key_value_bias2), out_sinfo=R.Tensor((1, 1, 7680), dtype="float16"))
            lv532 = R.call_tir(cls.fused_reshape8_reshape9, (lv240,), out_sinfo=R.Tensor((1, 96, 80), dtype="float16"))
            lv284 = R.call_dps_packed("vm.builtin.attention_kv_cache_attention_with_fused_qkv", (paged_kv_cache, R.prim_value(24), R.prim_value(T.float32(1.0)), lv532), out_sinfo=R.Tensor((1, 32, 80), dtype="float16"))
            lv533 = R.call_tir(cls.fused_reshape10_reshape11, (lv284,), out_sinfo=R.Tensor((1, 1, 2560), dtype="float16"))
            lv240_1 = R.call_tir(cls.fused_dequantize2_fused_NT_matmul11_add11_add12, (gpt_neox_layers_24_attention_dense_q_weight2, gpt_neox_layers_24_attention_dense_q_scale2, lv533, gpt_neox_layers_24_attention_dense_bias2, lv239_2), out_sinfo=R.Tensor((1, 1, 2560), dtype="float16"))
            layer_norm114 = R.call_tir(cls.layer_norm2, (lv240_1, gpt_neox_layers_24_post_attention_layernorm_weight2, gpt_neox_layers_24_post_attention_layernorm_bias2), out_sinfo=R.Tensor((1, 1, 2560), dtype="float16"))
            lv241 = R.call_tir(cls.fused_dequantize3_fused_NT_matmul12_add13_gelu2_cast6, (gpt_neox_layers_24_mlp_dense_h_to_4h_q_weight2, gpt_neox_layers_24_mlp_dense_h_to_4h_q_scale2, layer_norm114, gpt_neox_layers_24_mlp_dense_h_to_4h_bias2), out_sinfo=R.Tensor((1, 1, 10240), dtype="float16"))
            lv241_1 = R.call_tir(cls.fused_dequantize4_fused_NT_matmul13_add14_cast7_add12, (gpt_neox_layers_24_mlp_dense_4h_to_h_q_weight2, gpt_neox_layers_24_mlp_dense_4h_to_h_q_scale2, lv241, gpt_neox_layers_24_mlp_dense_4h_to_h_bias2, lv240_1), out_sinfo=R.Tensor((1, 1, 2560), dtype="float16"))
            layer_norm115 = R.call_tir(cls.layer_norm2, (lv241_1, gpt_neox_layers_25_input_layernorm_weight2, gpt_neox_layers_25_input_layernorm_bias2), out_sinfo=R.Tensor((1, 1, 2560), dtype="float16"))
            lv242 = R.call_tir(cls.fused_dequantize1_fused_NT_matmul10_add10, (gpt_neox_layers_25_attention_query_key_value_q_weight2, gpt_neox_layers_25_attention_query_key_value_q_scale2, layer_norm115, gpt_neox_layers_25_attention_query_key_value_bias2), out_sinfo=R.Tensor((1, 1, 7680), dtype="float16"))
            lv538 = R.call_tir(cls.fused_reshape8_reshape9, (lv242,), out_sinfo=R.Tensor((1, 96, 80), dtype="float16"))
            lv289 = R.call_dps_packed("vm.builtin.attention_kv_cache_attention_with_fused_qkv", (paged_kv_cache, R.prim_value(25), R.prim_value(T.float32(1.0)), lv538), out_sinfo=R.Tensor((1, 32, 80), dtype="float16"))
            lv539 = R.call_tir(cls.fused_reshape10_reshape11, (lv289,), out_sinfo=R.Tensor((1, 1, 2560), dtype="float16"))
            lv242_1 = R.call_tir(cls.fused_dequantize2_fused_NT_matmul11_add11_add12, (gpt_neox_layers_25_attention_dense_q_weight2, gpt_neox_layers_25_attention_dense_q_scale2, lv539, gpt_neox_layers_25_attention_dense_bias2, lv241_1), out_sinfo=R.Tensor((1, 1, 2560), dtype="float16"))
            layer_norm116 = R.call_tir(cls.layer_norm2, (lv242_1, gpt_neox_layers_25_post_attention_layernorm_weight2, gpt_neox_layers_25_post_attention_layernorm_bias2), out_sinfo=R.Tensor((1, 1, 2560), dtype="float16"))
            lv243 = R.call_tir(cls.fused_dequantize3_fused_NT_matmul12_add13_gelu2_cast6, (gpt_neox_layers_25_mlp_dense_h_to_4h_q_weight2, gpt_neox_layers_25_mlp_dense_h_to_4h_q_scale2, layer_norm116, gpt_neox_layers_25_mlp_dense_h_to_4h_bias2), out_sinfo=R.Tensor((1, 1, 10240), dtype="float16"))
            lv243_1 = R.call_tir(cls.fused_dequantize4_fused_NT_matmul13_add14_cast7_add12, (gpt_neox_layers_25_mlp_dense_4h_to_h_q_weight2, gpt_neox_layers_25_mlp_dense_4h_to_h_q_scale2, lv243, gpt_neox_layers_25_mlp_dense_4h_to_h_bias2, lv242_1), out_sinfo=R.Tensor((1, 1, 2560), dtype="float16"))
            layer_norm117 = R.call_tir(cls.layer_norm2, (lv243_1, gpt_neox_layers_26_input_layernorm_weight2, gpt_neox_layers_26_input_layernorm_bias2), out_sinfo=R.Tensor((1, 1, 2560), dtype="float16"))
            lv244_1 = R.call_tir(cls.fused_dequantize1_fused_NT_matmul10_add10, (gpt_neox_layers_26_attention_query_key_value_q_weight2, gpt_neox_layers_26_attention_query_key_value_q_scale2, layer_norm117, gpt_neox_layers_26_attention_query_key_value_bias2), out_sinfo=R.Tensor((1, 1, 7680), dtype="float16"))
            lv544 = R.call_tir(cls.fused_reshape8_reshape9, (lv244_1,), out_sinfo=R.Tensor((1, 96, 80), dtype="float16"))
            lv294 = R.call_dps_packed("vm.builtin.attention_kv_cache_attention_with_fused_qkv", (paged_kv_cache, R.prim_value(26), R.prim_value(T.float32(1.0)), lv544), out_sinfo=R.Tensor((1, 32, 80), dtype="float16"))
            lv545 = R.call_tir(cls.fused_reshape10_reshape11, (lv294,), out_sinfo=R.Tensor((1, 1, 2560), dtype="float16"))
            lv244_2 = R.call_tir(cls.fused_dequantize2_fused_NT_matmul11_add11_add12, (gpt_neox_layers_26_attention_dense_q_weight2, gpt_neox_layers_26_attention_dense_q_scale2, lv545, gpt_neox_layers_26_attention_dense_bias2, lv243_1), out_sinfo=R.Tensor((1, 1, 2560), dtype="float16"))
            layer_norm118 = R.call_tir(cls.layer_norm2, (lv244_2, gpt_neox_layers_26_post_attention_layernorm_weight2, gpt_neox_layers_26_post_attention_layernorm_bias2), out_sinfo=R.Tensor((1, 1, 2560), dtype="float16"))
            lv245 = R.call_tir(cls.fused_dequantize3_fused_NT_matmul12_add13_gelu2_cast6, (gpt_neox_layers_26_mlp_dense_h_to_4h_q_weight2, gpt_neox_layers_26_mlp_dense_h_to_4h_q_scale2, layer_norm118, gpt_neox_layers_26_mlp_dense_h_to_4h_bias2), out_sinfo=R.Tensor((1, 1, 10240), dtype="float16"))
            lv245_1 = R.call_tir(cls.fused_dequantize4_fused_NT_matmul13_add14_cast7_add12, (gpt_neox_layers_26_mlp_dense_4h_to_h_q_weight2, gpt_neox_layers_26_mlp_dense_4h_to_h_q_scale2, lv245, gpt_neox_layers_26_mlp_dense_4h_to_h_bias2, lv244_2), out_sinfo=R.Tensor((1, 1, 2560), dtype="float16"))
            layer_norm119 = R.call_tir(cls.layer_norm2, (lv245_1, gpt_neox_layers_27_input_layernorm_weight2, gpt_neox_layers_27_input_layernorm_bias2), out_sinfo=R.Tensor((1, 1, 2560), dtype="float16"))
            lv246 = R.call_tir(cls.fused_dequantize1_fused_NT_matmul10_add10, (gpt_neox_layers_27_attention_query_key_value_q_weight2, gpt_neox_layers_27_attention_query_key_value_q_scale2, layer_norm119, gpt_neox_layers_27_attention_query_key_value_bias2), out_sinfo=R.Tensor((1, 1, 7680), dtype="float16"))
            lv550 = R.call_tir(cls.fused_reshape8_reshape9, (lv246,), out_sinfo=R.Tensor((1, 96, 80), dtype="float16"))
            lv299 = R.call_dps_packed("vm.builtin.attention_kv_cache_attention_with_fused_qkv", (paged_kv_cache, R.prim_value(27), R.prim_value(T.float32(1.0)), lv550), out_sinfo=R.Tensor((1, 32, 80), dtype="float16"))
            lv551 = R.call_tir(cls.fused_reshape10_reshape11, (lv299,), out_sinfo=R.Tensor((1, 1, 2560), dtype="float16"))
            lv246_1 = R.call_tir(cls.fused_dequantize2_fused_NT_matmul11_add11_add12, (gpt_neox_layers_27_attention_dense_q_weight2, gpt_neox_layers_27_attention_dense_q_scale2, lv551, gpt_neox_layers_27_attention_dense_bias2, lv245_1), out_sinfo=R.Tensor((1, 1, 2560), dtype="float16"))
            layer_norm120 = R.call_tir(cls.layer_norm2, (lv246_1, gpt_neox_layers_27_post_attention_layernorm_weight2, gpt_neox_layers_27_post_attention_layernorm_bias2), out_sinfo=R.Tensor((1, 1, 2560), dtype="float16"))
            lv247 = R.call_tir(cls.fused_dequantize3_fused_NT_matmul12_add13_gelu2_cast6, (gpt_neox_layers_27_mlp_dense_h_to_4h_q_weight2, gpt_neox_layers_27_mlp_dense_h_to_4h_q_scale2, layer_norm120, gpt_neox_layers_27_mlp_dense_h_to_4h_bias2), out_sinfo=R.Tensor((1, 1, 10240), dtype="float16"))
            lv247_1 = R.call_tir(cls.fused_dequantize4_fused_NT_matmul13_add14_cast7_add12, (gpt_neox_layers_27_mlp_dense_4h_to_h_q_weight2, gpt_neox_layers_27_mlp_dense_4h_to_h_q_scale2, lv247, gpt_neox_layers_27_mlp_dense_4h_to_h_bias2, lv246_1), out_sinfo=R.Tensor((1, 1, 2560), dtype="float16"))
            layer_norm121 = R.call_tir(cls.layer_norm2, (lv247_1, gpt_neox_layers_28_input_layernorm_weight2, gpt_neox_layers_28_input_layernorm_bias2), out_sinfo=R.Tensor((1, 1, 2560), dtype="float16"))
            lv248 = R.call_tir(cls.fused_dequantize1_fused_NT_matmul10_add10, (gpt_neox_layers_28_attention_query_key_value_q_weight2, gpt_neox_layers_28_attention_query_key_value_q_scale2, layer_norm121, gpt_neox_layers_28_attention_query_key_value_bias2), out_sinfo=R.Tensor((1, 1, 7680), dtype="float16"))
            lv556 = R.call_tir(cls.fused_reshape8_reshape9, (lv248,), out_sinfo=R.Tensor((1, 96, 80), dtype="float16"))
            lv304 = R.call_dps_packed("vm.builtin.attention_kv_cache_attention_with_fused_qkv", (paged_kv_cache, R.prim_value(28), R.prim_value(T.float32(1.0)), lv556), out_sinfo=R.Tensor((1, 32, 80), dtype="float16"))
            lv557 = R.call_tir(cls.fused_reshape10_reshape11, (lv304,), out_sinfo=R.Tensor((1, 1, 2560), dtype="float16"))
            lv248_1 = R.call_tir(cls.fused_dequantize2_fused_NT_matmul11_add11_add12, (gpt_neox_layers_28_attention_dense_q_weight2, gpt_neox_layers_28_attention_dense_q_scale2, lv557, gpt_neox_layers_28_attention_dense_bias2, lv247_1), out_sinfo=R.Tensor((1, 1, 2560), dtype="float16"))
            layer_norm122 = R.call_tir(cls.layer_norm2, (lv248_1, gpt_neox_layers_28_post_attention_layernorm_weight2, gpt_neox_layers_28_post_attention_layernorm_bias2), out_sinfo=R.Tensor((1, 1, 2560), dtype="float16"))
            lv249_1 = R.call_tir(cls.fused_dequantize3_fused_NT_matmul12_add13_gelu2_cast6, (gpt_neox_layers_28_mlp_dense_h_to_4h_q_weight2, gpt_neox_layers_28_mlp_dense_h_to_4h_q_scale2, layer_norm122, gpt_neox_layers_28_mlp_dense_h_to_4h_bias2), out_sinfo=R.Tensor((1, 1, 10240), dtype="float16"))
            lv249_2 = R.call_tir(cls.fused_dequantize4_fused_NT_matmul13_add14_cast7_add12, (gpt_neox_layers_28_mlp_dense_4h_to_h_q_weight2, gpt_neox_layers_28_mlp_dense_4h_to_h_q_scale2, lv249_1, gpt_neox_layers_28_mlp_dense_4h_to_h_bias2, lv248_1), out_sinfo=R.Tensor((1, 1, 2560), dtype="float16"))
            layer_norm123 = R.call_tir(cls.layer_norm2, (lv249_2, gpt_neox_layers_29_input_layernorm_weight2, gpt_neox_layers_29_input_layernorm_bias2), out_sinfo=R.Tensor((1, 1, 2560), dtype="float16"))
            lv250 = R.call_tir(cls.fused_dequantize1_fused_NT_matmul10_add10, (gpt_neox_layers_29_attention_query_key_value_q_weight2, gpt_neox_layers_29_attention_query_key_value_q_scale2, layer_norm123, gpt_neox_layers_29_attention_query_key_value_bias2), out_sinfo=R.Tensor((1, 1, 7680), dtype="float16"))
            lv562 = R.call_tir(cls.fused_reshape8_reshape9, (lv250,), out_sinfo=R.Tensor((1, 96, 80), dtype="float16"))
            lv309 = R.call_dps_packed("vm.builtin.attention_kv_cache_attention_with_fused_qkv", (paged_kv_cache, R.prim_value(29), R.prim_value(T.float32(1.0)), lv562), out_sinfo=R.Tensor((1, 32, 80), dtype="float16"))
            lv563 = R.call_tir(cls.fused_reshape10_reshape11, (lv309,), out_sinfo=R.Tensor((1, 1, 2560), dtype="float16"))
            lv250_1 = R.call_tir(cls.fused_dequantize2_fused_NT_matmul11_add11_add12, (gpt_neox_layers_29_attention_dense_q_weight2, gpt_neox_layers_29_attention_dense_q_scale2, lv563, gpt_neox_layers_29_attention_dense_bias2, lv249_2), out_sinfo=R.Tensor((1, 1, 2560), dtype="float16"))
            layer_norm124 = R.call_tir(cls.layer_norm2, (lv250_1, gpt_neox_layers_29_post_attention_layernorm_weight2, gpt_neox_layers_29_post_attention_layernorm_bias2), out_sinfo=R.Tensor((1, 1, 2560), dtype="float16"))
            lv251 = R.call_tir(cls.fused_dequantize3_fused_NT_matmul12_add13_gelu2_cast6, (gpt_neox_layers_29_mlp_dense_h_to_4h_q_weight2, gpt_neox_layers_29_mlp_dense_h_to_4h_q_scale2, layer_norm124, gpt_neox_layers_29_mlp_dense_h_to_4h_bias2), out_sinfo=R.Tensor((1, 1, 10240), dtype="float16"))
            lv251_1 = R.call_tir(cls.fused_dequantize4_fused_NT_matmul13_add14_cast7_add12, (gpt_neox_layers_29_mlp_dense_4h_to_h_q_weight2, gpt_neox_layers_29_mlp_dense_4h_to_h_q_scale2, lv251, gpt_neox_layers_29_mlp_dense_4h_to_h_bias2, lv250_1), out_sinfo=R.Tensor((1, 1, 2560), dtype="float16"))
            layer_norm125 = R.call_tir(cls.layer_norm2, (lv251_1, gpt_neox_layers_30_input_layernorm_weight2, gpt_neox_layers_30_input_layernorm_bias2), out_sinfo=R.Tensor((1, 1, 2560), dtype="float16"))
            lv252 = R.call_tir(cls.fused_dequantize1_fused_NT_matmul10_add10, (gpt_neox_layers_30_attention_query_key_value_q_weight2, gpt_neox_layers_30_attention_query_key_value_q_scale2, layer_norm125, gpt_neox_layers_30_attention_query_key_value_bias2), out_sinfo=R.Tensor((1, 1, 7680), dtype="float16"))
            lv568 = R.call_tir(cls.fused_reshape8_reshape9, (lv252,), out_sinfo=R.Tensor((1, 96, 80), dtype="float16"))
            lv314 = R.call_dps_packed("vm.builtin.attention_kv_cache_attention_with_fused_qkv", (paged_kv_cache, R.prim_value(30), R.prim_value(T.float32(1.0)), lv568), out_sinfo=R.Tensor((1, 32, 80), dtype="float16"))
            lv569 = R.call_tir(cls.fused_reshape10_reshape11, (lv314,), out_sinfo=R.Tensor((1, 1, 2560), dtype="float16"))
            lv252_1 = R.call_tir(cls.fused_dequantize2_fused_NT_matmul11_add11_add12, (gpt_neox_layers_30_attention_dense_q_weight2, gpt_neox_layers_30_attention_dense_q_scale2, lv569, gpt_neox_layers_30_attention_dense_bias2, lv251_1), out_sinfo=R.Tensor((1, 1, 2560), dtype="float16"))
            layer_norm126 = R.call_tir(cls.layer_norm2, (lv252_1, gpt_neox_layers_30_post_attention_layernorm_weight2, gpt_neox_layers_30_post_attention_layernorm_bias2), out_sinfo=R.Tensor((1, 1, 2560), dtype="float16"))
            lv253 = R.call_tir(cls.fused_dequantize3_fused_NT_matmul12_add13_gelu2_cast6, (gpt_neox_layers_30_mlp_dense_h_to_4h_q_weight2, gpt_neox_layers_30_mlp_dense_h_to_4h_q_scale2, layer_norm126, gpt_neox_layers_30_mlp_dense_h_to_4h_bias2), out_sinfo=R.Tensor((1, 1, 10240), dtype="float16"))
            lv253_1 = R.call_tir(cls.fused_dequantize4_fused_NT_matmul13_add14_cast7_add12, (gpt_neox_layers_30_mlp_dense_4h_to_h_q_weight2, gpt_neox_layers_30_mlp_dense_4h_to_h_q_scale2, lv253, gpt_neox_layers_30_mlp_dense_4h_to_h_bias2, lv252_1), out_sinfo=R.Tensor((1, 1, 2560), dtype="float16"))
            layer_norm127 = R.call_tir(cls.layer_norm2, (lv253_1, gpt_neox_layers_31_input_layernorm_weight2, gpt_neox_layers_31_input_layernorm_bias2), out_sinfo=R.Tensor((1, 1, 2560), dtype="float16"))
            lv254_1 = R.call_tir(cls.fused_dequantize1_fused_NT_matmul10_add10, (gpt_neox_layers_31_attention_query_key_value_q_weight2, gpt_neox_layers_31_attention_query_key_value_q_scale2, layer_norm127, gpt_neox_layers_31_attention_query_key_value_bias2), out_sinfo=R.Tensor((1, 1, 7680), dtype="float16"))
            lv574 = R.call_tir(cls.fused_reshape8_reshape9, (lv254_1,), out_sinfo=R.Tensor((1, 96, 80), dtype="float16"))
            lv319 = R.call_dps_packed("vm.builtin.attention_kv_cache_attention_with_fused_qkv", (paged_kv_cache, R.prim_value(31), R.prim_value(T.float32(1.0)), lv574), out_sinfo=R.Tensor((1, 32, 80), dtype="float16"))
            lv575 = R.call_tir(cls.fused_reshape10_reshape11, (lv319,), out_sinfo=R.Tensor((1, 1, 2560), dtype="float16"))
            lv254_2 = R.call_tir(cls.fused_dequantize2_fused_NT_matmul11_add11_add12, (gpt_neox_layers_31_attention_dense_q_weight2, gpt_neox_layers_31_attention_dense_q_scale2, lv575, gpt_neox_layers_31_attention_dense_bias2, lv253_1), out_sinfo=R.Tensor((1, 1, 2560), dtype="float16"))
            layer_norm128 = R.call_tir(cls.layer_norm2, (lv254_2, gpt_neox_layers_31_post_attention_layernorm_weight2, gpt_neox_layers_31_post_attention_layernorm_bias2), out_sinfo=R.Tensor((1, 1, 2560), dtype="float16"))
            lv255 = R.call_tir(cls.fused_dequantize3_fused_NT_matmul12_add13_gelu2_cast6, (gpt_neox_layers_31_mlp_dense_h_to_4h_q_weight2, gpt_neox_layers_31_mlp_dense_h_to_4h_q_scale2, layer_norm128, gpt_neox_layers_31_mlp_dense_h_to_4h_bias2), out_sinfo=R.Tensor((1, 1, 10240), dtype="float16"))
            lv255_1 = R.call_tir(cls.fused_dequantize4_fused_NT_matmul13_add14_cast7_add12, (gpt_neox_layers_31_mlp_dense_4h_to_h_q_weight2, gpt_neox_layers_31_mlp_dense_4h_to_h_q_scale2, lv255, gpt_neox_layers_31_mlp_dense_4h_to_h_bias2, lv254_2), out_sinfo=R.Tensor((1, 1, 2560), dtype="float16"))
            layer_norm129 = R.call_tir(cls.layer_norm2, (lv255_1, gpt_neox_final_layer_norm_weight2, gpt_neox_final_layer_norm_bias2), out_sinfo=R.Tensor((1, 1, 2560), dtype="float16"))
            lv3 = R.call_tir(cls.fused_dequantize_fused_NT_matmul14_cast8, (embed_out_q_weight2, embed_out_q_scale2, layer_norm129), out_sinfo=R.Tensor((1, 1, vocab_size), dtype="float32"))
            gv2: R.Tuple(R.Tensor((1, 1, vocab_size), dtype="float32"), R.Object) = lv3, paged_kv_cache
            R.output(gv2)
        return gv2

    @R.function
    def embed(input_ids: R.Tensor(("seq_len",), dtype="int32"), packed_params: R.Tuple(R.Tensor(("vocab_size", 320), dtype="uint32"), R.Tensor(("vocab_size", 80), dtype="float16"), R.Tensor((2560,), dtype="float16"), R.Tensor((2560,), dtype="float16"), R.Tensor((2560,), dtype="float16"), R.Tensor((2560,), dtype="float16"), R.Tensor((7680, 320), dtype="uint32"), R.Tensor((7680, 80), dtype="float16"), R.Tensor((7680,), dtype="float16"), R.Tensor((2560, 320), dtype="uint32"), R.Tensor((2560, 80), dtype="float16"), R.Tensor((2560,), dtype="float16"), R.Tensor((10240, 320), dtype="uint32"), R.Tensor((10240, 80), dtype="float16"), R.Tensor((10240,), dtype="float32"), R.Tensor((2560, 1280), dtype="uint32"), R.Tensor((2560, 320), dtype="float16"), R.Tensor((2560,), dtype="float32"), R.Tensor((2560,), dtype="float16"), R.Tensor((2560,), dtype="float16"), R.Tensor((2560,), dtype="float16"), R.Tensor((2560,), dtype="float16"), R.Tensor((7680, 320), dtype="uint32"), R.Tensor((7680, 80), dtype="float16"), R.Tensor((7680,), dtype="float16"), R.Tensor((2560, 320), dtype="uint32"), R.Tensor((2560, 80), dtype="float16"), R.Tensor((2560,), dtype="float16"), R.Tensor((10240, 320), dtype="uint32"), R.Tensor((10240, 80), dtype="float16"), R.Tensor((10240,), dtype="float32"), R.Tensor((2560, 1280), dtype="uint32"), R.Tensor((2560, 320), dtype="float16"), R.Tensor((2560,), dtype="float32"), R.Tensor((2560,), dtype="float16"), R.Tensor((2560,), dtype="float16"), R.Tensor((2560,), dtype="float16"), R.Tensor((2560,), dtype="float16"), R.Tensor((7680, 320), dtype="uint32"), R.Tensor((7680, 80), dtype="float16"), R.Tensor((7680,), dtype="float16"), R.Tensor((2560, 320), dtype="uint32"), R.Tensor((2560, 80), dtype="float16"), R.Tensor((2560,), dtype="float16"), R.Tensor((10240, 320), dtype="uint32"), R.Tensor((10240, 80), dtype="float16"), R.Tensor((10240,), dtype="float32"), R.Tensor((2560, 1280), dtype="uint32"), R.Tensor((2560, 320), dtype="float16"), R.Tensor((2560,), dtype="float32"), R.Tensor((2560,), dtype="float16"), R.Tensor((2560,), dtype="float16"), R.Tensor((2560,), dtype="float16"), R.Tensor((2560,), dtype="float16"), R.Tensor((7680, 320), dtype="uint32"), R.Tensor((7680, 80), dtype="float16"), R.Tensor((7680,), dtype="float16"), R.Tensor((2560, 320), dtype="uint32"), R.Tensor((2560, 80), dtype="float16"), R.Tensor((2560,), dtype="float16"), R.Tensor((10240, 320), dtype="uint32"), R.Tensor((10240, 80), dtype="float16"), R.Tensor((10240,), dtype="float32"), R.Tensor((2560, 1280), dtype="uint32"), R.Tensor((2560, 320), dtype="float16"), R.Tensor((2560,), dtype="float32"), R.Tensor((2560,), dtype="float16"), R.Tensor((2560,), dtype="float16"), R.Tensor((2560,), dtype="float16"), R.Tensor((2560,), dtype="float16"), R.Tensor((7680, 320), dtype="uint32"), R.Tensor((7680, 80), dtype="float16"), R.Tensor((7680,), dtype="float16"), R.Tensor((2560, 320), dtype="uint32"), R.Tensor((2560, 80), dtype="float16"), R.Tensor((2560,), dtype="float16"), R.Tensor((10240, 320), dtype="uint32"), R.Tensor((10240, 80), dtype="float16"), R.Tensor((10240,), dtype="float32"), R.Tensor((2560, 1280), dtype="uint32"), R.Tensor((2560, 320), dtype="float16"), R.Tensor((2560,), dtype="float32"), R.Tensor((2560,), dtype="float16"), R.Tensor((2560,), dtype="float16"), R.Tensor((2560,), dtype="float16"), R.Tensor((2560,), dtype="float16"), R.Tensor((7680, 320), dtype="uint32"), R.Tensor((7680, 80), dtype="float16"), R.Tensor((7680,), dtype="float16"), R.Tensor((2560, 320), dtype="uint32"), R.Tensor((2560, 80), dtype="float16"), R.Tensor((2560,), dtype="float16"), R.Tensor((10240, 320), dtype="uint32"), R.Tensor((10240, 80), dtype="float16"), R.Tensor((10240,), dtype="float32"), R.Tensor((2560, 1280), dtype="uint32"), R.Tensor((2560, 320), dtype="float16"), R.Tensor((2560,), dtype="float32"), R.Tensor((2560,), dtype="float16"), R.Tensor((2560,), dtype="float16"), R.Tensor((2560,), dtype="float16"), R.Tensor((2560,), dtype="float16"), R.Tensor((7680, 320), dtype="uint32"), R.Tensor((7680, 80), dtype="float16"), R.Tensor((7680,), dtype="float16"), R.Tensor((2560, 320), dtype="uint32"), R.Tensor((2560, 80), dtype="float16"), R.Tensor((2560,), dtype="float16"), R.Tensor((10240, 320), dtype="uint32"), R.Tensor((10240, 80), dtype="float16"), R.Tensor((10240,), dtype="float32"), R.Tensor((2560, 1280), dtype="uint32"), R.Tensor((2560, 320), dtype="float16"), R.Tensor((2560,), dtype="float32"), R.Tensor((2560,), dtype="float16"), R.Tensor((2560,), dtype="float16"), R.Tensor((2560,), dtype="float16"), R.Tensor((2560,), dtype="float16"), R.Tensor((7680, 320), dtype="uint32"), R.Tensor((7680, 80), dtype="float16"), R.Tensor((7680,), dtype="float16"), R.Tensor((2560, 320), dtype="uint32"), R.Tensor((2560, 80), dtype="float16"), R.Tensor((2560,), dtype="float16"), R.Tensor((10240, 320), dtype="uint32"), R.Tensor((10240, 80), dtype="float16"), R.Tensor((10240,), dtype="float32"), R.Tensor((2560, 1280), dtype="uint32"), R.Tensor((2560, 320), dtype="float16"), R.Tensor((2560,), dtype="float32"), R.Tensor((2560,), dtype="float16"), R.Tensor((2560,), dtype="float16"), R.Tensor((2560,), dtype="float16"), R.Tensor((2560,), dtype="float16"), R.Tensor((7680, 320), dtype="uint32"), R.Tensor((7680, 80), dtype="float16"), R.Tensor((7680,), dtype="float16"), R.Tensor((2560, 320), dtype="uint32"), R.Tensor((2560, 80), dtype="float16"), R.Tensor((2560,), dtype="float16"), R.Tensor((10240, 320), dtype="uint32"), R.Tensor((10240, 80), dtype="float16"), R.Tensor((10240,), dtype="float32"), R.Tensor((2560, 1280), dtype="uint32"), R.Tensor((2560, 320), dtype="float16"), R.Tensor((2560,), dtype="float32"), R.Tensor((2560,), dtype="float16"), R.Tensor((2560,), dtype="float16"), R.Tensor((2560,), dtype="float16"), R.Tensor((2560,), dtype="float16"), R.Tensor((7680, 320), dtype="uint32"), R.Tensor((7680, 80), dtype="float16"), R.Tensor((7680,), dtype="float16"), R.Tensor((2560, 320), dtype="uint32"), R.Tensor((2560, 80), dtype="float16"), R.Tensor((2560,), dtype="float16"), R.Tensor((10240, 320), dtype="uint32"), R.Tensor((10240, 80), dtype="float16"), R.Tensor((10240,), dtype="float32"), R.Tensor((2560, 1280), dtype="uint32"), R.Tensor((2560, 320), dtype="float16"), R.Tensor((2560,), dtype="float32"), R.Tensor((2560,), dtype="float16"), R.Tensor((2560,), dtype="float16"), R.Tensor((2560,), dtype="float16"), R.Tensor((2560,), dtype="float16"), R.Tensor((7680, 320), dtype="uint32"), R.Tensor((7680, 80), dtype="float16"), R.Tensor((7680,), dtype="float16"), R.Tensor((2560, 320), dtype="uint32"), R.Tensor((2560, 80), dtype="float16"), R.Tensor((2560,), dtype="float16"), R.Tensor((10240, 320), dtype="uint32"), R.Tensor((10240, 80), dtype="float16"), R.Tensor((10240,), dtype="float32"), R.Tensor((2560, 1280), dtype="uint32"), R.Tensor((2560, 320), dtype="float16"), R.Tensor((2560,), dtype="float32"), R.Tensor((2560,), dtype="float16"), R.Tensor((2560,), dtype="float16"), R.Tensor((2560,), dtype="float16"), R.Tensor((2560,), dtype="float16"), R.Tensor((7680, 320), dtype="uint32"), R.Tensor((7680, 80), dtype="float16"), R.Tensor((7680,), dtype="float16"), R.Tensor((2560, 320), dtype="uint32"), R.Tensor((2560, 80), dtype="float16"), R.Tensor((2560,), dtype="float16"), R.Tensor((10240, 320), dtype="uint32"), R.Tensor((10240, 80), dtype="float16"), R.Tensor((10240,), dtype="float32"), R.Tensor((2560, 1280), dtype="uint32"), R.Tensor((2560, 320), dtype="float16"), R.Tensor((2560,), dtype="float32"), R.Tensor((2560,), dtype="float16"), R.Tensor((2560,), dtype="float16"), R.Tensor((2560,), dtype="float16"), R.Tensor((2560,), dtype="float16"), R.Tensor((7680, 320), dtype="uint32"), R.Tensor((7680, 80), dtype="float16"), R.Tensor((7680,), dtype="float16"), R.Tensor((2560, 320), dtype="uint32"), R.Tensor((2560, 80), dtype="float16"), R.Tensor((2560,), dtype="float16"), R.Tensor((10240, 320), dtype="uint32"), R.Tensor((10240, 80), dtype="float16"), R.Tensor((10240,), dtype="float32"), R.Tensor((2560, 1280), dtype="uint32"), R.Tensor((2560, 320), dtype="float16"), R.Tensor((2560,), dtype="float32"), R.Tensor((2560,), dtype="float16"), R.Tensor((2560,), dtype="float16"), R.Tensor((2560,), dtype="float16"), R.Tensor((2560,), dtype="float16"), R.Tensor((7680, 320), dtype="uint32"), R.Tensor((7680, 80), dtype="float16"), R.Tensor((7680,), dtype="float16"), R.Tensor((2560, 320), dtype="uint32"), R.Tensor((2560, 80), dtype="float16"), R.Tensor((2560,), dtype="float16"), R.Tensor((10240, 320), dtype="uint32"), R.Tensor((10240, 80), dtype="float16"), R.Tensor((10240,), dtype="float32"), R.Tensor((2560, 1280), dtype="uint32"), R.Tensor((2560, 320), dtype="float16"), R.Tensor((2560,), dtype="float32"), R.Tensor((2560,), dtype="float16"), R.Tensor((2560,), dtype="float16"), R.Tensor((2560,), dtype="float16"), R.Tensor((2560,), dtype="float16"), R.Tensor((7680, 320), dtype="uint32"), R.Tensor((7680, 80), dtype="float16"), R.Tensor((7680,), dtype="float16"), R.Tensor((2560, 320), dtype="uint32"), R.Tensor((2560, 80), dtype="float16"), R.Tensor((2560,), dtype="float16"), R.Tensor((10240, 320), dtype="uint32"), R.Tensor((10240, 80), dtype="float16"), R.Tensor((10240,), dtype="float32"), R.Tensor((2560, 1280), dtype="uint32"), R.Tensor((2560, 320), dtype="float16"), R.Tensor((2560,), dtype="float32"), R.Tensor((2560,), dtype="float16"), R.Tensor((2560,), dtype="float16"), R.Tensor((2560,), dtype="float16"), R.Tensor((2560,), dtype="float16"), R.Tensor((7680, 320), dtype="uint32"), R.Tensor((7680, 80), dtype="float16"), R.Tensor((7680,), dtype="float16"), R.Tensor((2560, 320), dtype="uint32"), R.Tensor((2560, 80), dtype="float16"), R.Tensor((2560,), dtype="float16"), R.Tensor((10240, 320), dtype="uint32"), R.Tensor((10240, 80), dtype="float16"), R.Tensor((10240,), dtype="float32"), R.Tensor((2560, 1280), dtype="uint32"), R.Tensor((2560, 320), dtype="float16"), R.Tensor((2560,), dtype="float32"), R.Tensor((2560,), dtype="float16"), R.Tensor((2560,), dtype="float16"), R.Tensor((2560,), dtype="float16"), R.Tensor((2560,), dtype="float16"), R.Tensor((7680, 320), dtype="uint32"), R.Tensor((7680, 80), dtype="float16"), R.Tensor((7680,), dtype="float16"), R.Tensor((2560, 320), dtype="uint32"), R.Tensor((2560, 80), dtype="float16"), R.Tensor((2560,), dtype="float16"), R.Tensor((10240, 320), dtype="uint32"), R.Tensor((10240, 80), dtype="float16"), R.Tensor((10240,), dtype="float32"), R.Tensor((2560, 1280), dtype="uint32"), R.Tensor((2560, 320), dtype="float16"), R.Tensor((2560,), dtype="float32"), R.Tensor((2560,), dtype="float16"), R.Tensor((2560,), dtype="float16"), R.Tensor((2560,), dtype="float16"), R.Tensor((2560,), dtype="float16"), R.Tensor((7680, 320), dtype="uint32"), R.Tensor((7680, 80), dtype="float16"), R.Tensor((7680,), dtype="float16"), R.Tensor((2560, 320), dtype="uint32"), R.Tensor((2560, 80), dtype="float16"), R.Tensor((2560,), dtype="float16"), R.Tensor((10240, 320), dtype="uint32"), R.Tensor((10240, 80), dtype="float16"), R.Tensor((10240,), dtype="float32"), R.Tensor((2560, 1280), dtype="uint32"), R.Tensor((2560, 320), dtype="float16"), R.Tensor((2560,), dtype="float32"), R.Tensor((2560,), dtype="float16"), R.Tensor((2560,), dtype="float16"), R.Tensor((2560,), dtype="float16"), R.Tensor((2560,), dtype="float16"), R.Tensor((7680, 320), dtype="uint32"), R.Tensor((7680, 80), dtype="float16"), R.Tensor((7680,), dtype="float16"), R.Tensor((2560, 320), dtype="uint32"), R.Tensor((2560, 80), dtype="float16"), R.Tensor((2560,), dtype="float16"), R.Tensor((10240, 320), dtype="uint32"), R.Tensor((10240, 80), dtype="float16"), R.Tensor((10240,), dtype="float32"), R.Tensor((2560, 1280), dtype="uint32"), R.Tensor((2560, 320), dtype="float16"), R.Tensor((2560,), dtype="float32"), R.Tensor((2560,), dtype="float16"), R.Tensor((2560,), dtype="float16"), R.Tensor((2560,), dtype="float16"), R.Tensor((2560,), dtype="float16"), R.Tensor((7680, 320), dtype="uint32"), R.Tensor((7680, 80), dtype="float16"), R.Tensor((7680,), dtype="float16"), R.Tensor((2560, 320), dtype="uint32"), R.Tensor((2560, 80), dtype="float16"), R.Tensor((2560,), dtype="float16"), R.Tensor((10240, 320), dtype="uint32"), R.Tensor((10240, 80), dtype="float16"), R.Tensor((10240,), dtype="float32"), R.Tensor((2560, 1280), dtype="uint32"), R.Tensor((2560, 320), dtype="float16"), R.Tensor((2560,), dtype="float32"), R.Tensor((2560,), dtype="float16"), R.Tensor((2560,), dtype="float16"), R.Tensor((2560,), dtype="float16"), R.Tensor((2560,), dtype="float16"), R.Tensor((7680, 320), dtype="uint32"), R.Tensor((7680, 80), dtype="float16"), R.Tensor((7680,), dtype="float16"), R.Tensor((2560, 320), dtype="uint32"), R.Tensor((2560, 80), dtype="float16"), R.Tensor((2560,), dtype="float16"), R.Tensor((10240, 320), dtype="uint32"), R.Tensor((10240, 80), dtype="float16"), R.Tensor((10240,), dtype="float32"), R.Tensor((2560, 1280), dtype="uint32"), R.Tensor((2560, 320), dtype="float16"), R.Tensor((2560,), dtype="float32"), R.Tensor((2560,), dtype="float16"), R.Tensor((2560,), dtype="float16"), R.Tensor((2560,), dtype="float16"), R.Tensor((2560,), dtype="float16"), R.Tensor((7680, 320), dtype="uint32"), R.Tensor((7680, 80), dtype="float16"), R.Tensor((7680,), dtype="float16"), R.Tensor((2560, 320), dtype="uint32"), R.Tensor((2560, 80), dtype="float16"), R.Tensor((2560,), dtype="float16"), R.Tensor((10240, 320), dtype="uint32"), R.Tensor((10240, 80), dtype="float16"), R.Tensor((10240,), dtype="float32"), R.Tensor((2560, 1280), dtype="uint32"), R.Tensor((2560, 320), dtype="float16"), R.Tensor((2560,), dtype="float32"), R.Tensor((2560,), dtype="float16"), R.Tensor((2560,), dtype="float16"), R.Tensor((2560,), dtype="float16"), R.Tensor((2560,), dtype="float16"), R.Tensor((7680, 320), dtype="uint32"), R.Tensor((7680, 80), dtype="float16"), R.Tensor((7680,), dtype="float16"), R.Tensor((2560, 320), dtype="uint32"), R.Tensor((2560, 80), dtype="float16"), R.Tensor((2560,), dtype="float16"), R.Tensor((10240, 320), dtype="uint32"), R.Tensor((10240, 80), dtype="float16"), R.Tensor((10240,), dtype="float32"), R.Tensor((2560, 1280), dtype="uint32"), R.Tensor((2560, 320), dtype="float16"), R.Tensor((2560,), dtype="float32"), R.Tensor((2560,), dtype="float16"), R.Tensor((2560,), dtype="float16"), R.Tensor((2560,), dtype="float16"), R.Tensor((2560,), dtype="float16"), R.Tensor((7680, 320), dtype="uint32"), R.Tensor((7680, 80), dtype="float16"), R.Tensor((7680,), dtype="float16"), R.Tensor((2560, 320), dtype="uint32"), R.Tensor((2560, 80), dtype="float16"), R.Tensor((2560,), dtype="float16"), R.Tensor((10240, 320), dtype="uint32"), R.Tensor((10240, 80), dtype="float16"), R.Tensor((10240,), dtype="float32"), R.Tensor((2560, 1280), dtype="uint32"), R.Tensor((2560, 320), dtype="float16"), R.Tensor((2560,), dtype="float32"), R.Tensor((2560,), dtype="float16"), R.Tensor((2560,), dtype="float16"), R.Tensor((2560,), dtype="float16"), R.Tensor((2560,), dtype="float16"), R.Tensor((7680, 320), dtype="uint32"), R.Tensor((7680, 80), dtype="float16"), R.Tensor((7680,), dtype="float16"), R.Tensor((2560, 320), dtype="uint32"), R.Tensor((2560, 80), dtype="float16"), R.Tensor((2560,), dtype="float16"), R.Tensor((10240, 320), dtype="uint32"), R.Tensor((10240, 80), dtype="float16"), R.Tensor((10240,), dtype="float32"), R.Tensor((2560, 1280), dtype="uint32"), R.Tensor((2560, 320), dtype="float16"), R.Tensor((2560,), dtype="float32"), R.Tensor((2560,), dtype="float16"), R.Tensor((2560,), dtype="float16"), R.Tensor((2560,), dtype="float16"), R.Tensor((2560,), dtype="float16"), R.Tensor((7680, 320), dtype="uint32"), R.Tensor((7680, 80), dtype="float16"), R.Tensor((7680,), dtype="float16"), R.Tensor((2560, 320), dtype="uint32"), R.Tensor((2560, 80), dtype="float16"), R.Tensor((2560,), dtype="float16"), R.Tensor((10240, 320), dtype="uint32"), R.Tensor((10240, 80), dtype="float16"), R.Tensor((10240,), dtype="float32"), R.Tensor((2560, 1280), dtype="uint32"), R.Tensor((2560, 320), dtype="float16"), R.Tensor((2560,), dtype="float32"), R.Tensor((2560,), dtype="float16"), R.Tensor((2560,), dtype="float16"), R.Tensor((2560,), dtype="float16"), R.Tensor((2560,), dtype="float16"), R.Tensor((7680, 320), dtype="uint32"), R.Tensor((7680, 80), dtype="float16"), R.Tensor((7680,), dtype="float16"), R.Tensor((2560, 320), dtype="uint32"), R.Tensor((2560, 80), dtype="float16"), R.Tensor((2560,), dtype="float16"), R.Tensor((10240, 320), dtype="uint32"), R.Tensor((10240, 80), dtype="float16"), R.Tensor((10240,), dtype="float32"), R.Tensor((2560, 1280), dtype="uint32"), R.Tensor((2560, 320), dtype="float16"), R.Tensor((2560,), dtype="float32"), R.Tensor((2560,), dtype="float16"), R.Tensor((2560,), dtype="float16"), R.Tensor((2560,), dtype="float16"), R.Tensor((2560,), dtype="float16"), R.Tensor((7680, 320), dtype="uint32"), R.Tensor((7680, 80), dtype="float16"), R.Tensor((7680,), dtype="float16"), R.Tensor((2560, 320), dtype="uint32"), R.Tensor((2560, 80), dtype="float16"), R.Tensor((2560,), dtype="float16"), R.Tensor((10240, 320), dtype="uint32"), R.Tensor((10240, 80), dtype="float16"), R.Tensor((10240,), dtype="float32"), R.Tensor((2560, 1280), dtype="uint32"), R.Tensor((2560, 320), dtype="float16"), R.Tensor((2560,), dtype="float32"), R.Tensor((2560,), dtype="float16"), R.Tensor((2560,), dtype="float16"), R.Tensor((2560,), dtype="float16"), R.Tensor((2560,), dtype="float16"), R.Tensor((7680, 320), dtype="uint32"), R.Tensor((7680, 80), dtype="float16"), R.Tensor((7680,), dtype="float16"), R.Tensor((2560, 320), dtype="uint32"), R.Tensor((2560, 80), dtype="float16"), R.Tensor((2560,), dtype="float16"), R.Tensor((10240, 320), dtype="uint32"), R.Tensor((10240, 80), dtype="float16"), R.Tensor((10240,), dtype="float32"), R.Tensor((2560, 1280), dtype="uint32"), R.Tensor((2560, 320), dtype="float16"), R.Tensor((2560,), dtype="float32"), R.Tensor((2560,), dtype="float16"), R.Tensor((2560,), dtype="float16"), R.Tensor((2560,), dtype="float16"), R.Tensor((2560,), dtype="float16"), R.Tensor((7680, 320), dtype="uint32"), R.Tensor((7680, 80), dtype="float16"), R.Tensor((7680,), dtype="float16"), R.Tensor((2560, 320), dtype="uint32"), R.Tensor((2560, 80), dtype="float16"), R.Tensor((2560,), dtype="float16"), R.Tensor((10240, 320), dtype="uint32"), R.Tensor((10240, 80), dtype="float16"), R.Tensor((10240,), dtype="float32"), R.Tensor((2560, 1280), dtype="uint32"), R.Tensor((2560, 320), dtype="float16"), R.Tensor((2560,), dtype="float32"), R.Tensor((2560,), dtype="float16"), R.Tensor((2560,), dtype="float16"), R.Tensor((2560,), dtype="float16"), R.Tensor((2560,), dtype="float16"), R.Tensor((7680, 320), dtype="uint32"), R.Tensor((7680, 80), dtype="float16"), R.Tensor((7680,), dtype="float16"), R.Tensor((2560, 320), dtype="uint32"), R.Tensor((2560, 80), dtype="float16"), R.Tensor((2560,), dtype="float16"), R.Tensor((10240, 320), dtype="uint32"), R.Tensor((10240, 80), dtype="float16"), R.Tensor((10240,), dtype="float32"), R.Tensor((2560, 1280), dtype="uint32"), R.Tensor((2560, 320), dtype="float16"), R.Tensor((2560,), dtype="float32"), R.Tensor((2560,), dtype="float16"), R.Tensor((2560,), dtype="float16"), R.Tensor((2560,), dtype="float16"), R.Tensor((2560,), dtype="float16"), R.Tensor((7680, 320), dtype="uint32"), R.Tensor((7680, 80), dtype="float16"), R.Tensor((7680,), dtype="float16"), R.Tensor((2560, 320), dtype="uint32"), R.Tensor((2560, 80), dtype="float16"), R.Tensor((2560,), dtype="float16"), R.Tensor((10240, 320), dtype="uint32"), R.Tensor((10240, 80), dtype="float16"), R.Tensor((10240,), dtype="float32"), R.Tensor((2560, 1280), dtype="uint32"), R.Tensor((2560, 320), dtype="float16"), R.Tensor((2560,), dtype="float32"), R.Tensor((2560,), dtype="float16"), R.Tensor((2560,), dtype="float16"), R.Tensor(("vocab_size", 320), dtype="uint32"), R.Tensor(("vocab_size", 80), dtype="float16"))) -> R.Tensor(("seq_len", 2560), dtype="float16"):
        seq_len = T.int64()
        vocab_size = T.int64()
        R.func_attr({"num_input": 1, "relax.memory_plan_dynamic_func_output": True, "tir_non_negative_var": ["vocab_size"], "tir_var_upper_bound": {"batch_size": 128, "seq_len": 2048, "total_seq_len": 2048}})
        cls = Module
        with R.dataflow():
            gpt_neox_embed_in_q_weight: R.Tensor((vocab_size, 320), dtype="uint32") = packed_params[0]
            gpt_neox_embed_in_q_scale: R.Tensor((vocab_size, 80), dtype="float16") = packed_params[1]
            gv = R.call_tir(cls.fused_dequantize_take1, (gpt_neox_embed_in_q_weight, gpt_neox_embed_in_q_scale, input_ids), out_sinfo=R.Tensor((seq_len, 2560), dtype="float16"))
            R.output(gv)
        return gv

    @R.function
    def multinomial_from_uniform(probs: R.Tensor(("batch_size", "vocab_size"), dtype="float32"), uniform_samples: R.Tensor(("num_samples",), dtype="float32"), sample_indices: R.Tensor(("num_samples",), dtype="int32")) -> R.Tensor(("num_samples",), dtype="int32"):
        num_samples = T.int64(is_size_var=True)
        batch_size = T.int64(is_size_var=True)
        vocab_size = T.int64(is_size_var=True)
        R.func_attr({"relax.memory_plan_dynamic_func_output": True, "tir_non_negative_var": ["vocab_size"], "tir_var_upper_bound": {"batch_size": 128, "num_positions": 768, "num_samples": 128}})
        cls = Module
        with R.dataflow():
            uniform_samples_1: R.Tensor((num_samples, 1), dtype="float32") = R.call_pure_packed("vm.builtin.reshape", uniform_samples, R.shape([num_samples, 1]), sinfo_args=(R.Tensor((num_samples, 1), dtype="float32"),))
            sample_indices_1: R.Tensor((num_samples, 1), dtype="int32") = R.call_pure_packed("vm.builtin.reshape", sample_indices, R.shape([num_samples, 1]), sinfo_args=(R.Tensor((num_samples, 1), dtype="int32"),))
            nn_multinomial_from_uniform = R.call_tir(cls.parallel_sampling_from_prob, (probs, uniform_samples_1, sample_indices_1), out_sinfo=R.Tensor((num_samples, 1), dtype="int32"))
            gv: R.Tensor((num_samples,), dtype="int32") = R.call_pure_packed("vm.builtin.reshape", nn_multinomial_from_uniform, R.shape([num_samples]), sinfo_args=(R.Tensor((num_samples,), dtype="int32"),))
            R.output(gv)
        return gv

    @R.function
    def prefill(input_embed: R.Tensor((1, "seq_len", 2560), dtype="float16"), paged_kv_cache: R.Object, packed_params: R.Tuple(R.Tensor(("vocab_size", 320), dtype="uint32"), R.Tensor(("vocab_size", 80), dtype="float16"), R.Tensor((2560,), dtype="float16"), R.Tensor((2560,), dtype="float16"), R.Tensor((2560,), dtype="float16"), R.Tensor((2560,), dtype="float16"), R.Tensor((7680, 320), dtype="uint32"), R.Tensor((7680, 80), dtype="float16"), R.Tensor((7680,), dtype="float16"), R.Tensor((2560, 320), dtype="uint32"), R.Tensor((2560, 80), dtype="float16"), R.Tensor((2560,), dtype="float16"), R.Tensor((10240, 320), dtype="uint32"), R.Tensor((10240, 80), dtype="float16"), R.Tensor((10240,), dtype="float32"), R.Tensor((2560, 1280), dtype="uint32"), R.Tensor((2560, 320), dtype="float16"), R.Tensor((2560,), dtype="float32"), R.Tensor((2560,), dtype="float16"), R.Tensor((2560,), dtype="float16"), R.Tensor((2560,), dtype="float16"), R.Tensor((2560,), dtype="float16"), R.Tensor((7680, 320), dtype="uint32"), R.Tensor((7680, 80), dtype="float16"), R.Tensor((7680,), dtype="float16"), R.Tensor((2560, 320), dtype="uint32"), R.Tensor((2560, 80), dtype="float16"), R.Tensor((2560,), dtype="float16"), R.Tensor((10240, 320), dtype="uint32"), R.Tensor((10240, 80), dtype="float16"), R.Tensor((10240,), dtype="float32"), R.Tensor((2560, 1280), dtype="uint32"), R.Tensor((2560, 320), dtype="float16"), R.Tensor((2560,), dtype="float32"), R.Tensor((2560,), dtype="float16"), R.Tensor((2560,), dtype="float16"), R.Tensor((2560,), dtype="float16"), R.Tensor((2560,), dtype="float16"), R.Tensor((7680, 320), dtype="uint32"), R.Tensor((7680, 80), dtype="float16"), R.Tensor((7680,), dtype="float16"), R.Tensor((2560, 320), dtype="uint32"), R.Tensor((2560, 80), dtype="float16"), R.Tensor((2560,), dtype="float16"), R.Tensor((10240, 320), dtype="uint32"), R.Tensor((10240, 80), dtype="float16"), R.Tensor((10240,), dtype="float32"), R.Tensor((2560, 1280), dtype="uint32"), R.Tensor((2560, 320), dtype="float16"), R.Tensor((2560,), dtype="float32"), R.Tensor((2560,), dtype="float16"), R.Tensor((2560,), dtype="float16"), R.Tensor((2560,), dtype="float16"), R.Tensor((2560,), dtype="float16"), R.Tensor((7680, 320), dtype="uint32"), R.Tensor((7680, 80), dtype="float16"), R.Tensor((7680,), dtype="float16"), R.Tensor((2560, 320), dtype="uint32"), R.Tensor((2560, 80), dtype="float16"), R.Tensor((2560,), dtype="float16"), R.Tensor((10240, 320), dtype="uint32"), R.Tensor((10240, 80), dtype="float16"), R.Tensor((10240,), dtype="float32"), R.Tensor((2560, 1280), dtype="uint32"), R.Tensor((2560, 320), dtype="float16"), R.Tensor((2560,), dtype="float32"), R.Tensor((2560,), dtype="float16"), R.Tensor((2560,), dtype="float16"), R.Tensor((2560,), dtype="float16"), R.Tensor((2560,), dtype="float16"), R.Tensor((7680, 320), dtype="uint32"), R.Tensor((7680, 80), dtype="float16"), R.Tensor((7680,), dtype="float16"), R.Tensor((2560, 320), dtype="uint32"), R.Tensor((2560, 80), dtype="float16"), R.Tensor((2560,), dtype="float16"), R.Tensor((10240, 320), dtype="uint32"), R.Tensor((10240, 80), dtype="float16"), R.Tensor((10240,), dtype="float32"), R.Tensor((2560, 1280), dtype="uint32"), R.Tensor((2560, 320), dtype="float16"), R.Tensor((2560,), dtype="float32"), R.Tensor((2560,), dtype="float16"), R.Tensor((2560,), dtype="float16"), R.Tensor((2560,), dtype="float16"), R.Tensor((2560,), dtype="float16"), R.Tensor((7680, 320), dtype="uint32"), R.Tensor((7680, 80), dtype="float16"), R.Tensor((7680,), dtype="float16"), R.Tensor((2560, 320), dtype="uint32"), R.Tensor((2560, 80), dtype="float16"), R.Tensor((2560,), dtype="float16"), R.Tensor((10240, 320), dtype="uint32"), R.Tensor((10240, 80), dtype="float16"), R.Tensor((10240,), dtype="float32"), R.Tensor((2560, 1280), dtype="uint32"), R.Tensor((2560, 320), dtype="float16"), R.Tensor((2560,), dtype="float32"), R.Tensor((2560,), dtype="float16"), R.Tensor((2560,), dtype="float16"), R.Tensor((2560,), dtype="float16"), R.Tensor((2560,), dtype="float16"), R.Tensor((7680, 320), dtype="uint32"), R.Tensor((7680, 80), dtype="float16"), R.Tensor((7680,), dtype="float16"), R.Tensor((2560, 320), dtype="uint32"), R.Tensor((2560, 80), dtype="float16"), R.Tensor((2560,), dtype="float16"), R.Tensor((10240, 320), dtype="uint32"), R.Tensor((10240, 80), dtype="float16"), R.Tensor((10240,), dtype="float32"), R.Tensor((2560, 1280), dtype="uint32"), R.Tensor((2560, 320), dtype="float16"), R.Tensor((2560,), dtype="float32"), R.Tensor((2560,), dtype="float16"), R.Tensor((2560,), dtype="float16"), R.Tensor((2560,), dtype="float16"), R.Tensor((2560,), dtype="float16"), R.Tensor((7680, 320), dtype="uint32"), R.Tensor((7680, 80), dtype="float16"), R.Tensor((7680,), dtype="float16"), R.Tensor((2560, 320), dtype="uint32"), R.Tensor((2560, 80), dtype="float16"), R.Tensor((2560,), dtype="float16"), R.Tensor((10240, 320), dtype="uint32"), R.Tensor((10240, 80), dtype="float16"), R.Tensor((10240,), dtype="float32"), R.Tensor((2560, 1280), dtype="uint32"), R.Tensor((2560, 320), dtype="float16"), R.Tensor((2560,), dtype="float32"), R.Tensor((2560,), dtype="float16"), R.Tensor((2560,), dtype="float16"), R.Tensor((2560,), dtype="float16"), R.Tensor((2560,), dtype="float16"), R.Tensor((7680, 320), dtype="uint32"), R.Tensor((7680, 80), dtype="float16"), R.Tensor((7680,), dtype="float16"), R.Tensor((2560, 320), dtype="uint32"), R.Tensor((2560, 80), dtype="float16"), R.Tensor((2560,), dtype="float16"), R.Tensor((10240, 320), dtype="uint32"), R.Tensor((10240, 80), dtype="float16"), R.Tensor((10240,), dtype="float32"), R.Tensor((2560, 1280), dtype="uint32"), R.Tensor((2560, 320), dtype="float16"), R.Tensor((2560,), dtype="float32"), R.Tensor((2560,), dtype="float16"), R.Tensor((2560,), dtype="float16"), R.Tensor((2560,), dtype="float16"), R.Tensor((2560,), dtype="float16"), R.Tensor((7680, 320), dtype="uint32"), R.Tensor((7680, 80), dtype="float16"), R.Tensor((7680,), dtype="float16"), R.Tensor((2560, 320), dtype="uint32"), R.Tensor((2560, 80), dtype="float16"), R.Tensor((2560,), dtype="float16"), R.Tensor((10240, 320), dtype="uint32"), R.Tensor((10240, 80), dtype="float16"), R.Tensor((10240,), dtype="float32"), R.Tensor((2560, 1280), dtype="uint32"), R.Tensor((2560, 320), dtype="float16"), R.Tensor((2560,), dtype="float32"), R.Tensor((2560,), dtype="float16"), R.Tensor((2560,), dtype="float16"), R.Tensor((2560,), dtype="float16"), R.Tensor((2560,), dtype="float16"), R.Tensor((7680, 320), dtype="uint32"), R.Tensor((7680, 80), dtype="float16"), R.Tensor((7680,), dtype="float16"), R.Tensor((2560, 320), dtype="uint32"), R.Tensor((2560, 80), dtype="float16"), R.Tensor((2560,), dtype="float16"), R.Tensor((10240, 320), dtype="uint32"), R.Tensor((10240, 80), dtype="float16"), R.Tensor((10240,), dtype="float32"), R.Tensor((2560, 1280), dtype="uint32"), R.Tensor((2560, 320), dtype="float16"), R.Tensor((2560,), dtype="float32"), R.Tensor((2560,), dtype="float16"), R.Tensor((2560,), dtype="float16"), R.Tensor((2560,), dtype="float16"), R.Tensor((2560,), dtype="float16"), R.Tensor((7680, 320), dtype="uint32"), R.Tensor((7680, 80), dtype="float16"), R.Tensor((7680,), dtype="float16"), R.Tensor((2560, 320), dtype="uint32"), R.Tensor((2560, 80), dtype="float16"), R.Tensor((2560,), dtype="float16"), R.Tensor((10240, 320), dtype="uint32"), R.Tensor((10240, 80), dtype="float16"), R.Tensor((10240,), dtype="float32"), R.Tensor((2560, 1280), dtype="uint32"), R.Tensor((2560, 320), dtype="float16"), R.Tensor((2560,), dtype="float32"), R.Tensor((2560,), dtype="float16"), R.Tensor((2560,), dtype="float16"), R.Tensor((2560,), dtype="float16"), R.Tensor((2560,), dtype="float16"), R.Tensor((7680, 320), dtype="uint32"), R.Tensor((7680, 80), dtype="float16"), R.Tensor((7680,), dtype="float16"), R.Tensor((2560, 320), dtype="uint32"), R.Tensor((2560, 80), dtype="float16"), R.Tensor((2560,), dtype="float16"), R.Tensor((10240, 320), dtype="uint32"), R.Tensor((10240, 80), dtype="float16"), R.Tensor((10240,), dtype="float32"), R.Tensor((2560, 1280), dtype="uint32"), R.Tensor((2560, 320), dtype="float16"), R.Tensor((2560,), dtype="float32"), R.Tensor((2560,), dtype="float16"), R.Tensor((2560,), dtype="float16"), R.Tensor((2560,), dtype="float16"), R.Tensor((2560,), dtype="float16"), R.Tensor((7680, 320), dtype="uint32"), R.Tensor((7680, 80), dtype="float16"), R.Tensor((7680,), dtype="float16"), R.Tensor((2560, 320), dtype="uint32"), R.Tensor((2560, 80), dtype="float16"), R.Tensor((2560,), dtype="float16"), R.Tensor((10240, 320), dtype="uint32"), R.Tensor((10240, 80), dtype="float16"), R.Tensor((10240,), dtype="float32"), R.Tensor((2560, 1280), dtype="uint32"), R.Tensor((2560, 320), dtype="float16"), R.Tensor((2560,), dtype="float32"), R.Tensor((2560,), dtype="float16"), R.Tensor((2560,), dtype="float16"), R.Tensor((2560,), dtype="float16"), R.Tensor((2560,), dtype="float16"), R.Tensor((7680, 320), dtype="uint32"), R.Tensor((7680, 80), dtype="float16"), R.Tensor((7680,), dtype="float16"), R.Tensor((2560, 320), dtype="uint32"), R.Tensor((2560, 80), dtype="float16"), R.Tensor((2560,), dtype="float16"), R.Tensor((10240, 320), dtype="uint32"), R.Tensor((10240, 80), dtype="float16"), R.Tensor((10240,), dtype="float32"), R.Tensor((2560, 1280), dtype="uint32"), R.Tensor((2560, 320), dtype="float16"), R.Tensor((2560,), dtype="float32"), R.Tensor((2560,), dtype="float16"), R.Tensor((2560,), dtype="float16"), R.Tensor((2560,), dtype="float16"), R.Tensor((2560,), dtype="float16"), R.Tensor((7680, 320), dtype="uint32"), R.Tensor((7680, 80), dtype="float16"), R.Tensor((7680,), dtype="float16"), R.Tensor((2560, 320), dtype="uint32"), R.Tensor((2560, 80), dtype="float16"), R.Tensor((2560,), dtype="float16"), R.Tensor((10240, 320), dtype="uint32"), R.Tensor((10240, 80), dtype="float16"), R.Tensor((10240,), dtype="float32"), R.Tensor((2560, 1280), dtype="uint32"), R.Tensor((2560, 320), dtype="float16"), R.Tensor((2560,), dtype="float32"), R.Tensor((2560,), dtype="float16"), R.Tensor((2560,), dtype="float16"), R.Tensor((2560,), dtype="float16"), R.Tensor((2560,), dtype="float16"), R.Tensor((7680, 320), dtype="uint32"), R.Tensor((7680, 80), dtype="float16"), R.Tensor((7680,), dtype="float16"), R.Tensor((2560, 320), dtype="uint32"), R.Tensor((2560, 80), dtype="float16"), R.Tensor((2560,), dtype="float16"), R.Tensor((10240, 320), dtype="uint32"), R.Tensor((10240, 80), dtype="float16"), R.Tensor((10240,), dtype="float32"), R.Tensor((2560, 1280), dtype="uint32"), R.Tensor((2560, 320), dtype="float16"), R.Tensor((2560,), dtype="float32"), R.Tensor((2560,), dtype="float16"), R.Tensor((2560,), dtype="float16"), R.Tensor((2560,), dtype="float16"), R.Tensor((2560,), dtype="float16"), R.Tensor((7680, 320), dtype="uint32"), R.Tensor((7680, 80), dtype="float16"), R.Tensor((7680,), dtype="float16"), R.Tensor((2560, 320), dtype="uint32"), R.Tensor((2560, 80), dtype="float16"), R.Tensor((2560,), dtype="float16"), R.Tensor((10240, 320), dtype="uint32"), R.Tensor((10240, 80), dtype="float16"), R.Tensor((10240,), dtype="float32"), R.Tensor((2560, 1280), dtype="uint32"), R.Tensor((2560, 320), dtype="float16"), R.Tensor((2560,), dtype="float32"), R.Tensor((2560,), dtype="float16"), R.Tensor((2560,), dtype="float16"), R.Tensor((2560,), dtype="float16"), R.Tensor((2560,), dtype="float16"), R.Tensor((7680, 320), dtype="uint32"), R.Tensor((7680, 80), dtype="float16"), R.Tensor((7680,), dtype="float16"), R.Tensor((2560, 320), dtype="uint32"), R.Tensor((2560, 80), dtype="float16"), R.Tensor((2560,), dtype="float16"), R.Tensor((10240, 320), dtype="uint32"), R.Tensor((10240, 80), dtype="float16"), R.Tensor((10240,), dtype="float32"), R.Tensor((2560, 1280), dtype="uint32"), R.Tensor((2560, 320), dtype="float16"), R.Tensor((2560,), dtype="float32"), R.Tensor((2560,), dtype="float16"), R.Tensor((2560,), dtype="float16"), R.Tensor((2560,), dtype="float16"), R.Tensor((2560,), dtype="float16"), R.Tensor((7680, 320), dtype="uint32"), R.Tensor((7680, 80), dtype="float16"), R.Tensor((7680,), dtype="float16"), R.Tensor((2560, 320), dtype="uint32"), R.Tensor((2560, 80), dtype="float16"), R.Tensor((2560,), dtype="float16"), R.Tensor((10240, 320), dtype="uint32"), R.Tensor((10240, 80), dtype="float16"), R.Tensor((10240,), dtype="float32"), R.Tensor((2560, 1280), dtype="uint32"), R.Tensor((2560, 320), dtype="float16"), R.Tensor((2560,), dtype="float32"), R.Tensor((2560,), dtype="float16"), R.Tensor((2560,), dtype="float16"), R.Tensor((2560,), dtype="float16"), R.Tensor((2560,), dtype="float16"), R.Tensor((7680, 320), dtype="uint32"), R.Tensor((7680, 80), dtype="float16"), R.Tensor((7680,), dtype="float16"), R.Tensor((2560, 320), dtype="uint32"), R.Tensor((2560, 80), dtype="float16"), R.Tensor((2560,), dtype="float16"), R.Tensor((10240, 320), dtype="uint32"), R.Tensor((10240, 80), dtype="float16"), R.Tensor((10240,), dtype="float32"), R.Tensor((2560, 1280), dtype="uint32"), R.Tensor((2560, 320), dtype="float16"), R.Tensor((2560,), dtype="float32"), R.Tensor((2560,), dtype="float16"), R.Tensor((2560,), dtype="float16"), R.Tensor((2560,), dtype="float16"), R.Tensor((2560,), dtype="float16"), R.Tensor((7680, 320), dtype="uint32"), R.Tensor((7680, 80), dtype="float16"), R.Tensor((7680,), dtype="float16"), R.Tensor((2560, 320), dtype="uint32"), R.Tensor((2560, 80), dtype="float16"), R.Tensor((2560,), dtype="float16"), R.Tensor((10240, 320), dtype="uint32"), R.Tensor((10240, 80), dtype="float16"), R.Tensor((10240,), dtype="float32"), R.Tensor((2560, 1280), dtype="uint32"), R.Tensor((2560, 320), dtype="float16"), R.Tensor((2560,), dtype="float32"), R.Tensor((2560,), dtype="float16"), R.Tensor((2560,), dtype="float16"), R.Tensor((2560,), dtype="float16"), R.Tensor((2560,), dtype="float16"), R.Tensor((7680, 320), dtype="uint32"), R.Tensor((7680, 80), dtype="float16"), R.Tensor((7680,), dtype="float16"), R.Tensor((2560, 320), dtype="uint32"), R.Tensor((2560, 80), dtype="float16"), R.Tensor((2560,), dtype="float16"), R.Tensor((10240, 320), dtype="uint32"), R.Tensor((10240, 80), dtype="float16"), R.Tensor((10240,), dtype="float32"), R.Tensor((2560, 1280), dtype="uint32"), R.Tensor((2560, 320), dtype="float16"), R.Tensor((2560,), dtype="float32"), R.Tensor((2560,), dtype="float16"), R.Tensor((2560,), dtype="float16"), R.Tensor((2560,), dtype="float16"), R.Tensor((2560,), dtype="float16"), R.Tensor((7680, 320), dtype="uint32"), R.Tensor((7680, 80), dtype="float16"), R.Tensor((7680,), dtype="float16"), R.Tensor((2560, 320), dtype="uint32"), R.Tensor((2560, 80), dtype="float16"), R.Tensor((2560,), dtype="float16"), R.Tensor((10240, 320), dtype="uint32"), R.Tensor((10240, 80), dtype="float16"), R.Tensor((10240,), dtype="float32"), R.Tensor((2560, 1280), dtype="uint32"), R.Tensor((2560, 320), dtype="float16"), R.Tensor((2560,), dtype="float32"), R.Tensor((2560,), dtype="float16"), R.Tensor((2560,), dtype="float16"), R.Tensor((2560,), dtype="float16"), R.Tensor((2560,), dtype="float16"), R.Tensor((7680, 320), dtype="uint32"), R.Tensor((7680, 80), dtype="float16"), R.Tensor((7680,), dtype="float16"), R.Tensor((2560, 320), dtype="uint32"), R.Tensor((2560, 80), dtype="float16"), R.Tensor((2560,), dtype="float16"), R.Tensor((10240, 320), dtype="uint32"), R.Tensor((10240, 80), dtype="float16"), R.Tensor((10240,), dtype="float32"), R.Tensor((2560, 1280), dtype="uint32"), R.Tensor((2560, 320), dtype="float16"), R.Tensor((2560,), dtype="float32"), R.Tensor((2560,), dtype="float16"), R.Tensor((2560,), dtype="float16"), R.Tensor((2560,), dtype="float16"), R.Tensor((2560,), dtype="float16"), R.Tensor((7680, 320), dtype="uint32"), R.Tensor((7680, 80), dtype="float16"), R.Tensor((7680,), dtype="float16"), R.Tensor((2560, 320), dtype="uint32"), R.Tensor((2560, 80), dtype="float16"), R.Tensor((2560,), dtype="float16"), R.Tensor((10240, 320), dtype="uint32"), R.Tensor((10240, 80), dtype="float16"), R.Tensor((10240,), dtype="float32"), R.Tensor((2560, 1280), dtype="uint32"), R.Tensor((2560, 320), dtype="float16"), R.Tensor((2560,), dtype="float32"), R.Tensor((2560,), dtype="float16"), R.Tensor((2560,), dtype="float16"), R.Tensor((2560,), dtype="float16"), R.Tensor((2560,), dtype="float16"), R.Tensor((7680, 320), dtype="uint32"), R.Tensor((7680, 80), dtype="float16"), R.Tensor((7680,), dtype="float16"), R.Tensor((2560, 320), dtype="uint32"), R.Tensor((2560, 80), dtype="float16"), R.Tensor((2560,), dtype="float16"), R.Tensor((10240, 320), dtype="uint32"), R.Tensor((10240, 80), dtype="float16"), R.Tensor((10240,), dtype="float32"), R.Tensor((2560, 1280), dtype="uint32"), R.Tensor((2560, 320), dtype="float16"), R.Tensor((2560,), dtype="float32"), R.Tensor((2560,), dtype="float16"), R.Tensor((2560,), dtype="float16"), R.Tensor((2560,), dtype="float16"), R.Tensor((2560,), dtype="float16"), R.Tensor((7680, 320), dtype="uint32"), R.Tensor((7680, 80), dtype="float16"), R.Tensor((7680,), dtype="float16"), R.Tensor((2560, 320), dtype="uint32"), R.Tensor((2560, 80), dtype="float16"), R.Tensor((2560,), dtype="float16"), R.Tensor((10240, 320), dtype="uint32"), R.Tensor((10240, 80), dtype="float16"), R.Tensor((10240,), dtype="float32"), R.Tensor((2560, 1280), dtype="uint32"), R.Tensor((2560, 320), dtype="float16"), R.Tensor((2560,), dtype="float32"), R.Tensor((2560,), dtype="float16"), R.Tensor((2560,), dtype="float16"), R.Tensor((2560,), dtype="float16"), R.Tensor((2560,), dtype="float16"), R.Tensor((7680, 320), dtype="uint32"), R.Tensor((7680, 80), dtype="float16"), R.Tensor((7680,), dtype="float16"), R.Tensor((2560, 320), dtype="uint32"), R.Tensor((2560, 80), dtype="float16"), R.Tensor((2560,), dtype="float16"), R.Tensor((10240, 320), dtype="uint32"), R.Tensor((10240, 80), dtype="float16"), R.Tensor((10240,), dtype="float32"), R.Tensor((2560, 1280), dtype="uint32"), R.Tensor((2560, 320), dtype="float16"), R.Tensor((2560,), dtype="float32"), R.Tensor((2560,), dtype="float16"), R.Tensor((2560,), dtype="float16"), R.Tensor((2560,), dtype="float16"), R.Tensor((2560,), dtype="float16"), R.Tensor((7680, 320), dtype="uint32"), R.Tensor((7680, 80), dtype="float16"), R.Tensor((7680,), dtype="float16"), R.Tensor((2560, 320), dtype="uint32"), R.Tensor((2560, 80), dtype="float16"), R.Tensor((2560,), dtype="float16"), R.Tensor((10240, 320), dtype="uint32"), R.Tensor((10240, 80), dtype="float16"), R.Tensor((10240,), dtype="float32"), R.Tensor((2560, 1280), dtype="uint32"), R.Tensor((2560, 320), dtype="float16"), R.Tensor((2560,), dtype="float32"), R.Tensor((2560,), dtype="float16"), R.Tensor((2560,), dtype="float16"), R.Tensor((2560,), dtype="float16"), R.Tensor((2560,), dtype="float16"), R.Tensor((7680, 320), dtype="uint32"), R.Tensor((7680, 80), dtype="float16"), R.Tensor((7680,), dtype="float16"), R.Tensor((2560, 320), dtype="uint32"), R.Tensor((2560, 80), dtype="float16"), R.Tensor((2560,), dtype="float16"), R.Tensor((10240, 320), dtype="uint32"), R.Tensor((10240, 80), dtype="float16"), R.Tensor((10240,), dtype="float32"), R.Tensor((2560, 1280), dtype="uint32"), R.Tensor((2560, 320), dtype="float16"), R.Tensor((2560,), dtype="float32"), R.Tensor((2560,), dtype="float16"), R.Tensor((2560,), dtype="float16"), R.Tensor((2560,), dtype="float16"), R.Tensor((2560,), dtype="float16"), R.Tensor((7680, 320), dtype="uint32"), R.Tensor((7680, 80), dtype="float16"), R.Tensor((7680,), dtype="float16"), R.Tensor((2560, 320), dtype="uint32"), R.Tensor((2560, 80), dtype="float16"), R.Tensor((2560,), dtype="float16"), R.Tensor((10240, 320), dtype="uint32"), R.Tensor((10240, 80), dtype="float16"), R.Tensor((10240,), dtype="float32"), R.Tensor((2560, 1280), dtype="uint32"), R.Tensor((2560, 320), dtype="float16"), R.Tensor((2560,), dtype="float32"), R.Tensor((2560,), dtype="float16"), R.Tensor((2560,), dtype="float16"), R.Tensor(("vocab_size", 320), dtype="uint32"), R.Tensor(("vocab_size", 80), dtype="float16"))) -> R.Tuple(R.Tensor((1, 1, "vocab_size"), dtype="float32"), R.Object):
        vocab_size = T.int64()
        seq_len = T.int64()
        R.func_attr({"num_input": 2, "pipeline_parallel_stages": 1, "relax.memory_plan_dynamic_func_output": True, "tir_non_negative_var": ["vocab_size"], "tir_var_upper_bound": {"batch_size": 128, "seq_len": 2048, "total_seq_len": 2048}})
        cls = Module
        with R.dataflow():
            gpt_neox_layers_0_input_layernorm_weight1: R.Tensor((2560,), dtype="float16") = packed_params[2]
            gpt_neox_layers_0_input_layernorm_bias1: R.Tensor((2560,), dtype="float16") = packed_params[3]
            gpt_neox_layers_0_post_attention_layernorm_weight1: R.Tensor((2560,), dtype="float16") = packed_params[4]
            gpt_neox_layers_0_post_attention_layernorm_bias1: R.Tensor((2560,), dtype="float16") = packed_params[5]
            gpt_neox_layers_0_attention_query_key_value_q_weight1: R.Tensor((7680, 320), dtype="uint32") = packed_params[6]
            gpt_neox_layers_0_attention_query_key_value_q_scale1: R.Tensor((7680, 80), dtype="float16") = packed_params[7]
            gpt_neox_layers_0_attention_query_key_value_bias1: R.Tensor((7680,), dtype="float16") = packed_params[8]
            gpt_neox_layers_0_attention_dense_q_weight1: R.Tensor((2560, 320), dtype="uint32") = packed_params[9]
            gpt_neox_layers_0_attention_dense_q_scale1: R.Tensor((2560, 80), dtype="float16") = packed_params[10]
            gpt_neox_layers_0_attention_dense_bias1: R.Tensor((2560,), dtype="float16") = packed_params[11]
            gpt_neox_layers_0_mlp_dense_h_to_4h_q_weight1: R.Tensor((10240, 320), dtype="uint32") = packed_params[12]
            gpt_neox_layers_0_mlp_dense_h_to_4h_q_scale1: R.Tensor((10240, 80), dtype="float16") = packed_params[13]
            gpt_neox_layers_0_mlp_dense_h_to_4h_bias1: R.Tensor((10240,), dtype="float32") = packed_params[14]
            gpt_neox_layers_0_mlp_dense_4h_to_h_q_weight1: R.Tensor((2560, 1280), dtype="uint32") = packed_params[15]
            gpt_neox_layers_0_mlp_dense_4h_to_h_q_scale1: R.Tensor((2560, 320), dtype="float16") = packed_params[16]
            gpt_neox_layers_0_mlp_dense_4h_to_h_bias1: R.Tensor((2560,), dtype="float32") = packed_params[17]
            gpt_neox_layers_1_input_layernorm_weight1: R.Tensor((2560,), dtype="float16") = packed_params[18]
            gpt_neox_layers_1_input_layernorm_bias1: R.Tensor((2560,), dtype="float16") = packed_params[19]
            gpt_neox_layers_1_post_attention_layernorm_weight1: R.Tensor((2560,), dtype="float16") = packed_params[20]
            gpt_neox_layers_1_post_attention_layernorm_bias1: R.Tensor((2560,), dtype="float16") = packed_params[21]
            gpt_neox_layers_1_attention_query_key_value_q_weight1: R.Tensor((7680, 320), dtype="uint32") = packed_params[22]
            gpt_neox_layers_1_attention_query_key_value_q_scale1: R.Tensor((7680, 80), dtype="float16") = packed_params[23]
            gpt_neox_layers_1_attention_query_key_value_bias1: R.Tensor((7680,), dtype="float16") = packed_params[24]
            gpt_neox_layers_1_attention_dense_q_weight1: R.Tensor((2560, 320), dtype="uint32") = packed_params[25]
            gpt_neox_layers_1_attention_dense_q_scale1: R.Tensor((2560, 80), dtype="float16") = packed_params[26]
            gpt_neox_layers_1_attention_dense_bias1: R.Tensor((2560,), dtype="float16") = packed_params[27]
            gpt_neox_layers_1_mlp_dense_h_to_4h_q_weight1: R.Tensor((10240, 320), dtype="uint32") = packed_params[28]
            gpt_neox_layers_1_mlp_dense_h_to_4h_q_scale1: R.Tensor((10240, 80), dtype="float16") = packed_params[29]
            gpt_neox_layers_1_mlp_dense_h_to_4h_bias1: R.Tensor((10240,), dtype="float32") = packed_params[30]
            gpt_neox_layers_1_mlp_dense_4h_to_h_q_weight1: R.Tensor((2560, 1280), dtype="uint32") = packed_params[31]
            gpt_neox_layers_1_mlp_dense_4h_to_h_q_scale1: R.Tensor((2560, 320), dtype="float16") = packed_params[32]
            gpt_neox_layers_1_mlp_dense_4h_to_h_bias1: R.Tensor((2560,), dtype="float32") = packed_params[33]
            gpt_neox_layers_2_input_layernorm_weight1: R.Tensor((2560,), dtype="float16") = packed_params[34]
            gpt_neox_layers_2_input_layernorm_bias1: R.Tensor((2560,), dtype="float16") = packed_params[35]
            gpt_neox_layers_2_post_attention_layernorm_weight1: R.Tensor((2560,), dtype="float16") = packed_params[36]
            gpt_neox_layers_2_post_attention_layernorm_bias1: R.Tensor((2560,), dtype="float16") = packed_params[37]
            gpt_neox_layers_2_attention_query_key_value_q_weight1: R.Tensor((7680, 320), dtype="uint32") = packed_params[38]
            gpt_neox_layers_2_attention_query_key_value_q_scale1: R.Tensor((7680, 80), dtype="float16") = packed_params[39]
            gpt_neox_layers_2_attention_query_key_value_bias1: R.Tensor((7680,), dtype="float16") = packed_params[40]
            gpt_neox_layers_2_attention_dense_q_weight1: R.Tensor((2560, 320), dtype="uint32") = packed_params[41]
            gpt_neox_layers_2_attention_dense_q_scale1: R.Tensor((2560, 80), dtype="float16") = packed_params[42]
            gpt_neox_layers_2_attention_dense_bias1: R.Tensor((2560,), dtype="float16") = packed_params[43]
            gpt_neox_layers_2_mlp_dense_h_to_4h_q_weight1: R.Tensor((10240, 320), dtype="uint32") = packed_params[44]
            gpt_neox_layers_2_mlp_dense_h_to_4h_q_scale1: R.Tensor((10240, 80), dtype="float16") = packed_params[45]
            gpt_neox_layers_2_mlp_dense_h_to_4h_bias1: R.Tensor((10240,), dtype="float32") = packed_params[46]
            gpt_neox_layers_2_mlp_dense_4h_to_h_q_weight1: R.Tensor((2560, 1280), dtype="uint32") = packed_params[47]
            gpt_neox_layers_2_mlp_dense_4h_to_h_q_scale1: R.Tensor((2560, 320), dtype="float16") = packed_params[48]
            gpt_neox_layers_2_mlp_dense_4h_to_h_bias1: R.Tensor((2560,), dtype="float32") = packed_params[49]
            gpt_neox_layers_3_input_layernorm_weight1: R.Tensor((2560,), dtype="float16") = packed_params[50]
            gpt_neox_layers_3_input_layernorm_bias1: R.Tensor((2560,), dtype="float16") = packed_params[51]
            gpt_neox_layers_3_post_attention_layernorm_weight1: R.Tensor((2560,), dtype="float16") = packed_params[52]
            gpt_neox_layers_3_post_attention_layernorm_bias1: R.Tensor((2560,), dtype="float16") = packed_params[53]
            gpt_neox_layers_3_attention_query_key_value_q_weight1: R.Tensor((7680, 320), dtype="uint32") = packed_params[54]
            gpt_neox_layers_3_attention_query_key_value_q_scale1: R.Tensor((7680, 80), dtype="float16") = packed_params[55]
            gpt_neox_layers_3_attention_query_key_value_bias1: R.Tensor((7680,), dtype="float16") = packed_params[56]
            gpt_neox_layers_3_attention_dense_q_weight1: R.Tensor((2560, 320), dtype="uint32") = packed_params[57]
            gpt_neox_layers_3_attention_dense_q_scale1: R.Tensor((2560, 80), dtype="float16") = packed_params[58]
            gpt_neox_layers_3_attention_dense_bias1: R.Tensor((2560,), dtype="float16") = packed_params[59]
            gpt_neox_layers_3_mlp_dense_h_to_4h_q_weight1: R.Tensor((10240, 320), dtype="uint32") = packed_params[60]
            gpt_neox_layers_3_mlp_dense_h_to_4h_q_scale1: R.Tensor((10240, 80), dtype="float16") = packed_params[61]
            gpt_neox_layers_3_mlp_dense_h_to_4h_bias1: R.Tensor((10240,), dtype="float32") = packed_params[62]
            gpt_neox_layers_3_mlp_dense_4h_to_h_q_weight1: R.Tensor((2560, 1280), dtype="uint32") = packed_params[63]
            gpt_neox_layers_3_mlp_dense_4h_to_h_q_scale1: R.Tensor((2560, 320), dtype="float16") = packed_params[64]
            gpt_neox_layers_3_mlp_dense_4h_to_h_bias1: R.Tensor((2560,), dtype="float32") = packed_params[65]
            gpt_neox_layers_4_input_layernorm_weight1: R.Tensor((2560,), dtype="float16") = packed_params[66]
            gpt_neox_layers_4_input_layernorm_bias1: R.Tensor((2560,), dtype="float16") = packed_params[67]
            gpt_neox_layers_4_post_attention_layernorm_weight1: R.Tensor((2560,), dtype="float16") = packed_params[68]
            gpt_neox_layers_4_post_attention_layernorm_bias1: R.Tensor((2560,), dtype="float16") = packed_params[69]
            gpt_neox_layers_4_attention_query_key_value_q_weight1: R.Tensor((7680, 320), dtype="uint32") = packed_params[70]
            gpt_neox_layers_4_attention_query_key_value_q_scale1: R.Tensor((7680, 80), dtype="float16") = packed_params[71]
            gpt_neox_layers_4_attention_query_key_value_bias1: R.Tensor((7680,), dtype="float16") = packed_params[72]
            gpt_neox_layers_4_attention_dense_q_weight1: R.Tensor((2560, 320), dtype="uint32") = packed_params[73]
            gpt_neox_layers_4_attention_dense_q_scale1: R.Tensor((2560, 80), dtype="float16") = packed_params[74]
            gpt_neox_layers_4_attention_dense_bias1: R.Tensor((2560,), dtype="float16") = packed_params[75]
            gpt_neox_layers_4_mlp_dense_h_to_4h_q_weight1: R.Tensor((10240, 320), dtype="uint32") = packed_params[76]
            gpt_neox_layers_4_mlp_dense_h_to_4h_q_scale1: R.Tensor((10240, 80), dtype="float16") = packed_params[77]
            gpt_neox_layers_4_mlp_dense_h_to_4h_bias1: R.Tensor((10240,), dtype="float32") = packed_params[78]
            gpt_neox_layers_4_mlp_dense_4h_to_h_q_weight1: R.Tensor((2560, 1280), dtype="uint32") = packed_params[79]
            gpt_neox_layers_4_mlp_dense_4h_to_h_q_scale1: R.Tensor((2560, 320), dtype="float16") = packed_params[80]
            gpt_neox_layers_4_mlp_dense_4h_to_h_bias1: R.Tensor((2560,), dtype="float32") = packed_params[81]
            gpt_neox_layers_5_input_layernorm_weight1: R.Tensor((2560,), dtype="float16") = packed_params[82]
            gpt_neox_layers_5_input_layernorm_bias1: R.Tensor((2560,), dtype="float16") = packed_params[83]
            gpt_neox_layers_5_post_attention_layernorm_weight1: R.Tensor((2560,), dtype="float16") = packed_params[84]
            gpt_neox_layers_5_post_attention_layernorm_bias1: R.Tensor((2560,), dtype="float16") = packed_params[85]
            gpt_neox_layers_5_attention_query_key_value_q_weight1: R.Tensor((7680, 320), dtype="uint32") = packed_params[86]
            gpt_neox_layers_5_attention_query_key_value_q_scale1: R.Tensor((7680, 80), dtype="float16") = packed_params[87]
            gpt_neox_layers_5_attention_query_key_value_bias1: R.Tensor((7680,), dtype="float16") = packed_params[88]
            gpt_neox_layers_5_attention_dense_q_weight1: R.Tensor((2560, 320), dtype="uint32") = packed_params[89]
            gpt_neox_layers_5_attention_dense_q_scale1: R.Tensor((2560, 80), dtype="float16") = packed_params[90]
            gpt_neox_layers_5_attention_dense_bias1: R.Tensor((2560,), dtype="float16") = packed_params[91]
            gpt_neox_layers_5_mlp_dense_h_to_4h_q_weight1: R.Tensor((10240, 320), dtype="uint32") = packed_params[92]
            gpt_neox_layers_5_mlp_dense_h_to_4h_q_scale1: R.Tensor((10240, 80), dtype="float16") = packed_params[93]
            gpt_neox_layers_5_mlp_dense_h_to_4h_bias1: R.Tensor((10240,), dtype="float32") = packed_params[94]
            gpt_neox_layers_5_mlp_dense_4h_to_h_q_weight1: R.Tensor((2560, 1280), dtype="uint32") = packed_params[95]
            gpt_neox_layers_5_mlp_dense_4h_to_h_q_scale1: R.Tensor((2560, 320), dtype="float16") = packed_params[96]
            gpt_neox_layers_5_mlp_dense_4h_to_h_bias1: R.Tensor((2560,), dtype="float32") = packed_params[97]
            gpt_neox_layers_6_input_layernorm_weight1: R.Tensor((2560,), dtype="float16") = packed_params[98]
            gpt_neox_layers_6_input_layernorm_bias1: R.Tensor((2560,), dtype="float16") = packed_params[99]
            gpt_neox_layers_6_post_attention_layernorm_weight1: R.Tensor((2560,), dtype="float16") = packed_params[100]
            gpt_neox_layers_6_post_attention_layernorm_bias1: R.Tensor((2560,), dtype="float16") = packed_params[101]
            gpt_neox_layers_6_attention_query_key_value_q_weight1: R.Tensor((7680, 320), dtype="uint32") = packed_params[102]
            gpt_neox_layers_6_attention_query_key_value_q_scale1: R.Tensor((7680, 80), dtype="float16") = packed_params[103]
            gpt_neox_layers_6_attention_query_key_value_bias1: R.Tensor((7680,), dtype="float16") = packed_params[104]
            gpt_neox_layers_6_attention_dense_q_weight1: R.Tensor((2560, 320), dtype="uint32") = packed_params[105]
            gpt_neox_layers_6_attention_dense_q_scale1: R.Tensor((2560, 80), dtype="float16") = packed_params[106]
            gpt_neox_layers_6_attention_dense_bias1: R.Tensor((2560,), dtype="float16") = packed_params[107]
            gpt_neox_layers_6_mlp_dense_h_to_4h_q_weight1: R.Tensor((10240, 320), dtype="uint32") = packed_params[108]
            gpt_neox_layers_6_mlp_dense_h_to_4h_q_scale1: R.Tensor((10240, 80), dtype="float16") = packed_params[109]
            gpt_neox_layers_6_mlp_dense_h_to_4h_bias1: R.Tensor((10240,), dtype="float32") = packed_params[110]
            gpt_neox_layers_6_mlp_dense_4h_to_h_q_weight1: R.Tensor((2560, 1280), dtype="uint32") = packed_params[111]
            gpt_neox_layers_6_mlp_dense_4h_to_h_q_scale1: R.Tensor((2560, 320), dtype="float16") = packed_params[112]
            gpt_neox_layers_6_mlp_dense_4h_to_h_bias1: R.Tensor((2560,), dtype="float32") = packed_params[113]
            gpt_neox_layers_7_input_layernorm_weight1: R.Tensor((2560,), dtype="float16") = packed_params[114]
            gpt_neox_layers_7_input_layernorm_bias1: R.Tensor((2560,), dtype="float16") = packed_params[115]
            gpt_neox_layers_7_post_attention_layernorm_weight1: R.Tensor((2560,), dtype="float16") = packed_params[116]
            gpt_neox_layers_7_post_attention_layernorm_bias1: R.Tensor((2560,), dtype="float16") = packed_params[117]
            gpt_neox_layers_7_attention_query_key_value_q_weight1: R.Tensor((7680, 320), dtype="uint32") = packed_params[118]
            gpt_neox_layers_7_attention_query_key_value_q_scale1: R.Tensor((7680, 80), dtype="float16") = packed_params[119]
            gpt_neox_layers_7_attention_query_key_value_bias1: R.Tensor((7680,), dtype="float16") = packed_params[120]
            gpt_neox_layers_7_attention_dense_q_weight1: R.Tensor((2560, 320), dtype="uint32") = packed_params[121]
            gpt_neox_layers_7_attention_dense_q_scale1: R.Tensor((2560, 80), dtype="float16") = packed_params[122]
            gpt_neox_layers_7_attention_dense_bias1: R.Tensor((2560,), dtype="float16") = packed_params[123]
            gpt_neox_layers_7_mlp_dense_h_to_4h_q_weight1: R.Tensor((10240, 320), dtype="uint32") = packed_params[124]
            gpt_neox_layers_7_mlp_dense_h_to_4h_q_scale1: R.Tensor((10240, 80), dtype="float16") = packed_params[125]
            gpt_neox_layers_7_mlp_dense_h_to_4h_bias1: R.Tensor((10240,), dtype="float32") = packed_params[126]
            gpt_neox_layers_7_mlp_dense_4h_to_h_q_weight1: R.Tensor((2560, 1280), dtype="uint32") = packed_params[127]
            gpt_neox_layers_7_mlp_dense_4h_to_h_q_scale1: R.Tensor((2560, 320), dtype="float16") = packed_params[128]
            gpt_neox_layers_7_mlp_dense_4h_to_h_bias1: R.Tensor((2560,), dtype="float32") = packed_params[129]
            gpt_neox_layers_8_input_layernorm_weight1: R.Tensor((2560,), dtype="float16") = packed_params[130]
            gpt_neox_layers_8_input_layernorm_bias1: R.Tensor((2560,), dtype="float16") = packed_params[131]
            gpt_neox_layers_8_post_attention_layernorm_weight1: R.Tensor((2560,), dtype="float16") = packed_params[132]
            gpt_neox_layers_8_post_attention_layernorm_bias1: R.Tensor((2560,), dtype="float16") = packed_params[133]
            gpt_neox_layers_8_attention_query_key_value_q_weight1: R.Tensor((7680, 320), dtype="uint32") = packed_params[134]
            gpt_neox_layers_8_attention_query_key_value_q_scale1: R.Tensor((7680, 80), dtype="float16") = packed_params[135]
            gpt_neox_layers_8_attention_query_key_value_bias1: R.Tensor((7680,), dtype="float16") = packed_params[136]
            gpt_neox_layers_8_attention_dense_q_weight1: R.Tensor((2560, 320), dtype="uint32") = packed_params[137]
            gpt_neox_layers_8_attention_dense_q_scale1: R.Tensor((2560, 80), dtype="float16") = packed_params[138]
            gpt_neox_layers_8_attention_dense_bias1: R.Tensor((2560,), dtype="float16") = packed_params[139]
            gpt_neox_layers_8_mlp_dense_h_to_4h_q_weight1: R.Tensor((10240, 320), dtype="uint32") = packed_params[140]
            gpt_neox_layers_8_mlp_dense_h_to_4h_q_scale1: R.Tensor((10240, 80), dtype="float16") = packed_params[141]
            gpt_neox_layers_8_mlp_dense_h_to_4h_bias1: R.Tensor((10240,), dtype="float32") = packed_params[142]
            gpt_neox_layers_8_mlp_dense_4h_to_h_q_weight1: R.Tensor((2560, 1280), dtype="uint32") = packed_params[143]
            gpt_neox_layers_8_mlp_dense_4h_to_h_q_scale1: R.Tensor((2560, 320), dtype="float16") = packed_params[144]
            gpt_neox_layers_8_mlp_dense_4h_to_h_bias1: R.Tensor((2560,), dtype="float32") = packed_params[145]
            gpt_neox_layers_9_input_layernorm_weight1: R.Tensor((2560,), dtype="float16") = packed_params[146]
            gpt_neox_layers_9_input_layernorm_bias1: R.Tensor((2560,), dtype="float16") = packed_params[147]
            gpt_neox_layers_9_post_attention_layernorm_weight1: R.Tensor((2560,), dtype="float16") = packed_params[148]
            gpt_neox_layers_9_post_attention_layernorm_bias1: R.Tensor((2560,), dtype="float16") = packed_params[149]
            gpt_neox_layers_9_attention_query_key_value_q_weight1: R.Tensor((7680, 320), dtype="uint32") = packed_params[150]
            gpt_neox_layers_9_attention_query_key_value_q_scale1: R.Tensor((7680, 80), dtype="float16") = packed_params[151]
            gpt_neox_layers_9_attention_query_key_value_bias1: R.Tensor((7680,), dtype="float16") = packed_params[152]
            gpt_neox_layers_9_attention_dense_q_weight1: R.Tensor((2560, 320), dtype="uint32") = packed_params[153]
            gpt_neox_layers_9_attention_dense_q_scale1: R.Tensor((2560, 80), dtype="float16") = packed_params[154]
            gpt_neox_layers_9_attention_dense_bias1: R.Tensor((2560,), dtype="float16") = packed_params[155]
            gpt_neox_layers_9_mlp_dense_h_to_4h_q_weight1: R.Tensor((10240, 320), dtype="uint32") = packed_params[156]
            gpt_neox_layers_9_mlp_dense_h_to_4h_q_scale1: R.Tensor((10240, 80), dtype="float16") = packed_params[157]
            gpt_neox_layers_9_mlp_dense_h_to_4h_bias1: R.Tensor((10240,), dtype="float32") = packed_params[158]
            gpt_neox_layers_9_mlp_dense_4h_to_h_q_weight1: R.Tensor((2560, 1280), dtype="uint32") = packed_params[159]
            gpt_neox_layers_9_mlp_dense_4h_to_h_q_scale1: R.Tensor((2560, 320), dtype="float16") = packed_params[160]
            gpt_neox_layers_9_mlp_dense_4h_to_h_bias1: R.Tensor((2560,), dtype="float32") = packed_params[161]
            gpt_neox_layers_10_input_layernorm_weight1: R.Tensor((2560,), dtype="float16") = packed_params[162]
            gpt_neox_layers_10_input_layernorm_bias1: R.Tensor((2560,), dtype="float16") = packed_params[163]
            gpt_neox_layers_10_post_attention_layernorm_weight1: R.Tensor((2560,), dtype="float16") = packed_params[164]
            gpt_neox_layers_10_post_attention_layernorm_bias1: R.Tensor((2560,), dtype="float16") = packed_params[165]
            gpt_neox_layers_10_attention_query_key_value_q_weight1: R.Tensor((7680, 320), dtype="uint32") = packed_params[166]
            gpt_neox_layers_10_attention_query_key_value_q_scale1: R.Tensor((7680, 80), dtype="float16") = packed_params[167]
            gpt_neox_layers_10_attention_query_key_value_bias1: R.Tensor((7680,), dtype="float16") = packed_params[168]
            gpt_neox_layers_10_attention_dense_q_weight1: R.Tensor((2560, 320), dtype="uint32") = packed_params[169]
            gpt_neox_layers_10_attention_dense_q_scale1: R.Tensor((2560, 80), dtype="float16") = packed_params[170]
            gpt_neox_layers_10_attention_dense_bias1: R.Tensor((2560,), dtype="float16") = packed_params[171]
            gpt_neox_layers_10_mlp_dense_h_to_4h_q_weight1: R.Tensor((10240, 320), dtype="uint32") = packed_params[172]
            gpt_neox_layers_10_mlp_dense_h_to_4h_q_scale1: R.Tensor((10240, 80), dtype="float16") = packed_params[173]
            gpt_neox_layers_10_mlp_dense_h_to_4h_bias1: R.Tensor((10240,), dtype="float32") = packed_params[174]
            gpt_neox_layers_10_mlp_dense_4h_to_h_q_weight1: R.Tensor((2560, 1280), dtype="uint32") = packed_params[175]
            gpt_neox_layers_10_mlp_dense_4h_to_h_q_scale1: R.Tensor((2560, 320), dtype="float16") = packed_params[176]
            gpt_neox_layers_10_mlp_dense_4h_to_h_bias1: R.Tensor((2560,), dtype="float32") = packed_params[177]
            gpt_neox_layers_11_input_layernorm_weight1: R.Tensor((2560,), dtype="float16") = packed_params[178]
            gpt_neox_layers_11_input_layernorm_bias1: R.Tensor((2560,), dtype="float16") = packed_params[179]
            gpt_neox_layers_11_post_attention_layernorm_weight1: R.Tensor((2560,), dtype="float16") = packed_params[180]
            gpt_neox_layers_11_post_attention_layernorm_bias1: R.Tensor((2560,), dtype="float16") = packed_params[181]
            gpt_neox_layers_11_attention_query_key_value_q_weight1: R.Tensor((7680, 320), dtype="uint32") = packed_params[182]
            gpt_neox_layers_11_attention_query_key_value_q_scale1: R.Tensor((7680, 80), dtype="float16") = packed_params[183]
            gpt_neox_layers_11_attention_query_key_value_bias1: R.Tensor((7680,), dtype="float16") = packed_params[184]
            gpt_neox_layers_11_attention_dense_q_weight1: R.Tensor((2560, 320), dtype="uint32") = packed_params[185]
            gpt_neox_layers_11_attention_dense_q_scale1: R.Tensor((2560, 80), dtype="float16") = packed_params[186]
            gpt_neox_layers_11_attention_dense_bias1: R.Tensor((2560,), dtype="float16") = packed_params[187]
            gpt_neox_layers_11_mlp_dense_h_to_4h_q_weight1: R.Tensor((10240, 320), dtype="uint32") = packed_params[188]
            gpt_neox_layers_11_mlp_dense_h_to_4h_q_scale1: R.Tensor((10240, 80), dtype="float16") = packed_params[189]
            gpt_neox_layers_11_mlp_dense_h_to_4h_bias1: R.Tensor((10240,), dtype="float32") = packed_params[190]
            gpt_neox_layers_11_mlp_dense_4h_to_h_q_weight1: R.Tensor((2560, 1280), dtype="uint32") = packed_params[191]
            gpt_neox_layers_11_mlp_dense_4h_to_h_q_scale1: R.Tensor((2560, 320), dtype="float16") = packed_params[192]
            gpt_neox_layers_11_mlp_dense_4h_to_h_bias1: R.Tensor((2560,), dtype="float32") = packed_params[193]
            gpt_neox_layers_12_input_layernorm_weight1: R.Tensor((2560,), dtype="float16") = packed_params[194]
            gpt_neox_layers_12_input_layernorm_bias1: R.Tensor((2560,), dtype="float16") = packed_params[195]
            gpt_neox_layers_12_post_attention_layernorm_weight1: R.Tensor((2560,), dtype="float16") = packed_params[196]
            gpt_neox_layers_12_post_attention_layernorm_bias1: R.Tensor((2560,), dtype="float16") = packed_params[197]
            gpt_neox_layers_12_attention_query_key_value_q_weight1: R.Tensor((7680, 320), dtype="uint32") = packed_params[198]
            gpt_neox_layers_12_attention_query_key_value_q_scale1: R.Tensor((7680, 80), dtype="float16") = packed_params[199]
            gpt_neox_layers_12_attention_query_key_value_bias1: R.Tensor((7680,), dtype="float16") = packed_params[200]
            gpt_neox_layers_12_attention_dense_q_weight1: R.Tensor((2560, 320), dtype="uint32") = packed_params[201]
            gpt_neox_layers_12_attention_dense_q_scale1: R.Tensor((2560, 80), dtype="float16") = packed_params[202]
            gpt_neox_layers_12_attention_dense_bias1: R.Tensor((2560,), dtype="float16") = packed_params[203]
            gpt_neox_layers_12_mlp_dense_h_to_4h_q_weight1: R.Tensor((10240, 320), dtype="uint32") = packed_params[204]
            gpt_neox_layers_12_mlp_dense_h_to_4h_q_scale1: R.Tensor((10240, 80), dtype="float16") = packed_params[205]
            gpt_neox_layers_12_mlp_dense_h_to_4h_bias1: R.Tensor((10240,), dtype="float32") = packed_params[206]
            gpt_neox_layers_12_mlp_dense_4h_to_h_q_weight1: R.Tensor((2560, 1280), dtype="uint32") = packed_params[207]
            gpt_neox_layers_12_mlp_dense_4h_to_h_q_scale1: R.Tensor((2560, 320), dtype="float16") = packed_params[208]
            gpt_neox_layers_12_mlp_dense_4h_to_h_bias1: R.Tensor((2560,), dtype="float32") = packed_params[209]
            gpt_neox_layers_13_input_layernorm_weight1: R.Tensor((2560,), dtype="float16") = packed_params[210]
            gpt_neox_layers_13_input_layernorm_bias1: R.Tensor((2560,), dtype="float16") = packed_params[211]
            gpt_neox_layers_13_post_attention_layernorm_weight1: R.Tensor((2560,), dtype="float16") = packed_params[212]
            gpt_neox_layers_13_post_attention_layernorm_bias1: R.Tensor((2560,), dtype="float16") = packed_params[213]
            gpt_neox_layers_13_attention_query_key_value_q_weight1: R.Tensor((7680, 320), dtype="uint32") = packed_params[214]
            gpt_neox_layers_13_attention_query_key_value_q_scale1: R.Tensor((7680, 80), dtype="float16") = packed_params[215]
            gpt_neox_layers_13_attention_query_key_value_bias1: R.Tensor((7680,), dtype="float16") = packed_params[216]
            gpt_neox_layers_13_attention_dense_q_weight1: R.Tensor((2560, 320), dtype="uint32") = packed_params[217]
            gpt_neox_layers_13_attention_dense_q_scale1: R.Tensor((2560, 80), dtype="float16") = packed_params[218]
            gpt_neox_layers_13_attention_dense_bias1: R.Tensor((2560,), dtype="float16") = packed_params[219]
            gpt_neox_layers_13_mlp_dense_h_to_4h_q_weight1: R.Tensor((10240, 320), dtype="uint32") = packed_params[220]
            gpt_neox_layers_13_mlp_dense_h_to_4h_q_scale1: R.Tensor((10240, 80), dtype="float16") = packed_params[221]
            gpt_neox_layers_13_mlp_dense_h_to_4h_bias1: R.Tensor((10240,), dtype="float32") = packed_params[222]
            gpt_neox_layers_13_mlp_dense_4h_to_h_q_weight1: R.Tensor((2560, 1280), dtype="uint32") = packed_params[223]
            gpt_neox_layers_13_mlp_dense_4h_to_h_q_scale1: R.Tensor((2560, 320), dtype="float16") = packed_params[224]
            gpt_neox_layers_13_mlp_dense_4h_to_h_bias1: R.Tensor((2560,), dtype="float32") = packed_params[225]
            gpt_neox_layers_14_input_layernorm_weight1: R.Tensor((2560,), dtype="float16") = packed_params[226]
            gpt_neox_layers_14_input_layernorm_bias1: R.Tensor((2560,), dtype="float16") = packed_params[227]
            gpt_neox_layers_14_post_attention_layernorm_weight1: R.Tensor((2560,), dtype="float16") = packed_params[228]
            gpt_neox_layers_14_post_attention_layernorm_bias1: R.Tensor((2560,), dtype="float16") = packed_params[229]
            gpt_neox_layers_14_attention_query_key_value_q_weight1: R.Tensor((7680, 320), dtype="uint32") = packed_params[230]
            gpt_neox_layers_14_attention_query_key_value_q_scale1: R.Tensor((7680, 80), dtype="float16") = packed_params[231]
            gpt_neox_layers_14_attention_query_key_value_bias1: R.Tensor((7680,), dtype="float16") = packed_params[232]
            gpt_neox_layers_14_attention_dense_q_weight1: R.Tensor((2560, 320), dtype="uint32") = packed_params[233]
            gpt_neox_layers_14_attention_dense_q_scale1: R.Tensor((2560, 80), dtype="float16") = packed_params[234]
            gpt_neox_layers_14_attention_dense_bias1: R.Tensor((2560,), dtype="float16") = packed_params[235]
            gpt_neox_layers_14_mlp_dense_h_to_4h_q_weight1: R.Tensor((10240, 320), dtype="uint32") = packed_params[236]
            gpt_neox_layers_14_mlp_dense_h_to_4h_q_scale1: R.Tensor((10240, 80), dtype="float16") = packed_params[237]
            gpt_neox_layers_14_mlp_dense_h_to_4h_bias1: R.Tensor((10240,), dtype="float32") = packed_params[238]
            gpt_neox_layers_14_mlp_dense_4h_to_h_q_weight1: R.Tensor((2560, 1280), dtype="uint32") = packed_params[239]
            gpt_neox_layers_14_mlp_dense_4h_to_h_q_scale1: R.Tensor((2560, 320), dtype="float16") = packed_params[240]
            gpt_neox_layers_14_mlp_dense_4h_to_h_bias1: R.Tensor((2560,), dtype="float32") = packed_params[241]
            gpt_neox_layers_15_input_layernorm_weight1: R.Tensor((2560,), dtype="float16") = packed_params[242]
            gpt_neox_layers_15_input_layernorm_bias1: R.Tensor((2560,), dtype="float16") = packed_params[243]
            gpt_neox_layers_15_post_attention_layernorm_weight1: R.Tensor((2560,), dtype="float16") = packed_params[244]
            gpt_neox_layers_15_post_attention_layernorm_bias1: R.Tensor((2560,), dtype="float16") = packed_params[245]
            gpt_neox_layers_15_attention_query_key_value_q_weight1: R.Tensor((7680, 320), dtype="uint32") = packed_params[246]
            gpt_neox_layers_15_attention_query_key_value_q_scale1: R.Tensor((7680, 80), dtype="float16") = packed_params[247]
            gpt_neox_layers_15_attention_query_key_value_bias1: R.Tensor((7680,), dtype="float16") = packed_params[248]
            gpt_neox_layers_15_attention_dense_q_weight1: R.Tensor((2560, 320), dtype="uint32") = packed_params[249]
            gpt_neox_layers_15_attention_dense_q_scale1: R.Tensor((2560, 80), dtype="float16") = packed_params[250]
            gpt_neox_layers_15_attention_dense_bias1: R.Tensor((2560,), dtype="float16") = packed_params[251]
            gpt_neox_layers_15_mlp_dense_h_to_4h_q_weight1: R.Tensor((10240, 320), dtype="uint32") = packed_params[252]
            gpt_neox_layers_15_mlp_dense_h_to_4h_q_scale1: R.Tensor((10240, 80), dtype="float16") = packed_params[253]
            gpt_neox_layers_15_mlp_dense_h_to_4h_bias1: R.Tensor((10240,), dtype="float32") = packed_params[254]
            gpt_neox_layers_15_mlp_dense_4h_to_h_q_weight1: R.Tensor((2560, 1280), dtype="uint32") = packed_params[255]
            gpt_neox_layers_15_mlp_dense_4h_to_h_q_scale1: R.Tensor((2560, 320), dtype="float16") = packed_params[256]
            gpt_neox_layers_15_mlp_dense_4h_to_h_bias1: R.Tensor((2560,), dtype="float32") = packed_params[257]
            gpt_neox_layers_16_input_layernorm_weight1: R.Tensor((2560,), dtype="float16") = packed_params[258]
            gpt_neox_layers_16_input_layernorm_bias1: R.Tensor((2560,), dtype="float16") = packed_params[259]
            gpt_neox_layers_16_post_attention_layernorm_weight1: R.Tensor((2560,), dtype="float16") = packed_params[260]
            gpt_neox_layers_16_post_attention_layernorm_bias1: R.Tensor((2560,), dtype="float16") = packed_params[261]
            gpt_neox_layers_16_attention_query_key_value_q_weight1: R.Tensor((7680, 320), dtype="uint32") = packed_params[262]
            gpt_neox_layers_16_attention_query_key_value_q_scale1: R.Tensor((7680, 80), dtype="float16") = packed_params[263]
            gpt_neox_layers_16_attention_query_key_value_bias1: R.Tensor((7680,), dtype="float16") = packed_params[264]
            gpt_neox_layers_16_attention_dense_q_weight1: R.Tensor((2560, 320), dtype="uint32") = packed_params[265]
            gpt_neox_layers_16_attention_dense_q_scale1: R.Tensor((2560, 80), dtype="float16") = packed_params[266]
            gpt_neox_layers_16_attention_dense_bias1: R.Tensor((2560,), dtype="float16") = packed_params[267]
            gpt_neox_layers_16_mlp_dense_h_to_4h_q_weight1: R.Tensor((10240, 320), dtype="uint32") = packed_params[268]
            gpt_neox_layers_16_mlp_dense_h_to_4h_q_scale1: R.Tensor((10240, 80), dtype="float16") = packed_params[269]
            gpt_neox_layers_16_mlp_dense_h_to_4h_bias1: R.Tensor((10240,), dtype="float32") = packed_params[270]
            gpt_neox_layers_16_mlp_dense_4h_to_h_q_weight1: R.Tensor((2560, 1280), dtype="uint32") = packed_params[271]
            gpt_neox_layers_16_mlp_dense_4h_to_h_q_scale1: R.Tensor((2560, 320), dtype="float16") = packed_params[272]
            gpt_neox_layers_16_mlp_dense_4h_to_h_bias1: R.Tensor((2560,), dtype="float32") = packed_params[273]
            gpt_neox_layers_17_input_layernorm_weight1: R.Tensor((2560,), dtype="float16") = packed_params[274]
            gpt_neox_layers_17_input_layernorm_bias1: R.Tensor((2560,), dtype="float16") = packed_params[275]
            gpt_neox_layers_17_post_attention_layernorm_weight1: R.Tensor((2560,), dtype="float16") = packed_params[276]
            gpt_neox_layers_17_post_attention_layernorm_bias1: R.Tensor((2560,), dtype="float16") = packed_params[277]
            gpt_neox_layers_17_attention_query_key_value_q_weight1: R.Tensor((7680, 320), dtype="uint32") = packed_params[278]
            gpt_neox_layers_17_attention_query_key_value_q_scale1: R.Tensor((7680, 80), dtype="float16") = packed_params[279]
            gpt_neox_layers_17_attention_query_key_value_bias1: R.Tensor((7680,), dtype="float16") = packed_params[280]
            gpt_neox_layers_17_attention_dense_q_weight1: R.Tensor((2560, 320), dtype="uint32") = packed_params[281]
            gpt_neox_layers_17_attention_dense_q_scale1: R.Tensor((2560, 80), dtype="float16") = packed_params[282]
            gpt_neox_layers_17_attention_dense_bias1: R.Tensor((2560,), dtype="float16") = packed_params[283]
            gpt_neox_layers_17_mlp_dense_h_to_4h_q_weight1: R.Tensor((10240, 320), dtype="uint32") = packed_params[284]
            gpt_neox_layers_17_mlp_dense_h_to_4h_q_scale1: R.Tensor((10240, 80), dtype="float16") = packed_params[285]
            gpt_neox_layers_17_mlp_dense_h_to_4h_bias1: R.Tensor((10240,), dtype="float32") = packed_params[286]
            gpt_neox_layers_17_mlp_dense_4h_to_h_q_weight1: R.Tensor((2560, 1280), dtype="uint32") = packed_params[287]
            gpt_neox_layers_17_mlp_dense_4h_to_h_q_scale1: R.Tensor((2560, 320), dtype="float16") = packed_params[288]
            gpt_neox_layers_17_mlp_dense_4h_to_h_bias1: R.Tensor((2560,), dtype="float32") = packed_params[289]
            gpt_neox_layers_18_input_layernorm_weight1: R.Tensor((2560,), dtype="float16") = packed_params[290]
            gpt_neox_layers_18_input_layernorm_bias1: R.Tensor((2560,), dtype="float16") = packed_params[291]
            gpt_neox_layers_18_post_attention_layernorm_weight1: R.Tensor((2560,), dtype="float16") = packed_params[292]
            gpt_neox_layers_18_post_attention_layernorm_bias1: R.Tensor((2560,), dtype="float16") = packed_params[293]
            gpt_neox_layers_18_attention_query_key_value_q_weight1: R.Tensor((7680, 320), dtype="uint32") = packed_params[294]
            gpt_neox_layers_18_attention_query_key_value_q_scale1: R.Tensor((7680, 80), dtype="float16") = packed_params[295]
            gpt_neox_layers_18_attention_query_key_value_bias1: R.Tensor((7680,), dtype="float16") = packed_params[296]
            gpt_neox_layers_18_attention_dense_q_weight1: R.Tensor((2560, 320), dtype="uint32") = packed_params[297]
            gpt_neox_layers_18_attention_dense_q_scale1: R.Tensor((2560, 80), dtype="float16") = packed_params[298]
            gpt_neox_layers_18_attention_dense_bias1: R.Tensor((2560,), dtype="float16") = packed_params[299]
            gpt_neox_layers_18_mlp_dense_h_to_4h_q_weight1: R.Tensor((10240, 320), dtype="uint32") = packed_params[300]
            gpt_neox_layers_18_mlp_dense_h_to_4h_q_scale1: R.Tensor((10240, 80), dtype="float16") = packed_params[301]
            gpt_neox_layers_18_mlp_dense_h_to_4h_bias1: R.Tensor((10240,), dtype="float32") = packed_params[302]
            gpt_neox_layers_18_mlp_dense_4h_to_h_q_weight1: R.Tensor((2560, 1280), dtype="uint32") = packed_params[303]
            gpt_neox_layers_18_mlp_dense_4h_to_h_q_scale1: R.Tensor((2560, 320), dtype="float16") = packed_params[304]
            gpt_neox_layers_18_mlp_dense_4h_to_h_bias1: R.Tensor((2560,), dtype="float32") = packed_params[305]
            gpt_neox_layers_19_input_layernorm_weight1: R.Tensor((2560,), dtype="float16") = packed_params[306]
            gpt_neox_layers_19_input_layernorm_bias1: R.Tensor((2560,), dtype="float16") = packed_params[307]
            gpt_neox_layers_19_post_attention_layernorm_weight1: R.Tensor((2560,), dtype="float16") = packed_params[308]
            gpt_neox_layers_19_post_attention_layernorm_bias1: R.Tensor((2560,), dtype="float16") = packed_params[309]
            gpt_neox_layers_19_attention_query_key_value_q_weight1: R.Tensor((7680, 320), dtype="uint32") = packed_params[310]
            gpt_neox_layers_19_attention_query_key_value_q_scale1: R.Tensor((7680, 80), dtype="float16") = packed_params[311]
            gpt_neox_layers_19_attention_query_key_value_bias1: R.Tensor((7680,), dtype="float16") = packed_params[312]
            gpt_neox_layers_19_attention_dense_q_weight1: R.Tensor((2560, 320), dtype="uint32") = packed_params[313]
            gpt_neox_layers_19_attention_dense_q_scale1: R.Tensor((2560, 80), dtype="float16") = packed_params[314]
            gpt_neox_layers_19_attention_dense_bias1: R.Tensor((2560,), dtype="float16") = packed_params[315]
            gpt_neox_layers_19_mlp_dense_h_to_4h_q_weight1: R.Tensor((10240, 320), dtype="uint32") = packed_params[316]
            gpt_neox_layers_19_mlp_dense_h_to_4h_q_scale1: R.Tensor((10240, 80), dtype="float16") = packed_params[317]
            gpt_neox_layers_19_mlp_dense_h_to_4h_bias1: R.Tensor((10240,), dtype="float32") = packed_params[318]
            gpt_neox_layers_19_mlp_dense_4h_to_h_q_weight1: R.Tensor((2560, 1280), dtype="uint32") = packed_params[319]
            gpt_neox_layers_19_mlp_dense_4h_to_h_q_scale1: R.Tensor((2560, 320), dtype="float16") = packed_params[320]
            gpt_neox_layers_19_mlp_dense_4h_to_h_bias1: R.Tensor((2560,), dtype="float32") = packed_params[321]
            gpt_neox_layers_20_input_layernorm_weight1: R.Tensor((2560,), dtype="float16") = packed_params[322]
            gpt_neox_layers_20_input_layernorm_bias1: R.Tensor((2560,), dtype="float16") = packed_params[323]
            gpt_neox_layers_20_post_attention_layernorm_weight1: R.Tensor((2560,), dtype="float16") = packed_params[324]
            gpt_neox_layers_20_post_attention_layernorm_bias1: R.Tensor((2560,), dtype="float16") = packed_params[325]
            gpt_neox_layers_20_attention_query_key_value_q_weight1: R.Tensor((7680, 320), dtype="uint32") = packed_params[326]
            gpt_neox_layers_20_attention_query_key_value_q_scale1: R.Tensor((7680, 80), dtype="float16") = packed_params[327]
            gpt_neox_layers_20_attention_query_key_value_bias1: R.Tensor((7680,), dtype="float16") = packed_params[328]
            gpt_neox_layers_20_attention_dense_q_weight1: R.Tensor((2560, 320), dtype="uint32") = packed_params[329]
            gpt_neox_layers_20_attention_dense_q_scale1: R.Tensor((2560, 80), dtype="float16") = packed_params[330]
            gpt_neox_layers_20_attention_dense_bias1: R.Tensor((2560,), dtype="float16") = packed_params[331]
            gpt_neox_layers_20_mlp_dense_h_to_4h_q_weight1: R.Tensor((10240, 320), dtype="uint32") = packed_params[332]
            gpt_neox_layers_20_mlp_dense_h_to_4h_q_scale1: R.Tensor((10240, 80), dtype="float16") = packed_params[333]
            gpt_neox_layers_20_mlp_dense_h_to_4h_bias1: R.Tensor((10240,), dtype="float32") = packed_params[334]
            gpt_neox_layers_20_mlp_dense_4h_to_h_q_weight1: R.Tensor((2560, 1280), dtype="uint32") = packed_params[335]
            gpt_neox_layers_20_mlp_dense_4h_to_h_q_scale1: R.Tensor((2560, 320), dtype="float16") = packed_params[336]
            gpt_neox_layers_20_mlp_dense_4h_to_h_bias1: R.Tensor((2560,), dtype="float32") = packed_params[337]
            gpt_neox_layers_21_input_layernorm_weight1: R.Tensor((2560,), dtype="float16") = packed_params[338]
            gpt_neox_layers_21_input_layernorm_bias1: R.Tensor((2560,), dtype="float16") = packed_params[339]
            gpt_neox_layers_21_post_attention_layernorm_weight1: R.Tensor((2560,), dtype="float16") = packed_params[340]
            gpt_neox_layers_21_post_attention_layernorm_bias1: R.Tensor((2560,), dtype="float16") = packed_params[341]
            gpt_neox_layers_21_attention_query_key_value_q_weight1: R.Tensor((7680, 320), dtype="uint32") = packed_params[342]
            gpt_neox_layers_21_attention_query_key_value_q_scale1: R.Tensor((7680, 80), dtype="float16") = packed_params[343]
            gpt_neox_layers_21_attention_query_key_value_bias1: R.Tensor((7680,), dtype="float16") = packed_params[344]
            gpt_neox_layers_21_attention_dense_q_weight1: R.Tensor((2560, 320), dtype="uint32") = packed_params[345]
            gpt_neox_layers_21_attention_dense_q_scale1: R.Tensor((2560, 80), dtype="float16") = packed_params[346]
            gpt_neox_layers_21_attention_dense_bias1: R.Tensor((2560,), dtype="float16") = packed_params[347]
            gpt_neox_layers_21_mlp_dense_h_to_4h_q_weight1: R.Tensor((10240, 320), dtype="uint32") = packed_params[348]
            gpt_neox_layers_21_mlp_dense_h_to_4h_q_scale1: R.Tensor((10240, 80), dtype="float16") = packed_params[349]
            gpt_neox_layers_21_mlp_dense_h_to_4h_bias1: R.Tensor((10240,), dtype="float32") = packed_params[350]
            gpt_neox_layers_21_mlp_dense_4h_to_h_q_weight1: R.Tensor((2560, 1280), dtype="uint32") = packed_params[351]
            gpt_neox_layers_21_mlp_dense_4h_to_h_q_scale1: R.Tensor((2560, 320), dtype="float16") = packed_params[352]
            gpt_neox_layers_21_mlp_dense_4h_to_h_bias1: R.Tensor((2560,), dtype="float32") = packed_params[353]
            gpt_neox_layers_22_input_layernorm_weight1: R.Tensor((2560,), dtype="float16") = packed_params[354]
            gpt_neox_layers_22_input_layernorm_bias1: R.Tensor((2560,), dtype="float16") = packed_params[355]
            gpt_neox_layers_22_post_attention_layernorm_weight1: R.Tensor((2560,), dtype="float16") = packed_params[356]
            gpt_neox_layers_22_post_attention_layernorm_bias1: R.Tensor((2560,), dtype="float16") = packed_params[357]
            gpt_neox_layers_22_attention_query_key_value_q_weight1: R.Tensor((7680, 320), dtype="uint32") = packed_params[358]
            gpt_neox_layers_22_attention_query_key_value_q_scale1: R.Tensor((7680, 80), dtype="float16") = packed_params[359]
            gpt_neox_layers_22_attention_query_key_value_bias1: R.Tensor((7680,), dtype="float16") = packed_params[360]
            gpt_neox_layers_22_attention_dense_q_weight1: R.Tensor((2560, 320), dtype="uint32") = packed_params[361]
            gpt_neox_layers_22_attention_dense_q_scale1: R.Tensor((2560, 80), dtype="float16") = packed_params[362]
            gpt_neox_layers_22_attention_dense_bias1: R.Tensor((2560,), dtype="float16") = packed_params[363]
            gpt_neox_layers_22_mlp_dense_h_to_4h_q_weight1: R.Tensor((10240, 320), dtype="uint32") = packed_params[364]
            gpt_neox_layers_22_mlp_dense_h_to_4h_q_scale1: R.Tensor((10240, 80), dtype="float16") = packed_params[365]
            gpt_neox_layers_22_mlp_dense_h_to_4h_bias1: R.Tensor((10240,), dtype="float32") = packed_params[366]
            gpt_neox_layers_22_mlp_dense_4h_to_h_q_weight1: R.Tensor((2560, 1280), dtype="uint32") = packed_params[367]
            gpt_neox_layers_22_mlp_dense_4h_to_h_q_scale1: R.Tensor((2560, 320), dtype="float16") = packed_params[368]
            gpt_neox_layers_22_mlp_dense_4h_to_h_bias1: R.Tensor((2560,), dtype="float32") = packed_params[369]
            gpt_neox_layers_23_input_layernorm_weight1: R.Tensor((2560,), dtype="float16") = packed_params[370]
            gpt_neox_layers_23_input_layernorm_bias1: R.Tensor((2560,), dtype="float16") = packed_params[371]
            gpt_neox_layers_23_post_attention_layernorm_weight1: R.Tensor((2560,), dtype="float16") = packed_params[372]
            gpt_neox_layers_23_post_attention_layernorm_bias1: R.Tensor((2560,), dtype="float16") = packed_params[373]
            gpt_neox_layers_23_attention_query_key_value_q_weight1: R.Tensor((7680, 320), dtype="uint32") = packed_params[374]
            gpt_neox_layers_23_attention_query_key_value_q_scale1: R.Tensor((7680, 80), dtype="float16") = packed_params[375]
            gpt_neox_layers_23_attention_query_key_value_bias1: R.Tensor((7680,), dtype="float16") = packed_params[376]
            gpt_neox_layers_23_attention_dense_q_weight1: R.Tensor((2560, 320), dtype="uint32") = packed_params[377]
            gpt_neox_layers_23_attention_dense_q_scale1: R.Tensor((2560, 80), dtype="float16") = packed_params[378]
            gpt_neox_layers_23_attention_dense_bias1: R.Tensor((2560,), dtype="float16") = packed_params[379]
            gpt_neox_layers_23_mlp_dense_h_to_4h_q_weight1: R.Tensor((10240, 320), dtype="uint32") = packed_params[380]
            gpt_neox_layers_23_mlp_dense_h_to_4h_q_scale1: R.Tensor((10240, 80), dtype="float16") = packed_params[381]
            gpt_neox_layers_23_mlp_dense_h_to_4h_bias1: R.Tensor((10240,), dtype="float32") = packed_params[382]
            gpt_neox_layers_23_mlp_dense_4h_to_h_q_weight1: R.Tensor((2560, 1280), dtype="uint32") = packed_params[383]
            gpt_neox_layers_23_mlp_dense_4h_to_h_q_scale1: R.Tensor((2560, 320), dtype="float16") = packed_params[384]
            gpt_neox_layers_23_mlp_dense_4h_to_h_bias1: R.Tensor((2560,), dtype="float32") = packed_params[385]
            gpt_neox_layers_24_input_layernorm_weight1: R.Tensor((2560,), dtype="float16") = packed_params[386]
            gpt_neox_layers_24_input_layernorm_bias1: R.Tensor((2560,), dtype="float16") = packed_params[387]
            gpt_neox_layers_24_post_attention_layernorm_weight1: R.Tensor((2560,), dtype="float16") = packed_params[388]
            gpt_neox_layers_24_post_attention_layernorm_bias1: R.Tensor((2560,), dtype="float16") = packed_params[389]
            gpt_neox_layers_24_attention_query_key_value_q_weight1: R.Tensor((7680, 320), dtype="uint32") = packed_params[390]
            gpt_neox_layers_24_attention_query_key_value_q_scale1: R.Tensor((7680, 80), dtype="float16") = packed_params[391]
            gpt_neox_layers_24_attention_query_key_value_bias1: R.Tensor((7680,), dtype="float16") = packed_params[392]
            gpt_neox_layers_24_attention_dense_q_weight1: R.Tensor((2560, 320), dtype="uint32") = packed_params[393]
            gpt_neox_layers_24_attention_dense_q_scale1: R.Tensor((2560, 80), dtype="float16") = packed_params[394]
            gpt_neox_layers_24_attention_dense_bias1: R.Tensor((2560,), dtype="float16") = packed_params[395]
            gpt_neox_layers_24_mlp_dense_h_to_4h_q_weight1: R.Tensor((10240, 320), dtype="uint32") = packed_params[396]
            gpt_neox_layers_24_mlp_dense_h_to_4h_q_scale1: R.Tensor((10240, 80), dtype="float16") = packed_params[397]
            gpt_neox_layers_24_mlp_dense_h_to_4h_bias1: R.Tensor((10240,), dtype="float32") = packed_params[398]
            gpt_neox_layers_24_mlp_dense_4h_to_h_q_weight1: R.Tensor((2560, 1280), dtype="uint32") = packed_params[399]
            gpt_neox_layers_24_mlp_dense_4h_to_h_q_scale1: R.Tensor((2560, 320), dtype="float16") = packed_params[400]
            gpt_neox_layers_24_mlp_dense_4h_to_h_bias1: R.Tensor((2560,), dtype="float32") = packed_params[401]
            gpt_neox_layers_25_input_layernorm_weight1: R.Tensor((2560,), dtype="float16") = packed_params[402]
            gpt_neox_layers_25_input_layernorm_bias1: R.Tensor((2560,), dtype="float16") = packed_params[403]
            gpt_neox_layers_25_post_attention_layernorm_weight1: R.Tensor((2560,), dtype="float16") = packed_params[404]
            gpt_neox_layers_25_post_attention_layernorm_bias1: R.Tensor((2560,), dtype="float16") = packed_params[405]
            gpt_neox_layers_25_attention_query_key_value_q_weight1: R.Tensor((7680, 320), dtype="uint32") = packed_params[406]
            gpt_neox_layers_25_attention_query_key_value_q_scale1: R.Tensor((7680, 80), dtype="float16") = packed_params[407]
            gpt_neox_layers_25_attention_query_key_value_bias1: R.Tensor((7680,), dtype="float16") = packed_params[408]
            gpt_neox_layers_25_attention_dense_q_weight1: R.Tensor((2560, 320), dtype="uint32") = packed_params[409]
            gpt_neox_layers_25_attention_dense_q_scale1: R.Tensor((2560, 80), dtype="float16") = packed_params[410]
            gpt_neox_layers_25_attention_dense_bias1: R.Tensor((2560,), dtype="float16") = packed_params[411]
            gpt_neox_layers_25_mlp_dense_h_to_4h_q_weight1: R.Tensor((10240, 320), dtype="uint32") = packed_params[412]
            gpt_neox_layers_25_mlp_dense_h_to_4h_q_scale1: R.Tensor((10240, 80), dtype="float16") = packed_params[413]
            gpt_neox_layers_25_mlp_dense_h_to_4h_bias1: R.Tensor((10240,), dtype="float32") = packed_params[414]
            gpt_neox_layers_25_mlp_dense_4h_to_h_q_weight1: R.Tensor((2560, 1280), dtype="uint32") = packed_params[415]
            gpt_neox_layers_25_mlp_dense_4h_to_h_q_scale1: R.Tensor((2560, 320), dtype="float16") = packed_params[416]
            gpt_neox_layers_25_mlp_dense_4h_to_h_bias1: R.Tensor((2560,), dtype="float32") = packed_params[417]
            gpt_neox_layers_26_input_layernorm_weight1: R.Tensor((2560,), dtype="float16") = packed_params[418]
            gpt_neox_layers_26_input_layernorm_bias1: R.Tensor((2560,), dtype="float16") = packed_params[419]
            gpt_neox_layers_26_post_attention_layernorm_weight1: R.Tensor((2560,), dtype="float16") = packed_params[420]
            gpt_neox_layers_26_post_attention_layernorm_bias1: R.Tensor((2560,), dtype="float16") = packed_params[421]
            gpt_neox_layers_26_attention_query_key_value_q_weight1: R.Tensor((7680, 320), dtype="uint32") = packed_params[422]
            gpt_neox_layers_26_attention_query_key_value_q_scale1: R.Tensor((7680, 80), dtype="float16") = packed_params[423]
            gpt_neox_layers_26_attention_query_key_value_bias1: R.Tensor((7680,), dtype="float16") = packed_params[424]
            gpt_neox_layers_26_attention_dense_q_weight1: R.Tensor((2560, 320), dtype="uint32") = packed_params[425]
            gpt_neox_layers_26_attention_dense_q_scale1: R.Tensor((2560, 80), dtype="float16") = packed_params[426]
            gpt_neox_layers_26_attention_dense_bias1: R.Tensor((2560,), dtype="float16") = packed_params[427]
            gpt_neox_layers_26_mlp_dense_h_to_4h_q_weight1: R.Tensor((10240, 320), dtype="uint32") = packed_params[428]
            gpt_neox_layers_26_mlp_dense_h_to_4h_q_scale1: R.Tensor((10240, 80), dtype="float16") = packed_params[429]
            gpt_neox_layers_26_mlp_dense_h_to_4h_bias1: R.Tensor((10240,), dtype="float32") = packed_params[430]
            gpt_neox_layers_26_mlp_dense_4h_to_h_q_weight1: R.Tensor((2560, 1280), dtype="uint32") = packed_params[431]
            gpt_neox_layers_26_mlp_dense_4h_to_h_q_scale1: R.Tensor((2560, 320), dtype="float16") = packed_params[432]
            gpt_neox_layers_26_mlp_dense_4h_to_h_bias1: R.Tensor((2560,), dtype="float32") = packed_params[433]
            gpt_neox_layers_27_input_layernorm_weight1: R.Tensor((2560,), dtype="float16") = packed_params[434]
            gpt_neox_layers_27_input_layernorm_bias1: R.Tensor((2560,), dtype="float16") = packed_params[435]
            gpt_neox_layers_27_post_attention_layernorm_weight1: R.Tensor((2560,), dtype="float16") = packed_params[436]
            gpt_neox_layers_27_post_attention_layernorm_bias1: R.Tensor((2560,), dtype="float16") = packed_params[437]
            gpt_neox_layers_27_attention_query_key_value_q_weight1: R.Tensor((7680, 320), dtype="uint32") = packed_params[438]
            gpt_neox_layers_27_attention_query_key_value_q_scale1: R.Tensor((7680, 80), dtype="float16") = packed_params[439]
            gpt_neox_layers_27_attention_query_key_value_bias1: R.Tensor((7680,), dtype="float16") = packed_params[440]
            gpt_neox_layers_27_attention_dense_q_weight1: R.Tensor((2560, 320), dtype="uint32") = packed_params[441]
            gpt_neox_layers_27_attention_dense_q_scale1: R.Tensor((2560, 80), dtype="float16") = packed_params[442]
            gpt_neox_layers_27_attention_dense_bias1: R.Tensor((2560,), dtype="float16") = packed_params[443]
            gpt_neox_layers_27_mlp_dense_h_to_4h_q_weight1: R.Tensor((10240, 320), dtype="uint32") = packed_params[444]
            gpt_neox_layers_27_mlp_dense_h_to_4h_q_scale1: R.Tensor((10240, 80), dtype="float16") = packed_params[445]
            gpt_neox_layers_27_mlp_dense_h_to_4h_bias1: R.Tensor((10240,), dtype="float32") = packed_params[446]
            gpt_neox_layers_27_mlp_dense_4h_to_h_q_weight1: R.Tensor((2560, 1280), dtype="uint32") = packed_params[447]
            gpt_neox_layers_27_mlp_dense_4h_to_h_q_scale1: R.Tensor((2560, 320), dtype="float16") = packed_params[448]
            gpt_neox_layers_27_mlp_dense_4h_to_h_bias1: R.Tensor((2560,), dtype="float32") = packed_params[449]
            gpt_neox_layers_28_input_layernorm_weight1: R.Tensor((2560,), dtype="float16") = packed_params[450]
            gpt_neox_layers_28_input_layernorm_bias1: R.Tensor((2560,), dtype="float16") = packed_params[451]
            gpt_neox_layers_28_post_attention_layernorm_weight1: R.Tensor((2560,), dtype="float16") = packed_params[452]
            gpt_neox_layers_28_post_attention_layernorm_bias1: R.Tensor((2560,), dtype="float16") = packed_params[453]
            gpt_neox_layers_28_attention_query_key_value_q_weight1: R.Tensor((7680, 320), dtype="uint32") = packed_params[454]
            gpt_neox_layers_28_attention_query_key_value_q_scale1: R.Tensor((7680, 80), dtype="float16") = packed_params[455]
            gpt_neox_layers_28_attention_query_key_value_bias1: R.Tensor((7680,), dtype="float16") = packed_params[456]
            gpt_neox_layers_28_attention_dense_q_weight1: R.Tensor((2560, 320), dtype="uint32") = packed_params[457]
            gpt_neox_layers_28_attention_dense_q_scale1: R.Tensor((2560, 80), dtype="float16") = packed_params[458]
            gpt_neox_layers_28_attention_dense_bias1: R.Tensor((2560,), dtype="float16") = packed_params[459]
            gpt_neox_layers_28_mlp_dense_h_to_4h_q_weight1: R.Tensor((10240, 320), dtype="uint32") = packed_params[460]
            gpt_neox_layers_28_mlp_dense_h_to_4h_q_scale1: R.Tensor((10240, 80), dtype="float16") = packed_params[461]
            gpt_neox_layers_28_mlp_dense_h_to_4h_bias1: R.Tensor((10240,), dtype="float32") = packed_params[462]
            gpt_neox_layers_28_mlp_dense_4h_to_h_q_weight1: R.Tensor((2560, 1280), dtype="uint32") = packed_params[463]
            gpt_neox_layers_28_mlp_dense_4h_to_h_q_scale1: R.Tensor((2560, 320), dtype="float16") = packed_params[464]
            gpt_neox_layers_28_mlp_dense_4h_to_h_bias1: R.Tensor((2560,), dtype="float32") = packed_params[465]
            gpt_neox_layers_29_input_layernorm_weight1: R.Tensor((2560,), dtype="float16") = packed_params[466]
            gpt_neox_layers_29_input_layernorm_bias1: R.Tensor((2560,), dtype="float16") = packed_params[467]
            gpt_neox_layers_29_post_attention_layernorm_weight1: R.Tensor((2560,), dtype="float16") = packed_params[468]
            gpt_neox_layers_29_post_attention_layernorm_bias1: R.Tensor((2560,), dtype="float16") = packed_params[469]
            gpt_neox_layers_29_attention_query_key_value_q_weight1: R.Tensor((7680, 320), dtype="uint32") = packed_params[470]
            gpt_neox_layers_29_attention_query_key_value_q_scale1: R.Tensor((7680, 80), dtype="float16") = packed_params[471]
            gpt_neox_layers_29_attention_query_key_value_bias1: R.Tensor((7680,), dtype="float16") = packed_params[472]
            gpt_neox_layers_29_attention_dense_q_weight1: R.Tensor((2560, 320), dtype="uint32") = packed_params[473]
            gpt_neox_layers_29_attention_dense_q_scale1: R.Tensor((2560, 80), dtype="float16") = packed_params[474]
            gpt_neox_layers_29_attention_dense_bias1: R.Tensor((2560,), dtype="float16") = packed_params[475]
            gpt_neox_layers_29_mlp_dense_h_to_4h_q_weight1: R.Tensor((10240, 320), dtype="uint32") = packed_params[476]
            gpt_neox_layers_29_mlp_dense_h_to_4h_q_scale1: R.Tensor((10240, 80), dtype="float16") = packed_params[477]
            gpt_neox_layers_29_mlp_dense_h_to_4h_bias1: R.Tensor((10240,), dtype="float32") = packed_params[478]
            gpt_neox_layers_29_mlp_dense_4h_to_h_q_weight1: R.Tensor((2560, 1280), dtype="uint32") = packed_params[479]
            gpt_neox_layers_29_mlp_dense_4h_to_h_q_scale1: R.Tensor((2560, 320), dtype="float16") = packed_params[480]
            gpt_neox_layers_29_mlp_dense_4h_to_h_bias1: R.Tensor((2560,), dtype="float32") = packed_params[481]
            gpt_neox_layers_30_input_layernorm_weight1: R.Tensor((2560,), dtype="float16") = packed_params[482]
            gpt_neox_layers_30_input_layernorm_bias1: R.Tensor((2560,), dtype="float16") = packed_params[483]
            gpt_neox_layers_30_post_attention_layernorm_weight1: R.Tensor((2560,), dtype="float16") = packed_params[484]
            gpt_neox_layers_30_post_attention_layernorm_bias1: R.Tensor((2560,), dtype="float16") = packed_params[485]
            gpt_neox_layers_30_attention_query_key_value_q_weight1: R.Tensor((7680, 320), dtype="uint32") = packed_params[486]
            gpt_neox_layers_30_attention_query_key_value_q_scale1: R.Tensor((7680, 80), dtype="float16") = packed_params[487]
            gpt_neox_layers_30_attention_query_key_value_bias1: R.Tensor((7680,), dtype="float16") = packed_params[488]
            gpt_neox_layers_30_attention_dense_q_weight1: R.Tensor((2560, 320), dtype="uint32") = packed_params[489]
            gpt_neox_layers_30_attention_dense_q_scale1: R.Tensor((2560, 80), dtype="float16") = packed_params[490]
            gpt_neox_layers_30_attention_dense_bias1: R.Tensor((2560,), dtype="float16") = packed_params[491]
            gpt_neox_layers_30_mlp_dense_h_to_4h_q_weight1: R.Tensor((10240, 320), dtype="uint32") = packed_params[492]
            gpt_neox_layers_30_mlp_dense_h_to_4h_q_scale1: R.Tensor((10240, 80), dtype="float16") = packed_params[493]
            gpt_neox_layers_30_mlp_dense_h_to_4h_bias1: R.Tensor((10240,), dtype="float32") = packed_params[494]
            gpt_neox_layers_30_mlp_dense_4h_to_h_q_weight1: R.Tensor((2560, 1280), dtype="uint32") = packed_params[495]
            gpt_neox_layers_30_mlp_dense_4h_to_h_q_scale1: R.Tensor((2560, 320), dtype="float16") = packed_params[496]
            gpt_neox_layers_30_mlp_dense_4h_to_h_bias1: R.Tensor((2560,), dtype="float32") = packed_params[497]
            gpt_neox_layers_31_input_layernorm_weight1: R.Tensor((2560,), dtype="float16") = packed_params[498]
            gpt_neox_layers_31_input_layernorm_bias1: R.Tensor((2560,), dtype="float16") = packed_params[499]
            gpt_neox_layers_31_post_attention_layernorm_weight1: R.Tensor((2560,), dtype="float16") = packed_params[500]
            gpt_neox_layers_31_post_attention_layernorm_bias1: R.Tensor((2560,), dtype="float16") = packed_params[501]
            gpt_neox_layers_31_attention_query_key_value_q_weight1: R.Tensor((7680, 320), dtype="uint32") = packed_params[502]
            gpt_neox_layers_31_attention_query_key_value_q_scale1: R.Tensor((7680, 80), dtype="float16") = packed_params[503]
            gpt_neox_layers_31_attention_query_key_value_bias1: R.Tensor((7680,), dtype="float16") = packed_params[504]
            gpt_neox_layers_31_attention_dense_q_weight1: R.Tensor((2560, 320), dtype="uint32") = packed_params[505]
            gpt_neox_layers_31_attention_dense_q_scale1: R.Tensor((2560, 80), dtype="float16") = packed_params[506]
            gpt_neox_layers_31_attention_dense_bias1: R.Tensor((2560,), dtype="float16") = packed_params[507]
            gpt_neox_layers_31_mlp_dense_h_to_4h_q_weight1: R.Tensor((10240, 320), dtype="uint32") = packed_params[508]
            gpt_neox_layers_31_mlp_dense_h_to_4h_q_scale1: R.Tensor((10240, 80), dtype="float16") = packed_params[509]
            gpt_neox_layers_31_mlp_dense_h_to_4h_bias1: R.Tensor((10240,), dtype="float32") = packed_params[510]
            gpt_neox_layers_31_mlp_dense_4h_to_h_q_weight1: R.Tensor((2560, 1280), dtype="uint32") = packed_params[511]
            gpt_neox_layers_31_mlp_dense_4h_to_h_q_scale1: R.Tensor((2560, 320), dtype="float16") = packed_params[512]
            gpt_neox_layers_31_mlp_dense_4h_to_h_bias1: R.Tensor((2560,), dtype="float32") = packed_params[513]
            gpt_neox_final_layer_norm_weight1: R.Tensor((2560,), dtype="float16") = packed_params[514]
            gpt_neox_final_layer_norm_bias1: R.Tensor((2560,), dtype="float16") = packed_params[515]
            embed_out_q_weight1: R.Tensor((vocab_size, 320), dtype="uint32") = packed_params[516]
            embed_out_q_scale1: R.Tensor((vocab_size, 80), dtype="float16") = packed_params[517]
            layer_norm = R.call_tir(cls.layer_norm1, (input_embed, gpt_neox_layers_0_input_layernorm_weight1, gpt_neox_layers_0_input_layernorm_bias1), out_sinfo=R.Tensor((1, seq_len, 2560), dtype="float16"))
            lv256 = R.call_tir(cls.fused_dequantize1_fused_NT_matmul5_add5, (gpt_neox_layers_0_attention_query_key_value_q_weight1, gpt_neox_layers_0_attention_query_key_value_q_scale1, layer_norm, gpt_neox_layers_0_attention_query_key_value_bias1), out_sinfo=R.Tensor((1, seq_len, 7680), dtype="float16"))
            reshape = R.call_tir(cls.reshape4, (lv256,), out_sinfo=R.Tensor((1, seq_len, 96, 80), dtype="float16"))
            reshape1 = R.call_tir(cls.reshape5, (reshape,), out_sinfo=R.Tensor((seq_len, 96, 80), dtype="float16"))
            lv2 = R.call_dps_packed("vm.builtin.attention_kv_cache_attention_with_fused_qkv", (paged_kv_cache, R.prim_value(0), R.prim_value(T.float32(1.0)), reshape1), out_sinfo=R.Tensor((seq_len, 32, 80), dtype="float16"))
            reshape2 = R.call_tir(cls.reshape6, (lv2,), out_sinfo=R.Tensor((1, seq_len, 32, 80), dtype="float16"))
            reshape3 = R.call_tir(cls.reshape7, (reshape2,), out_sinfo=R.Tensor((1, seq_len, 2560), dtype="float16"))
            lv256_1 = R.call_tir(cls.fused_dequantize2_fused_NT_matmul6_add6_add7, (gpt_neox_layers_0_attention_dense_q_weight1, gpt_neox_layers_0_attention_dense_q_scale1, reshape3, gpt_neox_layers_0_attention_dense_bias1, input_embed), out_sinfo=R.Tensor((1, seq_len, 2560), dtype="float16"))
            layer_norm1 = R.call_tir(cls.layer_norm1, (lv256_1, gpt_neox_layers_0_post_attention_layernorm_weight1, gpt_neox_layers_0_post_attention_layernorm_bias1), out_sinfo=R.Tensor((1, seq_len, 2560), dtype="float16"))
            lv257 = R.call_tir(cls.fused_dequantize3_fused_NT_matmul7_add8_gelu1_cast3, (gpt_neox_layers_0_mlp_dense_h_to_4h_q_weight1, gpt_neox_layers_0_mlp_dense_h_to_4h_q_scale1, layer_norm1, gpt_neox_layers_0_mlp_dense_h_to_4h_bias1), out_sinfo=R.Tensor((1, seq_len, 10240), dtype="float16"))
            lv257_1 = R.call_tir(cls.fused_dequantize4_fused_NT_matmul8_add9_cast4_add7, (gpt_neox_layers_0_mlp_dense_4h_to_h_q_weight1, gpt_neox_layers_0_mlp_dense_4h_to_h_q_scale1, lv257, gpt_neox_layers_0_mlp_dense_4h_to_h_bias1, lv256_1), out_sinfo=R.Tensor((1, seq_len, 2560), dtype="float16"))
            layer_norm2 = R.call_tir(cls.layer_norm1, (lv257_1, gpt_neox_layers_1_input_layernorm_weight1, gpt_neox_layers_1_input_layernorm_bias1), out_sinfo=R.Tensor((1, seq_len, 2560), dtype="float16"))
            lv258 = R.call_tir(cls.fused_dequantize1_fused_NT_matmul5_add5, (gpt_neox_layers_1_attention_query_key_value_q_weight1, gpt_neox_layers_1_attention_query_key_value_q_scale1, layer_norm2, gpt_neox_layers_1_attention_query_key_value_bias1), out_sinfo=R.Tensor((1, seq_len, 7680), dtype="float16"))
            reshape4 = R.call_tir(cls.reshape4, (lv258,), out_sinfo=R.Tensor((1, seq_len, 96, 80), dtype="float16"))
            reshape5 = R.call_tir(cls.reshape5, (reshape4,), out_sinfo=R.Tensor((seq_len, 96, 80), dtype="float16"))
            lv7 = R.call_dps_packed("vm.builtin.attention_kv_cache_attention_with_fused_qkv", (paged_kv_cache, R.prim_value(1), R.prim_value(T.float32(1.0)), reshape5), out_sinfo=R.Tensor((seq_len, 32, 80), dtype="float16"))
            reshape6 = R.call_tir(cls.reshape6, (lv7,), out_sinfo=R.Tensor((1, seq_len, 32, 80), dtype="float16"))
            reshape7 = R.call_tir(cls.reshape7, (reshape6,), out_sinfo=R.Tensor((1, seq_len, 2560), dtype="float16"))
            lv258_1 = R.call_tir(cls.fused_dequantize2_fused_NT_matmul6_add6_add7, (gpt_neox_layers_1_attention_dense_q_weight1, gpt_neox_layers_1_attention_dense_q_scale1, reshape7, gpt_neox_layers_1_attention_dense_bias1, lv257_1), out_sinfo=R.Tensor((1, seq_len, 2560), dtype="float16"))
            layer_norm3 = R.call_tir(cls.layer_norm1, (lv258_1, gpt_neox_layers_1_post_attention_layernorm_weight1, gpt_neox_layers_1_post_attention_layernorm_bias1), out_sinfo=R.Tensor((1, seq_len, 2560), dtype="float16"))
            lv259 = R.call_tir(cls.fused_dequantize3_fused_NT_matmul7_add8_gelu1_cast3, (gpt_neox_layers_1_mlp_dense_h_to_4h_q_weight1, gpt_neox_layers_1_mlp_dense_h_to_4h_q_scale1, layer_norm3, gpt_neox_layers_1_mlp_dense_h_to_4h_bias1), out_sinfo=R.Tensor((1, seq_len, 10240), dtype="float16"))
            lv259_1 = R.call_tir(cls.fused_dequantize4_fused_NT_matmul8_add9_cast4_add7, (gpt_neox_layers_1_mlp_dense_4h_to_h_q_weight1, gpt_neox_layers_1_mlp_dense_4h_to_h_q_scale1, lv259, gpt_neox_layers_1_mlp_dense_4h_to_h_bias1, lv258_1), out_sinfo=R.Tensor((1, seq_len, 2560), dtype="float16"))
            layer_norm4 = R.call_tir(cls.layer_norm1, (lv259_1, gpt_neox_layers_2_input_layernorm_weight1, gpt_neox_layers_2_input_layernorm_bias1), out_sinfo=R.Tensor((1, seq_len, 2560), dtype="float16"))
            lv260 = R.call_tir(cls.fused_dequantize1_fused_NT_matmul5_add5, (gpt_neox_layers_2_attention_query_key_value_q_weight1, gpt_neox_layers_2_attention_query_key_value_q_scale1, layer_norm4, gpt_neox_layers_2_attention_query_key_value_bias1), out_sinfo=R.Tensor((1, seq_len, 7680), dtype="float16"))
            reshape8 = R.call_tir(cls.reshape4, (lv260,), out_sinfo=R.Tensor((1, seq_len, 96, 80), dtype="float16"))
            reshape9 = R.call_tir(cls.reshape5, (reshape8,), out_sinfo=R.Tensor((seq_len, 96, 80), dtype="float16"))
            lv12 = R.call_dps_packed("vm.builtin.attention_kv_cache_attention_with_fused_qkv", (paged_kv_cache, R.prim_value(2), R.prim_value(T.float32(1.0)), reshape9), out_sinfo=R.Tensor((seq_len, 32, 80), dtype="float16"))
            reshape10 = R.call_tir(cls.reshape6, (lv12,), out_sinfo=R.Tensor((1, seq_len, 32, 80), dtype="float16"))
            reshape11 = R.call_tir(cls.reshape7, (reshape10,), out_sinfo=R.Tensor((1, seq_len, 2560), dtype="float16"))
            lv260_1 = R.call_tir(cls.fused_dequantize2_fused_NT_matmul6_add6_add7, (gpt_neox_layers_2_attention_dense_q_weight1, gpt_neox_layers_2_attention_dense_q_scale1, reshape11, gpt_neox_layers_2_attention_dense_bias1, lv259_1), out_sinfo=R.Tensor((1, seq_len, 2560), dtype="float16"))
            layer_norm5 = R.call_tir(cls.layer_norm1, (lv260_1, gpt_neox_layers_2_post_attention_layernorm_weight1, gpt_neox_layers_2_post_attention_layernorm_bias1), out_sinfo=R.Tensor((1, seq_len, 2560), dtype="float16"))
            lv261 = R.call_tir(cls.fused_dequantize3_fused_NT_matmul7_add8_gelu1_cast3, (gpt_neox_layers_2_mlp_dense_h_to_4h_q_weight1, gpt_neox_layers_2_mlp_dense_h_to_4h_q_scale1, layer_norm5, gpt_neox_layers_2_mlp_dense_h_to_4h_bias1), out_sinfo=R.Tensor((1, seq_len, 10240), dtype="float16"))
            lv261_1 = R.call_tir(cls.fused_dequantize4_fused_NT_matmul8_add9_cast4_add7, (gpt_neox_layers_2_mlp_dense_4h_to_h_q_weight1, gpt_neox_layers_2_mlp_dense_4h_to_h_q_scale1, lv261, gpt_neox_layers_2_mlp_dense_4h_to_h_bias1, lv260_1), out_sinfo=R.Tensor((1, seq_len, 2560), dtype="float16"))
            layer_norm6 = R.call_tir(cls.layer_norm1, (lv261_1, gpt_neox_layers_3_input_layernorm_weight1, gpt_neox_layers_3_input_layernorm_bias1), out_sinfo=R.Tensor((1, seq_len, 2560), dtype="float16"))
            lv262 = R.call_tir(cls.fused_dequantize1_fused_NT_matmul5_add5, (gpt_neox_layers_3_attention_query_key_value_q_weight1, gpt_neox_layers_3_attention_query_key_value_q_scale1, layer_norm6, gpt_neox_layers_3_attention_query_key_value_bias1), out_sinfo=R.Tensor((1, seq_len, 7680), dtype="float16"))
            reshape12 = R.call_tir(cls.reshape4, (lv262,), out_sinfo=R.Tensor((1, seq_len, 96, 80), dtype="float16"))
            reshape13 = R.call_tir(cls.reshape5, (reshape12,), out_sinfo=R.Tensor((seq_len, 96, 80), dtype="float16"))
            lv17 = R.call_dps_packed("vm.builtin.attention_kv_cache_attention_with_fused_qkv", (paged_kv_cache, R.prim_value(3), R.prim_value(T.float32(1.0)), reshape13), out_sinfo=R.Tensor((seq_len, 32, 80), dtype="float16"))
            reshape14 = R.call_tir(cls.reshape6, (lv17,), out_sinfo=R.Tensor((1, seq_len, 32, 80), dtype="float16"))
            reshape15 = R.call_tir(cls.reshape7, (reshape14,), out_sinfo=R.Tensor((1, seq_len, 2560), dtype="float16"))
            lv262_1 = R.call_tir(cls.fused_dequantize2_fused_NT_matmul6_add6_add7, (gpt_neox_layers_3_attention_dense_q_weight1, gpt_neox_layers_3_attention_dense_q_scale1, reshape15, gpt_neox_layers_3_attention_dense_bias1, lv261_1), out_sinfo=R.Tensor((1, seq_len, 2560), dtype="float16"))
            layer_norm7 = R.call_tir(cls.layer_norm1, (lv262_1, gpt_neox_layers_3_post_attention_layernorm_weight1, gpt_neox_layers_3_post_attention_layernorm_bias1), out_sinfo=R.Tensor((1, seq_len, 2560), dtype="float16"))
            lv263 = R.call_tir(cls.fused_dequantize3_fused_NT_matmul7_add8_gelu1_cast3, (gpt_neox_layers_3_mlp_dense_h_to_4h_q_weight1, gpt_neox_layers_3_mlp_dense_h_to_4h_q_scale1, layer_norm7, gpt_neox_layers_3_mlp_dense_h_to_4h_bias1), out_sinfo=R.Tensor((1, seq_len, 10240), dtype="float16"))
            lv263_1 = R.call_tir(cls.fused_dequantize4_fused_NT_matmul8_add9_cast4_add7, (gpt_neox_layers_3_mlp_dense_4h_to_h_q_weight1, gpt_neox_layers_3_mlp_dense_4h_to_h_q_scale1, lv263, gpt_neox_layers_3_mlp_dense_4h_to_h_bias1, lv262_1), out_sinfo=R.Tensor((1, seq_len, 2560), dtype="float16"))
            layer_norm8 = R.call_tir(cls.layer_norm1, (lv263_1, gpt_neox_layers_4_input_layernorm_weight1, gpt_neox_layers_4_input_layernorm_bias1), out_sinfo=R.Tensor((1, seq_len, 2560), dtype="float16"))
            lv264 = R.call_tir(cls.fused_dequantize1_fused_NT_matmul5_add5, (gpt_neox_layers_4_attention_query_key_value_q_weight1, gpt_neox_layers_4_attention_query_key_value_q_scale1, layer_norm8, gpt_neox_layers_4_attention_query_key_value_bias1), out_sinfo=R.Tensor((1, seq_len, 7680), dtype="float16"))
            reshape16 = R.call_tir(cls.reshape4, (lv264,), out_sinfo=R.Tensor((1, seq_len, 96, 80), dtype="float16"))
            reshape17 = R.call_tir(cls.reshape5, (reshape16,), out_sinfo=R.Tensor((seq_len, 96, 80), dtype="float16"))
            lv22 = R.call_dps_packed("vm.builtin.attention_kv_cache_attention_with_fused_qkv", (paged_kv_cache, R.prim_value(4), R.prim_value(T.float32(1.0)), reshape17), out_sinfo=R.Tensor((seq_len, 32, 80), dtype="float16"))
            reshape18 = R.call_tir(cls.reshape6, (lv22,), out_sinfo=R.Tensor((1, seq_len, 32, 80), dtype="float16"))
            reshape19 = R.call_tir(cls.reshape7, (reshape18,), out_sinfo=R.Tensor((1, seq_len, 2560), dtype="float16"))
            lv264_1 = R.call_tir(cls.fused_dequantize2_fused_NT_matmul6_add6_add7, (gpt_neox_layers_4_attention_dense_q_weight1, gpt_neox_layers_4_attention_dense_q_scale1, reshape19, gpt_neox_layers_4_attention_dense_bias1, lv263_1), out_sinfo=R.Tensor((1, seq_len, 2560), dtype="float16"))
            layer_norm9 = R.call_tir(cls.layer_norm1, (lv264_1, gpt_neox_layers_4_post_attention_layernorm_weight1, gpt_neox_layers_4_post_attention_layernorm_bias1), out_sinfo=R.Tensor((1, seq_len, 2560), dtype="float16"))
            lv265 = R.call_tir(cls.fused_dequantize3_fused_NT_matmul7_add8_gelu1_cast3, (gpt_neox_layers_4_mlp_dense_h_to_4h_q_weight1, gpt_neox_layers_4_mlp_dense_h_to_4h_q_scale1, layer_norm9, gpt_neox_layers_4_mlp_dense_h_to_4h_bias1), out_sinfo=R.Tensor((1, seq_len, 10240), dtype="float16"))
            lv265_1 = R.call_tir(cls.fused_dequantize4_fused_NT_matmul8_add9_cast4_add7, (gpt_neox_layers_4_mlp_dense_4h_to_h_q_weight1, gpt_neox_layers_4_mlp_dense_4h_to_h_q_scale1, lv265, gpt_neox_layers_4_mlp_dense_4h_to_h_bias1, lv264_1), out_sinfo=R.Tensor((1, seq_len, 2560), dtype="float16"))
            layer_norm10 = R.call_tir(cls.layer_norm1, (lv265_1, gpt_neox_layers_5_input_layernorm_weight1, gpt_neox_layers_5_input_layernorm_bias1), out_sinfo=R.Tensor((1, seq_len, 2560), dtype="float16"))
            lv266 = R.call_tir(cls.fused_dequantize1_fused_NT_matmul5_add5, (gpt_neox_layers_5_attention_query_key_value_q_weight1, gpt_neox_layers_5_attention_query_key_value_q_scale1, layer_norm10, gpt_neox_layers_5_attention_query_key_value_bias1), out_sinfo=R.Tensor((1, seq_len, 7680), dtype="float16"))
            reshape20 = R.call_tir(cls.reshape4, (lv266,), out_sinfo=R.Tensor((1, seq_len, 96, 80), dtype="float16"))
            reshape21 = R.call_tir(cls.reshape5, (reshape20,), out_sinfo=R.Tensor((seq_len, 96, 80), dtype="float16"))
            lv27 = R.call_dps_packed("vm.builtin.attention_kv_cache_attention_with_fused_qkv", (paged_kv_cache, R.prim_value(5), R.prim_value(T.float32(1.0)), reshape21), out_sinfo=R.Tensor((seq_len, 32, 80), dtype="float16"))
            reshape22 = R.call_tir(cls.reshape6, (lv27,), out_sinfo=R.Tensor((1, seq_len, 32, 80), dtype="float16"))
            reshape23 = R.call_tir(cls.reshape7, (reshape22,), out_sinfo=R.Tensor((1, seq_len, 2560), dtype="float16"))
            lv266_1 = R.call_tir(cls.fused_dequantize2_fused_NT_matmul6_add6_add7, (gpt_neox_layers_5_attention_dense_q_weight1, gpt_neox_layers_5_attention_dense_q_scale1, reshape23, gpt_neox_layers_5_attention_dense_bias1, lv265_1), out_sinfo=R.Tensor((1, seq_len, 2560), dtype="float16"))
            layer_norm11 = R.call_tir(cls.layer_norm1, (lv266_1, gpt_neox_layers_5_post_attention_layernorm_weight1, gpt_neox_layers_5_post_attention_layernorm_bias1), out_sinfo=R.Tensor((1, seq_len, 2560), dtype="float16"))
            lv267 = R.call_tir(cls.fused_dequantize3_fused_NT_matmul7_add8_gelu1_cast3, (gpt_neox_layers_5_mlp_dense_h_to_4h_q_weight1, gpt_neox_layers_5_mlp_dense_h_to_4h_q_scale1, layer_norm11, gpt_neox_layers_5_mlp_dense_h_to_4h_bias1), out_sinfo=R.Tensor((1, seq_len, 10240), dtype="float16"))
            lv267_1 = R.call_tir(cls.fused_dequantize4_fused_NT_matmul8_add9_cast4_add7, (gpt_neox_layers_5_mlp_dense_4h_to_h_q_weight1, gpt_neox_layers_5_mlp_dense_4h_to_h_q_scale1, lv267, gpt_neox_layers_5_mlp_dense_4h_to_h_bias1, lv266_1), out_sinfo=R.Tensor((1, seq_len, 2560), dtype="float16"))
            layer_norm12 = R.call_tir(cls.layer_norm1, (lv267_1, gpt_neox_layers_6_input_layernorm_weight1, gpt_neox_layers_6_input_layernorm_bias1), out_sinfo=R.Tensor((1, seq_len, 2560), dtype="float16"))
            lv268 = R.call_tir(cls.fused_dequantize1_fused_NT_matmul5_add5, (gpt_neox_layers_6_attention_query_key_value_q_weight1, gpt_neox_layers_6_attention_query_key_value_q_scale1, layer_norm12, gpt_neox_layers_6_attention_query_key_value_bias1), out_sinfo=R.Tensor((1, seq_len, 7680), dtype="float16"))
            reshape24 = R.call_tir(cls.reshape4, (lv268,), out_sinfo=R.Tensor((1, seq_len, 96, 80), dtype="float16"))
            reshape25 = R.call_tir(cls.reshape5, (reshape24,), out_sinfo=R.Tensor((seq_len, 96, 80), dtype="float16"))
            lv32 = R.call_dps_packed("vm.builtin.attention_kv_cache_attention_with_fused_qkv", (paged_kv_cache, R.prim_value(6), R.prim_value(T.float32(1.0)), reshape25), out_sinfo=R.Tensor((seq_len, 32, 80), dtype="float16"))
            reshape26 = R.call_tir(cls.reshape6, (lv32,), out_sinfo=R.Tensor((1, seq_len, 32, 80), dtype="float16"))
            reshape27 = R.call_tir(cls.reshape7, (reshape26,), out_sinfo=R.Tensor((1, seq_len, 2560), dtype="float16"))
            lv268_1 = R.call_tir(cls.fused_dequantize2_fused_NT_matmul6_add6_add7, (gpt_neox_layers_6_attention_dense_q_weight1, gpt_neox_layers_6_attention_dense_q_scale1, reshape27, gpt_neox_layers_6_attention_dense_bias1, lv267_1), out_sinfo=R.Tensor((1, seq_len, 2560), dtype="float16"))
            layer_norm13 = R.call_tir(cls.layer_norm1, (lv268_1, gpt_neox_layers_6_post_attention_layernorm_weight1, gpt_neox_layers_6_post_attention_layernorm_bias1), out_sinfo=R.Tensor((1, seq_len, 2560), dtype="float16"))
            lv269 = R.call_tir(cls.fused_dequantize3_fused_NT_matmul7_add8_gelu1_cast3, (gpt_neox_layers_6_mlp_dense_h_to_4h_q_weight1, gpt_neox_layers_6_mlp_dense_h_to_4h_q_scale1, layer_norm13, gpt_neox_layers_6_mlp_dense_h_to_4h_bias1), out_sinfo=R.Tensor((1, seq_len, 10240), dtype="float16"))
            lv269_1 = R.call_tir(cls.fused_dequantize4_fused_NT_matmul8_add9_cast4_add7, (gpt_neox_layers_6_mlp_dense_4h_to_h_q_weight1, gpt_neox_layers_6_mlp_dense_4h_to_h_q_scale1, lv269, gpt_neox_layers_6_mlp_dense_4h_to_h_bias1, lv268_1), out_sinfo=R.Tensor((1, seq_len, 2560), dtype="float16"))
            layer_norm14 = R.call_tir(cls.layer_norm1, (lv269_1, gpt_neox_layers_7_input_layernorm_weight1, gpt_neox_layers_7_input_layernorm_bias1), out_sinfo=R.Tensor((1, seq_len, 2560), dtype="float16"))
            lv270 = R.call_tir(cls.fused_dequantize1_fused_NT_matmul5_add5, (gpt_neox_layers_7_attention_query_key_value_q_weight1, gpt_neox_layers_7_attention_query_key_value_q_scale1, layer_norm14, gpt_neox_layers_7_attention_query_key_value_bias1), out_sinfo=R.Tensor((1, seq_len, 7680), dtype="float16"))
            reshape28 = R.call_tir(cls.reshape4, (lv270,), out_sinfo=R.Tensor((1, seq_len, 96, 80), dtype="float16"))
            reshape29 = R.call_tir(cls.reshape5, (reshape28,), out_sinfo=R.Tensor((seq_len, 96, 80), dtype="float16"))
            lv37 = R.call_dps_packed("vm.builtin.attention_kv_cache_attention_with_fused_qkv", (paged_kv_cache, R.prim_value(7), R.prim_value(T.float32(1.0)), reshape29), out_sinfo=R.Tensor((seq_len, 32, 80), dtype="float16"))
            reshape30 = R.call_tir(cls.reshape6, (lv37,), out_sinfo=R.Tensor((1, seq_len, 32, 80), dtype="float16"))
            reshape31 = R.call_tir(cls.reshape7, (reshape30,), out_sinfo=R.Tensor((1, seq_len, 2560), dtype="float16"))
            lv270_1 = R.call_tir(cls.fused_dequantize2_fused_NT_matmul6_add6_add7, (gpt_neox_layers_7_attention_dense_q_weight1, gpt_neox_layers_7_attention_dense_q_scale1, reshape31, gpt_neox_layers_7_attention_dense_bias1, lv269_1), out_sinfo=R.Tensor((1, seq_len, 2560), dtype="float16"))
            layer_norm15 = R.call_tir(cls.layer_norm1, (lv270_1, gpt_neox_layers_7_post_attention_layernorm_weight1, gpt_neox_layers_7_post_attention_layernorm_bias1), out_sinfo=R.Tensor((1, seq_len, 2560), dtype="float16"))
            lv271 = R.call_tir(cls.fused_dequantize3_fused_NT_matmul7_add8_gelu1_cast3, (gpt_neox_layers_7_mlp_dense_h_to_4h_q_weight1, gpt_neox_layers_7_mlp_dense_h_to_4h_q_scale1, layer_norm15, gpt_neox_layers_7_mlp_dense_h_to_4h_bias1), out_sinfo=R.Tensor((1, seq_len, 10240), dtype="float16"))
            lv271_1 = R.call_tir(cls.fused_dequantize4_fused_NT_matmul8_add9_cast4_add7, (gpt_neox_layers_7_mlp_dense_4h_to_h_q_weight1, gpt_neox_layers_7_mlp_dense_4h_to_h_q_scale1, lv271, gpt_neox_layers_7_mlp_dense_4h_to_h_bias1, lv270_1), out_sinfo=R.Tensor((1, seq_len, 2560), dtype="float16"))
            layer_norm16 = R.call_tir(cls.layer_norm1, (lv271_1, gpt_neox_layers_8_input_layernorm_weight1, gpt_neox_layers_8_input_layernorm_bias1), out_sinfo=R.Tensor((1, seq_len, 2560), dtype="float16"))
            lv272 = R.call_tir(cls.fused_dequantize1_fused_NT_matmul5_add5, (gpt_neox_layers_8_attention_query_key_value_q_weight1, gpt_neox_layers_8_attention_query_key_value_q_scale1, layer_norm16, gpt_neox_layers_8_attention_query_key_value_bias1), out_sinfo=R.Tensor((1, seq_len, 7680), dtype="float16"))
            reshape32 = R.call_tir(cls.reshape4, (lv272,), out_sinfo=R.Tensor((1, seq_len, 96, 80), dtype="float16"))
            reshape33 = R.call_tir(cls.reshape5, (reshape32,), out_sinfo=R.Tensor((seq_len, 96, 80), dtype="float16"))
            lv42 = R.call_dps_packed("vm.builtin.attention_kv_cache_attention_with_fused_qkv", (paged_kv_cache, R.prim_value(8), R.prim_value(T.float32(1.0)), reshape33), out_sinfo=R.Tensor((seq_len, 32, 80), dtype="float16"))
            reshape34 = R.call_tir(cls.reshape6, (lv42,), out_sinfo=R.Tensor((1, seq_len, 32, 80), dtype="float16"))
            reshape35 = R.call_tir(cls.reshape7, (reshape34,), out_sinfo=R.Tensor((1, seq_len, 2560), dtype="float16"))
            lv272_1 = R.call_tir(cls.fused_dequantize2_fused_NT_matmul6_add6_add7, (gpt_neox_layers_8_attention_dense_q_weight1, gpt_neox_layers_8_attention_dense_q_scale1, reshape35, gpt_neox_layers_8_attention_dense_bias1, lv271_1), out_sinfo=R.Tensor((1, seq_len, 2560), dtype="float16"))
            layer_norm17 = R.call_tir(cls.layer_norm1, (lv272_1, gpt_neox_layers_8_post_attention_layernorm_weight1, gpt_neox_layers_8_post_attention_layernorm_bias1), out_sinfo=R.Tensor((1, seq_len, 2560), dtype="float16"))
            lv273 = R.call_tir(cls.fused_dequantize3_fused_NT_matmul7_add8_gelu1_cast3, (gpt_neox_layers_8_mlp_dense_h_to_4h_q_weight1, gpt_neox_layers_8_mlp_dense_h_to_4h_q_scale1, layer_norm17, gpt_neox_layers_8_mlp_dense_h_to_4h_bias1), out_sinfo=R.Tensor((1, seq_len, 10240), dtype="float16"))
            lv273_1 = R.call_tir(cls.fused_dequantize4_fused_NT_matmul8_add9_cast4_add7, (gpt_neox_layers_8_mlp_dense_4h_to_h_q_weight1, gpt_neox_layers_8_mlp_dense_4h_to_h_q_scale1, lv273, gpt_neox_layers_8_mlp_dense_4h_to_h_bias1, lv272_1), out_sinfo=R.Tensor((1, seq_len, 2560), dtype="float16"))
            layer_norm18 = R.call_tir(cls.layer_norm1, (lv273_1, gpt_neox_layers_9_input_layernorm_weight1, gpt_neox_layers_9_input_layernorm_bias1), out_sinfo=R.Tensor((1, seq_len, 2560), dtype="float16"))
            lv274 = R.call_tir(cls.fused_dequantize1_fused_NT_matmul5_add5, (gpt_neox_layers_9_attention_query_key_value_q_weight1, gpt_neox_layers_9_attention_query_key_value_q_scale1, layer_norm18, gpt_neox_layers_9_attention_query_key_value_bias1), out_sinfo=R.Tensor((1, seq_len, 7680), dtype="float16"))
            reshape36 = R.call_tir(cls.reshape4, (lv274,), out_sinfo=R.Tensor((1, seq_len, 96, 80), dtype="float16"))
            reshape37 = R.call_tir(cls.reshape5, (reshape36,), out_sinfo=R.Tensor((seq_len, 96, 80), dtype="float16"))
            lv47 = R.call_dps_packed("vm.builtin.attention_kv_cache_attention_with_fused_qkv", (paged_kv_cache, R.prim_value(9), R.prim_value(T.float32(1.0)), reshape37), out_sinfo=R.Tensor((seq_len, 32, 80), dtype="float16"))
            reshape38 = R.call_tir(cls.reshape6, (lv47,), out_sinfo=R.Tensor((1, seq_len, 32, 80), dtype="float16"))
            reshape39 = R.call_tir(cls.reshape7, (reshape38,), out_sinfo=R.Tensor((1, seq_len, 2560), dtype="float16"))
            lv274_1 = R.call_tir(cls.fused_dequantize2_fused_NT_matmul6_add6_add7, (gpt_neox_layers_9_attention_dense_q_weight1, gpt_neox_layers_9_attention_dense_q_scale1, reshape39, gpt_neox_layers_9_attention_dense_bias1, lv273_1), out_sinfo=R.Tensor((1, seq_len, 2560), dtype="float16"))
            layer_norm19 = R.call_tir(cls.layer_norm1, (lv274_1, gpt_neox_layers_9_post_attention_layernorm_weight1, gpt_neox_layers_9_post_attention_layernorm_bias1), out_sinfo=R.Tensor((1, seq_len, 2560), dtype="float16"))
            lv275 = R.call_tir(cls.fused_dequantize3_fused_NT_matmul7_add8_gelu1_cast3, (gpt_neox_layers_9_mlp_dense_h_to_4h_q_weight1, gpt_neox_layers_9_mlp_dense_h_to_4h_q_scale1, layer_norm19, gpt_neox_layers_9_mlp_dense_h_to_4h_bias1), out_sinfo=R.Tensor((1, seq_len, 10240), dtype="float16"))
            lv275_1 = R.call_tir(cls.fused_dequantize4_fused_NT_matmul8_add9_cast4_add7, (gpt_neox_layers_9_mlp_dense_4h_to_h_q_weight1, gpt_neox_layers_9_mlp_dense_4h_to_h_q_scale1, lv275, gpt_neox_layers_9_mlp_dense_4h_to_h_bias1, lv274_1), out_sinfo=R.Tensor((1, seq_len, 2560), dtype="float16"))
            layer_norm20 = R.call_tir(cls.layer_norm1, (lv275_1, gpt_neox_layers_10_input_layernorm_weight1, gpt_neox_layers_10_input_layernorm_bias1), out_sinfo=R.Tensor((1, seq_len, 2560), dtype="float16"))
            lv276 = R.call_tir(cls.fused_dequantize1_fused_NT_matmul5_add5, (gpt_neox_layers_10_attention_query_key_value_q_weight1, gpt_neox_layers_10_attention_query_key_value_q_scale1, layer_norm20, gpt_neox_layers_10_attention_query_key_value_bias1), out_sinfo=R.Tensor((1, seq_len, 7680), dtype="float16"))
            reshape40 = R.call_tir(cls.reshape4, (lv276,), out_sinfo=R.Tensor((1, seq_len, 96, 80), dtype="float16"))
            reshape41 = R.call_tir(cls.reshape5, (reshape40,), out_sinfo=R.Tensor((seq_len, 96, 80), dtype="float16"))
            lv52 = R.call_dps_packed("vm.builtin.attention_kv_cache_attention_with_fused_qkv", (paged_kv_cache, R.prim_value(10), R.prim_value(T.float32(1.0)), reshape41), out_sinfo=R.Tensor((seq_len, 32, 80), dtype="float16"))
            reshape42 = R.call_tir(cls.reshape6, (lv52,), out_sinfo=R.Tensor((1, seq_len, 32, 80), dtype="float16"))
            reshape43 = R.call_tir(cls.reshape7, (reshape42,), out_sinfo=R.Tensor((1, seq_len, 2560), dtype="float16"))
            lv276_1 = R.call_tir(cls.fused_dequantize2_fused_NT_matmul6_add6_add7, (gpt_neox_layers_10_attention_dense_q_weight1, gpt_neox_layers_10_attention_dense_q_scale1, reshape43, gpt_neox_layers_10_attention_dense_bias1, lv275_1), out_sinfo=R.Tensor((1, seq_len, 2560), dtype="float16"))
            layer_norm21 = R.call_tir(cls.layer_norm1, (lv276_1, gpt_neox_layers_10_post_attention_layernorm_weight1, gpt_neox_layers_10_post_attention_layernorm_bias1), out_sinfo=R.Tensor((1, seq_len, 2560), dtype="float16"))
            lv277 = R.call_tir(cls.fused_dequantize3_fused_NT_matmul7_add8_gelu1_cast3, (gpt_neox_layers_10_mlp_dense_h_to_4h_q_weight1, gpt_neox_layers_10_mlp_dense_h_to_4h_q_scale1, layer_norm21, gpt_neox_layers_10_mlp_dense_h_to_4h_bias1), out_sinfo=R.Tensor((1, seq_len, 10240), dtype="float16"))
            lv277_1 = R.call_tir(cls.fused_dequantize4_fused_NT_matmul8_add9_cast4_add7, (gpt_neox_layers_10_mlp_dense_4h_to_h_q_weight1, gpt_neox_layers_10_mlp_dense_4h_to_h_q_scale1, lv277, gpt_neox_layers_10_mlp_dense_4h_to_h_bias1, lv276_1), out_sinfo=R.Tensor((1, seq_len, 2560), dtype="float16"))
            layer_norm22 = R.call_tir(cls.layer_norm1, (lv277_1, gpt_neox_layers_11_input_layernorm_weight1, gpt_neox_layers_11_input_layernorm_bias1), out_sinfo=R.Tensor((1, seq_len, 2560), dtype="float16"))
            lv278 = R.call_tir(cls.fused_dequantize1_fused_NT_matmul5_add5, (gpt_neox_layers_11_attention_query_key_value_q_weight1, gpt_neox_layers_11_attention_query_key_value_q_scale1, layer_norm22, gpt_neox_layers_11_attention_query_key_value_bias1), out_sinfo=R.Tensor((1, seq_len, 7680), dtype="float16"))
            reshape44 = R.call_tir(cls.reshape4, (lv278,), out_sinfo=R.Tensor((1, seq_len, 96, 80), dtype="float16"))
            reshape45 = R.call_tir(cls.reshape5, (reshape44,), out_sinfo=R.Tensor((seq_len, 96, 80), dtype="float16"))
            lv57 = R.call_dps_packed("vm.builtin.attention_kv_cache_attention_with_fused_qkv", (paged_kv_cache, R.prim_value(11), R.prim_value(T.float32(1.0)), reshape45), out_sinfo=R.Tensor((seq_len, 32, 80), dtype="float16"))
            reshape46 = R.call_tir(cls.reshape6, (lv57,), out_sinfo=R.Tensor((1, seq_len, 32, 80), dtype="float16"))
            reshape47 = R.call_tir(cls.reshape7, (reshape46,), out_sinfo=R.Tensor((1, seq_len, 2560), dtype="float16"))
            lv278_1 = R.call_tir(cls.fused_dequantize2_fused_NT_matmul6_add6_add7, (gpt_neox_layers_11_attention_dense_q_weight1, gpt_neox_layers_11_attention_dense_q_scale1, reshape47, gpt_neox_layers_11_attention_dense_bias1, lv277_1), out_sinfo=R.Tensor((1, seq_len, 2560), dtype="float16"))
            layer_norm23 = R.call_tir(cls.layer_norm1, (lv278_1, gpt_neox_layers_11_post_attention_layernorm_weight1, gpt_neox_layers_11_post_attention_layernorm_bias1), out_sinfo=R.Tensor((1, seq_len, 2560), dtype="float16"))
            lv279 = R.call_tir(cls.fused_dequantize3_fused_NT_matmul7_add8_gelu1_cast3, (gpt_neox_layers_11_mlp_dense_h_to_4h_q_weight1, gpt_neox_layers_11_mlp_dense_h_to_4h_q_scale1, layer_norm23, gpt_neox_layers_11_mlp_dense_h_to_4h_bias1), out_sinfo=R.Tensor((1, seq_len, 10240), dtype="float16"))
            lv279_1 = R.call_tir(cls.fused_dequantize4_fused_NT_matmul8_add9_cast4_add7, (gpt_neox_layers_11_mlp_dense_4h_to_h_q_weight1, gpt_neox_layers_11_mlp_dense_4h_to_h_q_scale1, lv279, gpt_neox_layers_11_mlp_dense_4h_to_h_bias1, lv278_1), out_sinfo=R.Tensor((1, seq_len, 2560), dtype="float16"))
            layer_norm24 = R.call_tir(cls.layer_norm1, (lv279_1, gpt_neox_layers_12_input_layernorm_weight1, gpt_neox_layers_12_input_layernorm_bias1), out_sinfo=R.Tensor((1, seq_len, 2560), dtype="float16"))
            lv280 = R.call_tir(cls.fused_dequantize1_fused_NT_matmul5_add5, (gpt_neox_layers_12_attention_query_key_value_q_weight1, gpt_neox_layers_12_attention_query_key_value_q_scale1, layer_norm24, gpt_neox_layers_12_attention_query_key_value_bias1), out_sinfo=R.Tensor((1, seq_len, 7680), dtype="float16"))
            reshape48 = R.call_tir(cls.reshape4, (lv280,), out_sinfo=R.Tensor((1, seq_len, 96, 80), dtype="float16"))
            reshape49 = R.call_tir(cls.reshape5, (reshape48,), out_sinfo=R.Tensor((seq_len, 96, 80), dtype="float16"))
            lv62 = R.call_dps_packed("vm.builtin.attention_kv_cache_attention_with_fused_qkv", (paged_kv_cache, R.prim_value(12), R.prim_value(T.float32(1.0)), reshape49), out_sinfo=R.Tensor((seq_len, 32, 80), dtype="float16"))
            reshape50 = R.call_tir(cls.reshape6, (lv62,), out_sinfo=R.Tensor((1, seq_len, 32, 80), dtype="float16"))
            reshape51 = R.call_tir(cls.reshape7, (reshape50,), out_sinfo=R.Tensor((1, seq_len, 2560), dtype="float16"))
            lv280_1 = R.call_tir(cls.fused_dequantize2_fused_NT_matmul6_add6_add7, (gpt_neox_layers_12_attention_dense_q_weight1, gpt_neox_layers_12_attention_dense_q_scale1, reshape51, gpt_neox_layers_12_attention_dense_bias1, lv279_1), out_sinfo=R.Tensor((1, seq_len, 2560), dtype="float16"))
            layer_norm25 = R.call_tir(cls.layer_norm1, (lv280_1, gpt_neox_layers_12_post_attention_layernorm_weight1, gpt_neox_layers_12_post_attention_layernorm_bias1), out_sinfo=R.Tensor((1, seq_len, 2560), dtype="float16"))
            lv281 = R.call_tir(cls.fused_dequantize3_fused_NT_matmul7_add8_gelu1_cast3, (gpt_neox_layers_12_mlp_dense_h_to_4h_q_weight1, gpt_neox_layers_12_mlp_dense_h_to_4h_q_scale1, layer_norm25, gpt_neox_layers_12_mlp_dense_h_to_4h_bias1), out_sinfo=R.Tensor((1, seq_len, 10240), dtype="float16"))
            lv281_1 = R.call_tir(cls.fused_dequantize4_fused_NT_matmul8_add9_cast4_add7, (gpt_neox_layers_12_mlp_dense_4h_to_h_q_weight1, gpt_neox_layers_12_mlp_dense_4h_to_h_q_scale1, lv281, gpt_neox_layers_12_mlp_dense_4h_to_h_bias1, lv280_1), out_sinfo=R.Tensor((1, seq_len, 2560), dtype="float16"))
            layer_norm26 = R.call_tir(cls.layer_norm1, (lv281_1, gpt_neox_layers_13_input_layernorm_weight1, gpt_neox_layers_13_input_layernorm_bias1), out_sinfo=R.Tensor((1, seq_len, 2560), dtype="float16"))
            lv282 = R.call_tir(cls.fused_dequantize1_fused_NT_matmul5_add5, (gpt_neox_layers_13_attention_query_key_value_q_weight1, gpt_neox_layers_13_attention_query_key_value_q_scale1, layer_norm26, gpt_neox_layers_13_attention_query_key_value_bias1), out_sinfo=R.Tensor((1, seq_len, 7680), dtype="float16"))
            reshape52 = R.call_tir(cls.reshape4, (lv282,), out_sinfo=R.Tensor((1, seq_len, 96, 80), dtype="float16"))
            reshape53 = R.call_tir(cls.reshape5, (reshape52,), out_sinfo=R.Tensor((seq_len, 96, 80), dtype="float16"))
            lv67 = R.call_dps_packed("vm.builtin.attention_kv_cache_attention_with_fused_qkv", (paged_kv_cache, R.prim_value(13), R.prim_value(T.float32(1.0)), reshape53), out_sinfo=R.Tensor((seq_len, 32, 80), dtype="float16"))
            reshape54 = R.call_tir(cls.reshape6, (lv67,), out_sinfo=R.Tensor((1, seq_len, 32, 80), dtype="float16"))
            reshape55 = R.call_tir(cls.reshape7, (reshape54,), out_sinfo=R.Tensor((1, seq_len, 2560), dtype="float16"))
            lv282_1 = R.call_tir(cls.fused_dequantize2_fused_NT_matmul6_add6_add7, (gpt_neox_layers_13_attention_dense_q_weight1, gpt_neox_layers_13_attention_dense_q_scale1, reshape55, gpt_neox_layers_13_attention_dense_bias1, lv281_1), out_sinfo=R.Tensor((1, seq_len, 2560), dtype="float16"))
            layer_norm27 = R.call_tir(cls.layer_norm1, (lv282_1, gpt_neox_layers_13_post_attention_layernorm_weight1, gpt_neox_layers_13_post_attention_layernorm_bias1), out_sinfo=R.Tensor((1, seq_len, 2560), dtype="float16"))
            lv283 = R.call_tir(cls.fused_dequantize3_fused_NT_matmul7_add8_gelu1_cast3, (gpt_neox_layers_13_mlp_dense_h_to_4h_q_weight1, gpt_neox_layers_13_mlp_dense_h_to_4h_q_scale1, layer_norm27, gpt_neox_layers_13_mlp_dense_h_to_4h_bias1), out_sinfo=R.Tensor((1, seq_len, 10240), dtype="float16"))
            lv283_1 = R.call_tir(cls.fused_dequantize4_fused_NT_matmul8_add9_cast4_add7, (gpt_neox_layers_13_mlp_dense_4h_to_h_q_weight1, gpt_neox_layers_13_mlp_dense_4h_to_h_q_scale1, lv283, gpt_neox_layers_13_mlp_dense_4h_to_h_bias1, lv282_1), out_sinfo=R.Tensor((1, seq_len, 2560), dtype="float16"))
            layer_norm28 = R.call_tir(cls.layer_norm1, (lv283_1, gpt_neox_layers_14_input_layernorm_weight1, gpt_neox_layers_14_input_layernorm_bias1), out_sinfo=R.Tensor((1, seq_len, 2560), dtype="float16"))
            lv284 = R.call_tir(cls.fused_dequantize1_fused_NT_matmul5_add5, (gpt_neox_layers_14_attention_query_key_value_q_weight1, gpt_neox_layers_14_attention_query_key_value_q_scale1, layer_norm28, gpt_neox_layers_14_attention_query_key_value_bias1), out_sinfo=R.Tensor((1, seq_len, 7680), dtype="float16"))
            reshape56 = R.call_tir(cls.reshape4, (lv284,), out_sinfo=R.Tensor((1, seq_len, 96, 80), dtype="float16"))
            reshape57 = R.call_tir(cls.reshape5, (reshape56,), out_sinfo=R.Tensor((seq_len, 96, 80), dtype="float16"))
            lv72 = R.call_dps_packed("vm.builtin.attention_kv_cache_attention_with_fused_qkv", (paged_kv_cache, R.prim_value(14), R.prim_value(T.float32(1.0)), reshape57), out_sinfo=R.Tensor((seq_len, 32, 80), dtype="float16"))
            reshape58 = R.call_tir(cls.reshape6, (lv72,), out_sinfo=R.Tensor((1, seq_len, 32, 80), dtype="float16"))
            reshape59 = R.call_tir(cls.reshape7, (reshape58,), out_sinfo=R.Tensor((1, seq_len, 2560), dtype="float16"))
            lv284_1 = R.call_tir(cls.fused_dequantize2_fused_NT_matmul6_add6_add7, (gpt_neox_layers_14_attention_dense_q_weight1, gpt_neox_layers_14_attention_dense_q_scale1, reshape59, gpt_neox_layers_14_attention_dense_bias1, lv283_1), out_sinfo=R.Tensor((1, seq_len, 2560), dtype="float16"))
            layer_norm29 = R.call_tir(cls.layer_norm1, (lv284_1, gpt_neox_layers_14_post_attention_layernorm_weight1, gpt_neox_layers_14_post_attention_layernorm_bias1), out_sinfo=R.Tensor((1, seq_len, 2560), dtype="float16"))
            lv285 = R.call_tir(cls.fused_dequantize3_fused_NT_matmul7_add8_gelu1_cast3, (gpt_neox_layers_14_mlp_dense_h_to_4h_q_weight1, gpt_neox_layers_14_mlp_dense_h_to_4h_q_scale1, layer_norm29, gpt_neox_layers_14_mlp_dense_h_to_4h_bias1), out_sinfo=R.Tensor((1, seq_len, 10240), dtype="float16"))
            lv285_1 = R.call_tir(cls.fused_dequantize4_fused_NT_matmul8_add9_cast4_add7, (gpt_neox_layers_14_mlp_dense_4h_to_h_q_weight1, gpt_neox_layers_14_mlp_dense_4h_to_h_q_scale1, lv285, gpt_neox_layers_14_mlp_dense_4h_to_h_bias1, lv284_1), out_sinfo=R.Tensor((1, seq_len, 2560), dtype="float16"))
            layer_norm30 = R.call_tir(cls.layer_norm1, (lv285_1, gpt_neox_layers_15_input_layernorm_weight1, gpt_neox_layers_15_input_layernorm_bias1), out_sinfo=R.Tensor((1, seq_len, 2560), dtype="float16"))
            lv286 = R.call_tir(cls.fused_dequantize1_fused_NT_matmul5_add5, (gpt_neox_layers_15_attention_query_key_value_q_weight1, gpt_neox_layers_15_attention_query_key_value_q_scale1, layer_norm30, gpt_neox_layers_15_attention_query_key_value_bias1), out_sinfo=R.Tensor((1, seq_len, 7680), dtype="float16"))
            reshape60 = R.call_tir(cls.reshape4, (lv286,), out_sinfo=R.Tensor((1, seq_len, 96, 80), dtype="float16"))
            reshape61 = R.call_tir(cls.reshape5, (reshape60,), out_sinfo=R.Tensor((seq_len, 96, 80), dtype="float16"))
            lv77 = R.call_dps_packed("vm.builtin.attention_kv_cache_attention_with_fused_qkv", (paged_kv_cache, R.prim_value(15), R.prim_value(T.float32(1.0)), reshape61), out_sinfo=R.Tensor((seq_len, 32, 80), dtype="float16"))
            reshape62 = R.call_tir(cls.reshape6, (lv77,), out_sinfo=R.Tensor((1, seq_len, 32, 80), dtype="float16"))
            reshape63 = R.call_tir(cls.reshape7, (reshape62,), out_sinfo=R.Tensor((1, seq_len, 2560), dtype="float16"))
            lv286_1 = R.call_tir(cls.fused_dequantize2_fused_NT_matmul6_add6_add7, (gpt_neox_layers_15_attention_dense_q_weight1, gpt_neox_layers_15_attention_dense_q_scale1, reshape63, gpt_neox_layers_15_attention_dense_bias1, lv285_1), out_sinfo=R.Tensor((1, seq_len, 2560), dtype="float16"))
            layer_norm31 = R.call_tir(cls.layer_norm1, (lv286_1, gpt_neox_layers_15_post_attention_layernorm_weight1, gpt_neox_layers_15_post_attention_layernorm_bias1), out_sinfo=R.Tensor((1, seq_len, 2560), dtype="float16"))
            lv287 = R.call_tir(cls.fused_dequantize3_fused_NT_matmul7_add8_gelu1_cast3, (gpt_neox_layers_15_mlp_dense_h_to_4h_q_weight1, gpt_neox_layers_15_mlp_dense_h_to_4h_q_scale1, layer_norm31, gpt_neox_layers_15_mlp_dense_h_to_4h_bias1), out_sinfo=R.Tensor((1, seq_len, 10240), dtype="float16"))
            lv287_1 = R.call_tir(cls.fused_dequantize4_fused_NT_matmul8_add9_cast4_add7, (gpt_neox_layers_15_mlp_dense_4h_to_h_q_weight1, gpt_neox_layers_15_mlp_dense_4h_to_h_q_scale1, lv287, gpt_neox_layers_15_mlp_dense_4h_to_h_bias1, lv286_1), out_sinfo=R.Tensor((1, seq_len, 2560), dtype="float16"))
            layer_norm32 = R.call_tir(cls.layer_norm1, (lv287_1, gpt_neox_layers_16_input_layernorm_weight1, gpt_neox_layers_16_input_layernorm_bias1), out_sinfo=R.Tensor((1, seq_len, 2560), dtype="float16"))
            lv288 = R.call_tir(cls.fused_dequantize1_fused_NT_matmul5_add5, (gpt_neox_layers_16_attention_query_key_value_q_weight1, gpt_neox_layers_16_attention_query_key_value_q_scale1, layer_norm32, gpt_neox_layers_16_attention_query_key_value_bias1), out_sinfo=R.Tensor((1, seq_len, 7680), dtype="float16"))
            reshape64 = R.call_tir(cls.reshape4, (lv288,), out_sinfo=R.Tensor((1, seq_len, 96, 80), dtype="float16"))
            reshape65 = R.call_tir(cls.reshape5, (reshape64,), out_sinfo=R.Tensor((seq_len, 96, 80), dtype="float16"))
            lv82 = R.call_dps_packed("vm.builtin.attention_kv_cache_attention_with_fused_qkv", (paged_kv_cache, R.prim_value(16), R.prim_value(T.float32(1.0)), reshape65), out_sinfo=R.Tensor((seq_len, 32, 80), dtype="float16"))
            reshape66 = R.call_tir(cls.reshape6, (lv82,), out_sinfo=R.Tensor((1, seq_len, 32, 80), dtype="float16"))
            reshape67 = R.call_tir(cls.reshape7, (reshape66,), out_sinfo=R.Tensor((1, seq_len, 2560), dtype="float16"))
            lv288_1 = R.call_tir(cls.fused_dequantize2_fused_NT_matmul6_add6_add7, (gpt_neox_layers_16_attention_dense_q_weight1, gpt_neox_layers_16_attention_dense_q_scale1, reshape67, gpt_neox_layers_16_attention_dense_bias1, lv287_1), out_sinfo=R.Tensor((1, seq_len, 2560), dtype="float16"))
            layer_norm33 = R.call_tir(cls.layer_norm1, (lv288_1, gpt_neox_layers_16_post_attention_layernorm_weight1, gpt_neox_layers_16_post_attention_layernorm_bias1), out_sinfo=R.Tensor((1, seq_len, 2560), dtype="float16"))
            lv289 = R.call_tir(cls.fused_dequantize3_fused_NT_matmul7_add8_gelu1_cast3, (gpt_neox_layers_16_mlp_dense_h_to_4h_q_weight1, gpt_neox_layers_16_mlp_dense_h_to_4h_q_scale1, layer_norm33, gpt_neox_layers_16_mlp_dense_h_to_4h_bias1), out_sinfo=R.Tensor((1, seq_len, 10240), dtype="float16"))
            lv289_1 = R.call_tir(cls.fused_dequantize4_fused_NT_matmul8_add9_cast4_add7, (gpt_neox_layers_16_mlp_dense_4h_to_h_q_weight1, gpt_neox_layers_16_mlp_dense_4h_to_h_q_scale1, lv289, gpt_neox_layers_16_mlp_dense_4h_to_h_bias1, lv288_1), out_sinfo=R.Tensor((1, seq_len, 2560), dtype="float16"))
            layer_norm34 = R.call_tir(cls.layer_norm1, (lv289_1, gpt_neox_layers_17_input_layernorm_weight1, gpt_neox_layers_17_input_layernorm_bias1), out_sinfo=R.Tensor((1, seq_len, 2560), dtype="float16"))
            lv290 = R.call_tir(cls.fused_dequantize1_fused_NT_matmul5_add5, (gpt_neox_layers_17_attention_query_key_value_q_weight1, gpt_neox_layers_17_attention_query_key_value_q_scale1, layer_norm34, gpt_neox_layers_17_attention_query_key_value_bias1), out_sinfo=R.Tensor((1, seq_len, 7680), dtype="float16"))
            reshape68 = R.call_tir(cls.reshape4, (lv290,), out_sinfo=R.Tensor((1, seq_len, 96, 80), dtype="float16"))
            reshape69 = R.call_tir(cls.reshape5, (reshape68,), out_sinfo=R.Tensor((seq_len, 96, 80), dtype="float16"))
            lv87 = R.call_dps_packed("vm.builtin.attention_kv_cache_attention_with_fused_qkv", (paged_kv_cache, R.prim_value(17), R.prim_value(T.float32(1.0)), reshape69), out_sinfo=R.Tensor((seq_len, 32, 80), dtype="float16"))
            reshape70 = R.call_tir(cls.reshape6, (lv87,), out_sinfo=R.Tensor((1, seq_len, 32, 80), dtype="float16"))
            reshape71 = R.call_tir(cls.reshape7, (reshape70,), out_sinfo=R.Tensor((1, seq_len, 2560), dtype="float16"))
            lv290_1 = R.call_tir(cls.fused_dequantize2_fused_NT_matmul6_add6_add7, (gpt_neox_layers_17_attention_dense_q_weight1, gpt_neox_layers_17_attention_dense_q_scale1, reshape71, gpt_neox_layers_17_attention_dense_bias1, lv289_1), out_sinfo=R.Tensor((1, seq_len, 2560), dtype="float16"))
            layer_norm35 = R.call_tir(cls.layer_norm1, (lv290_1, gpt_neox_layers_17_post_attention_layernorm_weight1, gpt_neox_layers_17_post_attention_layernorm_bias1), out_sinfo=R.Tensor((1, seq_len, 2560), dtype="float16"))
            lv291 = R.call_tir(cls.fused_dequantize3_fused_NT_matmul7_add8_gelu1_cast3, (gpt_neox_layers_17_mlp_dense_h_to_4h_q_weight1, gpt_neox_layers_17_mlp_dense_h_to_4h_q_scale1, layer_norm35, gpt_neox_layers_17_mlp_dense_h_to_4h_bias1), out_sinfo=R.Tensor((1, seq_len, 10240), dtype="float16"))
            lv291_1 = R.call_tir(cls.fused_dequantize4_fused_NT_matmul8_add9_cast4_add7, (gpt_neox_layers_17_mlp_dense_4h_to_h_q_weight1, gpt_neox_layers_17_mlp_dense_4h_to_h_q_scale1, lv291, gpt_neox_layers_17_mlp_dense_4h_to_h_bias1, lv290_1), out_sinfo=R.Tensor((1, seq_len, 2560), dtype="float16"))
            layer_norm36 = R.call_tir(cls.layer_norm1, (lv291_1, gpt_neox_layers_18_input_layernorm_weight1, gpt_neox_layers_18_input_layernorm_bias1), out_sinfo=R.Tensor((1, seq_len, 2560), dtype="float16"))
            lv292 = R.call_tir(cls.fused_dequantize1_fused_NT_matmul5_add5, (gpt_neox_layers_18_attention_query_key_value_q_weight1, gpt_neox_layers_18_attention_query_key_value_q_scale1, layer_norm36, gpt_neox_layers_18_attention_query_key_value_bias1), out_sinfo=R.Tensor((1, seq_len, 7680), dtype="float16"))
            reshape72 = R.call_tir(cls.reshape4, (lv292,), out_sinfo=R.Tensor((1, seq_len, 96, 80), dtype="float16"))
            reshape73 = R.call_tir(cls.reshape5, (reshape72,), out_sinfo=R.Tensor((seq_len, 96, 80), dtype="float16"))
            lv92 = R.call_dps_packed("vm.builtin.attention_kv_cache_attention_with_fused_qkv", (paged_kv_cache, R.prim_value(18), R.prim_value(T.float32(1.0)), reshape73), out_sinfo=R.Tensor((seq_len, 32, 80), dtype="float16"))
            reshape74 = R.call_tir(cls.reshape6, (lv92,), out_sinfo=R.Tensor((1, seq_len, 32, 80), dtype="float16"))
            reshape75 = R.call_tir(cls.reshape7, (reshape74,), out_sinfo=R.Tensor((1, seq_len, 2560), dtype="float16"))
            lv292_1 = R.call_tir(cls.fused_dequantize2_fused_NT_matmul6_add6_add7, (gpt_neox_layers_18_attention_dense_q_weight1, gpt_neox_layers_18_attention_dense_q_scale1, reshape75, gpt_neox_layers_18_attention_dense_bias1, lv291_1), out_sinfo=R.Tensor((1, seq_len, 2560), dtype="float16"))
            layer_norm37 = R.call_tir(cls.layer_norm1, (lv292_1, gpt_neox_layers_18_post_attention_layernorm_weight1, gpt_neox_layers_18_post_attention_layernorm_bias1), out_sinfo=R.Tensor((1, seq_len, 2560), dtype="float16"))
            lv293 = R.call_tir(cls.fused_dequantize3_fused_NT_matmul7_add8_gelu1_cast3, (gpt_neox_layers_18_mlp_dense_h_to_4h_q_weight1, gpt_neox_layers_18_mlp_dense_h_to_4h_q_scale1, layer_norm37, gpt_neox_layers_18_mlp_dense_h_to_4h_bias1), out_sinfo=R.Tensor((1, seq_len, 10240), dtype="float16"))
            lv293_1 = R.call_tir(cls.fused_dequantize4_fused_NT_matmul8_add9_cast4_add7, (gpt_neox_layers_18_mlp_dense_4h_to_h_q_weight1, gpt_neox_layers_18_mlp_dense_4h_to_h_q_scale1, lv293, gpt_neox_layers_18_mlp_dense_4h_to_h_bias1, lv292_1), out_sinfo=R.Tensor((1, seq_len, 2560), dtype="float16"))
            layer_norm38 = R.call_tir(cls.layer_norm1, (lv293_1, gpt_neox_layers_19_input_layernorm_weight1, gpt_neox_layers_19_input_layernorm_bias1), out_sinfo=R.Tensor((1, seq_len, 2560), dtype="float16"))
            lv294 = R.call_tir(cls.fused_dequantize1_fused_NT_matmul5_add5, (gpt_neox_layers_19_attention_query_key_value_q_weight1, gpt_neox_layers_19_attention_query_key_value_q_scale1, layer_norm38, gpt_neox_layers_19_attention_query_key_value_bias1), out_sinfo=R.Tensor((1, seq_len, 7680), dtype="float16"))
            reshape76 = R.call_tir(cls.reshape4, (lv294,), out_sinfo=R.Tensor((1, seq_len, 96, 80), dtype="float16"))
            reshape77 = R.call_tir(cls.reshape5, (reshape76,), out_sinfo=R.Tensor((seq_len, 96, 80), dtype="float16"))
            lv97 = R.call_dps_packed("vm.builtin.attention_kv_cache_attention_with_fused_qkv", (paged_kv_cache, R.prim_value(19), R.prim_value(T.float32(1.0)), reshape77), out_sinfo=R.Tensor((seq_len, 32, 80), dtype="float16"))
            reshape78 = R.call_tir(cls.reshape6, (lv97,), out_sinfo=R.Tensor((1, seq_len, 32, 80), dtype="float16"))
            reshape79 = R.call_tir(cls.reshape7, (reshape78,), out_sinfo=R.Tensor((1, seq_len, 2560), dtype="float16"))
            lv294_1 = R.call_tir(cls.fused_dequantize2_fused_NT_matmul6_add6_add7, (gpt_neox_layers_19_attention_dense_q_weight1, gpt_neox_layers_19_attention_dense_q_scale1, reshape79, gpt_neox_layers_19_attention_dense_bias1, lv293_1), out_sinfo=R.Tensor((1, seq_len, 2560), dtype="float16"))
            layer_norm39 = R.call_tir(cls.layer_norm1, (lv294_1, gpt_neox_layers_19_post_attention_layernorm_weight1, gpt_neox_layers_19_post_attention_layernorm_bias1), out_sinfo=R.Tensor((1, seq_len, 2560), dtype="float16"))
            lv295 = R.call_tir(cls.fused_dequantize3_fused_NT_matmul7_add8_gelu1_cast3, (gpt_neox_layers_19_mlp_dense_h_to_4h_q_weight1, gpt_neox_layers_19_mlp_dense_h_to_4h_q_scale1, layer_norm39, gpt_neox_layers_19_mlp_dense_h_to_4h_bias1), out_sinfo=R.Tensor((1, seq_len, 10240), dtype="float16"))
            lv295_1 = R.call_tir(cls.fused_dequantize4_fused_NT_matmul8_add9_cast4_add7, (gpt_neox_layers_19_mlp_dense_4h_to_h_q_weight1, gpt_neox_layers_19_mlp_dense_4h_to_h_q_scale1, lv295, gpt_neox_layers_19_mlp_dense_4h_to_h_bias1, lv294_1), out_sinfo=R.Tensor((1, seq_len, 2560), dtype="float16"))
            layer_norm40 = R.call_tir(cls.layer_norm1, (lv295_1, gpt_neox_layers_20_input_layernorm_weight1, gpt_neox_layers_20_input_layernorm_bias1), out_sinfo=R.Tensor((1, seq_len, 2560), dtype="float16"))
            lv296 = R.call_tir(cls.fused_dequantize1_fused_NT_matmul5_add5, (gpt_neox_layers_20_attention_query_key_value_q_weight1, gpt_neox_layers_20_attention_query_key_value_q_scale1, layer_norm40, gpt_neox_layers_20_attention_query_key_value_bias1), out_sinfo=R.Tensor((1, seq_len, 7680), dtype="float16"))
            reshape80 = R.call_tir(cls.reshape4, (lv296,), out_sinfo=R.Tensor((1, seq_len, 96, 80), dtype="float16"))
            reshape81 = R.call_tir(cls.reshape5, (reshape80,), out_sinfo=R.Tensor((seq_len, 96, 80), dtype="float16"))
            lv102 = R.call_dps_packed("vm.builtin.attention_kv_cache_attention_with_fused_qkv", (paged_kv_cache, R.prim_value(20), R.prim_value(T.float32(1.0)), reshape81), out_sinfo=R.Tensor((seq_len, 32, 80), dtype="float16"))
            reshape82 = R.call_tir(cls.reshape6, (lv102,), out_sinfo=R.Tensor((1, seq_len, 32, 80), dtype="float16"))
            reshape83 = R.call_tir(cls.reshape7, (reshape82,), out_sinfo=R.Tensor((1, seq_len, 2560), dtype="float16"))
            lv296_1 = R.call_tir(cls.fused_dequantize2_fused_NT_matmul6_add6_add7, (gpt_neox_layers_20_attention_dense_q_weight1, gpt_neox_layers_20_attention_dense_q_scale1, reshape83, gpt_neox_layers_20_attention_dense_bias1, lv295_1), out_sinfo=R.Tensor((1, seq_len, 2560), dtype="float16"))
            layer_norm41 = R.call_tir(cls.layer_norm1, (lv296_1, gpt_neox_layers_20_post_attention_layernorm_weight1, gpt_neox_layers_20_post_attention_layernorm_bias1), out_sinfo=R.Tensor((1, seq_len, 2560), dtype="float16"))
            lv297 = R.call_tir(cls.fused_dequantize3_fused_NT_matmul7_add8_gelu1_cast3, (gpt_neox_layers_20_mlp_dense_h_to_4h_q_weight1, gpt_neox_layers_20_mlp_dense_h_to_4h_q_scale1, layer_norm41, gpt_neox_layers_20_mlp_dense_h_to_4h_bias1), out_sinfo=R.Tensor((1, seq_len, 10240), dtype="float16"))
            lv297_1 = R.call_tir(cls.fused_dequantize4_fused_NT_matmul8_add9_cast4_add7, (gpt_neox_layers_20_mlp_dense_4h_to_h_q_weight1, gpt_neox_layers_20_mlp_dense_4h_to_h_q_scale1, lv297, gpt_neox_layers_20_mlp_dense_4h_to_h_bias1, lv296_1), out_sinfo=R.Tensor((1, seq_len, 2560), dtype="float16"))
            layer_norm42 = R.call_tir(cls.layer_norm1, (lv297_1, gpt_neox_layers_21_input_layernorm_weight1, gpt_neox_layers_21_input_layernorm_bias1), out_sinfo=R.Tensor((1, seq_len, 2560), dtype="float16"))
            lv298 = R.call_tir(cls.fused_dequantize1_fused_NT_matmul5_add5, (gpt_neox_layers_21_attention_query_key_value_q_weight1, gpt_neox_layers_21_attention_query_key_value_q_scale1, layer_norm42, gpt_neox_layers_21_attention_query_key_value_bias1), out_sinfo=R.Tensor((1, seq_len, 7680), dtype="float16"))
            reshape84 = R.call_tir(cls.reshape4, (lv298,), out_sinfo=R.Tensor((1, seq_len, 96, 80), dtype="float16"))
            reshape85 = R.call_tir(cls.reshape5, (reshape84,), out_sinfo=R.Tensor((seq_len, 96, 80), dtype="float16"))
            lv107 = R.call_dps_packed("vm.builtin.attention_kv_cache_attention_with_fused_qkv", (paged_kv_cache, R.prim_value(21), R.prim_value(T.float32(1.0)), reshape85), out_sinfo=R.Tensor((seq_len, 32, 80), dtype="float16"))
            reshape86 = R.call_tir(cls.reshape6, (lv107,), out_sinfo=R.Tensor((1, seq_len, 32, 80), dtype="float16"))
            reshape87 = R.call_tir(cls.reshape7, (reshape86,), out_sinfo=R.Tensor((1, seq_len, 2560), dtype="float16"))
            lv298_1 = R.call_tir(cls.fused_dequantize2_fused_NT_matmul6_add6_add7, (gpt_neox_layers_21_attention_dense_q_weight1, gpt_neox_layers_21_attention_dense_q_scale1, reshape87, gpt_neox_layers_21_attention_dense_bias1, lv297_1), out_sinfo=R.Tensor((1, seq_len, 2560), dtype="float16"))
            layer_norm43 = R.call_tir(cls.layer_norm1, (lv298_1, gpt_neox_layers_21_post_attention_layernorm_weight1, gpt_neox_layers_21_post_attention_layernorm_bias1), out_sinfo=R.Tensor((1, seq_len, 2560), dtype="float16"))
            lv299 = R.call_tir(cls.fused_dequantize3_fused_NT_matmul7_add8_gelu1_cast3, (gpt_neox_layers_21_mlp_dense_h_to_4h_q_weight1, gpt_neox_layers_21_mlp_dense_h_to_4h_q_scale1, layer_norm43, gpt_neox_layers_21_mlp_dense_h_to_4h_bias1), out_sinfo=R.Tensor((1, seq_len, 10240), dtype="float16"))
            lv299_1 = R.call_tir(cls.fused_dequantize4_fused_NT_matmul8_add9_cast4_add7, (gpt_neox_layers_21_mlp_dense_4h_to_h_q_weight1, gpt_neox_layers_21_mlp_dense_4h_to_h_q_scale1, lv299, gpt_neox_layers_21_mlp_dense_4h_to_h_bias1, lv298_1), out_sinfo=R.Tensor((1, seq_len, 2560), dtype="float16"))
            layer_norm44 = R.call_tir(cls.layer_norm1, (lv299_1, gpt_neox_layers_22_input_layernorm_weight1, gpt_neox_layers_22_input_layernorm_bias1), out_sinfo=R.Tensor((1, seq_len, 2560), dtype="float16"))
            lv300 = R.call_tir(cls.fused_dequantize1_fused_NT_matmul5_add5, (gpt_neox_layers_22_attention_query_key_value_q_weight1, gpt_neox_layers_22_attention_query_key_value_q_scale1, layer_norm44, gpt_neox_layers_22_attention_query_key_value_bias1), out_sinfo=R.Tensor((1, seq_len, 7680), dtype="float16"))
            reshape88 = R.call_tir(cls.reshape4, (lv300,), out_sinfo=R.Tensor((1, seq_len, 96, 80), dtype="float16"))
            reshape89 = R.call_tir(cls.reshape5, (reshape88,), out_sinfo=R.Tensor((seq_len, 96, 80), dtype="float16"))
            lv112 = R.call_dps_packed("vm.builtin.attention_kv_cache_attention_with_fused_qkv", (paged_kv_cache, R.prim_value(22), R.prim_value(T.float32(1.0)), reshape89), out_sinfo=R.Tensor((seq_len, 32, 80), dtype="float16"))
            reshape90 = R.call_tir(cls.reshape6, (lv112,), out_sinfo=R.Tensor((1, seq_len, 32, 80), dtype="float16"))
            reshape91 = R.call_tir(cls.reshape7, (reshape90,), out_sinfo=R.Tensor((1, seq_len, 2560), dtype="float16"))
            lv300_1 = R.call_tir(cls.fused_dequantize2_fused_NT_matmul6_add6_add7, (gpt_neox_layers_22_attention_dense_q_weight1, gpt_neox_layers_22_attention_dense_q_scale1, reshape91, gpt_neox_layers_22_attention_dense_bias1, lv299_1), out_sinfo=R.Tensor((1, seq_len, 2560), dtype="float16"))
            layer_norm45 = R.call_tir(cls.layer_norm1, (lv300_1, gpt_neox_layers_22_post_attention_layernorm_weight1, gpt_neox_layers_22_post_attention_layernorm_bias1), out_sinfo=R.Tensor((1, seq_len, 2560), dtype="float16"))
            lv301 = R.call_tir(cls.fused_dequantize3_fused_NT_matmul7_add8_gelu1_cast3, (gpt_neox_layers_22_mlp_dense_h_to_4h_q_weight1, gpt_neox_layers_22_mlp_dense_h_to_4h_q_scale1, layer_norm45, gpt_neox_layers_22_mlp_dense_h_to_4h_bias1), out_sinfo=R.Tensor((1, seq_len, 10240), dtype="float16"))
            lv301_1 = R.call_tir(cls.fused_dequantize4_fused_NT_matmul8_add9_cast4_add7, (gpt_neox_layers_22_mlp_dense_4h_to_h_q_weight1, gpt_neox_layers_22_mlp_dense_4h_to_h_q_scale1, lv301, gpt_neox_layers_22_mlp_dense_4h_to_h_bias1, lv300_1), out_sinfo=R.Tensor((1, seq_len, 2560), dtype="float16"))
            layer_norm46 = R.call_tir(cls.layer_norm1, (lv301_1, gpt_neox_layers_23_input_layernorm_weight1, gpt_neox_layers_23_input_layernorm_bias1), out_sinfo=R.Tensor((1, seq_len, 2560), dtype="float16"))
            lv302 = R.call_tir(cls.fused_dequantize1_fused_NT_matmul5_add5, (gpt_neox_layers_23_attention_query_key_value_q_weight1, gpt_neox_layers_23_attention_query_key_value_q_scale1, layer_norm46, gpt_neox_layers_23_attention_query_key_value_bias1), out_sinfo=R.Tensor((1, seq_len, 7680), dtype="float16"))
            reshape92 = R.call_tir(cls.reshape4, (lv302,), out_sinfo=R.Tensor((1, seq_len, 96, 80), dtype="float16"))
            reshape93 = R.call_tir(cls.reshape5, (reshape92,), out_sinfo=R.Tensor((seq_len, 96, 80), dtype="float16"))
            lv117 = R.call_dps_packed("vm.builtin.attention_kv_cache_attention_with_fused_qkv", (paged_kv_cache, R.prim_value(23), R.prim_value(T.float32(1.0)), reshape93), out_sinfo=R.Tensor((seq_len, 32, 80), dtype="float16"))
            reshape94 = R.call_tir(cls.reshape6, (lv117,), out_sinfo=R.Tensor((1, seq_len, 32, 80), dtype="float16"))
            reshape95 = R.call_tir(cls.reshape7, (reshape94,), out_sinfo=R.Tensor((1, seq_len, 2560), dtype="float16"))
            lv302_1 = R.call_tir(cls.fused_dequantize2_fused_NT_matmul6_add6_add7, (gpt_neox_layers_23_attention_dense_q_weight1, gpt_neox_layers_23_attention_dense_q_scale1, reshape95, gpt_neox_layers_23_attention_dense_bias1, lv301_1), out_sinfo=R.Tensor((1, seq_len, 2560), dtype="float16"))
            layer_norm47 = R.call_tir(cls.layer_norm1, (lv302_1, gpt_neox_layers_23_post_attention_layernorm_weight1, gpt_neox_layers_23_post_attention_layernorm_bias1), out_sinfo=R.Tensor((1, seq_len, 2560), dtype="float16"))
            lv303 = R.call_tir(cls.fused_dequantize3_fused_NT_matmul7_add8_gelu1_cast3, (gpt_neox_layers_23_mlp_dense_h_to_4h_q_weight1, gpt_neox_layers_23_mlp_dense_h_to_4h_q_scale1, layer_norm47, gpt_neox_layers_23_mlp_dense_h_to_4h_bias1), out_sinfo=R.Tensor((1, seq_len, 10240), dtype="float16"))
            lv303_1 = R.call_tir(cls.fused_dequantize4_fused_NT_matmul8_add9_cast4_add7, (gpt_neox_layers_23_mlp_dense_4h_to_h_q_weight1, gpt_neox_layers_23_mlp_dense_4h_to_h_q_scale1, lv303, gpt_neox_layers_23_mlp_dense_4h_to_h_bias1, lv302_1), out_sinfo=R.Tensor((1, seq_len, 2560), dtype="float16"))
            layer_norm48 = R.call_tir(cls.layer_norm1, (lv303_1, gpt_neox_layers_24_input_layernorm_weight1, gpt_neox_layers_24_input_layernorm_bias1), out_sinfo=R.Tensor((1, seq_len, 2560), dtype="float16"))
            lv304 = R.call_tir(cls.fused_dequantize1_fused_NT_matmul5_add5, (gpt_neox_layers_24_attention_query_key_value_q_weight1, gpt_neox_layers_24_attention_query_key_value_q_scale1, layer_norm48, gpt_neox_layers_24_attention_query_key_value_bias1), out_sinfo=R.Tensor((1, seq_len, 7680), dtype="float16"))
            reshape96 = R.call_tir(cls.reshape4, (lv304,), out_sinfo=R.Tensor((1, seq_len, 96, 80), dtype="float16"))
            reshape97 = R.call_tir(cls.reshape5, (reshape96,), out_sinfo=R.Tensor((seq_len, 96, 80), dtype="float16"))
            lv122 = R.call_dps_packed("vm.builtin.attention_kv_cache_attention_with_fused_qkv", (paged_kv_cache, R.prim_value(24), R.prim_value(T.float32(1.0)), reshape97), out_sinfo=R.Tensor((seq_len, 32, 80), dtype="float16"))
            reshape98 = R.call_tir(cls.reshape6, (lv122,), out_sinfo=R.Tensor((1, seq_len, 32, 80), dtype="float16"))
            reshape99 = R.call_tir(cls.reshape7, (reshape98,), out_sinfo=R.Tensor((1, seq_len, 2560), dtype="float16"))
            lv304_1 = R.call_tir(cls.fused_dequantize2_fused_NT_matmul6_add6_add7, (gpt_neox_layers_24_attention_dense_q_weight1, gpt_neox_layers_24_attention_dense_q_scale1, reshape99, gpt_neox_layers_24_attention_dense_bias1, lv303_1), out_sinfo=R.Tensor((1, seq_len, 2560), dtype="float16"))
            layer_norm49 = R.call_tir(cls.layer_norm1, (lv304_1, gpt_neox_layers_24_post_attention_layernorm_weight1, gpt_neox_layers_24_post_attention_layernorm_bias1), out_sinfo=R.Tensor((1, seq_len, 2560), dtype="float16"))
            lv305 = R.call_tir(cls.fused_dequantize3_fused_NT_matmul7_add8_gelu1_cast3, (gpt_neox_layers_24_mlp_dense_h_to_4h_q_weight1, gpt_neox_layers_24_mlp_dense_h_to_4h_q_scale1, layer_norm49, gpt_neox_layers_24_mlp_dense_h_to_4h_bias1), out_sinfo=R.Tensor((1, seq_len, 10240), dtype="float16"))
            lv305_1 = R.call_tir(cls.fused_dequantize4_fused_NT_matmul8_add9_cast4_add7, (gpt_neox_layers_24_mlp_dense_4h_to_h_q_weight1, gpt_neox_layers_24_mlp_dense_4h_to_h_q_scale1, lv305, gpt_neox_layers_24_mlp_dense_4h_to_h_bias1, lv304_1), out_sinfo=R.Tensor((1, seq_len, 2560), dtype="float16"))
            layer_norm50 = R.call_tir(cls.layer_norm1, (lv305_1, gpt_neox_layers_25_input_layernorm_weight1, gpt_neox_layers_25_input_layernorm_bias1), out_sinfo=R.Tensor((1, seq_len, 2560), dtype="float16"))
            lv306 = R.call_tir(cls.fused_dequantize1_fused_NT_matmul5_add5, (gpt_neox_layers_25_attention_query_key_value_q_weight1, gpt_neox_layers_25_attention_query_key_value_q_scale1, layer_norm50, gpt_neox_layers_25_attention_query_key_value_bias1), out_sinfo=R.Tensor((1, seq_len, 7680), dtype="float16"))
            reshape100 = R.call_tir(cls.reshape4, (lv306,), out_sinfo=R.Tensor((1, seq_len, 96, 80), dtype="float16"))
            reshape101 = R.call_tir(cls.reshape5, (reshape100,), out_sinfo=R.Tensor((seq_len, 96, 80), dtype="float16"))
            lv127 = R.call_dps_packed("vm.builtin.attention_kv_cache_attention_with_fused_qkv", (paged_kv_cache, R.prim_value(25), R.prim_value(T.float32(1.0)), reshape101), out_sinfo=R.Tensor((seq_len, 32, 80), dtype="float16"))
            reshape102 = R.call_tir(cls.reshape6, (lv127,), out_sinfo=R.Tensor((1, seq_len, 32, 80), dtype="float16"))
            reshape103 = R.call_tir(cls.reshape7, (reshape102,), out_sinfo=R.Tensor((1, seq_len, 2560), dtype="float16"))
            lv306_1 = R.call_tir(cls.fused_dequantize2_fused_NT_matmul6_add6_add7, (gpt_neox_layers_25_attention_dense_q_weight1, gpt_neox_layers_25_attention_dense_q_scale1, reshape103, gpt_neox_layers_25_attention_dense_bias1, lv305_1), out_sinfo=R.Tensor((1, seq_len, 2560), dtype="float16"))
            layer_norm51 = R.call_tir(cls.layer_norm1, (lv306_1, gpt_neox_layers_25_post_attention_layernorm_weight1, gpt_neox_layers_25_post_attention_layernorm_bias1), out_sinfo=R.Tensor((1, seq_len, 2560), dtype="float16"))
            lv307 = R.call_tir(cls.fused_dequantize3_fused_NT_matmul7_add8_gelu1_cast3, (gpt_neox_layers_25_mlp_dense_h_to_4h_q_weight1, gpt_neox_layers_25_mlp_dense_h_to_4h_q_scale1, layer_norm51, gpt_neox_layers_25_mlp_dense_h_to_4h_bias1), out_sinfo=R.Tensor((1, seq_len, 10240), dtype="float16"))
            lv307_1 = R.call_tir(cls.fused_dequantize4_fused_NT_matmul8_add9_cast4_add7, (gpt_neox_layers_25_mlp_dense_4h_to_h_q_weight1, gpt_neox_layers_25_mlp_dense_4h_to_h_q_scale1, lv307, gpt_neox_layers_25_mlp_dense_4h_to_h_bias1, lv306_1), out_sinfo=R.Tensor((1, seq_len, 2560), dtype="float16"))
            layer_norm52 = R.call_tir(cls.layer_norm1, (lv307_1, gpt_neox_layers_26_input_layernorm_weight1, gpt_neox_layers_26_input_layernorm_bias1), out_sinfo=R.Tensor((1, seq_len, 2560), dtype="float16"))
            lv308 = R.call_tir(cls.fused_dequantize1_fused_NT_matmul5_add5, (gpt_neox_layers_26_attention_query_key_value_q_weight1, gpt_neox_layers_26_attention_query_key_value_q_scale1, layer_norm52, gpt_neox_layers_26_attention_query_key_value_bias1), out_sinfo=R.Tensor((1, seq_len, 7680), dtype="float16"))
            reshape104 = R.call_tir(cls.reshape4, (lv308,), out_sinfo=R.Tensor((1, seq_len, 96, 80), dtype="float16"))
            reshape105 = R.call_tir(cls.reshape5, (reshape104,), out_sinfo=R.Tensor((seq_len, 96, 80), dtype="float16"))
            lv132 = R.call_dps_packed("vm.builtin.attention_kv_cache_attention_with_fused_qkv", (paged_kv_cache, R.prim_value(26), R.prim_value(T.float32(1.0)), reshape105), out_sinfo=R.Tensor((seq_len, 32, 80), dtype="float16"))
            reshape106 = R.call_tir(cls.reshape6, (lv132,), out_sinfo=R.Tensor((1, seq_len, 32, 80), dtype="float16"))
            reshape107 = R.call_tir(cls.reshape7, (reshape106,), out_sinfo=R.Tensor((1, seq_len, 2560), dtype="float16"))
            lv308_1 = R.call_tir(cls.fused_dequantize2_fused_NT_matmul6_add6_add7, (gpt_neox_layers_26_attention_dense_q_weight1, gpt_neox_layers_26_attention_dense_q_scale1, reshape107, gpt_neox_layers_26_attention_dense_bias1, lv307_1), out_sinfo=R.Tensor((1, seq_len, 2560), dtype="float16"))
            layer_norm53 = R.call_tir(cls.layer_norm1, (lv308_1, gpt_neox_layers_26_post_attention_layernorm_weight1, gpt_neox_layers_26_post_attention_layernorm_bias1), out_sinfo=R.Tensor((1, seq_len, 2560), dtype="float16"))
            lv309 = R.call_tir(cls.fused_dequantize3_fused_NT_matmul7_add8_gelu1_cast3, (gpt_neox_layers_26_mlp_dense_h_to_4h_q_weight1, gpt_neox_layers_26_mlp_dense_h_to_4h_q_scale1, layer_norm53, gpt_neox_layers_26_mlp_dense_h_to_4h_bias1), out_sinfo=R.Tensor((1, seq_len, 10240), dtype="float16"))
            lv309_1 = R.call_tir(cls.fused_dequantize4_fused_NT_matmul8_add9_cast4_add7, (gpt_neox_layers_26_mlp_dense_4h_to_h_q_weight1, gpt_neox_layers_26_mlp_dense_4h_to_h_q_scale1, lv309, gpt_neox_layers_26_mlp_dense_4h_to_h_bias1, lv308_1), out_sinfo=R.Tensor((1, seq_len, 2560), dtype="float16"))
            layer_norm54 = R.call_tir(cls.layer_norm1, (lv309_1, gpt_neox_layers_27_input_layernorm_weight1, gpt_neox_layers_27_input_layernorm_bias1), out_sinfo=R.Tensor((1, seq_len, 2560), dtype="float16"))
            lv310 = R.call_tir(cls.fused_dequantize1_fused_NT_matmul5_add5, (gpt_neox_layers_27_attention_query_key_value_q_weight1, gpt_neox_layers_27_attention_query_key_value_q_scale1, layer_norm54, gpt_neox_layers_27_attention_query_key_value_bias1), out_sinfo=R.Tensor((1, seq_len, 7680), dtype="float16"))
            reshape108 = R.call_tir(cls.reshape4, (lv310,), out_sinfo=R.Tensor((1, seq_len, 96, 80), dtype="float16"))
            reshape109 = R.call_tir(cls.reshape5, (reshape108,), out_sinfo=R.Tensor((seq_len, 96, 80), dtype="float16"))
            lv137 = R.call_dps_packed("vm.builtin.attention_kv_cache_attention_with_fused_qkv", (paged_kv_cache, R.prim_value(27), R.prim_value(T.float32(1.0)), reshape109), out_sinfo=R.Tensor((seq_len, 32, 80), dtype="float16"))
            reshape110 = R.call_tir(cls.reshape6, (lv137,), out_sinfo=R.Tensor((1, seq_len, 32, 80), dtype="float16"))
            reshape111 = R.call_tir(cls.reshape7, (reshape110,), out_sinfo=R.Tensor((1, seq_len, 2560), dtype="float16"))
            lv310_1 = R.call_tir(cls.fused_dequantize2_fused_NT_matmul6_add6_add7, (gpt_neox_layers_27_attention_dense_q_weight1, gpt_neox_layers_27_attention_dense_q_scale1, reshape111, gpt_neox_layers_27_attention_dense_bias1, lv309_1), out_sinfo=R.Tensor((1, seq_len, 2560), dtype="float16"))
            layer_norm55 = R.call_tir(cls.layer_norm1, (lv310_1, gpt_neox_layers_27_post_attention_layernorm_weight1, gpt_neox_layers_27_post_attention_layernorm_bias1), out_sinfo=R.Tensor((1, seq_len, 2560), dtype="float16"))
            lv311 = R.call_tir(cls.fused_dequantize3_fused_NT_matmul7_add8_gelu1_cast3, (gpt_neox_layers_27_mlp_dense_h_to_4h_q_weight1, gpt_neox_layers_27_mlp_dense_h_to_4h_q_scale1, layer_norm55, gpt_neox_layers_27_mlp_dense_h_to_4h_bias1), out_sinfo=R.Tensor((1, seq_len, 10240), dtype="float16"))
            lv311_1 = R.call_tir(cls.fused_dequantize4_fused_NT_matmul8_add9_cast4_add7, (gpt_neox_layers_27_mlp_dense_4h_to_h_q_weight1, gpt_neox_layers_27_mlp_dense_4h_to_h_q_scale1, lv311, gpt_neox_layers_27_mlp_dense_4h_to_h_bias1, lv310_1), out_sinfo=R.Tensor((1, seq_len, 2560), dtype="float16"))
            layer_norm56 = R.call_tir(cls.layer_norm1, (lv311_1, gpt_neox_layers_28_input_layernorm_weight1, gpt_neox_layers_28_input_layernorm_bias1), out_sinfo=R.Tensor((1, seq_len, 2560), dtype="float16"))
            lv312 = R.call_tir(cls.fused_dequantize1_fused_NT_matmul5_add5, (gpt_neox_layers_28_attention_query_key_value_q_weight1, gpt_neox_layers_28_attention_query_key_value_q_scale1, layer_norm56, gpt_neox_layers_28_attention_query_key_value_bias1), out_sinfo=R.Tensor((1, seq_len, 7680), dtype="float16"))
            reshape112 = R.call_tir(cls.reshape4, (lv312,), out_sinfo=R.Tensor((1, seq_len, 96, 80), dtype="float16"))
            reshape113 = R.call_tir(cls.reshape5, (reshape112,), out_sinfo=R.Tensor((seq_len, 96, 80), dtype="float16"))
            lv142 = R.call_dps_packed("vm.builtin.attention_kv_cache_attention_with_fused_qkv", (paged_kv_cache, R.prim_value(28), R.prim_value(T.float32(1.0)), reshape113), out_sinfo=R.Tensor((seq_len, 32, 80), dtype="float16"))
            reshape114 = R.call_tir(cls.reshape6, (lv142,), out_sinfo=R.Tensor((1, seq_len, 32, 80), dtype="float16"))
            reshape115 = R.call_tir(cls.reshape7, (reshape114,), out_sinfo=R.Tensor((1, seq_len, 2560), dtype="float16"))
            lv312_1 = R.call_tir(cls.fused_dequantize2_fused_NT_matmul6_add6_add7, (gpt_neox_layers_28_attention_dense_q_weight1, gpt_neox_layers_28_attention_dense_q_scale1, reshape115, gpt_neox_layers_28_attention_dense_bias1, lv311_1), out_sinfo=R.Tensor((1, seq_len, 2560), dtype="float16"))
            layer_norm57 = R.call_tir(cls.layer_norm1, (lv312_1, gpt_neox_layers_28_post_attention_layernorm_weight1, gpt_neox_layers_28_post_attention_layernorm_bias1), out_sinfo=R.Tensor((1, seq_len, 2560), dtype="float16"))
            lv313 = R.call_tir(cls.fused_dequantize3_fused_NT_matmul7_add8_gelu1_cast3, (gpt_neox_layers_28_mlp_dense_h_to_4h_q_weight1, gpt_neox_layers_28_mlp_dense_h_to_4h_q_scale1, layer_norm57, gpt_neox_layers_28_mlp_dense_h_to_4h_bias1), out_sinfo=R.Tensor((1, seq_len, 10240), dtype="float16"))
            lv313_1 = R.call_tir(cls.fused_dequantize4_fused_NT_matmul8_add9_cast4_add7, (gpt_neox_layers_28_mlp_dense_4h_to_h_q_weight1, gpt_neox_layers_28_mlp_dense_4h_to_h_q_scale1, lv313, gpt_neox_layers_28_mlp_dense_4h_to_h_bias1, lv312_1), out_sinfo=R.Tensor((1, seq_len, 2560), dtype="float16"))
            layer_norm58 = R.call_tir(cls.layer_norm1, (lv313_1, gpt_neox_layers_29_input_layernorm_weight1, gpt_neox_layers_29_input_layernorm_bias1), out_sinfo=R.Tensor((1, seq_len, 2560), dtype="float16"))
            lv314 = R.call_tir(cls.fused_dequantize1_fused_NT_matmul5_add5, (gpt_neox_layers_29_attention_query_key_value_q_weight1, gpt_neox_layers_29_attention_query_key_value_q_scale1, layer_norm58, gpt_neox_layers_29_attention_query_key_value_bias1), out_sinfo=R.Tensor((1, seq_len, 7680), dtype="float16"))
            reshape116 = R.call_tir(cls.reshape4, (lv314,), out_sinfo=R.Tensor((1, seq_len, 96, 80), dtype="float16"))
            reshape117 = R.call_tir(cls.reshape5, (reshape116,), out_sinfo=R.Tensor((seq_len, 96, 80), dtype="float16"))
            lv147 = R.call_dps_packed("vm.builtin.attention_kv_cache_attention_with_fused_qkv", (paged_kv_cache, R.prim_value(29), R.prim_value(T.float32(1.0)), reshape117), out_sinfo=R.Tensor((seq_len, 32, 80), dtype="float16"))
            reshape118 = R.call_tir(cls.reshape6, (lv147,), out_sinfo=R.Tensor((1, seq_len, 32, 80), dtype="float16"))
            reshape119 = R.call_tir(cls.reshape7, (reshape118,), out_sinfo=R.Tensor((1, seq_len, 2560), dtype="float16"))
            lv314_1 = R.call_tir(cls.fused_dequantize2_fused_NT_matmul6_add6_add7, (gpt_neox_layers_29_attention_dense_q_weight1, gpt_neox_layers_29_attention_dense_q_scale1, reshape119, gpt_neox_layers_29_attention_dense_bias1, lv313_1), out_sinfo=R.Tensor((1, seq_len, 2560), dtype="float16"))
            layer_norm59 = R.call_tir(cls.layer_norm1, (lv314_1, gpt_neox_layers_29_post_attention_layernorm_weight1, gpt_neox_layers_29_post_attention_layernorm_bias1), out_sinfo=R.Tensor((1, seq_len, 2560), dtype="float16"))
            lv315 = R.call_tir(cls.fused_dequantize3_fused_NT_matmul7_add8_gelu1_cast3, (gpt_neox_layers_29_mlp_dense_h_to_4h_q_weight1, gpt_neox_layers_29_mlp_dense_h_to_4h_q_scale1, layer_norm59, gpt_neox_layers_29_mlp_dense_h_to_4h_bias1), out_sinfo=R.Tensor((1, seq_len, 10240), dtype="float16"))
            lv315_1 = R.call_tir(cls.fused_dequantize4_fused_NT_matmul8_add9_cast4_add7, (gpt_neox_layers_29_mlp_dense_4h_to_h_q_weight1, gpt_neox_layers_29_mlp_dense_4h_to_h_q_scale1, lv315, gpt_neox_layers_29_mlp_dense_4h_to_h_bias1, lv314_1), out_sinfo=R.Tensor((1, seq_len, 2560), dtype="float16"))
            layer_norm60 = R.call_tir(cls.layer_norm1, (lv315_1, gpt_neox_layers_30_input_layernorm_weight1, gpt_neox_layers_30_input_layernorm_bias1), out_sinfo=R.Tensor((1, seq_len, 2560), dtype="float16"))
            lv316 = R.call_tir(cls.fused_dequantize1_fused_NT_matmul5_add5, (gpt_neox_layers_30_attention_query_key_value_q_weight1, gpt_neox_layers_30_attention_query_key_value_q_scale1, layer_norm60, gpt_neox_layers_30_attention_query_key_value_bias1), out_sinfo=R.Tensor((1, seq_len, 7680), dtype="float16"))
            reshape120 = R.call_tir(cls.reshape4, (lv316,), out_sinfo=R.Tensor((1, seq_len, 96, 80), dtype="float16"))
            reshape121 = R.call_tir(cls.reshape5, (reshape120,), out_sinfo=R.Tensor((seq_len, 96, 80), dtype="float16"))
            lv152 = R.call_dps_packed("vm.builtin.attention_kv_cache_attention_with_fused_qkv", (paged_kv_cache, R.prim_value(30), R.prim_value(T.float32(1.0)), reshape121), out_sinfo=R.Tensor((seq_len, 32, 80), dtype="float16"))
            reshape122 = R.call_tir(cls.reshape6, (lv152,), out_sinfo=R.Tensor((1, seq_len, 32, 80), dtype="float16"))
            reshape123 = R.call_tir(cls.reshape7, (reshape122,), out_sinfo=R.Tensor((1, seq_len, 2560), dtype="float16"))
            lv316_1 = R.call_tir(cls.fused_dequantize2_fused_NT_matmul6_add6_add7, (gpt_neox_layers_30_attention_dense_q_weight1, gpt_neox_layers_30_attention_dense_q_scale1, reshape123, gpt_neox_layers_30_attention_dense_bias1, lv315_1), out_sinfo=R.Tensor((1, seq_len, 2560), dtype="float16"))
            layer_norm61 = R.call_tir(cls.layer_norm1, (lv316_1, gpt_neox_layers_30_post_attention_layernorm_weight1, gpt_neox_layers_30_post_attention_layernorm_bias1), out_sinfo=R.Tensor((1, seq_len, 2560), dtype="float16"))
            lv317 = R.call_tir(cls.fused_dequantize3_fused_NT_matmul7_add8_gelu1_cast3, (gpt_neox_layers_30_mlp_dense_h_to_4h_q_weight1, gpt_neox_layers_30_mlp_dense_h_to_4h_q_scale1, layer_norm61, gpt_neox_layers_30_mlp_dense_h_to_4h_bias1), out_sinfo=R.Tensor((1, seq_len, 10240), dtype="float16"))
            lv317_1 = R.call_tir(cls.fused_dequantize4_fused_NT_matmul8_add9_cast4_add7, (gpt_neox_layers_30_mlp_dense_4h_to_h_q_weight1, gpt_neox_layers_30_mlp_dense_4h_to_h_q_scale1, lv317, gpt_neox_layers_30_mlp_dense_4h_to_h_bias1, lv316_1), out_sinfo=R.Tensor((1, seq_len, 2560), dtype="float16"))
            layer_norm62 = R.call_tir(cls.layer_norm1, (lv317_1, gpt_neox_layers_31_input_layernorm_weight1, gpt_neox_layers_31_input_layernorm_bias1), out_sinfo=R.Tensor((1, seq_len, 2560), dtype="float16"))
            lv318 = R.call_tir(cls.fused_dequantize1_fused_NT_matmul5_add5, (gpt_neox_layers_31_attention_query_key_value_q_weight1, gpt_neox_layers_31_attention_query_key_value_q_scale1, layer_norm62, gpt_neox_layers_31_attention_query_key_value_bias1), out_sinfo=R.Tensor((1, seq_len, 7680), dtype="float16"))
            reshape124 = R.call_tir(cls.reshape4, (lv318,), out_sinfo=R.Tensor((1, seq_len, 96, 80), dtype="float16"))
            reshape125 = R.call_tir(cls.reshape5, (reshape124,), out_sinfo=R.Tensor((seq_len, 96, 80), dtype="float16"))
            lv157 = R.call_dps_packed("vm.builtin.attention_kv_cache_attention_with_fused_qkv", (paged_kv_cache, R.prim_value(31), R.prim_value(T.float32(1.0)), reshape125), out_sinfo=R.Tensor((seq_len, 32, 80), dtype="float16"))
            reshape126 = R.call_tir(cls.reshape6, (lv157,), out_sinfo=R.Tensor((1, seq_len, 32, 80), dtype="float16"))
            reshape127 = R.call_tir(cls.reshape7, (reshape126,), out_sinfo=R.Tensor((1, seq_len, 2560), dtype="float16"))
            lv318_1 = R.call_tir(cls.fused_dequantize2_fused_NT_matmul6_add6_add7, (gpt_neox_layers_31_attention_dense_q_weight1, gpt_neox_layers_31_attention_dense_q_scale1, reshape127, gpt_neox_layers_31_attention_dense_bias1, lv317_1), out_sinfo=R.Tensor((1, seq_len, 2560), dtype="float16"))
            layer_norm63 = R.call_tir(cls.layer_norm1, (lv318_1, gpt_neox_layers_31_post_attention_layernorm_weight1, gpt_neox_layers_31_post_attention_layernorm_bias1), out_sinfo=R.Tensor((1, seq_len, 2560), dtype="float16"))
            lv319 = R.call_tir(cls.fused_dequantize3_fused_NT_matmul7_add8_gelu1_cast3, (gpt_neox_layers_31_mlp_dense_h_to_4h_q_weight1, gpt_neox_layers_31_mlp_dense_h_to_4h_q_scale1, layer_norm63, gpt_neox_layers_31_mlp_dense_h_to_4h_bias1), out_sinfo=R.Tensor((1, seq_len, 10240), dtype="float16"))
            lv319_1 = R.call_tir(cls.fused_dequantize4_fused_NT_matmul8_add9_cast4_add7, (gpt_neox_layers_31_mlp_dense_4h_to_h_q_weight1, gpt_neox_layers_31_mlp_dense_4h_to_h_q_scale1, lv319, gpt_neox_layers_31_mlp_dense_4h_to_h_bias1, lv318_1), out_sinfo=R.Tensor((1, seq_len, 2560), dtype="float16"))
            layer_norm64 = R.call_tir(cls.layer_norm1, (lv319_1, gpt_neox_final_layer_norm_weight1, gpt_neox_final_layer_norm_bias1), out_sinfo=R.Tensor((1, seq_len, 2560), dtype="float16"))
            lv161 = R.call_tir(cls.index, (layer_norm64,), out_sinfo=R.Tensor((1, 1, 2560), dtype="float16"))
            lv4 = R.call_tir(cls.fused_dequantize_fused_NT_matmul14_cast8, (embed_out_q_weight1, embed_out_q_scale1, lv161), out_sinfo=R.Tensor((1, 1, vocab_size), dtype="float32"))
            gv1: R.Tuple(R.Tensor((1, 1, vocab_size), dtype="float32"), R.Object) = lv4, paged_kv_cache
            R.output(gv1)
        return gv1

    @R.function
    def renormalize_by_top_p(probs: R.Tensor(("batch_size", "vocab_size"), dtype="float32"), top_p: R.Tensor(("batch_size",), dtype="float32"), init_pivots: R.Tensor(("batch_size", 3), dtype="float32")) -> R.Tensor(("batch_size", "vocab_size"), dtype="float32"):
        batch_size = T.int64(is_size_var=True)
        vocab_size = T.int64(is_size_var=True)
        R.func_attr({"relax.memory_plan_dynamic_func_output": True, "tir_non_negative_var": ["vocab_size"], "tir_var_upper_bound": {"batch_size": 128, "num_positions": 768, "num_samples": 128}})
        cls = Module
        with R.dataflow():
            lv6 = R.call_tir(cls.top_p_pivot_cutoff, (probs, top_p, init_pivots), out_sinfo=[R.Tensor((batch_size,), dtype="float32"), R.Tensor((batch_size,), dtype="float32")])
            lv7: R.Tensor((batch_size,), dtype="float32") = lv6[0]
            lv8: R.Tensor((batch_size,), dtype="float32") = lv6[1]
            gv5 = R.call_tir(cls.top_p_renorm_after_cutoff, (probs, lv7, lv8), out_sinfo=R.Tensor((batch_size, vocab_size), dtype="float32"))
            R.output(gv5)
        return gv5

    @R.function
    def sample_with_top_p(sorted_probs: R.Tensor(("batch_size", "vocab_size"), dtype="float32"), sorted_indices: R.Tensor(("batch_size", "vocab_size"), dtype="int32"), uniform_samples: R.Tensor(("num_samples",), dtype="float32"), sample_indices: R.Tensor(("num_samples",), dtype="int32"), top_p: R.Tensor(("batch_size",), dtype="float32")) -> R.Tensor(("num_samples",), dtype="int32"):
        num_samples = T.int64(is_size_var=True)
        batch_size = T.int64(is_size_var=True)
        vocab_size = T.int64(is_size_var=True)
        R.func_attr({"relax.memory_plan_dynamic_func_output": True, "tir_non_negative_var": ["vocab_size"], "tir_var_upper_bound": {"batch_size": 128, "num_positions": 768, "num_samples": 128}})
        cls = Module
        with R.dataflow():
            uniform_samples1: R.Tensor((num_samples, 1), dtype="float32") = R.call_pure_packed("vm.builtin.reshape", uniform_samples, R.shape([num_samples, 1]), sinfo_args=(R.Tensor((num_samples, 1), dtype="float32"),))
            sample_indices1: R.Tensor((num_samples, 1), dtype="int32") = R.call_pure_packed("vm.builtin.reshape", sample_indices, R.shape([num_samples, 1]), sinfo_args=(R.Tensor((num_samples, 1), dtype="int32"),))
            sample_indices2: R.Tensor((batch_size, 1), dtype="float32") = R.call_pure_packed("vm.builtin.reshape", top_p, R.shape([batch_size, 1]), sinfo_args=(R.Tensor((batch_size, 1), dtype="float32"),))
            lv3 = R.call_tir(cls.full, R.tuple(), out_sinfo=R.Tensor((batch_size, 1), dtype="int32"), tir_vars=R.shape([vocab_size]))
            lv: R.Tensor((batch_size, vocab_size), dtype="float32") = R.call_pure_packed("vm.builtin.reshape", sorted_probs, R.shape([batch_size, vocab_size]), sinfo_args=(R.Tensor((batch_size, vocab_size), dtype="float32"),))
            lv1 = R.call_tir(cls.gpu_2d_continuous_cumsum, (lv,), out_sinfo=R.Tensor((batch_size, vocab_size), dtype="float32"))
            cumsum: R.Tensor((batch_size, vocab_size), dtype="float32") = R.call_pure_packed("vm.builtin.reshape", lv1, R.shape([batch_size, vocab_size]), sinfo_args=(R.Tensor((batch_size, vocab_size), dtype="float32"),))
            lv4 = R.call_tir(cls.get_renorm_prob, (cumsum, sample_indices2, lv3), out_sinfo=R.Tensor((batch_size, 1), dtype="float32"))
            lv5 = R.call_tir(cls.get_index_from_sorted, (cumsum, sorted_indices, lv4, uniform_samples1, sample_indices1), out_sinfo=R.Tensor((num_samples, 1), dtype="int32"))
            gv2: R.Tensor((num_samples,), dtype="int32") = R.call_pure_packed("vm.builtin.reshape", lv5, R.shape([num_samples]), sinfo_args=(R.Tensor((num_samples,), dtype="int32"),))
            R.output(gv2)
        return gv2

    @R.function
    def sampler_take_probs(unsorted_probs: R.Tensor(("batch_size", "vocab_size"), dtype="float32"), sorted_indices: R.Tensor(("batch_size", "vocab_size"), dtype="int32"), sample_indices: R.Tensor(("num_samples",), dtype="int32"), sampling_result: R.Tensor(("num_samples",), dtype="int32"), lobprob_offsets: R.Tensor(("num_positions",), dtype="int32")) -> R.Tuple(R.Tensor(("num_samples",), dtype="float32"), R.Tensor(("num_positions",), dtype="float32"), R.Tensor(("num_positions",), dtype="int32")):
        num_samples = T.int64(is_size_var=True)
        num_positions = T.int64(is_size_var=True)
        batch_size = T.int64(is_size_var=True)
        vocab_size = T.int64(is_size_var=True)
        R.func_attr({"relax.memory_plan_dynamic_func_output": True, "tir_non_negative_var": ["vocab_size"], "tir_var_upper_bound": {"batch_size": 128, "num_positions": 768, "num_samples": 128}})
        cls = Module
        with R.dataflow():
            gv3 = R.call_tir(cls.sampler_take_probs_tir, (unsorted_probs, sorted_indices, sample_indices, sampling_result, lobprob_offsets), out_sinfo=[R.Tensor((num_samples,), dtype="float32"), R.Tensor((num_positions,), dtype="float32"), R.Tensor((num_positions,), dtype="int32")])
            R.output(gv3)
        return gv3

    @R.function
    def sampler_verify_draft_tokens(draft_probs: R.Tensor(("num_nodes", "vocab_size"), dtype="float32"), draft_tokens: R.Tensor(("num_nodes",), dtype="int32"), model_probs: R.Tensor(("num_nodes", "vocab_size"), dtype="float32"), token_tree_first_child: R.Tensor(("num_nodes",), dtype="int32"), token_tree_next_sibling: R.Tensor(("num_nodes",), dtype="int32"), uniform_samples: R.Tensor(("num_nodes",), dtype="float32"), token_tree_parent_ptr: R.Tensor(("nbatch",), dtype="int32")) -> R.Tuple(R.Tensor(("num_nodes", "vocab_size"), dtype="float32"), R.Tensor(("nbatch",), dtype="int32")):
        num_nodes = T.int64(is_size_var=True)
        vocab_size = T.int64(is_size_var=True)
        nbatch = T.int64(is_size_var=True)
        R.func_attr({"relax.memory_plan_dynamic_func_output": True, "tir_non_negative_var": ["vocab_size"], "tir_var_upper_bound": {"batch_size": 128, "num_positions": 768, "num_samples": 128}})
        cls = Module
        with R.dataflow():
            gv4: R.Tuple(R.Tensor((num_nodes, vocab_size), dtype="float32"), R.Tensor((nbatch,), dtype="int32")) = R.call_tir_inplace(cls.batch_verify_on_gpu_single_kernel, (draft_probs, draft_tokens, model_probs, token_tree_first_child, token_tree_next_sibling, uniform_samples, token_tree_parent_ptr), out_sinfo=[R.Tensor((num_nodes, vocab_size), dtype="float32"), R.Tensor((nbatch,), dtype="int32")], inplace_indices=[2, 6])
            R.output(gv4)
        return gv4

    @R.function
    def softmax_with_temperature(logits: R.Tensor(("batch_size", 1, "vocab_size"), dtype="float32"), temperature: R.Tensor(("batch_size",), dtype="float32")) -> R.Tensor(("batch_size", 1, "vocab_size"), dtype="float32"):
        batch_size = T.int64(is_size_var=True)
        vocab_size = T.int64(is_size_var=True)
        R.func_attr({"relax.memory_plan_dynamic_func_output": True, "tir_non_negative_var": ["vocab_size"], "tir_var_upper_bound": {"batch_size": 128, "seq_len": 2048, "total_seq_len": 2048}})
        cls = Module
        with R.dataflow():
            lv: R.Tensor((batch_size, vocab_size), dtype="float32") = R.call_pure_packed("vm.builtin.reshape", logits, R.shape([batch_size, vocab_size]), sinfo_args=(R.Tensor((batch_size, vocab_size), dtype="float32"),))
            lv1 = R.call_tir(cls.chunk_lse, (lv, temperature), out_sinfo=[R.Tensor((batch_size, (vocab_size + 4096 - 1) // 4096), dtype="float32"), R.Tensor((batch_size, (vocab_size + 4096 - 1) // 4096), dtype="float32")])
            lv2: R.Tensor((batch_size, (vocab_size + 4096 - 1) // 4096), dtype="float32") = lv1[0]
            lv3: R.Tensor((batch_size, (vocab_size + 4096 - 1) // 4096), dtype="float32") = lv1[1]
            lv4 = R.call_tir(cls.softmax_with_chunked_sum, (lv, temperature, lv2, lv3), out_sinfo=R.Tensor((batch_size, vocab_size), dtype="float32"))
            gv: R.Tensor((batch_size, 1, vocab_size), dtype="float32") = R.call_pure_packed("vm.builtin.reshape", lv4, R.shape([batch_size, 1, vocab_size]), sinfo_args=(R.Tensor((batch_size, 1, vocab_size), dtype="float32"),))
            R.output(gv)
        return gv